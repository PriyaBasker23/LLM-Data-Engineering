{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b720c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"\"\n",
    "#get token from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47069d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: farm-haystack in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.23.0)\n",
      "Requirement already satisfied: boilerpy3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (1.0.7)\n",
      "Requirement already satisfied: events in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (0.5)\n",
      "Requirement already satisfied: httpx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (0.26.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (4.19.1)\n",
      "Requirement already satisfied: lazy-imports==0.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (0.3.1)\n",
      "Requirement already satisfied: more-itertools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (10.1.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (3.2)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (1.5.3)\n",
      "Requirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (10.1.0)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (3.11.0)\n",
      "Requirement already satisfied: posthog in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (3.1.0)\n",
      "Requirement already satisfied: prompthub-py==4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (4.0.0)\n",
      "Requirement already satisfied: pydantic<2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (1.10.13)\n",
      "Requirement already satisfied: quantulum3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (0.9.0)\n",
      "Requirement already satisfied: rank-bm25 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (0.2.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (2.31.0)\n",
      "Requirement already satisfied: requests-cache<1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (0.9.8)\n",
      "Requirement already satisfied: scikit-learn>=1.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (1.3.2)\n",
      "Requirement already satisfied: sseclient-py in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (1.8.0)\n",
      "Requirement already satisfied: tenacity in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (0.5.2)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (4.66.1)\n",
      "Requirement already satisfied: transformers==4.35.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack) (4.35.2)\n",
      "Requirement already satisfied: pyyaml<7.0,>=6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from prompthub-py==4.0.0->farm-haystack) (6.0.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.35.2->farm-haystack) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.35.2->farm-haystack) (0.20.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.35.2->farm-haystack) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.35.2->farm-haystack) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.35.2->farm-haystack) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.35.2->farm-haystack) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.35.2->farm-haystack) (0.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<2->farm-haystack) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->farm-haystack) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->farm-haystack) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->farm-haystack) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->farm-haystack) (2023.7.22)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests-cache<1.0.0->farm-haystack) (1.4.4)\n",
      "Requirement already satisfied: attrs>=21.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests-cache<1.0.0->farm-haystack) (23.1.0)\n",
      "Requirement already satisfied: cattrs>=22.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests-cache<1.0.0->farm-haystack) (23.2.3)\n",
      "Requirement already satisfied: url-normalize>=1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests-cache<1.0.0->farm-haystack) (1.4.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=1.3.0->farm-haystack) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=1.3.0->farm-haystack) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=1.3.0->farm-haystack) (3.2.0)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx->farm-haystack) (4.0.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx->farm-haystack) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx->farm-haystack) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpcore==1.*->httpx->farm-haystack) (0.14.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->farm-haystack) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->farm-haystack) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->farm-haystack) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->farm-haystack) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->farm-haystack) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from posthog->farm-haystack) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from posthog->farm-haystack) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from posthog->farm-haystack) (2.2.1)\n",
      "Requirement already satisfied: inflect in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from quantulum3->farm-haystack) (7.0.0)\n",
      "Requirement already satisfied: num2words in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from quantulum3->farm-haystack) (0.5.13)\n",
      "Requirement already satisfied: exceptiongroup>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cattrs>=22.2->requests-cache<1.0.0->farm-haystack) (1.1.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2->farm-haystack) (2023.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.35.2->farm-haystack) (3.1.1)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from num2words->quantulum3->farm-haystack) (0.6.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: farm-haystack[inference] in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.23.0)\n",
      "Requirement already satisfied: boilerpy3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (1.0.7)\n",
      "Requirement already satisfied: events in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (0.5)\n",
      "Requirement already satisfied: httpx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (0.26.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (4.19.1)\n",
      "Requirement already satisfied: lazy-imports==0.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (0.3.1)\n",
      "Requirement already satisfied: more-itertools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (10.1.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (3.2)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (1.5.3)\n",
      "Requirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (10.1.0)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (3.11.0)\n",
      "Requirement already satisfied: posthog in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (3.1.0)\n",
      "Requirement already satisfied: prompthub-py==4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (4.0.0)\n",
      "Requirement already satisfied: pydantic<2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (1.10.13)\n",
      "Requirement already satisfied: quantulum3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (0.9.0)\n",
      "Requirement already satisfied: rank-bm25 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (0.2.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (2.31.0)\n",
      "Requirement already satisfied: requests-cache<1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (0.9.8)\n",
      "Requirement already satisfied: scikit-learn>=1.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (1.3.2)\n",
      "Requirement already satisfied: sseclient-py in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (1.8.0)\n",
      "Requirement already satisfied: tenacity in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (0.5.2)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (4.66.1)\n",
      "Requirement already satisfied: transformers==4.35.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (4.35.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (0.20.1)\n",
      "Requirement already satisfied: sentence-transformers>=2.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from farm-haystack[inference]) (2.2.2)\n",
      "Requirement already satisfied: pyyaml<7.0,>=6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from prompthub-py==4.0.0->farm-haystack[inference]) (6.0.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.35.2->farm-haystack[inference]) (3.12.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.35.2->farm-haystack[inference]) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.35.2->farm-haystack[inference]) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.35.2->farm-haystack[inference]) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.35.2->farm-haystack[inference]) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.35.2->farm-haystack[inference]) (0.4.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers[sentencepiece,torch]==4.35.2; extra == \"inference\"->farm-haystack[inference]) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers[sentencepiece,torch]==4.35.2; extra == \"inference\"->farm-haystack[inference]) (4.24.4)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers[sentencepiece,torch]==4.35.2; extra == \"inference\"->farm-haystack[inference]) (2.0.1)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers[sentencepiece,torch]==4.35.2; extra == \"inference\"->farm-haystack[inference]) (0.25.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.5.0->farm-haystack[inference]) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.5.0->farm-haystack[inference]) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->farm-haystack[inference]) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->farm-haystack[inference]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->farm-haystack[inference]) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->farm-haystack[inference]) (2023.7.22)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests-cache<1.0.0->farm-haystack[inference]) (1.4.4)\n",
      "Requirement already satisfied: attrs>=21.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests-cache<1.0.0->farm-haystack[inference]) (23.1.0)\n",
      "Requirement already satisfied: cattrs>=22.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests-cache<1.0.0->farm-haystack[inference]) (23.2.3)\n",
      "Requirement already satisfied: url-normalize>=1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests-cache<1.0.0->farm-haystack[inference]) (1.4.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=1.3.0->farm-haystack[inference]) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=1.3.0->farm-haystack[inference]) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=1.3.0->farm-haystack[inference]) (3.2.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence-transformers>=2.2.0->farm-haystack[inference]) (0.15.2)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence-transformers>=2.2.0->farm-haystack[inference]) (3.8.1)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx->farm-haystack[inference]) (4.0.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx->farm-haystack[inference]) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx->farm-haystack[inference]) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpcore==1.*->httpx->farm-haystack[inference]) (0.14.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->farm-haystack[inference]) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->farm-haystack[inference]) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->farm-haystack[inference]) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->farm-haystack[inference]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->farm-haystack[inference]) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from posthog->farm-haystack[inference]) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from posthog->farm-haystack[inference]) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from posthog->farm-haystack[inference]) (2.2.1)\n",
      "Requirement already satisfied: inflect in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from quantulum3->farm-haystack[inference]) (7.0.0)\n",
      "Requirement already satisfied: num2words in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from quantulum3->farm-haystack[inference]) (0.5.13)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate>=0.20.3->transformers[sentencepiece,torch]==4.35.2; extra == \"inference\"->farm-haystack[inference]) (5.9.5)\n",
      "Requirement already satisfied: exceptiongroup>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cattrs>=22.2->requests-cache<1.0.0->farm-haystack[inference]) (1.1.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.35.2->farm-haystack[inference]) (3.1.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[sentencepiece,torch]==4.35.2; extra == \"inference\"->farm-haystack[inference]) (1.12)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[sentencepiece,torch]==4.35.2; extra == \"inference\"->farm-haystack[inference]) (3.1.2)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk->sentence-transformers>=2.2.0->farm-haystack[inference]) (8.1.7)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from num2words->quantulum3->farm-haystack[inference]) (0.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[sentencepiece,torch]==4.35.2; extra == \"inference\"->farm-haystack[inference]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[sentencepiece,torch]==4.35.2; extra == \"inference\"->farm-haystack[inference]) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install farm-haystack\n",
    "!pip install farm-haystack[inference]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f1013f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PreProcessor,PromptModel, PromptTemplate, PromptNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96bfc5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (3.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07307369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import io\n",
    "import PyPDF2\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2aceaf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Get the SageMaker execution role\n",
    "role = \"arn:aws:iam::684969100054:role/service-role/SageMaker-MLOpsEngineer\"\n",
    "\n",
    "# Create an S3 client using SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "s3_client = sagemaker_session.boto_session.client('s3')\n",
    "\n",
    "def read_from_s3(bucket_name, file_key):\n",
    "    try:\n",
    "        # Read the file from S3\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "        content = response['Body'].read()\n",
    "\n",
    "        return content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path_text):\n",
    "    text = \"\"\n",
    "    pdf_reader = PyPDF2.PdfReader(BytesIO(pdf_path_text))\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        text += page.extract_text()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bea8bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace 'your_bucket_name' and 'path/to/your/file.pdf' with your S3 bucket and file path\n",
    "bucket_name = 'data-platform-firebreak-llm-unstructured'\n",
    "file_key = 'data/economy.pdf'\n",
    "\n",
    "pdf_file_path=bucket_name+\"/\"+file_key\n",
    "# Example usage\n",
    "pdf_content = read_from_s3(bucket_name, file_key)\n",
    "\n",
    "pdf_text = extract_text_from_pdf(pdf_content)\n",
    "\n",
    "text = pdf_text.replace(\"\\n\", \" \")\n",
    "print(len(text))\n",
    "\n",
    "# pdf_text[10000:19999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6918ff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts saved to scraped_texts.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "visited_urls = set()\n",
    "\n",
    "def scrape_page(url, depth=0, max_depth=1, all_texts=[]):\n",
    "    if depth > max_depth or url in visited_urls:\n",
    "        return all_texts\n",
    "\n",
    "    visited_urls.add(url)\n",
    "#     print(url)\n",
    "\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        page.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return all_texts\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    all_texts.append(text)\n",
    "\n",
    "    links = soup.find_all('a')\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href and is_valid_link(href, url):\n",
    "            absolute_url = urljoin(url, href)\n",
    "            scrape_page(absolute_url, depth + 1, max_depth, all_texts)\n",
    "\n",
    "    return all_texts\n",
    "\n",
    "def is_valid_link(href, base_url):\n",
    "    # Ignore fragment identifiers\n",
    "    if href.startswith('#'):\n",
    "        return False\n",
    "    if href.startswith('mailto:'):\n",
    "        return False\n",
    "    # Construct absolute URL from relative URL\n",
    "    absoluteURL=urljoin(base_url, href)\n",
    "#     print(absoluteURL)\n",
    "    return absoluteURL\n",
    "\n",
    "\n",
    "start_url = \"https://user-guidance.analytical-platform.service.justice.gov.uk/\"\n",
    "all_texts = scrape_page(start_url)\n",
    "\n",
    "\n",
    "output_file = 'scraped_texts.txt'\n",
    "\n",
    "# Open the file in write mode and write each text to the file\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    for text in all_texts:\n",
    "        file.write(text + '\\n\\n')  # Adding two line breaks as a separator\n",
    "\n",
    "print(f\"Texts saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa7d427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=open(output_file,mode='r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ab9e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from haystack import Document\n",
    "\n",
    "doc = Document(\n",
    "    content=text,\n",
    "    meta={\"pdf_path\": pdf_file_path}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6188ed91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n**Initializing the PreProcessor** : With processor = PreProcessor(...), configuring  preprocessor to  prepare documents.\\n\\n   - **Parameters:*\\n     - `clean_empty_lines`: remove empty lines for optimal cleaning.   \\n     - `clean_whitespace`:  eliminate whitespace\\n     - `clean_header_footer`: treat any header and footer elements.\\n     - `split_by`: We split the text by word. 📊\\n     - `split_length`: We set the split length to 500 words. 📏\\n     - `split_respect_sentence_boundary`: We respect sentence boundaries when splitting. 🗣️\\n     - `split_overlap`: No overlap when splitting. 🚫\\n\\n\\n **Processing the Document List:**  `preprocessed_docs = processor.process(docs)`, is applying  preprocessor to the document list.\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "**Initializing the PreProcessor** : With processor = PreProcessor(...), configuring  preprocessor to  prepare documents.\n",
    "\n",
    "   - **Parameters:*\n",
    "     - `clean_empty_lines`: remove empty lines for optimal cleaning.   \n",
    "     - `clean_whitespace`:  eliminate whitespace\n",
    "     - `clean_header_footer`: treat any header and footer elements.\n",
    "     - `split_by`: We split the text by word. 📊\n",
    "     - `split_length`: We set the split length to 500 words. 📏\n",
    "     - `split_respect_sentence_boundary`: We respect sentence boundaries when splitting. 🗣️\n",
    "     - `split_overlap`: No overlap when splitting. 🚫\n",
    "\n",
    "\n",
    " **Processing the Document List:**  `preprocessed_docs = processor.process(docs)`, is applying  preprocessor to the document list.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2e694be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 1/1 [00:02<00:00,  2.05s/docs]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Document: {'content': 'Introduction - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 0}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7ed9b5e8ea65a5f836eca59a0f429e85'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 1}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7a1bbf0601e5e6586cff17ef311f612'}>,\n",
       " <Document: {'content': 'Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Overview The Analytical Platform (AP) is a data analysis platform made up of tools, packages and datasets for creating applications that utilise data within the Ministry of Justice (MoJ). The AP provides development environments in both Python (JupyterLab) and R (RStudio), allowing you multiple ways to query, analyse and model data. This site provides instructions on how to configure and use the AP. Intended users Primarily intended for data analysts in the Data and Analytical Services Directorate, the Analytical Platform also hosts users from:\\n-   Criminal Injury Claims (CICA)\\n-   HM Courts & Tribunals Service (HMCTS)\\n-   HM Prison and Probation Service (HMPPS)\\n-   Legal Aid Agency (LAA)\\n-   Office of the Public Guardian (OPG) We can also host other MoJ organisations. Contact us to discuss your options. Knowledge requirements The Analytical Platform incorporates a variety of technical tools and concepts. While our community provide basic training materials on how to use some of these, to use the platform, as a minimum we recommend you have working knowledge of the following: Amazon Athena and S3: to create, manipulate and query data GitHub and GitHub actions: to manage your application code Python or R: to develop applications on the Analytical Platform SQL: to query and transform data Benefits In additional to Python and R compatibility, benefits of using the Analytical Platform include: modern data tools and services : the ability to freely install packages from CRAN and PyPI to perform advanced analytical techniques, such as text mining, predictive analytics and data visualisation compatiblity with current cloud data services, such as Amazon Athena, Glue and Redshift, offering scalability and a managed service at commodity pay-as-you-go prices centralised data : our Data Engineering team converts raw data from operational systems into structures and excerpts we hold data files in Amazon S3 for ease of use, to load into your code or run SQL queries directly using Amazon Athena users can also upload data to the AP from other sources and share them with granular access controls, subject to normal data protection processes; for more information, see [Information governance][information-governance.md] reproducible analysis : the AP provides tools to develop reproducible analytical pipelines (RAPs) to automate time–consuming and repetitive tasks, allowing you to focus on interpreting the results with the following elements: when datasets are imported into the AP, snapshots of them are taken and versioned standardised system libraries in GitHub a standardised virtual machine that can run R Studio or Jupyter, or code running in an explicitly defined Dockerfile secure environments : we host the Analytical Platform in a cloud-based ecosystem that is easy to access remotely from all MoJ IT systems. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 2}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a61306c35cc867609911c3e306c4220'}>,\n",
       " <Document: {'content': 'Designed for data at security classifications OFFICIAL and OFFICIAL-SENSITIVE, we follow NCSC Cloud Security Principles, implementing features such as: two-factor authentication data encryption at rest and in transit granular access control extensive tracking of user behaviour, user privilege requests/changes and data flows multiple isolation levels between users and system components resilience and high availability to provide optimal performance and uptime Note : The Analytical Platform does not currently provide the following:\\n- production apps at scale\\n- management information\\n- real-time data; however, the Airflow tool can schedule data processing as frequently as every few minutes\\n- pure data archival: Amazon S3, which the AP uses for data storage, does not offer index or search facilities\\n- we can set up a custom bucket policy to archive data to S3-IA or Glacier but recommend exploring SaaS alternatives, such as SharePoint or Google Drive This page was last reviewed on 8 December 2022.\\n\\nIt needs to be reviewed again on 8 June 2023\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 8 June 2023\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nIntroduction - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 3}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bf4c749f676d3f2c72a2ad5680a5e067'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 4}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Overview The Analytical Platform (AP) is a data analysis platform made up of tools, packages and datasets for creating applications that utilise data within the Ministry of Justice (MoJ). The AP provides development environments in both Python (JupyterLab) and R (RStudio), allowing you multiple ways to query, analyse and model data. This site provides instructions on how to configure and use the AP. Intended users Primarily intended for data analysts in the Data and Analytical Services Directorate, the Analytical Platform also hosts users from:\\n-   Criminal Injury Claims (CICA)\\n-   HM Courts & Tribunals Service (HMCTS)\\n-   HM Prison and Probation Service (HMPPS)\\n-   Legal Aid Agency (LAA)\\n-   Office of the Public Guardian (OPG) We can also host other MoJ organisations. Contact us to discuss your options. Knowledge requirements The Analytical Platform incorporates a variety of technical tools and concepts. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 5}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7c9390edaf6797aa89f470d17019a8f8'}>,\n",
       " <Document: {'content': 'While our community provide basic training materials on how to use some of these, to use the platform, as a minimum we recommend you have working knowledge of the following: Amazon Athena and S3: to create, manipulate and query data GitHub and GitHub actions: to manage your application code Python or R: to develop applications on the Analytical Platform SQL: to query and transform data Benefits In additional to Python and R compatibility, benefits of using the Analytical Platform include: modern data tools and services : the ability to freely install packages from CRAN and PyPI to perform advanced analytical techniques, such as text mining, predictive analytics and data visualisation compatiblity with current cloud data services, such as Amazon Athena, Glue and Redshift, offering scalability and a managed service at commodity pay-as-you-go prices centralised data : our Data Engineering team converts raw data from operational systems into structures and excerpts we hold data files in Amazon S3 for ease of use, to load into your code or run SQL queries directly using Amazon Athena users can also upload data to the AP from other sources and share them with granular access controls, subject to normal data protection processes; for more information, see [Information governance][information-governance.md] reproducible analysis : the AP provides tools to develop reproducible analytical pipelines (RAPs) to automate time–consuming and repetitive tasks, allowing you to focus on interpreting the results with the following elements: when datasets are imported into the AP, snapshots of them are taken and versioned standardised system libraries in GitHub a standardised virtual machine that can run R Studio or Jupyter, or code running in an explicitly defined Dockerfile secure environments : we host the Analytical Platform in a cloud-based ecosystem that is easy to access remotely from all MoJ IT systems. Designed for data at security classifications OFFICIAL and OFFICIAL-SENSITIVE, we follow NCSC Cloud Security Principles, implementing features such as: two-factor authentication data encryption at rest and in transit granular access control extensive tracking of user behaviour, user privilege requests/changes and data flows multiple isolation levels between users and system components resilience and high availability to provide optimal performance and uptime Note : The Analytical Platform does not currently provide the following:\\n- production apps at scale\\n- management information\\n- real-time data; however, the Airflow tool can schedule data processing as frequently as every few minutes\\n- pure data archival: Amazon S3, which the AP uses for data storage, does not offer index or search facilities\\n- we can set up a custom bucket policy to archive data to S3-IA or Glacier but recommend exploring SaaS alternatives, such as SharePoint or Google Drive This page was last reviewed on 8 December 2022.\\n\\nIt needs to be reviewed again on 8 June 2023\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 8 June 2023\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 6}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd738f72616c18046291c883bdc704ea1'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAnalytical Platform Control Panel - Sign In with Auth0 Login with code First time logging in? If this is your first time logging into the platform, the next screen you see will prompt you to set up two factor authentication (2FA). Please see here in the user guidance for step by step instructions. Need help with two factor authentication? Note that there are two layers of two factor authentication (2FA) in action on the platform: Your Github account must have 2FA enabled. When you log in to Github, your session will stay active for a month before you need to re-enter your 2FA code . Your Github username identifies you to the platform, and we use this identity to control access to data and other resources once you’ve logged into the platform. You therefore must be logged into Github to use the platform. Your Analytical Platform account has a separate 2FA step. You will be prompted to set this up the first time you access the platform. This code must be entered once a day . This security step lets you log into the platform and use it. For further information please see here in the platform user guidance.\\n\\nIntroduction - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 7}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'de80dca77c669296107bc606539de481'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 8}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Overview The Analytical Platform (AP) is a data analysis platform made up of tools, packages and datasets for creating applications that utilise data within the Ministry of Justice (MoJ). The AP provides development environments in both Python (JupyterLab) and R (RStudio), allowing you multiple ways to query, analyse and model data. This site provides instructions on how to configure and use the AP. Intended users Primarily intended for data analysts in the Data and Analytical Services Directorate, the Analytical Platform also hosts users from:\\n-   Criminal Injury Claims (CICA)\\n-   HM Courts & Tribunals Service (HMCTS)\\n-   HM Prison and Probation Service (HMPPS)\\n-   Legal Aid Agency (LAA)\\n-   Office of the Public Guardian (OPG) We can also host other MoJ organisations. Contact us to discuss your options. Knowledge requirements The Analytical Platform incorporates a variety of technical tools and concepts. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 9}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7c9390edaf6797aa89f470d17019a8f8'}>,\n",
       " <Document: {'content': 'While our community provide basic training materials on how to use some of these, to use the platform, as a minimum we recommend you have working knowledge of the following: Amazon Athena and S3: to create, manipulate and query data GitHub and GitHub actions: to manage your application code Python or R: to develop applications on the Analytical Platform SQL: to query and transform data Benefits In additional to Python and R compatibility, benefits of using the Analytical Platform include: modern data tools and services : the ability to freely install packages from CRAN and PyPI to perform advanced analytical techniques, such as text mining, predictive analytics and data visualisation compatiblity with current cloud data services, such as Amazon Athena, Glue and Redshift, offering scalability and a managed service at commodity pay-as-you-go prices centralised data : our Data Engineering team converts raw data from operational systems into structures and excerpts we hold data files in Amazon S3 for ease of use, to load into your code or run SQL queries directly using Amazon Athena users can also upload data to the AP from other sources and share them with granular access controls, subject to normal data protection processes; for more information, see [Information governance][information-governance.md] reproducible analysis : the AP provides tools to develop reproducible analytical pipelines (RAPs) to automate time–consuming and repetitive tasks, allowing you to focus on interpreting the results with the following elements: when datasets are imported into the AP, snapshots of them are taken and versioned standardised system libraries in GitHub a standardised virtual machine that can run R Studio or Jupyter, or code running in an explicitly defined Dockerfile secure environments : we host the Analytical Platform in a cloud-based ecosystem that is easy to access remotely from all MoJ IT systems. Designed for data at security classifications OFFICIAL and OFFICIAL-SENSITIVE, we follow NCSC Cloud Security Principles, implementing features such as: two-factor authentication data encryption at rest and in transit granular access control extensive tracking of user behaviour, user privilege requests/changes and data flows multiple isolation levels between users and system components resilience and high availability to provide optimal performance and uptime Note : The Analytical Platform does not currently provide the following:\\n- production apps at scale\\n- management information\\n- real-time data; however, the Airflow tool can schedule data processing as frequently as every few minutes\\n- pure data archival: Amazon S3, which the AP uses for data storage, does not offer index or search facilities\\n- we can set up a custom bucket policy to archive data to S3-IA or Glacier but recommend exploring SaaS alternatives, such as SharePoint or Google Drive This page was last reviewed on 8 December 2022.\\n\\nIt needs to be reviewed again on 8 June 2023\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 8 June 2023\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 10}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd738f72616c18046291c883bdc704ea1'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nIntroduction - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 11}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '50999ea853a1d9e69dce5cdad38c1ae9'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 12}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Overview The Analytical Platform (AP) is a data analysis platform made up of tools, packages and datasets for creating applications that utilise data within the Ministry of Justice (MoJ). The AP provides development environments in both Python (JupyterLab) and R (RStudio), allowing you multiple ways to query, analyse and model data. This site provides instructions on how to configure and use the AP. Intended users Primarily intended for data analysts in the Data and Analytical Services Directorate, the Analytical Platform also hosts users from:\\n-   Criminal Injury Claims (CICA)\\n-   HM Courts & Tribunals Service (HMCTS)\\n-   HM Prison and Probation Service (HMPPS)\\n-   Legal Aid Agency (LAA)\\n-   Office of the Public Guardian (OPG) We can also host other MoJ organisations. Contact us to discuss your options. Knowledge requirements The Analytical Platform incorporates a variety of technical tools and concepts. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 13}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7c9390edaf6797aa89f470d17019a8f8'}>,\n",
       " <Document: {'content': 'While our community provide basic training materials on how to use some of these, to use the platform, as a minimum we recommend you have working knowledge of the following: Amazon Athena and S3: to create, manipulate and query data GitHub and GitHub actions: to manage your application code Python or R: to develop applications on the Analytical Platform SQL: to query and transform data Benefits In additional to Python and R compatibility, benefits of using the Analytical Platform include: modern data tools and services : the ability to freely install packages from CRAN and PyPI to perform advanced analytical techniques, such as text mining, predictive analytics and data visualisation compatiblity with current cloud data services, such as Amazon Athena, Glue and Redshift, offering scalability and a managed service at commodity pay-as-you-go prices centralised data : our Data Engineering team converts raw data from operational systems into structures and excerpts we hold data files in Amazon S3 for ease of use, to load into your code or run SQL queries directly using Amazon Athena users can also upload data to the AP from other sources and share them with granular access controls, subject to normal data protection processes; for more information, see [Information governance][information-governance.md] reproducible analysis : the AP provides tools to develop reproducible analytical pipelines (RAPs) to automate time–consuming and repetitive tasks, allowing you to focus on interpreting the results with the following elements: when datasets are imported into the AP, snapshots of them are taken and versioned standardised system libraries in GitHub a standardised virtual machine that can run R Studio or Jupyter, or code running in an explicitly defined Dockerfile secure environments : we host the Analytical Platform in a cloud-based ecosystem that is easy to access remotely from all MoJ IT systems. Designed for data at security classifications OFFICIAL and OFFICIAL-SENSITIVE, we follow NCSC Cloud Security Principles, implementing features such as: two-factor authentication data encryption at rest and in transit granular access control extensive tracking of user behaviour, user privilege requests/changes and data flows multiple isolation levels between users and system components resilience and high availability to provide optimal performance and uptime Note : The Analytical Platform does not currently provide the following:\\n- production apps at scale\\n- management information\\n- real-time data; however, the Airflow tool can schedule data processing as frequently as every few minutes\\n- pure data archival: Amazon S3, which the AP uses for data storage, does not offer index or search facilities\\n- we can set up a custom bucket policy to archive data to S3-IA or Glacier but recommend exploring SaaS alternatives, such as SharePoint or Google Drive This page was last reviewed on 8 December 2022.\\n\\nIt needs to be reviewed again on 8 June 2023\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 8 June 2023\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 14}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd738f72616c18046291c883bdc704ea1'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nIntroduction - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 15}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '50999ea853a1d9e69dce5cdad38c1ae9'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 16}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Overview The Analytical Platform (AP) is a data analysis platform made up of tools, packages and datasets for creating applications that utilise data within the Ministry of Justice (MoJ). The AP provides development environments in both Python (JupyterLab) and R (RStudio), allowing you multiple ways to query, analyse and model data. This site provides instructions on how to configure and use the AP. Intended users Primarily intended for data analysts in the Data and Analytical Services Directorate, the Analytical Platform also hosts users from:\\n-   Criminal Injury Claims (CICA)\\n-   HM Courts & Tribunals Service (HMCTS)\\n-   HM Prison and Probation Service (HMPPS)\\n-   Legal Aid Agency (LAA)\\n-   Office of the Public Guardian (OPG) We can also host other MoJ organisations. Contact us to discuss your options. Knowledge requirements The Analytical Platform incorporates a variety of technical tools and concepts. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 17}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7c9390edaf6797aa89f470d17019a8f8'}>,\n",
       " <Document: {'content': 'While our community provide basic training materials on how to use some of these, to use the platform, as a minimum we recommend you have working knowledge of the following: Amazon Athena and S3: to create, manipulate and query data GitHub and GitHub actions: to manage your application code Python or R: to develop applications on the Analytical Platform SQL: to query and transform data Benefits In additional to Python and R compatibility, benefits of using the Analytical Platform include: modern data tools and services : the ability to freely install packages from CRAN and PyPI to perform advanced analytical techniques, such as text mining, predictive analytics and data visualisation compatiblity with current cloud data services, such as Amazon Athena, Glue and Redshift, offering scalability and a managed service at commodity pay-as-you-go prices centralised data : our Data Engineering team converts raw data from operational systems into structures and excerpts we hold data files in Amazon S3 for ease of use, to load into your code or run SQL queries directly using Amazon Athena users can also upload data to the AP from other sources and share them with granular access controls, subject to normal data protection processes; for more information, see [Information governance][information-governance.md] reproducible analysis : the AP provides tools to develop reproducible analytical pipelines (RAPs) to automate time–consuming and repetitive tasks, allowing you to focus on interpreting the results with the following elements: when datasets are imported into the AP, snapshots of them are taken and versioned standardised system libraries in GitHub a standardised virtual machine that can run R Studio or Jupyter, or code running in an explicitly defined Dockerfile secure environments : we host the Analytical Platform in a cloud-based ecosystem that is easy to access remotely from all MoJ IT systems. Designed for data at security classifications OFFICIAL and OFFICIAL-SENSITIVE, we follow NCSC Cloud Security Principles, implementing features such as: two-factor authentication data encryption at rest and in transit granular access control extensive tracking of user behaviour, user privilege requests/changes and data flows multiple isolation levels between users and system components resilience and high availability to provide optimal performance and uptime Note : The Analytical Platform does not currently provide the following:\\n- production apps at scale\\n- management information\\n- real-time data; however, the Airflow tool can schedule data processing as frequently as every few minutes\\n- pure data archival: Amazon S3, which the AP uses for data storage, does not offer index or search facilities\\n- we can set up a custom bucket policy to archive data to S3-IA or Glacier but recommend exploring SaaS alternatives, such as SharePoint or Google Drive This page was last reviewed on 8 December 2022.\\n\\nIt needs to be reviewed again on 8 June 2023\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 8 June 2023\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 18}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd738f72616c18046291c883bdc704ea1'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nGet Started - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 19}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '44e9ad7620ef4eacc3ae7bccf9eb3f9b'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 20}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Quickstart guide This guide provides the instructions to set up the main accounts and services you need to use the Analytical Platform. Once you complete it, you can: access the Analytical Platform Control Panel explore data on the Analytical Platform begin developing your application in either JupyterLab or RStudio contribute to the Analytical Platform User Guidance Before you begin To use this guide, you need the following: a Ministry of Justice-issued Office 365 account a Ministry of Justice-issued laptop you can install apps on a mobile device you can install apps on access to the Justice Digital workspace on Slack Complete this guide in order, following each step closely.\\nIf you encounter issues, preferably raise a ticket on GitHub issues A member of the Analytical Platform team will contact you. 1. Read Terms of Use For Analytical Platform best practice, you need to follow certain guidelines. Bookmark the following pages and ensure you follow them before you begin using the platform: Acceptable use policy : covers the way you should use the Analytical Platform and its associated tools and services Data and Analytical Services Directorate’s (DASD) coding standards : principles outlining how you should write and review code MoJ Analytical IT Tools Strategy : describes recommended ways of working on the Analytical Platform 2. Create Slack account We use Slack to communicate status updates, such as scheduled maintenance for the Analytical Platform. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 21}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1480ea2de9ddd118c3002bbbb5448161'}>,\n",
       " <Document: {'content': 'You can also use it to communicate with our support team and the Analytical Platform user community. There are two workspaces we recommend joining: Justice Digital and ASD . To join, while signed in to your work email, navigate to each workspace and request to join. Note that workspace moderators only consider users from the following email addresses: @justice.gsi.gov.uk @digital.justice.gov.uk @cjs.gsi.gov.uk @noms.gsi.gov.uk @legalaid.gsi.gov.uk @justice.gov.uk @judiciary.uk Note : You can only access the Justice Digital and ASD workspaces using DOM1 and MoJ Digital and Technology MacBooks. If you use Quantum, you will need to access Slack using a mobile device instead. Join Slack channels Conversations in Slack are organised into channels, which each have a specific topic. After getting access to Slack workspace you need to separately join channels. Channels can be: public (all users can join) or private (users can only join by invitation) or be created based on teams, projects, locations, tools and techniques among others. There are several public channels that are widely used and may be useful to join: the #analytical-platform-support channel is used for general discussion of the Analytical Platform – it is also monitored by the Analytical Platform team, who can help out with any technical queries or requests. Also used to request new or existing apps and app data sources the #ask-data-engineering channel is used for general discussion of data engineering and for getting in touch with the data engineering team with any technical queries or requests (such as airflow DAG reviews, database access, data discovery tool, etc). the #git , #r and #python channels can be used to get support from other users with any technical queries or questions – the # intro_r channel is aimed specifically at new users of R the #data_science channel is used for general discussion of data science tools and techniques the #general channel is used for any discussions that don’t fit anywhere else the #10sc channel can be used to get in touch with other people working at 10SC and to ask any questions about the building There are lots of other channels you can join or you can set up a new one if you think something is missing. You may also wish to set up private channels to discuss specific projects and can direct message individual users. 3. Create GitHub account Using your work email address (ending either justice.gov.uk or digital.justice.gov.uk ), sign up for a GitHub account . See the GitHub documentation for instructions. If you already have a GitHub account that you wish to use for work within the Ministry of Justice, you can use it provided you associate your work email address with the account in your user settings. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 22}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e983bb24dfe42b5372249e2ad43aebba'}>,\n",
       " <Document: {'content': 'Ensure that: your account uses the free plan subscription you choose a username that does not contain upper-case characters (for good practice) you set your username to associate with your Git commits for every repository; see the GitHub documentation for instructions you follow the Ministry of Justice’s best practice guidelines when setting your password you configure two-factor authentication (2FA) on your mobile device; we recommend using either Google Authenticator or Microsoft Authenticator for 2FA For more information on 2FA within the Ministry of Justice, see the MoJ’s Security Guidance 4. Access the Analytical Platform Once you have your GitHub account, there are two more steps to complete before you can access the Analytical Platform: joining the MoJ Analytical Services GitHub organisation and signing in to the Analytical Platform’s Control Panel. Join MoJ Analytical Services To request an invitation Analytical Platform GitHub Organisation, please head over to the #ask-operations-engineering slack channel. Ask to be added to the analytical-services GitHub Organisation by providing your GitHub username in the message. The Operations Engineering Team will invite you to join the MoJ Analytical Services GitHub organisation. Example 👋 Could I please be added to the Analytical Services GitHub Org, my github name is github user name thank you. If you do not receive a response within 24 hours, request access in either the #ask-operations-engineering Slack channel or email here , providing your GitHub username in your message. Sign in to the Control Panel The main entry point to the Analytical Platform is the Control Panel . From there, you configure core tools such as JupyterLab and RStudio. When you access the Control Panel first time, a prompt will appear, requiring you to configure 2FA using your mobile device. Note that while you use your GitHub account to access the Control Panel, this 2FA is separate from the one you use to log in to GitHub. You may need to disable browser extensions such as Dark Mode during the 2FA setup process. After you log in to the Control Panel for the first time, you can begin requesting access to data on the platform. 5. Set up JupyterLab Note : Only follow this step if you want to use the Analytical Platform as a Python-based user. If you want to use R instead, proceed to step 6 to configure RStudio instead. If you want to build and deploy applications on the Analytical Platform using Python, you need to set up JupyterLab, the Python-based IDE (Integrated Development Environment) on the Analytical Platform. To set up JupyterLab, navigate to the Control Panel . Under Jupyter Lab Data Science [Git extension] , use the drop-down menu to select the version of JupyterLab you want to deploy. Unless you need to use a specific version, we recommend selecting the first option in the list. This will deploy JupyterLab on a virtual machine in your browser, with all of the required components it requires to run. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 23}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2ed652cdd4e0b230203ca17cc8837035'}>,\n",
       " <Document: {'content': 'Create and add JupyterLab SSH key to GitHub To access GitHub repositories from JupyterLab, you need an SSH key to enable secure communication and authentication between the two services.\\nDo not try to use an existing SSH key; each tool you use requires a unique key. To create an SSH key in JupyterLab: Open JupyerLab from the Analytical Platform Control Panel Select the + icon in the file browser to open a new Launcher tab Navigate to the Other section and select Terminal Run the following command in your terminal, replacing your_email@example.com with the email address you used to sign up for GitHub: $ ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" The response will ask you to choose a directory to save the key in; press Enter to accept the default location The response will also ask you to set a passphrase; press Enter to not set a passphrase. To view the SSH key, run: $ cat /home/jovyan/.ssh/id_rsa.pub Copy the SSH key to your clipboard You then need to add the SSH key to GitHub; see the GitHub documentation for instructions. 6. Set up RStudio Note : Only follow this step if you want to use the Analytical Platform as an R-based user. RStudio supports Python, but we recommend using JupyterLab for Python instead. If you configured JupyterLab in the previous step, proceed to step 7. If you want to build and deploy applications on the Analytical Platform using R, you need to set up RStudio, the R-based IDE (Integrated Development Environment) the Analytical Platform provides. To set up RStudio, navigate to the Control Panel . Under RStudio , use the drop-down menu to select the version of RStudio you want to deploy. Unless you need to use a specific version, we recommend selecting the first option in the list. This will deploy RStudio on a virtual machine in your browser, with all of the required components it requires to run. Create and add RStudio SSH key to GitHub So you can access GitHub repositories from RStudio, you need an SSH key to connect the two. Do not try to use an existing SSH key; each tool you use requires a unique key. To create an SSH key in RStudio: Open RStudio from the Analytical Platform Control Panel Navigate to Tools>Global Options In the Options window, select Git/SVN in the navigation menu Select Create RSA key and then Create When the Information window appears, select Close Select View public key and copy the SSH key to your clipboard You then need to add the SSH key to GitHub; see the GitHub documentation for instructions. Now that you have completed this guide you are ready to begin using the Analytical Platform. See training for a list of different resources to help you begin using the platform. This page was last reviewed on 2 June 2023.\\n\\nIt needs to be reviewed again on 2 June 2024\\nby the page owner #analytical-platform-support . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 24}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'edf51f74fd1aae20a540e8836f60c341'}>,\n",
       " <Document: {'content': 'This page was set to be reviewed before 2 June 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nGet Started - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 25}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd90805b5d8f011b759ffafcd7210fc73'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 26}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Quickstart guide This guide provides the instructions to set up the main accounts and services you need to use the Analytical Platform. Once you complete it, you can: access the Analytical Platform Control Panel explore data on the Analytical Platform begin developing your application in either JupyterLab or RStudio contribute to the Analytical Platform User Guidance Before you begin To use this guide, you need the following: a Ministry of Justice-issued Office 365 account a Ministry of Justice-issued laptop you can install apps on a mobile device you can install apps on access to the Justice Digital workspace on Slack Complete this guide in order, following each step closely.\\nIf you encounter issues, preferably raise a ticket on GitHub issues A member of the Analytical Platform team will contact you. 1. Read Terms of Use For Analytical Platform best practice, you need to follow certain guidelines. Bookmark the following pages and ensure you follow them before you begin using the platform: Acceptable use policy : covers the way you should use the Analytical Platform and its associated tools and services Data and Analytical Services Directorate’s (DASD) coding standards : principles outlining how you should write and review code MoJ Analytical IT Tools Strategy : describes recommended ways of working on the Analytical Platform 2. Create Slack account We use Slack to communicate status updates, such as scheduled maintenance for the Analytical Platform. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 27}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1480ea2de9ddd118c3002bbbb5448161'}>,\n",
       " <Document: {'content': 'You can also use it to communicate with our support team and the Analytical Platform user community. There are two workspaces we recommend joining: Justice Digital and ASD . To join, while signed in to your work email, navigate to each workspace and request to join. Note that workspace moderators only consider users from the following email addresses: @justice.gsi.gov.uk @digital.justice.gov.uk @cjs.gsi.gov.uk @noms.gsi.gov.uk @legalaid.gsi.gov.uk @justice.gov.uk @judiciary.uk Note : You can only access the Justice Digital and ASD workspaces using DOM1 and MoJ Digital and Technology MacBooks. If you use Quantum, you will need to access Slack using a mobile device instead. Join Slack channels Conversations in Slack are organised into channels, which each have a specific topic. After getting access to Slack workspace you need to separately join channels. Channels can be: public (all users can join) or private (users can only join by invitation) or be created based on teams, projects, locations, tools and techniques among others. There are several public channels that are widely used and may be useful to join: the #analytical-platform-support channel is used for general discussion of the Analytical Platform – it is also monitored by the Analytical Platform team, who can help out with any technical queries or requests. Also used to request new or existing apps and app data sources the #ask-data-engineering channel is used for general discussion of data engineering and for getting in touch with the data engineering team with any technical queries or requests (such as airflow DAG reviews, database access, data discovery tool, etc). the #git , #r and #python channels can be used to get support from other users with any technical queries or questions – the # intro_r channel is aimed specifically at new users of R the #data_science channel is used for general discussion of data science tools and techniques the #general channel is used for any discussions that don’t fit anywhere else the #10sc channel can be used to get in touch with other people working at 10SC and to ask any questions about the building There are lots of other channels you can join or you can set up a new one if you think something is missing. You may also wish to set up private channels to discuss specific projects and can direct message individual users. 3. Create GitHub account Using your work email address (ending either justice.gov.uk or digital.justice.gov.uk ), sign up for a GitHub account . See the GitHub documentation for instructions. If you already have a GitHub account that you wish to use for work within the Ministry of Justice, you can use it provided you associate your work email address with the account in your user settings. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 28}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e983bb24dfe42b5372249e2ad43aebba'}>,\n",
       " <Document: {'content': 'Ensure that: your account uses the free plan subscription you choose a username that does not contain upper-case characters (for good practice) you set your username to associate with your Git commits for every repository; see the GitHub documentation for instructions you follow the Ministry of Justice’s best practice guidelines when setting your password you configure two-factor authentication (2FA) on your mobile device; we recommend using either Google Authenticator or Microsoft Authenticator for 2FA For more information on 2FA within the Ministry of Justice, see the MoJ’s Security Guidance 4. Access the Analytical Platform Once you have your GitHub account, there are two more steps to complete before you can access the Analytical Platform: joining the MoJ Analytical Services GitHub organisation and signing in to the Analytical Platform’s Control Panel. Join MoJ Analytical Services To request an invitation Analytical Platform GitHub Organisation, please head over to the #ask-operations-engineering slack channel. Ask to be added to the analytical-services GitHub Organisation by providing your GitHub username in the message. The Operations Engineering Team will invite you to join the MoJ Analytical Services GitHub organisation. Example 👋 Could I please be added to the Analytical Services GitHub Org, my github name is github user name thank you. If you do not receive a response within 24 hours, request access in either the #ask-operations-engineering Slack channel or email here , providing your GitHub username in your message. Sign in to the Control Panel The main entry point to the Analytical Platform is the Control Panel . From there, you configure core tools such as JupyterLab and RStudio. When you access the Control Panel first time, a prompt will appear, requiring you to configure 2FA using your mobile device. Note that while you use your GitHub account to access the Control Panel, this 2FA is separate from the one you use to log in to GitHub. You may need to disable browser extensions such as Dark Mode during the 2FA setup process. After you log in to the Control Panel for the first time, you can begin requesting access to data on the platform. 5. Set up JupyterLab Note : Only follow this step if you want to use the Analytical Platform as a Python-based user. If you want to use R instead, proceed to step 6 to configure RStudio instead. If you want to build and deploy applications on the Analytical Platform using Python, you need to set up JupyterLab, the Python-based IDE (Integrated Development Environment) on the Analytical Platform. To set up JupyterLab, navigate to the Control Panel . Under Jupyter Lab Data Science [Git extension] , use the drop-down menu to select the version of JupyterLab you want to deploy. Unless you need to use a specific version, we recommend selecting the first option in the list. This will deploy JupyterLab on a virtual machine in your browser, with all of the required components it requires to run. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 29}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2ed652cdd4e0b230203ca17cc8837035'}>,\n",
       " <Document: {'content': 'Create and add JupyterLab SSH key to GitHub To access GitHub repositories from JupyterLab, you need an SSH key to enable secure communication and authentication between the two services.\\nDo not try to use an existing SSH key; each tool you use requires a unique key. To create an SSH key in JupyterLab: Open JupyerLab from the Analytical Platform Control Panel Select the + icon in the file browser to open a new Launcher tab Navigate to the Other section and select Terminal Run the following command in your terminal, replacing your_email@example.com with the email address you used to sign up for GitHub: $ ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" The response will ask you to choose a directory to save the key in; press Enter to accept the default location The response will also ask you to set a passphrase; press Enter to not set a passphrase. To view the SSH key, run: $ cat /home/jovyan/.ssh/id_rsa.pub Copy the SSH key to your clipboard You then need to add the SSH key to GitHub; see the GitHub documentation for instructions. 6. Set up RStudio Note : Only follow this step if you want to use the Analytical Platform as an R-based user. RStudio supports Python, but we recommend using JupyterLab for Python instead. If you configured JupyterLab in the previous step, proceed to step 7. If you want to build and deploy applications on the Analytical Platform using R, you need to set up RStudio, the R-based IDE (Integrated Development Environment) the Analytical Platform provides. To set up RStudio, navigate to the Control Panel . Under RStudio , use the drop-down menu to select the version of RStudio you want to deploy. Unless you need to use a specific version, we recommend selecting the first option in the list. This will deploy RStudio on a virtual machine in your browser, with all of the required components it requires to run. Create and add RStudio SSH key to GitHub So you can access GitHub repositories from RStudio, you need an SSH key to connect the two. Do not try to use an existing SSH key; each tool you use requires a unique key. To create an SSH key in RStudio: Open RStudio from the Analytical Platform Control Panel Navigate to Tools>Global Options In the Options window, select Git/SVN in the navigation menu Select Create RSA key and then Create When the Information window appears, select Close Select View public key and copy the SSH key to your clipboard You then need to add the SSH key to GitHub; see the GitHub documentation for instructions. Now that you have completed this guide you are ready to begin using the Analytical Platform. See training for a list of different resources to help you begin using the platform. This page was last reviewed on 2 June 2023.\\n\\nIt needs to be reviewed again on 2 June 2024\\nby the page owner #analytical-platform-support . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 30}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'edf51f74fd1aae20a540e8836f60c341'}>,\n",
       " <Document: {'content': 'This page was set to be reviewed before 2 June 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nGet Started - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 31}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd90805b5d8f011b759ffafcd7210fc73'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 32}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Quickstart guide This guide provides the instructions to set up the main accounts and services you need to use the Analytical Platform. Once you complete it, you can: access the Analytical Platform Control Panel explore data on the Analytical Platform begin developing your application in either JupyterLab or RStudio contribute to the Analytical Platform User Guidance Before you begin To use this guide, you need the following: a Ministry of Justice-issued Office 365 account a Ministry of Justice-issued laptop you can install apps on a mobile device you can install apps on access to the Justice Digital workspace on Slack Complete this guide in order, following each step closely.\\nIf you encounter issues, preferably raise a ticket on GitHub issues A member of the Analytical Platform team will contact you. 1. Read Terms of Use For Analytical Platform best practice, you need to follow certain guidelines. Bookmark the following pages and ensure you follow them before you begin using the platform: Acceptable use policy : covers the way you should use the Analytical Platform and its associated tools and services Data and Analytical Services Directorate’s (DASD) coding standards : principles outlining how you should write and review code MoJ Analytical IT Tools Strategy : describes recommended ways of working on the Analytical Platform 2. Create Slack account We use Slack to communicate status updates, such as scheduled maintenance for the Analytical Platform. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 33}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1480ea2de9ddd118c3002bbbb5448161'}>,\n",
       " <Document: {'content': 'You can also use it to communicate with our support team and the Analytical Platform user community. There are two workspaces we recommend joining: Justice Digital and ASD . To join, while signed in to your work email, navigate to each workspace and request to join. Note that workspace moderators only consider users from the following email addresses: @justice.gsi.gov.uk @digital.justice.gov.uk @cjs.gsi.gov.uk @noms.gsi.gov.uk @legalaid.gsi.gov.uk @justice.gov.uk @judiciary.uk Note : You can only access the Justice Digital and ASD workspaces using DOM1 and MoJ Digital and Technology MacBooks. If you use Quantum, you will need to access Slack using a mobile device instead. Join Slack channels Conversations in Slack are organised into channels, which each have a specific topic. After getting access to Slack workspace you need to separately join channels. Channels can be: public (all users can join) or private (users can only join by invitation) or be created based on teams, projects, locations, tools and techniques among others. There are several public channels that are widely used and may be useful to join: the #analytical-platform-support channel is used for general discussion of the Analytical Platform – it is also monitored by the Analytical Platform team, who can help out with any technical queries or requests. Also used to request new or existing apps and app data sources the #ask-data-engineering channel is used for general discussion of data engineering and for getting in touch with the data engineering team with any technical queries or requests (such as airflow DAG reviews, database access, data discovery tool, etc). the #git , #r and #python channels can be used to get support from other users with any technical queries or questions – the # intro_r channel is aimed specifically at new users of R the #data_science channel is used for general discussion of data science tools and techniques the #general channel is used for any discussions that don’t fit anywhere else the #10sc channel can be used to get in touch with other people working at 10SC and to ask any questions about the building There are lots of other channels you can join or you can set up a new one if you think something is missing. You may also wish to set up private channels to discuss specific projects and can direct message individual users. 3. Create GitHub account Using your work email address (ending either justice.gov.uk or digital.justice.gov.uk ), sign up for a GitHub account . See the GitHub documentation for instructions. If you already have a GitHub account that you wish to use for work within the Ministry of Justice, you can use it provided you associate your work email address with the account in your user settings. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 34}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e983bb24dfe42b5372249e2ad43aebba'}>,\n",
       " <Document: {'content': 'Ensure that: your account uses the free plan subscription you choose a username that does not contain upper-case characters (for good practice) you set your username to associate with your Git commits for every repository; see the GitHub documentation for instructions you follow the Ministry of Justice’s best practice guidelines when setting your password you configure two-factor authentication (2FA) on your mobile device; we recommend using either Google Authenticator or Microsoft Authenticator for 2FA For more information on 2FA within the Ministry of Justice, see the MoJ’s Security Guidance 4. Access the Analytical Platform Once you have your GitHub account, there are two more steps to complete before you can access the Analytical Platform: joining the MoJ Analytical Services GitHub organisation and signing in to the Analytical Platform’s Control Panel. Join MoJ Analytical Services To request an invitation Analytical Platform GitHub Organisation, please head over to the #ask-operations-engineering slack channel. Ask to be added to the analytical-services GitHub Organisation by providing your GitHub username in the message. The Operations Engineering Team will invite you to join the MoJ Analytical Services GitHub organisation. Example 👋 Could I please be added to the Analytical Services GitHub Org, my github name is github user name thank you. If you do not receive a response within 24 hours, request access in either the #ask-operations-engineering Slack channel or email here , providing your GitHub username in your message. Sign in to the Control Panel The main entry point to the Analytical Platform is the Control Panel . From there, you configure core tools such as JupyterLab and RStudio. When you access the Control Panel first time, a prompt will appear, requiring you to configure 2FA using your mobile device. Note that while you use your GitHub account to access the Control Panel, this 2FA is separate from the one you use to log in to GitHub. You may need to disable browser extensions such as Dark Mode during the 2FA setup process. After you log in to the Control Panel for the first time, you can begin requesting access to data on the platform. 5. Set up JupyterLab Note : Only follow this step if you want to use the Analytical Platform as a Python-based user. If you want to use R instead, proceed to step 6 to configure RStudio instead. If you want to build and deploy applications on the Analytical Platform using Python, you need to set up JupyterLab, the Python-based IDE (Integrated Development Environment) on the Analytical Platform. To set up JupyterLab, navigate to the Control Panel . Under Jupyter Lab Data Science [Git extension] , use the drop-down menu to select the version of JupyterLab you want to deploy. Unless you need to use a specific version, we recommend selecting the first option in the list. This will deploy JupyterLab on a virtual machine in your browser, with all of the required components it requires to run. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 35}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2ed652cdd4e0b230203ca17cc8837035'}>,\n",
       " <Document: {'content': 'Create and add JupyterLab SSH key to GitHub To access GitHub repositories from JupyterLab, you need an SSH key to enable secure communication and authentication between the two services.\\nDo not try to use an existing SSH key; each tool you use requires a unique key. To create an SSH key in JupyterLab: Open JupyerLab from the Analytical Platform Control Panel Select the + icon in the file browser to open a new Launcher tab Navigate to the Other section and select Terminal Run the following command in your terminal, replacing your_email@example.com with the email address you used to sign up for GitHub: $ ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" The response will ask you to choose a directory to save the key in; press Enter to accept the default location The response will also ask you to set a passphrase; press Enter to not set a passphrase. To view the SSH key, run: $ cat /home/jovyan/.ssh/id_rsa.pub Copy the SSH key to your clipboard You then need to add the SSH key to GitHub; see the GitHub documentation for instructions. 6. Set up RStudio Note : Only follow this step if you want to use the Analytical Platform as an R-based user. RStudio supports Python, but we recommend using JupyterLab for Python instead. If you configured JupyterLab in the previous step, proceed to step 7. If you want to build and deploy applications on the Analytical Platform using R, you need to set up RStudio, the R-based IDE (Integrated Development Environment) the Analytical Platform provides. To set up RStudio, navigate to the Control Panel . Under RStudio , use the drop-down menu to select the version of RStudio you want to deploy. Unless you need to use a specific version, we recommend selecting the first option in the list. This will deploy RStudio on a virtual machine in your browser, with all of the required components it requires to run. Create and add RStudio SSH key to GitHub So you can access GitHub repositories from RStudio, you need an SSH key to connect the two. Do not try to use an existing SSH key; each tool you use requires a unique key. To create an SSH key in RStudio: Open RStudio from the Analytical Platform Control Panel Navigate to Tools>Global Options In the Options window, select Git/SVN in the navigation menu Select Create RSA key and then Create When the Information window appears, select Close Select View public key and copy the SSH key to your clipboard You then need to add the SSH key to GitHub; see the GitHub documentation for instructions. Now that you have completed this guide you are ready to begin using the Analytical Platform. See training for a list of different resources to help you begin using the platform. This page was last reviewed on 2 June 2023.\\n\\nIt needs to be reviewed again on 2 June 2024\\nby the page owner #analytical-platform-support . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 36}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'edf51f74fd1aae20a540e8836f60c341'}>,\n",
       " <Document: {'content': 'This page was set to be reviewed before 2 June 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nGet Started - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 37}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd90805b5d8f011b759ffafcd7210fc73'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 38}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Quickstart guide This guide provides the instructions to set up the main accounts and services you need to use the Analytical Platform. Once you complete it, you can: access the Analytical Platform Control Panel explore data on the Analytical Platform begin developing your application in either JupyterLab or RStudio contribute to the Analytical Platform User Guidance Before you begin To use this guide, you need the following: a Ministry of Justice-issued Office 365 account a Ministry of Justice-issued laptop you can install apps on a mobile device you can install apps on access to the Justice Digital workspace on Slack Complete this guide in order, following each step closely.\\nIf you encounter issues, preferably raise a ticket on GitHub issues A member of the Analytical Platform team will contact you. 1. Read Terms of Use For Analytical Platform best practice, you need to follow certain guidelines. Bookmark the following pages and ensure you follow them before you begin using the platform: Acceptable use policy : covers the way you should use the Analytical Platform and its associated tools and services Data and Analytical Services Directorate’s (DASD) coding standards : principles outlining how you should write and review code MoJ Analytical IT Tools Strategy : describes recommended ways of working on the Analytical Platform 2. Create Slack account We use Slack to communicate status updates, such as scheduled maintenance for the Analytical Platform. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 39}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1480ea2de9ddd118c3002bbbb5448161'}>,\n",
       " <Document: {'content': 'You can also use it to communicate with our support team and the Analytical Platform user community. There are two workspaces we recommend joining: Justice Digital and ASD . To join, while signed in to your work email, navigate to each workspace and request to join. Note that workspace moderators only consider users from the following email addresses: @justice.gsi.gov.uk @digital.justice.gov.uk @cjs.gsi.gov.uk @noms.gsi.gov.uk @legalaid.gsi.gov.uk @justice.gov.uk @judiciary.uk Note : You can only access the Justice Digital and ASD workspaces using DOM1 and MoJ Digital and Technology MacBooks. If you use Quantum, you will need to access Slack using a mobile device instead. Join Slack channels Conversations in Slack are organised into channels, which each have a specific topic. After getting access to Slack workspace you need to separately join channels. Channels can be: public (all users can join) or private (users can only join by invitation) or be created based on teams, projects, locations, tools and techniques among others. There are several public channels that are widely used and may be useful to join: the #analytical-platform-support channel is used for general discussion of the Analytical Platform – it is also monitored by the Analytical Platform team, who can help out with any technical queries or requests. Also used to request new or existing apps and app data sources the #ask-data-engineering channel is used for general discussion of data engineering and for getting in touch with the data engineering team with any technical queries or requests (such as airflow DAG reviews, database access, data discovery tool, etc). the #git , #r and #python channels can be used to get support from other users with any technical queries or questions – the # intro_r channel is aimed specifically at new users of R the #data_science channel is used for general discussion of data science tools and techniques the #general channel is used for any discussions that don’t fit anywhere else the #10sc channel can be used to get in touch with other people working at 10SC and to ask any questions about the building There are lots of other channels you can join or you can set up a new one if you think something is missing. You may also wish to set up private channels to discuss specific projects and can direct message individual users. 3. Create GitHub account Using your work email address (ending either justice.gov.uk or digital.justice.gov.uk ), sign up for a GitHub account . See the GitHub documentation for instructions. If you already have a GitHub account that you wish to use for work within the Ministry of Justice, you can use it provided you associate your work email address with the account in your user settings. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 40}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e983bb24dfe42b5372249e2ad43aebba'}>,\n",
       " <Document: {'content': 'Ensure that: your account uses the free plan subscription you choose a username that does not contain upper-case characters (for good practice) you set your username to associate with your Git commits for every repository; see the GitHub documentation for instructions you follow the Ministry of Justice’s best practice guidelines when setting your password you configure two-factor authentication (2FA) on your mobile device; we recommend using either Google Authenticator or Microsoft Authenticator for 2FA For more information on 2FA within the Ministry of Justice, see the MoJ’s Security Guidance 4. Access the Analytical Platform Once you have your GitHub account, there are two more steps to complete before you can access the Analytical Platform: joining the MoJ Analytical Services GitHub organisation and signing in to the Analytical Platform’s Control Panel. Join MoJ Analytical Services To request an invitation Analytical Platform GitHub Organisation, please head over to the #ask-operations-engineering slack channel. Ask to be added to the analytical-services GitHub Organisation by providing your GitHub username in the message. The Operations Engineering Team will invite you to join the MoJ Analytical Services GitHub organisation. Example 👋 Could I please be added to the Analytical Services GitHub Org, my github name is github user name thank you. If you do not receive a response within 24 hours, request access in either the #ask-operations-engineering Slack channel or email here , providing your GitHub username in your message. Sign in to the Control Panel The main entry point to the Analytical Platform is the Control Panel . From there, you configure core tools such as JupyterLab and RStudio. When you access the Control Panel first time, a prompt will appear, requiring you to configure 2FA using your mobile device. Note that while you use your GitHub account to access the Control Panel, this 2FA is separate from the one you use to log in to GitHub. You may need to disable browser extensions such as Dark Mode during the 2FA setup process. After you log in to the Control Panel for the first time, you can begin requesting access to data on the platform. 5. Set up JupyterLab Note : Only follow this step if you want to use the Analytical Platform as a Python-based user. If you want to use R instead, proceed to step 6 to configure RStudio instead. If you want to build and deploy applications on the Analytical Platform using Python, you need to set up JupyterLab, the Python-based IDE (Integrated Development Environment) on the Analytical Platform. To set up JupyterLab, navigate to the Control Panel . Under Jupyter Lab Data Science [Git extension] , use the drop-down menu to select the version of JupyterLab you want to deploy. Unless you need to use a specific version, we recommend selecting the first option in the list. This will deploy JupyterLab on a virtual machine in your browser, with all of the required components it requires to run. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 41}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2ed652cdd4e0b230203ca17cc8837035'}>,\n",
       " <Document: {'content': 'Create and add JupyterLab SSH key to GitHub To access GitHub repositories from JupyterLab, you need an SSH key to enable secure communication and authentication between the two services.\\nDo not try to use an existing SSH key; each tool you use requires a unique key. To create an SSH key in JupyterLab: Open JupyerLab from the Analytical Platform Control Panel Select the + icon in the file browser to open a new Launcher tab Navigate to the Other section and select Terminal Run the following command in your terminal, replacing your_email@example.com with the email address you used to sign up for GitHub: $ ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" The response will ask you to choose a directory to save the key in; press Enter to accept the default location The response will also ask you to set a passphrase; press Enter to not set a passphrase. To view the SSH key, run: $ cat /home/jovyan/.ssh/id_rsa.pub Copy the SSH key to your clipboard You then need to add the SSH key to GitHub; see the GitHub documentation for instructions. 6. Set up RStudio Note : Only follow this step if you want to use the Analytical Platform as an R-based user. RStudio supports Python, but we recommend using JupyterLab for Python instead. If you configured JupyterLab in the previous step, proceed to step 7. If you want to build and deploy applications on the Analytical Platform using R, you need to set up RStudio, the R-based IDE (Integrated Development Environment) the Analytical Platform provides. To set up RStudio, navigate to the Control Panel . Under RStudio , use the drop-down menu to select the version of RStudio you want to deploy. Unless you need to use a specific version, we recommend selecting the first option in the list. This will deploy RStudio on a virtual machine in your browser, with all of the required components it requires to run. Create and add RStudio SSH key to GitHub So you can access GitHub repositories from RStudio, you need an SSH key to connect the two. Do not try to use an existing SSH key; each tool you use requires a unique key. To create an SSH key in RStudio: Open RStudio from the Analytical Platform Control Panel Navigate to Tools>Global Options In the Options window, select Git/SVN in the navigation menu Select Create RSA key and then Create When the Information window appears, select Close Select View public key and copy the SSH key to your clipboard You then need to add the SSH key to GitHub; see the GitHub documentation for instructions. Now that you have completed this guide you are ready to begin using the Analytical Platform. See training for a list of different resources to help you begin using the platform. This page was last reviewed on 2 June 2023.\\n\\nIt needs to be reviewed again on 2 June 2024\\nby the page owner #analytical-platform-support . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 42}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'edf51f74fd1aae20a540e8836f60c341'}>,\n",
       " <Document: {'content': 'This page was set to be reviewed before 2 June 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nGet Started - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 43}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd90805b5d8f011b759ffafcd7210fc73'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 44}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Quickstart guide This guide provides the instructions to set up the main accounts and services you need to use the Analytical Platform. Once you complete it, you can: access the Analytical Platform Control Panel explore data on the Analytical Platform begin developing your application in either JupyterLab or RStudio contribute to the Analytical Platform User Guidance Before you begin To use this guide, you need the following: a Ministry of Justice-issued Office 365 account a Ministry of Justice-issued laptop you can install apps on a mobile device you can install apps on access to the Justice Digital workspace on Slack Complete this guide in order, following each step closely.\\nIf you encounter issues, preferably raise a ticket on GitHub issues A member of the Analytical Platform team will contact you. 1. Read Terms of Use For Analytical Platform best practice, you need to follow certain guidelines. Bookmark the following pages and ensure you follow them before you begin using the platform: Acceptable use policy : covers the way you should use the Analytical Platform and its associated tools and services Data and Analytical Services Directorate’s (DASD) coding standards : principles outlining how you should write and review code MoJ Analytical IT Tools Strategy : describes recommended ways of working on the Analytical Platform 2. Create Slack account We use Slack to communicate status updates, such as scheduled maintenance for the Analytical Platform. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 45}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1480ea2de9ddd118c3002bbbb5448161'}>,\n",
       " <Document: {'content': 'You can also use it to communicate with our support team and the Analytical Platform user community. There are two workspaces we recommend joining: Justice Digital and ASD . To join, while signed in to your work email, navigate to each workspace and request to join. Note that workspace moderators only consider users from the following email addresses: @justice.gsi.gov.uk @digital.justice.gov.uk @cjs.gsi.gov.uk @noms.gsi.gov.uk @legalaid.gsi.gov.uk @justice.gov.uk @judiciary.uk Note : You can only access the Justice Digital and ASD workspaces using DOM1 and MoJ Digital and Technology MacBooks. If you use Quantum, you will need to access Slack using a mobile device instead. Join Slack channels Conversations in Slack are organised into channels, which each have a specific topic. After getting access to Slack workspace you need to separately join channels. Channels can be: public (all users can join) or private (users can only join by invitation) or be created based on teams, projects, locations, tools and techniques among others. There are several public channels that are widely used and may be useful to join: the #analytical-platform-support channel is used for general discussion of the Analytical Platform – it is also monitored by the Analytical Platform team, who can help out with any technical queries or requests. Also used to request new or existing apps and app data sources the #ask-data-engineering channel is used for general discussion of data engineering and for getting in touch with the data engineering team with any technical queries or requests (such as airflow DAG reviews, database access, data discovery tool, etc). the #git , #r and #python channels can be used to get support from other users with any technical queries or questions – the # intro_r channel is aimed specifically at new users of R the #data_science channel is used for general discussion of data science tools and techniques the #general channel is used for any discussions that don’t fit anywhere else the #10sc channel can be used to get in touch with other people working at 10SC and to ask any questions about the building There are lots of other channels you can join or you can set up a new one if you think something is missing. You may also wish to set up private channels to discuss specific projects and can direct message individual users. 3. Create GitHub account Using your work email address (ending either justice.gov.uk or digital.justice.gov.uk ), sign up for a GitHub account . See the GitHub documentation for instructions. If you already have a GitHub account that you wish to use for work within the Ministry of Justice, you can use it provided you associate your work email address with the account in your user settings. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 46}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e983bb24dfe42b5372249e2ad43aebba'}>,\n",
       " <Document: {'content': 'Ensure that: your account uses the free plan subscription you choose a username that does not contain upper-case characters (for good practice) you set your username to associate with your Git commits for every repository; see the GitHub documentation for instructions you follow the Ministry of Justice’s best practice guidelines when setting your password you configure two-factor authentication (2FA) on your mobile device; we recommend using either Google Authenticator or Microsoft Authenticator for 2FA For more information on 2FA within the Ministry of Justice, see the MoJ’s Security Guidance 4. Access the Analytical Platform Once you have your GitHub account, there are two more steps to complete before you can access the Analytical Platform: joining the MoJ Analytical Services GitHub organisation and signing in to the Analytical Platform’s Control Panel. Join MoJ Analytical Services To request an invitation Analytical Platform GitHub Organisation, please head over to the #ask-operations-engineering slack channel. Ask to be added to the analytical-services GitHub Organisation by providing your GitHub username in the message. The Operations Engineering Team will invite you to join the MoJ Analytical Services GitHub organisation. Example 👋 Could I please be added to the Analytical Services GitHub Org, my github name is github user name thank you. If you do not receive a response within 24 hours, request access in either the #ask-operations-engineering Slack channel or email here , providing your GitHub username in your message. Sign in to the Control Panel The main entry point to the Analytical Platform is the Control Panel . From there, you configure core tools such as JupyterLab and RStudio. When you access the Control Panel first time, a prompt will appear, requiring you to configure 2FA using your mobile device. Note that while you use your GitHub account to access the Control Panel, this 2FA is separate from the one you use to log in to GitHub. You may need to disable browser extensions such as Dark Mode during the 2FA setup process. After you log in to the Control Panel for the first time, you can begin requesting access to data on the platform. 5. Set up JupyterLab Note : Only follow this step if you want to use the Analytical Platform as a Python-based user. If you want to use R instead, proceed to step 6 to configure RStudio instead. If you want to build and deploy applications on the Analytical Platform using Python, you need to set up JupyterLab, the Python-based IDE (Integrated Development Environment) on the Analytical Platform. To set up JupyterLab, navigate to the Control Panel . Under Jupyter Lab Data Science [Git extension] , use the drop-down menu to select the version of JupyterLab you want to deploy. Unless you need to use a specific version, we recommend selecting the first option in the list. This will deploy JupyterLab on a virtual machine in your browser, with all of the required components it requires to run. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 47}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2ed652cdd4e0b230203ca17cc8837035'}>,\n",
       " <Document: {'content': 'Create and add JupyterLab SSH key to GitHub To access GitHub repositories from JupyterLab, you need an SSH key to enable secure communication and authentication between the two services.\\nDo not try to use an existing SSH key; each tool you use requires a unique key. To create an SSH key in JupyterLab: Open JupyerLab from the Analytical Platform Control Panel Select the + icon in the file browser to open a new Launcher tab Navigate to the Other section and select Terminal Run the following command in your terminal, replacing your_email@example.com with the email address you used to sign up for GitHub: $ ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" The response will ask you to choose a directory to save the key in; press Enter to accept the default location The response will also ask you to set a passphrase; press Enter to not set a passphrase. To view the SSH key, run: $ cat /home/jovyan/.ssh/id_rsa.pub Copy the SSH key to your clipboard You then need to add the SSH key to GitHub; see the GitHub documentation for instructions. 6. Set up RStudio Note : Only follow this step if you want to use the Analytical Platform as an R-based user. RStudio supports Python, but we recommend using JupyterLab for Python instead. If you configured JupyterLab in the previous step, proceed to step 7. If you want to build and deploy applications on the Analytical Platform using R, you need to set up RStudio, the R-based IDE (Integrated Development Environment) the Analytical Platform provides. To set up RStudio, navigate to the Control Panel . Under RStudio , use the drop-down menu to select the version of RStudio you want to deploy. Unless you need to use a specific version, we recommend selecting the first option in the list. This will deploy RStudio on a virtual machine in your browser, with all of the required components it requires to run. Create and add RStudio SSH key to GitHub So you can access GitHub repositories from RStudio, you need an SSH key to connect the two. Do not try to use an existing SSH key; each tool you use requires a unique key. To create an SSH key in RStudio: Open RStudio from the Analytical Platform Control Panel Navigate to Tools>Global Options In the Options window, select Git/SVN in the navigation menu Select Create RSA key and then Create When the Information window appears, select Close Select View public key and copy the SSH key to your clipboard You then need to add the SSH key to GitHub; see the GitHub documentation for instructions. Now that you have completed this guide you are ready to begin using the Analytical Platform. See training for a list of different resources to help you begin using the platform. This page was last reviewed on 2 June 2023.\\n\\nIt needs to be reviewed again on 2 June 2024\\nby the page owner #analytical-platform-support . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 48}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'edf51f74fd1aae20a540e8836f60c341'}>,\n",
       " <Document: {'content': 'This page was set to be reviewed before 2 June 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nGet Started - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 49}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd90805b5d8f011b759ffafcd7210fc73'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 50}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Quickstart guide This guide provides the instructions to set up the main accounts and services you need to use the Analytical Platform. Once you complete it, you can: access the Analytical Platform Control Panel explore data on the Analytical Platform begin developing your application in either JupyterLab or RStudio contribute to the Analytical Platform User Guidance Before you begin To use this guide, you need the following: a Ministry of Justice-issued Office 365 account a Ministry of Justice-issued laptop you can install apps on a mobile device you can install apps on access to the Justice Digital workspace on Slack Complete this guide in order, following each step closely.\\nIf you encounter issues, preferably raise a ticket on GitHub issues A member of the Analytical Platform team will contact you. 1. Read Terms of Use For Analytical Platform best practice, you need to follow certain guidelines. Bookmark the following pages and ensure you follow them before you begin using the platform: Acceptable use policy : covers the way you should use the Analytical Platform and its associated tools and services Data and Analytical Services Directorate’s (DASD) coding standards : principles outlining how you should write and review code MoJ Analytical IT Tools Strategy : describes recommended ways of working on the Analytical Platform 2. Create Slack account We use Slack to communicate status updates, such as scheduled maintenance for the Analytical Platform. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 51}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1480ea2de9ddd118c3002bbbb5448161'}>,\n",
       " <Document: {'content': 'You can also use it to communicate with our support team and the Analytical Platform user community. There are two workspaces we recommend joining: Justice Digital and ASD . To join, while signed in to your work email, navigate to each workspace and request to join. Note that workspace moderators only consider users from the following email addresses: @justice.gsi.gov.uk @digital.justice.gov.uk @cjs.gsi.gov.uk @noms.gsi.gov.uk @legalaid.gsi.gov.uk @justice.gov.uk @judiciary.uk Note : You can only access the Justice Digital and ASD workspaces using DOM1 and MoJ Digital and Technology MacBooks. If you use Quantum, you will need to access Slack using a mobile device instead. Join Slack channels Conversations in Slack are organised into channels, which each have a specific topic. After getting access to Slack workspace you need to separately join channels. Channels can be: public (all users can join) or private (users can only join by invitation) or be created based on teams, projects, locations, tools and techniques among others. There are several public channels that are widely used and may be useful to join: the #analytical-platform-support channel is used for general discussion of the Analytical Platform – it is also monitored by the Analytical Platform team, who can help out with any technical queries or requests. Also used to request new or existing apps and app data sources the #ask-data-engineering channel is used for general discussion of data engineering and for getting in touch with the data engineering team with any technical queries or requests (such as airflow DAG reviews, database access, data discovery tool, etc). the #git , #r and #python channels can be used to get support from other users with any technical queries or questions – the # intro_r channel is aimed specifically at new users of R the #data_science channel is used for general discussion of data science tools and techniques the #general channel is used for any discussions that don’t fit anywhere else the #10sc channel can be used to get in touch with other people working at 10SC and to ask any questions about the building There are lots of other channels you can join or you can set up a new one if you think something is missing. You may also wish to set up private channels to discuss specific projects and can direct message individual users. 3. Create GitHub account Using your work email address (ending either justice.gov.uk or digital.justice.gov.uk ), sign up for a GitHub account . See the GitHub documentation for instructions. If you already have a GitHub account that you wish to use for work within the Ministry of Justice, you can use it provided you associate your work email address with the account in your user settings. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 52}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e983bb24dfe42b5372249e2ad43aebba'}>,\n",
       " <Document: {'content': 'Ensure that: your account uses the free plan subscription you choose a username that does not contain upper-case characters (for good practice) you set your username to associate with your Git commits for every repository; see the GitHub documentation for instructions you follow the Ministry of Justice’s best practice guidelines when setting your password you configure two-factor authentication (2FA) on your mobile device; we recommend using either Google Authenticator or Microsoft Authenticator for 2FA For more information on 2FA within the Ministry of Justice, see the MoJ’s Security Guidance 4. Access the Analytical Platform Once you have your GitHub account, there are two more steps to complete before you can access the Analytical Platform: joining the MoJ Analytical Services GitHub organisation and signing in to the Analytical Platform’s Control Panel. Join MoJ Analytical Services To request an invitation Analytical Platform GitHub Organisation, please head over to the #ask-operations-engineering slack channel. Ask to be added to the analytical-services GitHub Organisation by providing your GitHub username in the message. The Operations Engineering Team will invite you to join the MoJ Analytical Services GitHub organisation. Example 👋 Could I please be added to the Analytical Services GitHub Org, my github name is github user name thank you. If you do not receive a response within 24 hours, request access in either the #ask-operations-engineering Slack channel or email here , providing your GitHub username in your message. Sign in to the Control Panel The main entry point to the Analytical Platform is the Control Panel . From there, you configure core tools such as JupyterLab and RStudio. When you access the Control Panel first time, a prompt will appear, requiring you to configure 2FA using your mobile device. Note that while you use your GitHub account to access the Control Panel, this 2FA is separate from the one you use to log in to GitHub. You may need to disable browser extensions such as Dark Mode during the 2FA setup process. After you log in to the Control Panel for the first time, you can begin requesting access to data on the platform. 5. Set up JupyterLab Note : Only follow this step if you want to use the Analytical Platform as a Python-based user. If you want to use R instead, proceed to step 6 to configure RStudio instead. If you want to build and deploy applications on the Analytical Platform using Python, you need to set up JupyterLab, the Python-based IDE (Integrated Development Environment) on the Analytical Platform. To set up JupyterLab, navigate to the Control Panel . Under Jupyter Lab Data Science [Git extension] , use the drop-down menu to select the version of JupyterLab you want to deploy. Unless you need to use a specific version, we recommend selecting the first option in the list. This will deploy JupyterLab on a virtual machine in your browser, with all of the required components it requires to run. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 53}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2ed652cdd4e0b230203ca17cc8837035'}>,\n",
       " <Document: {'content': 'Create and add JupyterLab SSH key to GitHub To access GitHub repositories from JupyterLab, you need an SSH key to enable secure communication and authentication between the two services.\\nDo not try to use an existing SSH key; each tool you use requires a unique key. To create an SSH key in JupyterLab: Open JupyerLab from the Analytical Platform Control Panel Select the + icon in the file browser to open a new Launcher tab Navigate to the Other section and select Terminal Run the following command in your terminal, replacing your_email@example.com with the email address you used to sign up for GitHub: $ ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" The response will ask you to choose a directory to save the key in; press Enter to accept the default location The response will also ask you to set a passphrase; press Enter to not set a passphrase. To view the SSH key, run: $ cat /home/jovyan/.ssh/id_rsa.pub Copy the SSH key to your clipboard You then need to add the SSH key to GitHub; see the GitHub documentation for instructions. 6. Set up RStudio Note : Only follow this step if you want to use the Analytical Platform as an R-based user. RStudio supports Python, but we recommend using JupyterLab for Python instead. If you configured JupyterLab in the previous step, proceed to step 7. If you want to build and deploy applications on the Analytical Platform using R, you need to set up RStudio, the R-based IDE (Integrated Development Environment) the Analytical Platform provides. To set up RStudio, navigate to the Control Panel . Under RStudio , use the drop-down menu to select the version of RStudio you want to deploy. Unless you need to use a specific version, we recommend selecting the first option in the list. This will deploy RStudio on a virtual machine in your browser, with all of the required components it requires to run. Create and add RStudio SSH key to GitHub So you can access GitHub repositories from RStudio, you need an SSH key to connect the two. Do not try to use an existing SSH key; each tool you use requires a unique key. To create an SSH key in RStudio: Open RStudio from the Analytical Platform Control Panel Navigate to Tools>Global Options In the Options window, select Git/SVN in the navigation menu Select Create RSA key and then Create When the Information window appears, select Close Select View public key and copy the SSH key to your clipboard You then need to add the SSH key to GitHub; see the GitHub documentation for instructions. Now that you have completed this guide you are ready to begin using the Analytical Platform. See training for a list of different resources to help you begin using the platform. This page was last reviewed on 2 June 2023.\\n\\nIt needs to be reviewed again on 2 June 2024\\nby the page owner #analytical-platform-support . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 54}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'edf51f74fd1aae20a540e8836f60c341'}>,\n",
       " <Document: {'content': 'This page was set to be reviewed before 2 June 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nGet Started - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 55}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd90805b5d8f011b759ffafcd7210fc73'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 56}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Quickstart guide This guide provides the instructions to set up the main accounts and services you need to use the Analytical Platform. Once you complete it, you can: access the Analytical Platform Control Panel explore data on the Analytical Platform begin developing your application in either JupyterLab or RStudio contribute to the Analytical Platform User Guidance Before you begin To use this guide, you need the following: a Ministry of Justice-issued Office 365 account a Ministry of Justice-issued laptop you can install apps on a mobile device you can install apps on access to the Justice Digital workspace on Slack Complete this guide in order, following each step closely.\\nIf you encounter issues, preferably raise a ticket on GitHub issues A member of the Analytical Platform team will contact you. 1. Read Terms of Use For Analytical Platform best practice, you need to follow certain guidelines. Bookmark the following pages and ensure you follow them before you begin using the platform: Acceptable use policy : covers the way you should use the Analytical Platform and its associated tools and services Data and Analytical Services Directorate’s (DASD) coding standards : principles outlining how you should write and review code MoJ Analytical IT Tools Strategy : describes recommended ways of working on the Analytical Platform 2. Create Slack account We use Slack to communicate status updates, such as scheduled maintenance for the Analytical Platform. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 57}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1480ea2de9ddd118c3002bbbb5448161'}>,\n",
       " <Document: {'content': 'You can also use it to communicate with our support team and the Analytical Platform user community. There are two workspaces we recommend joining: Justice Digital and ASD . To join, while signed in to your work email, navigate to each workspace and request to join. Note that workspace moderators only consider users from the following email addresses: @justice.gsi.gov.uk @digital.justice.gov.uk @cjs.gsi.gov.uk @noms.gsi.gov.uk @legalaid.gsi.gov.uk @justice.gov.uk @judiciary.uk Note : You can only access the Justice Digital and ASD workspaces using DOM1 and MoJ Digital and Technology MacBooks. If you use Quantum, you will need to access Slack using a mobile device instead. Join Slack channels Conversations in Slack are organised into channels, which each have a specific topic. After getting access to Slack workspace you need to separately join channels. Channels can be: public (all users can join) or private (users can only join by invitation) or be created based on teams, projects, locations, tools and techniques among others. There are several public channels that are widely used and may be useful to join: the #analytical-platform-support channel is used for general discussion of the Analytical Platform – it is also monitored by the Analytical Platform team, who can help out with any technical queries or requests. Also used to request new or existing apps and app data sources the #ask-data-engineering channel is used for general discussion of data engineering and for getting in touch with the data engineering team with any technical queries or requests (such as airflow DAG reviews, database access, data discovery tool, etc). the #git , #r and #python channels can be used to get support from other users with any technical queries or questions – the # intro_r channel is aimed specifically at new users of R the #data_science channel is used for general discussion of data science tools and techniques the #general channel is used for any discussions that don’t fit anywhere else the #10sc channel can be used to get in touch with other people working at 10SC and to ask any questions about the building There are lots of other channels you can join or you can set up a new one if you think something is missing. You may also wish to set up private channels to discuss specific projects and can direct message individual users. 3. Create GitHub account Using your work email address (ending either justice.gov.uk or digital.justice.gov.uk ), sign up for a GitHub account . See the GitHub documentation for instructions. If you already have a GitHub account that you wish to use for work within the Ministry of Justice, you can use it provided you associate your work email address with the account in your user settings. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 58}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e983bb24dfe42b5372249e2ad43aebba'}>,\n",
       " <Document: {'content': 'Ensure that: your account uses the free plan subscription you choose a username that does not contain upper-case characters (for good practice) you set your username to associate with your Git commits for every repository; see the GitHub documentation for instructions you follow the Ministry of Justice’s best practice guidelines when setting your password you configure two-factor authentication (2FA) on your mobile device; we recommend using either Google Authenticator or Microsoft Authenticator for 2FA For more information on 2FA within the Ministry of Justice, see the MoJ’s Security Guidance 4. Access the Analytical Platform Once you have your GitHub account, there are two more steps to complete before you can access the Analytical Platform: joining the MoJ Analytical Services GitHub organisation and signing in to the Analytical Platform’s Control Panel. Join MoJ Analytical Services To request an invitation Analytical Platform GitHub Organisation, please head over to the #ask-operations-engineering slack channel. Ask to be added to the analytical-services GitHub Organisation by providing your GitHub username in the message. The Operations Engineering Team will invite you to join the MoJ Analytical Services GitHub organisation. Example 👋 Could I please be added to the Analytical Services GitHub Org, my github name is github user name thank you. If you do not receive a response within 24 hours, request access in either the #ask-operations-engineering Slack channel or email here , providing your GitHub username in your message. Sign in to the Control Panel The main entry point to the Analytical Platform is the Control Panel . From there, you configure core tools such as JupyterLab and RStudio. When you access the Control Panel first time, a prompt will appear, requiring you to configure 2FA using your mobile device. Note that while you use your GitHub account to access the Control Panel, this 2FA is separate from the one you use to log in to GitHub. You may need to disable browser extensions such as Dark Mode during the 2FA setup process. After you log in to the Control Panel for the first time, you can begin requesting access to data on the platform. 5. Set up JupyterLab Note : Only follow this step if you want to use the Analytical Platform as a Python-based user. If you want to use R instead, proceed to step 6 to configure RStudio instead. If you want to build and deploy applications on the Analytical Platform using Python, you need to set up JupyterLab, the Python-based IDE (Integrated Development Environment) on the Analytical Platform. To set up JupyterLab, navigate to the Control Panel . Under Jupyter Lab Data Science [Git extension] , use the drop-down menu to select the version of JupyterLab you want to deploy. Unless you need to use a specific version, we recommend selecting the first option in the list. This will deploy JupyterLab on a virtual machine in your browser, with all of the required components it requires to run. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 59}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2ed652cdd4e0b230203ca17cc8837035'}>,\n",
       " <Document: {'content': 'Create and add JupyterLab SSH key to GitHub To access GitHub repositories from JupyterLab, you need an SSH key to enable secure communication and authentication between the two services.\\nDo not try to use an existing SSH key; each tool you use requires a unique key. To create an SSH key in JupyterLab: Open JupyerLab from the Analytical Platform Control Panel Select the + icon in the file browser to open a new Launcher tab Navigate to the Other section and select Terminal Run the following command in your terminal, replacing your_email@example.com with the email address you used to sign up for GitHub: $ ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" The response will ask you to choose a directory to save the key in; press Enter to accept the default location The response will also ask you to set a passphrase; press Enter to not set a passphrase. To view the SSH key, run: $ cat /home/jovyan/.ssh/id_rsa.pub Copy the SSH key to your clipboard You then need to add the SSH key to GitHub; see the GitHub documentation for instructions. 6. Set up RStudio Note : Only follow this step if you want to use the Analytical Platform as an R-based user. RStudio supports Python, but we recommend using JupyterLab for Python instead. If you configured JupyterLab in the previous step, proceed to step 7. If you want to build and deploy applications on the Analytical Platform using R, you need to set up RStudio, the R-based IDE (Integrated Development Environment) the Analytical Platform provides. To set up RStudio, navigate to the Control Panel . Under RStudio , use the drop-down menu to select the version of RStudio you want to deploy. Unless you need to use a specific version, we recommend selecting the first option in the list. This will deploy RStudio on a virtual machine in your browser, with all of the required components it requires to run. Create and add RStudio SSH key to GitHub So you can access GitHub repositories from RStudio, you need an SSH key to connect the two. Do not try to use an existing SSH key; each tool you use requires a unique key. To create an SSH key in RStudio: Open RStudio from the Analytical Platform Control Panel Navigate to Tools>Global Options In the Options window, select Git/SVN in the navigation menu Select Create RSA key and then Create When the Information window appears, select Close Select View public key and copy the SSH key to your clipboard You then need to add the SSH key to GitHub; see the GitHub documentation for instructions. Now that you have completed this guide you are ready to begin using the Analytical Platform. See training for a list of different resources to help you begin using the platform. This page was last reviewed on 2 June 2023.\\n\\nIt needs to be reviewed again on 2 June 2024\\nby the page owner #analytical-platform-support . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 60}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'edf51f74fd1aae20a540e8836f60c341'}>,\n",
       " <Document: {'content': 'This page was set to be reviewed before 2 June 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nGet Started - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 61}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd90805b5d8f011b759ffafcd7210fc73'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 62}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Quickstart guide This guide provides the instructions to set up the main accounts and services you need to use the Analytical Platform. Once you complete it, you can: access the Analytical Platform Control Panel explore data on the Analytical Platform begin developing your application in either JupyterLab or RStudio contribute to the Analytical Platform User Guidance Before you begin To use this guide, you need the following: a Ministry of Justice-issued Office 365 account a Ministry of Justice-issued laptop you can install apps on a mobile device you can install apps on access to the Justice Digital workspace on Slack Complete this guide in order, following each step closely.\\nIf you encounter issues, preferably raise a ticket on GitHub issues A member of the Analytical Platform team will contact you. 1. Read Terms of Use For Analytical Platform best practice, you need to follow certain guidelines. Bookmark the following pages and ensure you follow them before you begin using the platform: Acceptable use policy : covers the way you should use the Analytical Platform and its associated tools and services Data and Analytical Services Directorate’s (DASD) coding standards : principles outlining how you should write and review code MoJ Analytical IT Tools Strategy : describes recommended ways of working on the Analytical Platform 2. Create Slack account We use Slack to communicate status updates, such as scheduled maintenance for the Analytical Platform. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 63}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1480ea2de9ddd118c3002bbbb5448161'}>,\n",
       " <Document: {'content': 'You can also use it to communicate with our support team and the Analytical Platform user community. There are two workspaces we recommend joining: Justice Digital and ASD . To join, while signed in to your work email, navigate to each workspace and request to join. Note that workspace moderators only consider users from the following email addresses: @justice.gsi.gov.uk @digital.justice.gov.uk @cjs.gsi.gov.uk @noms.gsi.gov.uk @legalaid.gsi.gov.uk @justice.gov.uk @judiciary.uk Note : You can only access the Justice Digital and ASD workspaces using DOM1 and MoJ Digital and Technology MacBooks. If you use Quantum, you will need to access Slack using a mobile device instead. Join Slack channels Conversations in Slack are organised into channels, which each have a specific topic. After getting access to Slack workspace you need to separately join channels. Channels can be: public (all users can join) or private (users can only join by invitation) or be created based on teams, projects, locations, tools and techniques among others. There are several public channels that are widely used and may be useful to join: the #analytical-platform-support channel is used for general discussion of the Analytical Platform – it is also monitored by the Analytical Platform team, who can help out with any technical queries or requests. Also used to request new or existing apps and app data sources the #ask-data-engineering channel is used for general discussion of data engineering and for getting in touch with the data engineering team with any technical queries or requests (such as airflow DAG reviews, database access, data discovery tool, etc). the #git , #r and #python channels can be used to get support from other users with any technical queries or questions – the # intro_r channel is aimed specifically at new users of R the #data_science channel is used for general discussion of data science tools and techniques the #general channel is used for any discussions that don’t fit anywhere else the #10sc channel can be used to get in touch with other people working at 10SC and to ask any questions about the building There are lots of other channels you can join or you can set up a new one if you think something is missing. You may also wish to set up private channels to discuss specific projects and can direct message individual users. 3. Create GitHub account Using your work email address (ending either justice.gov.uk or digital.justice.gov.uk ), sign up for a GitHub account . See the GitHub documentation for instructions. If you already have a GitHub account that you wish to use for work within the Ministry of Justice, you can use it provided you associate your work email address with the account in your user settings. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 64}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e983bb24dfe42b5372249e2ad43aebba'}>,\n",
       " <Document: {'content': 'Ensure that: your account uses the free plan subscription you choose a username that does not contain upper-case characters (for good practice) you set your username to associate with your Git commits for every repository; see the GitHub documentation for instructions you follow the Ministry of Justice’s best practice guidelines when setting your password you configure two-factor authentication (2FA) on your mobile device; we recommend using either Google Authenticator or Microsoft Authenticator for 2FA For more information on 2FA within the Ministry of Justice, see the MoJ’s Security Guidance 4. Access the Analytical Platform Once you have your GitHub account, there are two more steps to complete before you can access the Analytical Platform: joining the MoJ Analytical Services GitHub organisation and signing in to the Analytical Platform’s Control Panel. Join MoJ Analytical Services To request an invitation Analytical Platform GitHub Organisation, please head over to the #ask-operations-engineering slack channel. Ask to be added to the analytical-services GitHub Organisation by providing your GitHub username in the message. The Operations Engineering Team will invite you to join the MoJ Analytical Services GitHub organisation. Example 👋 Could I please be added to the Analytical Services GitHub Org, my github name is github user name thank you. If you do not receive a response within 24 hours, request access in either the #ask-operations-engineering Slack channel or email here , providing your GitHub username in your message. Sign in to the Control Panel The main entry point to the Analytical Platform is the Control Panel . From there, you configure core tools such as JupyterLab and RStudio. When you access the Control Panel first time, a prompt will appear, requiring you to configure 2FA using your mobile device. Note that while you use your GitHub account to access the Control Panel, this 2FA is separate from the one you use to log in to GitHub. You may need to disable browser extensions such as Dark Mode during the 2FA setup process. After you log in to the Control Panel for the first time, you can begin requesting access to data on the platform. 5. Set up JupyterLab Note : Only follow this step if you want to use the Analytical Platform as a Python-based user. If you want to use R instead, proceed to step 6 to configure RStudio instead. If you want to build and deploy applications on the Analytical Platform using Python, you need to set up JupyterLab, the Python-based IDE (Integrated Development Environment) on the Analytical Platform. To set up JupyterLab, navigate to the Control Panel . Under Jupyter Lab Data Science [Git extension] , use the drop-down menu to select the version of JupyterLab you want to deploy. Unless you need to use a specific version, we recommend selecting the first option in the list. This will deploy JupyterLab on a virtual machine in your browser, with all of the required components it requires to run. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 65}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2ed652cdd4e0b230203ca17cc8837035'}>,\n",
       " <Document: {'content': 'Create and add JupyterLab SSH key to GitHub To access GitHub repositories from JupyterLab, you need an SSH key to enable secure communication and authentication between the two services.\\nDo not try to use an existing SSH key; each tool you use requires a unique key. To create an SSH key in JupyterLab: Open JupyerLab from the Analytical Platform Control Panel Select the + icon in the file browser to open a new Launcher tab Navigate to the Other section and select Terminal Run the following command in your terminal, replacing your_email@example.com with the email address you used to sign up for GitHub: $ ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" The response will ask you to choose a directory to save the key in; press Enter to accept the default location The response will also ask you to set a passphrase; press Enter to not set a passphrase. To view the SSH key, run: $ cat /home/jovyan/.ssh/id_rsa.pub Copy the SSH key to your clipboard You then need to add the SSH key to GitHub; see the GitHub documentation for instructions. 6. Set up RStudio Note : Only follow this step if you want to use the Analytical Platform as an R-based user. RStudio supports Python, but we recommend using JupyterLab for Python instead. If you configured JupyterLab in the previous step, proceed to step 7. If you want to build and deploy applications on the Analytical Platform using R, you need to set up RStudio, the R-based IDE (Integrated Development Environment) the Analytical Platform provides. To set up RStudio, navigate to the Control Panel . Under RStudio , use the drop-down menu to select the version of RStudio you want to deploy. Unless you need to use a specific version, we recommend selecting the first option in the list. This will deploy RStudio on a virtual machine in your browser, with all of the required components it requires to run. Create and add RStudio SSH key to GitHub So you can access GitHub repositories from RStudio, you need an SSH key to connect the two. Do not try to use an existing SSH key; each tool you use requires a unique key. To create an SSH key in RStudio: Open RStudio from the Analytical Platform Control Panel Navigate to Tools>Global Options In the Options window, select Git/SVN in the navigation menu Select Create RSA key and then Create When the Information window appears, select Close Select View public key and copy the SSH key to your clipboard You then need to add the SSH key to GitHub; see the GitHub documentation for instructions. Now that you have completed this guide you are ready to begin using the Analytical Platform. See training for a list of different resources to help you begin using the platform. This page was last reviewed on 2 June 2023.\\n\\nIt needs to be reviewed again on 2 June 2024\\nby the page owner #analytical-platform-support . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 66}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'edf51f74fd1aae20a540e8836f60c341'}>,\n",
       " <Document: {'content': 'This page was set to be reviewed before 2 June 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 67}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c72742ee33b2ae2c749bdbde01f346d2'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 68}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data There are several different data sources on the Analytical Platform: Amazon S3 Curated databases Home directories Also see Demos on how to use our AWS Tools Getting the data you need The data you need may already exist on the Analytical Platform. The diagram below shows the process for getting access to it. More details on each step is contained in the guidance. If after following this you find out the data you need is not on the Platform, you may need to find it and upload it yourself. In that case please refer to the Information Governance section. FAQs There are also some FAQs on data access. This page was last reviewed on 10 November 2022.\\n\\nIt needs to be reviewed again on 10 November 2023\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 10 November 2023\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData FAQs - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 69}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e8cd47ca338ad00d4e67e7cfb8cb7667'}>,\n",
       " <Document: {'content': 'Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 70}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '28d347313af6c5d2b3caa9c411b897b0'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data FAQs Where do I find out what data is already on the Platform? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 71}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5fc34cf5566d8fad7a74532e211a6c60'}>,\n",
       " <Document: {'content': 'The data engineering team maintain a number of databases on the Platform (curated databases). The best way to find out about these is using the data discovery tool (access to the tool is now governed via GitHub; Analytical Platform users have access by default). The tool can be updated by anyone, so if you find something out about the data that isn’t already documented, please do add to it! In addition to this users can create their own S3 buckets which may have data useful to other teams, you may have to ask around to see if there is an existing dataset that may suit your needs. How do I gain access to existing data? Access to curated databases is granted via the database access repository . Have a read through the guidance on there. If you are finding the process a little tricky, please ask for help in #ask-data-engineering . A data engineer will happily guide you through things. If you are looking for access to a user created bucket, then the admin of that bucket should be able to grant you access. If you don’t know who the admin is, or they are not able to grant you access, then ask in the #analytical-platform-support Slack channel or via GitHub . If the bucket admin is unavailable, the Analytical Platform team will need to receive approval from your line manager before you can be given any access to the bucket. Where should I store my own data? Data should be stored in an s3 bucket. You can create a new s3 bucket in the control panel. Data can be uploaded manually via the AWS console (which can be accessed through the control panel) or you can write it from RStudio or JupyterLab. If your data contains anything that could be considered personal information, you must follow guidance from the data protection team which can be found on the intranet . How do I read/write data from an s3 bucket? Python/JupyterLab : You can read/write directly from s3 using pandas. However, to get the best representation of the column types in the resulting Pandas dataframe(s), you may wish to use mojap-arrow-pd-parser . R/Rstudio : Whilst initially the recommended package was botor ,\\nyou are also encouraged to try out Rs3tools a community\\nmaintained, R-native version of S3tools that removes some of the complexity around using Python. How do I query a database on the Platform? Databases on the AP use Amazon Athena which allow you to query data using SQL. You shouldn’t need to know about Athena in detail to query databases on the AP, but if you are interested you may wish to read more about it . There are three ways you can query data (there is more detail on all three of these in Data section of this guidance): The Amazon Athena workbench : If you log into the AWS console and click Services -> Athena, you’ll see the Athena workbench. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 72}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5e99bbe8c9a29a010e6ff58fc7cf464f'}>,\n",
       " <Document: {'content': 'This is good for testing your queries. If you get assumed-role/... is not authorized to perform: glue:GetDatabases , request database access . Python/JupyterLab : To run queries and/or read data into a pandas DataFrame, use pydbtools . More details are here . Remember to install the latest version! R/RStudio : There is currently no single recommended package for querying databases in R. There is dbtools which should work on the “old” platform. Rdbtools should work on the “new” platform, but is not officially supported so should be used with caution. I am running into memory issues, what should I do? You should do as much data manipulation in Athena/SQL as you possibly can, before reading into your analytical tool of choice. If you are using the curated databases consider filtering out unnecessary rows or columns, or aggregating results if appropriate. The function create_temp_table in pydbtools is particularly useful for helping with this. If the data is stored in your own s3 bucket, you may wish to create your own Athena database. How do I create my own Athena database? Athena workbench/JupyterLab/RStudio : You can run CREATE DATABASE and CREATE TABLE AS SELECT (CTAS) queries to create your own database and tables from data you have in s3. There are more details in this guidance or you can use what is provided by AWS . When running CTAS queries a key thing to remember is to specify the location (s3 bucket and path) of the data. There is a nice example here of setting up your own database. The tutorial is in python but the SQL can be ran from any tool on the AP. This page was last reviewed on 20 January 2022.\\n\\nIt needs to be reviewed again on 20 January 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 20 January 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData FAQs - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 73}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dedea87e65dcdc62cec12933c96dcdf1'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 74}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data FAQs Where do I find out what data is already on the Platform? The data engineering team maintain a number of databases on the Platform (curated databases). The best way to find out about these is using the data discovery tool (access to the tool is now governed via GitHub; Analytical Platform users have access by default). The tool can be updated by anyone, so if you find something out about the data that isn’t already documented, please do add to it! In addition to this users can create their own S3 buckets which may have data useful to other teams, you may have to ask around to see if there is an existing dataset that may suit your needs. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 75}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1bbcc9f153a0c45fca6ba052226929c'}>,\n",
       " <Document: {'content': 'How do I gain access to existing data? Access to curated databases is granted via the database access repository . Have a read through the guidance on there. If you are finding the process a little tricky, please ask for help in #ask-data-engineering . A data engineer will happily guide you through things. If you are looking for access to a user created bucket, then the admin of that bucket should be able to grant you access. If you don’t know who the admin is, or they are not able to grant you access, then ask in the #analytical-platform-support Slack channel or via GitHub . If the bucket admin is unavailable, the Analytical Platform team will need to receive approval from your line manager before you can be given any access to the bucket. Where should I store my own data? Data should be stored in an s3 bucket. You can create a new s3 bucket in the control panel. Data can be uploaded manually via the AWS console (which can be accessed through the control panel) or you can write it from RStudio or JupyterLab. If your data contains anything that could be considered personal information, you must follow guidance from the data protection team which can be found on the intranet . How do I read/write data from an s3 bucket? Python/JupyterLab : You can read/write directly from s3 using pandas. However, to get the best representation of the column types in the resulting Pandas dataframe(s), you may wish to use mojap-arrow-pd-parser . R/Rstudio : Whilst initially the recommended package was botor ,\\nyou are also encouraged to try out Rs3tools a community\\nmaintained, R-native version of S3tools that removes some of the complexity around using Python. How do I query a database on the Platform? Databases on the AP use Amazon Athena which allow you to query data using SQL. You shouldn’t need to know about Athena in detail to query databases on the AP, but if you are interested you may wish to read more about it . There are three ways you can query data (there is more detail on all three of these in Data section of this guidance): The Amazon Athena workbench : If you log into the AWS console and click Services -> Athena, you’ll see the Athena workbench. This is good for testing your queries. If you get assumed-role/... is not authorized to perform: glue:GetDatabases , request database access . Python/JupyterLab : To run queries and/or read data into a pandas DataFrame, use pydbtools . More details are here . Remember to install the latest version! R/RStudio : There is currently no single recommended package for querying databases in R. There is dbtools which should work on the “old” platform. Rdbtools should work on the “new” platform, but is not officially supported so should be used with caution. I am running into memory issues, what should I do? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 76}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9abf63db4ec02525453bf2b74358121a'}>,\n",
       " <Document: {'content': 'You should do as much data manipulation in Athena/SQL as you possibly can, before reading into your analytical tool of choice. If you are using the curated databases consider filtering out unnecessary rows or columns, or aggregating results if appropriate. The function create_temp_table in pydbtools is particularly useful for helping with this. If the data is stored in your own s3 bucket, you may wish to create your own Athena database. How do I create my own Athena database? Athena workbench/JupyterLab/RStudio : You can run CREATE DATABASE and CREATE TABLE AS SELECT (CTAS) queries to create your own database and tables from data you have in s3. There are more details in this guidance or you can use what is provided by AWS . When running CTAS queries a key thing to remember is to specify the location (s3 bucket and path) of the data. There is a nice example here of setting up your own database. The tutorial is in python but the SQL can be ran from any tool on the AP. This page was last reviewed on 20 January 2022.\\n\\nIt needs to be reviewed again on 20 January 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 20 January 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData FAQs - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 77}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '51d8eb2625d4ec79a526c3c86e7eb6aa'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 78}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data FAQs Where do I find out what data is already on the Platform? The data engineering team maintain a number of databases on the Platform (curated databases). The best way to find out about these is using the data discovery tool (access to the tool is now governed via GitHub; Analytical Platform users have access by default). The tool can be updated by anyone, so if you find something out about the data that isn’t already documented, please do add to it! In addition to this users can create their own S3 buckets which may have data useful to other teams, you may have to ask around to see if there is an existing dataset that may suit your needs. How do I gain access to existing data? Access to curated databases is granted via the database access repository . Have a read through the guidance on there. If you are finding the process a little tricky, please ask for help in #ask-data-engineering . A data engineer will happily guide you through things. If you are looking for access to a user created bucket, then the admin of that bucket should be able to grant you access. If you don’t know who the admin is, or they are not able to grant you access, then ask in the #analytical-platform-support Slack channel or via GitHub . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 79}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9054e9cf3612d4484a6a05f450078324'}>,\n",
       " <Document: {'content': 'If the bucket admin is unavailable, the Analytical Platform team will need to receive approval from your line manager before you can be given any access to the bucket. Where should I store my own data? Data should be stored in an s3 bucket. You can create a new s3 bucket in the control panel. Data can be uploaded manually via the AWS console (which can be accessed through the control panel) or you can write it from RStudio or JupyterLab. If your data contains anything that could be considered personal information, you must follow guidance from the data protection team which can be found on the intranet . How do I read/write data from an s3 bucket? Python/JupyterLab : You can read/write directly from s3 using pandas. However, to get the best representation of the column types in the resulting Pandas dataframe(s), you may wish to use mojap-arrow-pd-parser . R/Rstudio : Whilst initially the recommended package was botor ,\\nyou are also encouraged to try out Rs3tools a community\\nmaintained, R-native version of S3tools that removes some of the complexity around using Python. How do I query a database on the Platform? Databases on the AP use Amazon Athena which allow you to query data using SQL. You shouldn’t need to know about Athena in detail to query databases on the AP, but if you are interested you may wish to read more about it . There are three ways you can query data (there is more detail on all three of these in Data section of this guidance): The Amazon Athena workbench : If you log into the AWS console and click Services -> Athena, you’ll see the Athena workbench. This is good for testing your queries. If you get assumed-role/... is not authorized to perform: glue:GetDatabases , request database access . Python/JupyterLab : To run queries and/or read data into a pandas DataFrame, use pydbtools . More details are here . Remember to install the latest version! R/RStudio : There is currently no single recommended package for querying databases in R. There is dbtools which should work on the “old” platform. Rdbtools should work on the “new” platform, but is not officially supported so should be used with caution. I am running into memory issues, what should I do? You should do as much data manipulation in Athena/SQL as you possibly can, before reading into your analytical tool of choice. If you are using the curated databases consider filtering out unnecessary rows or columns, or aggregating results if appropriate. The function create_temp_table in pydbtools is particularly useful for helping with this. If the data is stored in your own s3 bucket, you may wish to create your own Athena database. How do I create my own Athena database? Athena workbench/JupyterLab/RStudio : You can run CREATE DATABASE and CREATE TABLE AS SELECT (CTAS) queries to create your own database and tables from data you have in s3. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 80}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '231c53b92bbba800ec17bf58d972f93'}>,\n",
       " <Document: {'content': 'There are more details in this guidance or you can use what is provided by AWS . When running CTAS queries a key thing to remember is to specify the location (s3 bucket and path) of the data. There is a nice example here of setting up your own database. The tutorial is in python but the SQL can be ran from any tool on the AP. This page was last reviewed on 20 January 2022.\\n\\nIt needs to be reviewed again on 20 January 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 20 January 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData FAQs - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 81}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6005420cbc15cb24c0b21bb7cf0fae43'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 82}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data FAQs Where do I find out what data is already on the Platform? The data engineering team maintain a number of databases on the Platform (curated databases). The best way to find out about these is using the data discovery tool (access to the tool is now governed via GitHub; Analytical Platform users have access by default). The tool can be updated by anyone, so if you find something out about the data that isn’t already documented, please do add to it! In addition to this users can create their own S3 buckets which may have data useful to other teams, you may have to ask around to see if there is an existing dataset that may suit your needs. How do I gain access to existing data? Access to curated databases is granted via the database access repository . Have a read through the guidance on there. If you are finding the process a little tricky, please ask for help in #ask-data-engineering . A data engineer will happily guide you through things. If you are looking for access to a user created bucket, then the admin of that bucket should be able to grant you access. If you don’t know who the admin is, or they are not able to grant you access, then ask in the #analytical-platform-support Slack channel or via GitHub . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 83}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9054e9cf3612d4484a6a05f450078324'}>,\n",
       " <Document: {'content': 'If the bucket admin is unavailable, the Analytical Platform team will need to receive approval from your line manager before you can be given any access to the bucket. Where should I store my own data? Data should be stored in an s3 bucket. You can create a new s3 bucket in the control panel. Data can be uploaded manually via the AWS console (which can be accessed through the control panel) or you can write it from RStudio or JupyterLab. If your data contains anything that could be considered personal information, you must follow guidance from the data protection team which can be found on the intranet . How do I read/write data from an s3 bucket? Python/JupyterLab : You can read/write directly from s3 using pandas. However, to get the best representation of the column types in the resulting Pandas dataframe(s), you may wish to use mojap-arrow-pd-parser . R/Rstudio : Whilst initially the recommended package was botor ,\\nyou are also encouraged to try out Rs3tools a community\\nmaintained, R-native version of S3tools that removes some of the complexity around using Python. How do I query a database on the Platform? Databases on the AP use Amazon Athena which allow you to query data using SQL. You shouldn’t need to know about Athena in detail to query databases on the AP, but if you are interested you may wish to read more about it . There are three ways you can query data (there is more detail on all three of these in Data section of this guidance): The Amazon Athena workbench : If you log into the AWS console and click Services -> Athena, you’ll see the Athena workbench. This is good for testing your queries. If you get assumed-role/... is not authorized to perform: glue:GetDatabases , request database access . Python/JupyterLab : To run queries and/or read data into a pandas DataFrame, use pydbtools . More details are here . Remember to install the latest version! R/RStudio : There is currently no single recommended package for querying databases in R. There is dbtools which should work on the “old” platform. Rdbtools should work on the “new” platform, but is not officially supported so should be used with caution. I am running into memory issues, what should I do? You should do as much data manipulation in Athena/SQL as you possibly can, before reading into your analytical tool of choice. If you are using the curated databases consider filtering out unnecessary rows or columns, or aggregating results if appropriate. The function create_temp_table in pydbtools is particularly useful for helping with this. If the data is stored in your own s3 bucket, you may wish to create your own Athena database. How do I create my own Athena database? Athena workbench/JupyterLab/RStudio : You can run CREATE DATABASE and CREATE TABLE AS SELECT (CTAS) queries to create your own database and tables from data you have in s3. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 84}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '231c53b92bbba800ec17bf58d972f93'}>,\n",
       " <Document: {'content': 'There are more details in this guidance or you can use what is provided by AWS . When running CTAS queries a key thing to remember is to specify the location (s3 bucket and path) of the data. There is a nice example here of setting up your own database. The tutorial is in python but the SQL can be ran from any tool on the AP. This page was last reviewed on 20 January 2022.\\n\\nIt needs to be reviewed again on 20 January 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 20 January 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData FAQs - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 85}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6005420cbc15cb24c0b21bb7cf0fae43'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 86}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data FAQs Where do I find out what data is already on the Platform? The data engineering team maintain a number of databases on the Platform (curated databases). The best way to find out about these is using the data discovery tool (access to the tool is now governed via GitHub; Analytical Platform users have access by default). The tool can be updated by anyone, so if you find something out about the data that isn’t already documented, please do add to it! In addition to this users can create their own S3 buckets which may have data useful to other teams, you may have to ask around to see if there is an existing dataset that may suit your needs. How do I gain access to existing data? Access to curated databases is granted via the database access repository . Have a read through the guidance on there. If you are finding the process a little tricky, please ask for help in #ask-data-engineering . A data engineer will happily guide you through things. If you are looking for access to a user created bucket, then the admin of that bucket should be able to grant you access. If you don’t know who the admin is, or they are not able to grant you access, then ask in the #analytical-platform-support Slack channel or via GitHub . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 87}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9054e9cf3612d4484a6a05f450078324'}>,\n",
       " <Document: {'content': 'If the bucket admin is unavailable, the Analytical Platform team will need to receive approval from your line manager before you can be given any access to the bucket. Where should I store my own data? Data should be stored in an s3 bucket. You can create a new s3 bucket in the control panel. Data can be uploaded manually via the AWS console (which can be accessed through the control panel) or you can write it from RStudio or JupyterLab. If your data contains anything that could be considered personal information, you must follow guidance from the data protection team which can be found on the intranet . How do I read/write data from an s3 bucket? Python/JupyterLab : You can read/write directly from s3 using pandas. However, to get the best representation of the column types in the resulting Pandas dataframe(s), you may wish to use mojap-arrow-pd-parser . R/Rstudio : Whilst initially the recommended package was botor ,\\nyou are also encouraged to try out Rs3tools a community\\nmaintained, R-native version of S3tools that removes some of the complexity around using Python. How do I query a database on the Platform? Databases on the AP use Amazon Athena which allow you to query data using SQL. You shouldn’t need to know about Athena in detail to query databases on the AP, but if you are interested you may wish to read more about it . There are three ways you can query data (there is more detail on all three of these in Data section of this guidance): The Amazon Athena workbench : If you log into the AWS console and click Services -> Athena, you’ll see the Athena workbench. This is good for testing your queries. If you get assumed-role/... is not authorized to perform: glue:GetDatabases , request database access . Python/JupyterLab : To run queries and/or read data into a pandas DataFrame, use pydbtools . More details are here . Remember to install the latest version! R/RStudio : There is currently no single recommended package for querying databases in R. There is dbtools which should work on the “old” platform. Rdbtools should work on the “new” platform, but is not officially supported so should be used with caution. I am running into memory issues, what should I do? You should do as much data manipulation in Athena/SQL as you possibly can, before reading into your analytical tool of choice. If you are using the curated databases consider filtering out unnecessary rows or columns, or aggregating results if appropriate. The function create_temp_table in pydbtools is particularly useful for helping with this. If the data is stored in your own s3 bucket, you may wish to create your own Athena database. How do I create my own Athena database? Athena workbench/JupyterLab/RStudio : You can run CREATE DATABASE and CREATE TABLE AS SELECT (CTAS) queries to create your own database and tables from data you have in s3. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 88}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '231c53b92bbba800ec17bf58d972f93'}>,\n",
       " <Document: {'content': 'There are more details in this guidance or you can use what is provided by AWS . When running CTAS queries a key thing to remember is to specify the location (s3 bucket and path) of the data. There is a nice example here of setting up your own database. The tutorial is in python but the SQL can be ran from any tool on the AP. This page was last reviewed on 20 January 2022.\\n\\nIt needs to be reviewed again on 20 January 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 20 January 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData FAQs - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 89}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6005420cbc15cb24c0b21bb7cf0fae43'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 90}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data FAQs Where do I find out what data is already on the Platform? The data engineering team maintain a number of databases on the Platform (curated databases). The best way to find out about these is using the data discovery tool (access to the tool is now governed via GitHub; Analytical Platform users have access by default). The tool can be updated by anyone, so if you find something out about the data that isn’t already documented, please do add to it! In addition to this users can create their own S3 buckets which may have data useful to other teams, you may have to ask around to see if there is an existing dataset that may suit your needs. How do I gain access to existing data? Access to curated databases is granted via the database access repository . Have a read through the guidance on there. If you are finding the process a little tricky, please ask for help in #ask-data-engineering . A data engineer will happily guide you through things. If you are looking for access to a user created bucket, then the admin of that bucket should be able to grant you access. If you don’t know who the admin is, or they are not able to grant you access, then ask in the #analytical-platform-support Slack channel or via GitHub . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 91}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9054e9cf3612d4484a6a05f450078324'}>,\n",
       " <Document: {'content': 'If the bucket admin is unavailable, the Analytical Platform team will need to receive approval from your line manager before you can be given any access to the bucket. Where should I store my own data? Data should be stored in an s3 bucket. You can create a new s3 bucket in the control panel. Data can be uploaded manually via the AWS console (which can be accessed through the control panel) or you can write it from RStudio or JupyterLab. If your data contains anything that could be considered personal information, you must follow guidance from the data protection team which can be found on the intranet . How do I read/write data from an s3 bucket? Python/JupyterLab : You can read/write directly from s3 using pandas. However, to get the best representation of the column types in the resulting Pandas dataframe(s), you may wish to use mojap-arrow-pd-parser . R/Rstudio : Whilst initially the recommended package was botor ,\\nyou are also encouraged to try out Rs3tools a community\\nmaintained, R-native version of S3tools that removes some of the complexity around using Python. How do I query a database on the Platform? Databases on the AP use Amazon Athena which allow you to query data using SQL. You shouldn’t need to know about Athena in detail to query databases on the AP, but if you are interested you may wish to read more about it . There are three ways you can query data (there is more detail on all three of these in Data section of this guidance): The Amazon Athena workbench : If you log into the AWS console and click Services -> Athena, you’ll see the Athena workbench. This is good for testing your queries. If you get assumed-role/... is not authorized to perform: glue:GetDatabases , request database access . Python/JupyterLab : To run queries and/or read data into a pandas DataFrame, use pydbtools . More details are here . Remember to install the latest version! R/RStudio : There is currently no single recommended package for querying databases in R. There is dbtools which should work on the “old” platform. Rdbtools should work on the “new” platform, but is not officially supported so should be used with caution. I am running into memory issues, what should I do? You should do as much data manipulation in Athena/SQL as you possibly can, before reading into your analytical tool of choice. If you are using the curated databases consider filtering out unnecessary rows or columns, or aggregating results if appropriate. The function create_temp_table in pydbtools is particularly useful for helping with this. If the data is stored in your own s3 bucket, you may wish to create your own Athena database. How do I create my own Athena database? Athena workbench/JupyterLab/RStudio : You can run CREATE DATABASE and CREATE TABLE AS SELECT (CTAS) queries to create your own database and tables from data you have in s3. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 92}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '231c53b92bbba800ec17bf58d972f93'}>,\n",
       " <Document: {'content': 'There are more details in this guidance or you can use what is provided by AWS . When running CTAS queries a key thing to remember is to specify the location (s3 bucket and path) of the data. There is a nice example here of setting up your own database. The tutorial is in python but the SQL can be ran from any tool on the AP. This page was last reviewed on 20 January 2022.\\n\\nIt needs to be reviewed again on 20 January 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 20 January 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData FAQs - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 93}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6005420cbc15cb24c0b21bb7cf0fae43'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 94}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data FAQs Where do I find out what data is already on the Platform? The data engineering team maintain a number of databases on the Platform (curated databases). The best way to find out about these is using the data discovery tool (access to the tool is now governed via GitHub; Analytical Platform users have access by default). The tool can be updated by anyone, so if you find something out about the data that isn’t already documented, please do add to it! In addition to this users can create their own S3 buckets which may have data useful to other teams, you may have to ask around to see if there is an existing dataset that may suit your needs. How do I gain access to existing data? Access to curated databases is granted via the database access repository . Have a read through the guidance on there. If you are finding the process a little tricky, please ask for help in #ask-data-engineering . A data engineer will happily guide you through things. If you are looking for access to a user created bucket, then the admin of that bucket should be able to grant you access. If you don’t know who the admin is, or they are not able to grant you access, then ask in the #analytical-platform-support Slack channel or via GitHub . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 95}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9054e9cf3612d4484a6a05f450078324'}>,\n",
       " <Document: {'content': 'If the bucket admin is unavailable, the Analytical Platform team will need to receive approval from your line manager before you can be given any access to the bucket. Where should I store my own data? Data should be stored in an s3 bucket. You can create a new s3 bucket in the control panel. Data can be uploaded manually via the AWS console (which can be accessed through the control panel) or you can write it from RStudio or JupyterLab. If your data contains anything that could be considered personal information, you must follow guidance from the data protection team which can be found on the intranet . How do I read/write data from an s3 bucket? Python/JupyterLab : You can read/write directly from s3 using pandas. However, to get the best representation of the column types in the resulting Pandas dataframe(s), you may wish to use mojap-arrow-pd-parser . R/Rstudio : Whilst initially the recommended package was botor ,\\nyou are also encouraged to try out Rs3tools a community\\nmaintained, R-native version of S3tools that removes some of the complexity around using Python. How do I query a database on the Platform? Databases on the AP use Amazon Athena which allow you to query data using SQL. You shouldn’t need to know about Athena in detail to query databases on the AP, but if you are interested you may wish to read more about it . There are three ways you can query data (there is more detail on all three of these in Data section of this guidance): The Amazon Athena workbench : If you log into the AWS console and click Services -> Athena, you’ll see the Athena workbench. This is good for testing your queries. If you get assumed-role/... is not authorized to perform: glue:GetDatabases , request database access . Python/JupyterLab : To run queries and/or read data into a pandas DataFrame, use pydbtools . More details are here . Remember to install the latest version! R/RStudio : There is currently no single recommended package for querying databases in R. There is dbtools which should work on the “old” platform. Rdbtools should work on the “new” platform, but is not officially supported so should be used with caution. I am running into memory issues, what should I do? You should do as much data manipulation in Athena/SQL as you possibly can, before reading into your analytical tool of choice. If you are using the curated databases consider filtering out unnecessary rows or columns, or aggregating results if appropriate. The function create_temp_table in pydbtools is particularly useful for helping with this. If the data is stored in your own s3 bucket, you may wish to create your own Athena database. How do I create my own Athena database? Athena workbench/JupyterLab/RStudio : You can run CREATE DATABASE and CREATE TABLE AS SELECT (CTAS) queries to create your own database and tables from data you have in s3. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 96}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '231c53b92bbba800ec17bf58d972f93'}>,\n",
       " <Document: {'content': 'There are more details in this guidance or you can use what is provided by AWS . When running CTAS queries a key thing to remember is to specify the location (s3 bucket and path) of the data. There is a nice example here of setting up your own database. The tutorial is in python but the SQL can be ran from any tool on the AP. This page was last reviewed on 20 January 2022.\\n\\nIt needs to be reviewed again on 20 January 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 20 January 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData FAQs - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 97}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6005420cbc15cb24c0b21bb7cf0fae43'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 98}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data FAQs Where do I find out what data is already on the Platform? The data engineering team maintain a number of databases on the Platform (curated databases). The best way to find out about these is using the data discovery tool (access to the tool is now governed via GitHub; Analytical Platform users have access by default). The tool can be updated by anyone, so if you find something out about the data that isn’t already documented, please do add to it! In addition to this users can create their own S3 buckets which may have data useful to other teams, you may have to ask around to see if there is an existing dataset that may suit your needs. How do I gain access to existing data? Access to curated databases is granted via the database access repository . Have a read through the guidance on there. If you are finding the process a little tricky, please ask for help in #ask-data-engineering . A data engineer will happily guide you through things. If you are looking for access to a user created bucket, then the admin of that bucket should be able to grant you access. If you don’t know who the admin is, or they are not able to grant you access, then ask in the #analytical-platform-support Slack channel or via GitHub . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 99}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9054e9cf3612d4484a6a05f450078324'}>,\n",
       " <Document: {'content': 'If the bucket admin is unavailable, the Analytical Platform team will need to receive approval from your line manager before you can be given any access to the bucket. Where should I store my own data? Data should be stored in an s3 bucket. You can create a new s3 bucket in the control panel. Data can be uploaded manually via the AWS console (which can be accessed through the control panel) or you can write it from RStudio or JupyterLab. If your data contains anything that could be considered personal information, you must follow guidance from the data protection team which can be found on the intranet . How do I read/write data from an s3 bucket? Python/JupyterLab : You can read/write directly from s3 using pandas. However, to get the best representation of the column types in the resulting Pandas dataframe(s), you may wish to use mojap-arrow-pd-parser . R/Rstudio : Whilst initially the recommended package was botor ,\\nyou are also encouraged to try out Rs3tools a community\\nmaintained, R-native version of S3tools that removes some of the complexity around using Python. How do I query a database on the Platform? Databases on the AP use Amazon Athena which allow you to query data using SQL. You shouldn’t need to know about Athena in detail to query databases on the AP, but if you are interested you may wish to read more about it . There are three ways you can query data (there is more detail on all three of these in Data section of this guidance): The Amazon Athena workbench : If you log into the AWS console and click Services -> Athena, you’ll see the Athena workbench. This is good for testing your queries. If you get assumed-role/... is not authorized to perform: glue:GetDatabases , request database access . Python/JupyterLab : To run queries and/or read data into a pandas DataFrame, use pydbtools . More details are here . Remember to install the latest version! R/RStudio : There is currently no single recommended package for querying databases in R. There is dbtools which should work on the “old” platform. Rdbtools should work on the “new” platform, but is not officially supported so should be used with caution. I am running into memory issues, what should I do? You should do as much data manipulation in Athena/SQL as you possibly can, before reading into your analytical tool of choice. If you are using the curated databases consider filtering out unnecessary rows or columns, or aggregating results if appropriate. The function create_temp_table in pydbtools is particularly useful for helping with this. If the data is stored in your own s3 bucket, you may wish to create your own Athena database. How do I create my own Athena database? Athena workbench/JupyterLab/RStudio : You can run CREATE DATABASE and CREATE TABLE AS SELECT (CTAS) queries to create your own database and tables from data you have in s3. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 100}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '231c53b92bbba800ec17bf58d972f93'}>,\n",
       " <Document: {'content': 'There are more details in this guidance or you can use what is provided by AWS . When running CTAS queries a key thing to remember is to specify the location (s3 bucket and path) of the data. There is a nice example here of setting up your own database. The tutorial is in python but the SQL can be ran from any tool on the AP. This page was last reviewed on 20 January 2022.\\n\\nIt needs to be reviewed again on 20 January 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 20 January 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nHome Directories - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 101}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '35055422f86ad5852f0faa95f0379612'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 102}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Home directories Home directories have two purposes: storing config files that are needed to allow you to use the AP storing local copies of code and analytical outputs while you are working on them Analysts are expected to keep their home directories clean and clear, and avoid storing large amounts of data there. Any files stored in home directories may be removed without warning by the AP admin team in order to maintain the functioning of the AP. In addition, we will periodically review home storage and may remove files from users with the largest storage without warning. This page was last reviewed on 18 May 2022.\\n\\nIt needs to be reviewed again on 18 November 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 18 November 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAmazon S3 - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 103}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ca526fa9c1c81e734fb1f4bcba99d8b6'}>,\n",
       " <Document: {'content': 'Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 104}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9ba4b74f4fa634ec10c04b4f7c578f23'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Amazon S3 What is Amazon S3? Amazon S3 is a web-based cloud storage platform. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 105}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd8987c7066b75bd938d680b487085194'}>,\n",
       " <Document: {'content': 'It is one of the primary file storage locations on the Analytical Platform, alongside individual users’ home directories. You should use your home directory to store working copies of code and analytical outputs. Where possible, you should store all data and final analytical outputs in Amazon S3, and final code in GitHub to facilitate collaboration. Data stored in Amazon S3 can be seamlessly integrated with other AWS services such as Amazon Athena and Amazon Glue. Working with Amazon S3 buckets Types of buckets Amazon S3 buckets are separated into two categories on the Analytical Platform. Warehouse data sources Warehouse data sources are used to store data that is accessed by code you run yourself, for example, in RStudio or JupyterLab. You can create warehouse data sources yourself and can provide access to other users you need to collaborate with. Webapp data sources Webapp data sources are used to store data that is accessed by code run by the Analytical Platform, for example by deployed apps or by Airflow pipelines. You cannot create webapp data sources yourself – you must ask the Analytical Platform team to create one on your behalf. If you request that a webapp data source is created when setting up a new app, the app will automatically be given read-only access. You will also be given admin access to the bucket and can provide access to other users you need to collaborate with. The Data Engineering team also manage some buckets that are not shown in the control panel and that are not available to standard users. These buckets are used to store incoming raw data, which may be processed or fed into curated data pipelines. For more information, contact the Data Engineering team on the #ask-data-engineering Slack channel. You can view the data sources you have access to in the control panel. Create a new warehouse data source You can only create new warehouse data sources in the Analytical Platform control panel. You cannot create new buckets directly in the Amazon S3 console. To create a new warehouse data source: Go to the Analytical Platform [control panel](https://controlpanel.services.analytical-platform.service.justice.gov.uk/. Select the Warehouse data tab. Select Create new warehouse data source . Enter a name for the warehouse data source – this must be prefixed with ‘alpha-’. Select Create data source . When you create a new warehouse data source, only you will initially have access. As an admin of the data source, you will be able to add and remove other users from the data access group as required. Further information on managing data access groups can be found here . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 106}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8a1096864d6074dc423c7bcad27ee480'}>,\n",
       " <Document: {'content': 'Data access levels Every bucket has three data access levels: Read only Read/write Admin – this provides read/write access and allows the user to add and remove other users from the bucket’s data access group Path specific access As well as choosing an access level, you can also restrict a user’s access to specific paths in a bucket by entering each path on a new line in the ‘Paths’ textarea field when adding the user to a data access group, taking care not to leave an empty new line after the last path. For example: /folder-one\\n/folder-two This would give the user access to only /folder-one and /folder-two in the bucket and nothing else. If you leave this field blank, the user will be able to access everything in the bucket. Request access to a bucket To gain access to a bucket (warehouse data source or webapp data source), you must be added to the relevant data access group. If you know an admin of the bucket you require access to, you should ask them to add you to the data access group. If you do not know any of the admins of the bucket you require access to, you can find a list of the GitHub usernames of all bucket admins on the Warehouse Data page of Control Panel (scroll down the page), or contact the Analytical Platform team via Slack , GitHub or email ( analytical_platform@digital.justice.gov.uk ). If all bucket admins are unavailable (e.g. have left the MoJ), the  Analytical Platform team will be able to grant you access to the datasource if the request is approved by your line manager. When requesting access to a bucket, you should specify the name of the bucket and the level of access you require. You should only request access to data that you have a genuine business need to access and should only request the lowest level of access required for you to complete your work. You may be required to demonstrate the business need for you to access a bucket if requested by a bucket admin or an information asset owner (IAO). Manage access to a bucket Bucket admins can manage access to warehouse data sources and webapp data sources in the Analytical Platform control panel . You cannot manage access to buckets directly in the Amazon S3 console. To manage access to a data source: Go to the Analytical Platform control panel . Select the Warehouse data tab or the Webapp data tab, as relevant. Select the name of the data source you want to manage. To add a new user to the data access group: Type the user’s GitHub username into the input field labelled Grant access to this data to other users . Select the user from the drop-down list. Select the required data access level. Either leave the ‘Paths’ field blank or enter a list of paths to provide restricted access, as described in the section above. Select Grant access . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 107}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eb0472237d2997672d2df631ac298f2d'}>,\n",
       " <Document: {'content': 'To edit the access level of a user: Select Edit access level next to the name of the user. Select required data access level. Either leave the ‘Paths’ field blank or enter a list of paths to provide restricted access, as described in the section above. Select Save . To remove a user from the data access group: Select Edit access level next to the name of the user. Select Revoke access . Interacting with Amazon S3 via the Analytical Platform You can upload files to Amazon S3 from your local computer or download files from Amazon S3 to your local computer using below tools Amazon S3 console RStudio JupyterLab When uploading files to Amazon S3, you should ensure that you follow all necessary information governance procedures. In particular, you must complete a data movement form when moving any data onto the Analytical Platform. Downloading the data from Amazon S3 to your local machine is also considered as data movement and therefore needs to be managed as such in accordance with the necessary information governance procedures, particularly for Personal Identifiable Information. Your options This section presents a comparison of the various tools available for accessing Amazon S3 on each platform; further details on setup and usage are given below. AWS Console The AWS S3 Console is a browser-based GUI tool. You can use the Amazon S3 console to view an overview of an object. The object overview in the console provides all the essential information for an object in one place. For further details, see the guide further down the page. RStudio There are two main options for interacting with files stored in AWS S3 buckets on the Analytical Platform via RStudio: Rs3tools and botor . Either of these options works well on the Analytical Platform, and you should pick whichever best suits your use-case. Rs3tools is an R-native community-developed MoJ project which consists of a set of helper tools to access Amazon S3 buckets on the Analytical Platform. The installation process for botor takes longer as it requires a Python environment ( botor is a wrapper around Python’s boto3 library). However, it contains a larger range of functionality. Generally, we recommend using Rs3tools unless there is a specific need for the additional functionality in botor . You may also see mentions of another tool, s3tools . s3tools is now deprecated and has been replaced by Rs3tools .More information is available in this ADR Record Most of the original functionality is available via Rs3tools , so this is a good replacement if you are looking to update older code that relied on the s3tools package.If you need the additional functionality available in botor , a guide to migration is available here . In addition, an RStudio plugin, s3browser is available if you only want to browse your files. For further details, see the sections below on Rs3tools , botor and s3browser . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 108}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '900c0c06885545a7f1feb8d8358272b7'}>,\n",
       " <Document: {'content': 'JupyterLab The main options for interacting with files stored in AWS S3 buckets on the Analytical Platform via JupyterLab are : Reading files : pandas , mojap-arrow-pd-parser Downloading / Uploading files : boto3 Installation and usage Amazon S3 Console You can use the Amazon S3 Console to upload/download files from/to your local computer (for example, personal or shared storage on DOM1 or Quantum) only. To upload files using the Amazon S3 Console: Log in to the AWS Management Console using your Analytical Platform account. Select Services from the menu bar. Select S3 from the drop down menu. Select the bucket and folder you want to upload files to. Select Upload . Select Add files or drag and drop the files you want to upload. Select Upload . Downloading a file using the Amazon S3 Console follows a similar process: Follow steps 1-3 from the list above. Navigate to the bucket and select the file you want to download. Select Download or Download as as appropriate. You can also directly navigate to a bucket in the AWS S3 Console by selecting Open on AWS in the Analytical Platform Control Panel. RStudio Rs3tools To install Rs3tools follow the guidance on their homepage . To upload files using Rs3Tools Writing files to S3 Rs3tools :: write_file_to_s3 ( \"my_downloaded_file.csv\" , \"alpha-everyone/delete/my_downloaded_file.csv\" , overwrite = TRUE ) # if file already exists, you recieve an error. overwrite=True enables it to overwrite the file Writing a dataframe to S3 in csv format Rs3tools :: write_df_to_csv_in_s3 ( dataframe_name , \"alpha-everyone/delete/iris.csv\" , overwrite = TRUE ) Downloading a file from S3 using Rs3Tools Rs3tools :: download_file_from_s3 ( \"alpha-everyone/s3tools_tests/iris_base.csv\" , \"my_downloaded_file.csv\" , overwrite = TRUE ) botor You will need to use the package manager renv to install botor .\\nTo get started with renv , see our guidance on the RStudio package management page . Then, go ahead with the botor installation (this is slightly different from the guidance on botor ‘s website as we use the renv package manager): renv :: use_python () ## at the prompt, choose to use python3 renv :: install ( \\'reticulate\\' ) Restart the session (Ctrl+Alt+F10 on a Windows machine). And then: reticulate :: py_install ( \\'boto3\\' ) renv :: install ( \\'botor\\' ) botor contains two functions for downloading or reading files from Amazon S3: s3_upload_file s3_write For example, to write a dataframe to csv, run the following code: library ( botor ) s3_write ( your_df , write.csv , \"s3://your_bucket/your_key.csv\" ) To read files, use one of the following: s3_download_file s3_read And use as follows: library ( botor ) your_df <- s3_read ( read.csv , \"s3://your_bucket/your_key.csv\" ) You can find out more about how to use these and other functions in the Migrating to botor appendix, the botor documentation or by using the help operator in RStudio (for example, ?botor::s3_write ). ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 109}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c7f0c2b5aa9fc3715f0204c060f2ebf9'}>,\n",
       " <Document: {'content': 's3browser You can install s3browser by running the following code: install.packages(\\'remotes\\')\\nlibrary(remotes)\\nremotes::install_github(\\'moj-analytical-services/s3browser\\') To open the browser, run: s3browser::file_explorer_s3() You can find out more about how to use s3browser on GitHub . JupyterLab You can read/write directly from s3 using pandas . However, to get the best representation of the column types in the resulting Pandas dataframe(s), you may wish to use mojap-arrow-pd-parser . mojap-arrow-pd-parser mojap-arrow-pd-parser provides easy csv, jsonl and parquet file readers. To install in terminal: pip install arrow-pd-parser To read/write a csv file from s3: from arrow_pd_parser import reader , writer # Specifying the reader Both reader statements are equivalent and call the same readers under the hood df1 = reader . read ( \"s3://bucket_name/data/all_types.csv\" , file_format = \"csv\" ) df2 = reader . csv . read ( \"s3://bucket_name/data/all_types.csv\" ) # You can also pass the reader args to the reader as kwargs df3 = reader . csv . read ( \"s3://bucket_name/data/all_types.csv\" , nrows = 2 ) # The writer API has the same functionality writer . write ( df1 , file_format = \"parquet\" ) writer . parquet . write ( df1 ) mojap-arrow-pd-parser infers the file type from the extension, so for example reader.read(\"s3://bucket_name/file.parquet\") would read a parquet file without need for specifying the file type. The package also has a lot of other functionality including specifying data types when reading (or writing). More details can be found in the package README . pandas You can use any of the pandas read functions (for example, read_csv or read_json ) to download data directly from Amazon S3. This requires that you have installed the pandas and s3fs packages. To install these, run the following code in a terminal: python -m pip install pandas s3fs As an example, to read a CSV, you should run the following code: import pandas as pd\\npd.read_csv(\\'s3://bucket_name/key\\') Here, you should substitute bucket_name with the name of the bucket and key with the path of the object in Amazon S3. boto3 You can also download or read objects using the boto3 package. You can install boto3 by running the following code in a terminal: pip install boto3 To download a file from Amazon S3, you should use the following code: import boto3 s3 = boto3 . resource ( \\'s3\\' ) s3 . Object ( \\'bucket_name\\' , \\'key\\' ). download_file ( \\'local_path\\' ) If you receive an ImportError , try restarting your kernel, so that Python recognises your boto3 installation. Here, you should substitute \\'bucket_name\\' with the name of the bucket, \\'key\\' with the path of the object in Amazon S3 and local_path with the local path where you would like to save the downloaded file. To upload a file to Amazon S3, you should use the following code: #Upload sample contents to s3 s3 = boto3 . client ( \\'s3\\' ) data = b \\'This is the content of the file uploaded from python boto3\\' file_name = \\'your_file_name.txt\\' response = s3 . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 110}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b05c65c174e7a9e6cf0795ccc5ff9f9a'}>,\n",
       " <Document: {'content': \"put_object ( Bucket = your_bucket_name , Body = data , Key = file_name ) print ( 'AWS response code for uploading file is ' + str ( response [ 'ResponseMetadata' ][ 'HTTPStatusCode' ])) You can find more information in the package documentation . AWS Data Wrangler You can also use AWS Wrangler to work with data stored in Amazon S3. More information can be found in the product documentation . This page was last reviewed on 22 June 2022.\\n\\nIt needs to be reviewed again on 22 September 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 22 September 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAmazon S3 - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 111}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '54c5dde7d6683d406660ae820b1c6935'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 112}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Amazon S3 What is Amazon S3? Amazon S3 is a web-based cloud storage platform. It is one of the primary file storage locations on the Analytical Platform, alongside individual users’ home directories. You should use your home directory to store working copies of code and analytical outputs. Where possible, you should store all data and final analytical outputs in Amazon S3, and final code in GitHub to facilitate collaboration. Data stored in Amazon S3 can be seamlessly integrated with other AWS services such as Amazon Athena and Amazon Glue. Working with Amazon S3 buckets Types of buckets Amazon S3 buckets are separated into two categories on the Analytical Platform. Warehouse data sources Warehouse data sources are used to store data that is accessed by code you run yourself, for example, in RStudio or JupyterLab. You can create warehouse data sources yourself and can provide access to other users you need to collaborate with. Webapp data sources Webapp data sources are used to store data that is accessed by code run by the Analytical Platform, for example by deployed apps or by Airflow pipelines. You cannot create webapp data sources yourself – you must ask the Analytical Platform team to create one on your behalf. If you request that a webapp data source is created when setting up a new app, the app will automatically be given read-only access. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 113}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '83dcd8bacbfcdfc4bceecafd9bde7a44'}>,\n",
       " <Document: {'content': 'You will also be given admin access to the bucket and can provide access to other users you need to collaborate with. The Data Engineering team also manage some buckets that are not shown in the control panel and that are not available to standard users. These buckets are used to store incoming raw data, which may be processed or fed into curated data pipelines. For more information, contact the Data Engineering team on the #ask-data-engineering Slack channel. You can view the data sources you have access to in the control panel. Create a new warehouse data source You can only create new warehouse data sources in the Analytical Platform control panel. You cannot create new buckets directly in the Amazon S3 console. To create a new warehouse data source: Go to the Analytical Platform [control panel](https://controlpanel.services.analytical-platform.service.justice.gov.uk/. Select the Warehouse data tab. Select Create new warehouse data source . Enter a name for the warehouse data source – this must be prefixed with ‘alpha-’. Select Create data source . When you create a new warehouse data source, only you will initially have access. As an admin of the data source, you will be able to add and remove other users from the data access group as required. Further information on managing data access groups can be found here . Data access levels Every bucket has three data access levels: Read only Read/write Admin – this provides read/write access and allows the user to add and remove other users from the bucket’s data access group Path specific access As well as choosing an access level, you can also restrict a user’s access to specific paths in a bucket by entering each path on a new line in the ‘Paths’ textarea field when adding the user to a data access group, taking care not to leave an empty new line after the last path. For example: /folder-one\\n/folder-two This would give the user access to only /folder-one and /folder-two in the bucket and nothing else. If you leave this field blank, the user will be able to access everything in the bucket. Request access to a bucket To gain access to a bucket (warehouse data source or webapp data source), you must be added to the relevant data access group. If you know an admin of the bucket you require access to, you should ask them to add you to the data access group. If you do not know any of the admins of the bucket you require access to, you can find a list of the GitHub usernames of all bucket admins on the Warehouse Data page of Control Panel (scroll down the page), or contact the Analytical Platform team via Slack , GitHub or email ( analytical_platform@digital.justice.gov.uk ). If all bucket admins are unavailable (e.g. have left the MoJ), the  Analytical Platform team will be able to grant you access to the datasource if the request is approved by your line manager. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 114}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a4292e412a26ee26bcda7091d52946de'}>,\n",
       " <Document: {'content': 'When requesting access to a bucket, you should specify the name of the bucket and the level of access you require. You should only request access to data that you have a genuine business need to access and should only request the lowest level of access required for you to complete your work. You may be required to demonstrate the business need for you to access a bucket if requested by a bucket admin or an information asset owner (IAO). Manage access to a bucket Bucket admins can manage access to warehouse data sources and webapp data sources in the Analytical Platform control panel . You cannot manage access to buckets directly in the Amazon S3 console. To manage access to a data source: Go to the Analytical Platform control panel . Select the Warehouse data tab or the Webapp data tab, as relevant. Select the name of the data source you want to manage. To add a new user to the data access group: Type the user’s GitHub username into the input field labelled Grant access to this data to other users . Select the user from the drop-down list. Select the required data access level. Either leave the ‘Paths’ field blank or enter a list of paths to provide restricted access, as described in the section above. Select Grant access . To edit the access level of a user: Select Edit access level next to the name of the user. Select required data access level. Either leave the ‘Paths’ field blank or enter a list of paths to provide restricted access, as described in the section above. Select Save . To remove a user from the data access group: Select Edit access level next to the name of the user. Select Revoke access . Interacting with Amazon S3 via the Analytical Platform You can upload files to Amazon S3 from your local computer or download files from Amazon S3 to your local computer using below tools Amazon S3 console RStudio JupyterLab When uploading files to Amazon S3, you should ensure that you follow all necessary information governance procedures. In particular, you must complete a data movement form when moving any data onto the Analytical Platform. Downloading the data from Amazon S3 to your local machine is also considered as data movement and therefore needs to be managed as such in accordance with the necessary information governance procedures, particularly for Personal Identifiable Information. Your options This section presents a comparison of the various tools available for accessing Amazon S3 on each platform; further details on setup and usage are given below. AWS Console The AWS S3 Console is a browser-based GUI tool. You can use the Amazon S3 console to view an overview of an object. The object overview in the console provides all the essential information for an object in one place. For further details, see the guide further down the page. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 115}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '117f2b884434c5a0f9a59e1729c5da14'}>,\n",
       " <Document: {'content': 'RStudio There are two main options for interacting with files stored in AWS S3 buckets on the Analytical Platform via RStudio: Rs3tools and botor . Either of these options works well on the Analytical Platform, and you should pick whichever best suits your use-case. Rs3tools is an R-native community-developed MoJ project which consists of a set of helper tools to access Amazon S3 buckets on the Analytical Platform. The installation process for botor takes longer as it requires a Python environment ( botor is a wrapper around Python’s boto3 library). However, it contains a larger range of functionality. Generally, we recommend using Rs3tools unless there is a specific need for the additional functionality in botor . You may also see mentions of another tool, s3tools . s3tools is now deprecated and has been replaced by Rs3tools .More information is available in this ADR Record Most of the original functionality is available via Rs3tools , so this is a good replacement if you are looking to update older code that relied on the s3tools package.If you need the additional functionality available in botor , a guide to migration is available here . In addition, an RStudio plugin, s3browser is available if you only want to browse your files. For further details, see the sections below on Rs3tools , botor and s3browser . JupyterLab The main options for interacting with files stored in AWS S3 buckets on the Analytical Platform via JupyterLab are : Reading files : pandas , mojap-arrow-pd-parser Downloading / Uploading files : boto3 Installation and usage Amazon S3 Console You can use the Amazon S3 Console to upload/download files from/to your local computer (for example, personal or shared storage on DOM1 or Quantum) only. To upload files using the Amazon S3 Console: Log in to the AWS Management Console using your Analytical Platform account. Select Services from the menu bar. Select S3 from the drop down menu. Select the bucket and folder you want to upload files to. Select Upload . Select Add files or drag and drop the files you want to upload. Select Upload . Downloading a file using the Amazon S3 Console follows a similar process: Follow steps 1-3 from the list above. Navigate to the bucket and select the file you want to download. Select Download or Download as as appropriate. You can also directly navigate to a bucket in the AWS S3 Console by selecting Open on AWS in the Analytical Platform Control Panel. RStudio Rs3tools To install Rs3tools follow the guidance on their homepage . To upload files using Rs3Tools Writing files to S3 Rs3tools :: write_file_to_s3 ( \"my_downloaded_file.csv\" , \"alpha-everyone/delete/my_downloaded_file.csv\" , overwrite = TRUE ) # if file already exists, you recieve an error. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 116}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '540ada5f0dcfc569fdc2f3d58f6ef711'}>,\n",
       " <Document: {'content': 'overwrite=True enables it to overwrite the file Writing a dataframe to S3 in csv format Rs3tools :: write_df_to_csv_in_s3 ( dataframe_name , \"alpha-everyone/delete/iris.csv\" , overwrite = TRUE ) Downloading a file from S3 using Rs3Tools Rs3tools :: download_file_from_s3 ( \"alpha-everyone/s3tools_tests/iris_base.csv\" , \"my_downloaded_file.csv\" , overwrite = TRUE ) botor You will need to use the package manager renv to install botor .\\nTo get started with renv , see our guidance on the RStudio package management page . Then, go ahead with the botor installation (this is slightly different from the guidance on botor ‘s website as we use the renv package manager): renv :: use_python () ## at the prompt, choose to use python3 renv :: install ( \\'reticulate\\' ) Restart the session (Ctrl+Alt+F10 on a Windows machine). And then: reticulate :: py_install ( \\'boto3\\' ) renv :: install ( \\'botor\\' ) botor contains two functions for downloading or reading files from Amazon S3: s3_upload_file s3_write For example, to write a dataframe to csv, run the following code: library ( botor ) s3_write ( your_df , write.csv , \"s3://your_bucket/your_key.csv\" ) To read files, use one of the following: s3_download_file s3_read And use as follows: library ( botor ) your_df <- s3_read ( read.csv , \"s3://your_bucket/your_key.csv\" ) You can find out more about how to use these and other functions in the Migrating to botor appendix, the botor documentation or by using the help operator in RStudio (for example, ?botor::s3_write ). s3browser You can install s3browser by running the following code: install.packages(\\'remotes\\')\\nlibrary(remotes)\\nremotes::install_github(\\'moj-analytical-services/s3browser\\') To open the browser, run: s3browser::file_explorer_s3() You can find out more about how to use s3browser on GitHub . JupyterLab You can read/write directly from s3 using pandas . However, to get the best representation of the column types in the resulting Pandas dataframe(s), you may wish to use mojap-arrow-pd-parser . mojap-arrow-pd-parser mojap-arrow-pd-parser provides easy csv, jsonl and parquet file readers. To install in terminal: pip install arrow-pd-parser To read/write a csv file from s3: from arrow_pd_parser import reader , writer # Specifying the reader Both reader statements are equivalent and call the same readers under the hood df1 = reader . read ( \"s3://bucket_name/data/all_types.csv\" , file_format = \"csv\" ) df2 = reader . csv . read ( \"s3://bucket_name/data/all_types.csv\" ) # You can also pass the reader args to the reader as kwargs df3 = reader . csv . read ( \"s3://bucket_name/data/all_types.csv\" , nrows = 2 ) # The writer API has the same functionality writer . write ( df1 , file_format = \"parquet\" ) writer . parquet . write ( df1 ) mojap-arrow-pd-parser infers the file type from the extension, so for example reader.read(\"s3://bucket_name/file.parquet\") would read a parquet file without need for specifying the file type. The package also has a lot of other functionality including specifying data types when reading (or writing). More details can be found in the package README . pandas You can use any of the pandas read functions (for example, read_csv or read_json ) to download data directly from Amazon S3. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 117}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ec311e0fc3e781fde56b16195d9d266a'}>,\n",
       " <Document: {'content': \"This requires that you have installed the pandas and s3fs packages. To install these, run the following code in a terminal: python -m pip install pandas s3fs As an example, to read a CSV, you should run the following code: import pandas as pd\\npd.read_csv('s3://bucket_name/key') Here, you should substitute bucket_name with the name of the bucket and key with the path of the object in Amazon S3. boto3 You can also download or read objects using the boto3 package. You can install boto3 by running the following code in a terminal: pip install boto3 To download a file from Amazon S3, you should use the following code: import boto3 s3 = boto3 . resource ( 's3' ) s3 . Object ( 'bucket_name' , 'key' ). download_file ( 'local_path' ) If you receive an ImportError , try restarting your kernel, so that Python recognises your boto3 installation. Here, you should substitute 'bucket_name' with the name of the bucket, 'key' with the path of the object in Amazon S3 and local_path with the local path where you would like to save the downloaded file. To upload a file to Amazon S3, you should use the following code: #Upload sample contents to s3 s3 = boto3 . client ( 's3' ) data = b 'This is the content of the file uploaded from python boto3' file_name = 'your_file_name.txt' response = s3 . put_object ( Bucket = your_bucket_name , Body = data , Key = file_name ) print ( 'AWS response code for uploading file is ' + str ( response [ 'ResponseMetadata' ][ 'HTTPStatusCode' ])) You can find more information in the package documentation . AWS Data Wrangler You can also use AWS Wrangler to work with data stored in Amazon S3. More information can be found in the product documentation . This page was last reviewed on 22 June 2022.\\n\\nIt needs to be reviewed again on 22 September 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 22 September 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAmazon S3 - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 118}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e91ff9ef28ca4a87f21fff6727314cf6'}>,\n",
       " <Document: {'content': 'I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 119}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '165700989970d698eee8f1a9a15a7bc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Amazon S3 What is Amazon S3? Amazon S3 is a web-based cloud storage platform. It is one of the primary file storage locations on the Analytical Platform, alongside individual users’ home directories. You should use your home directory to store working copies of code and analytical outputs. Where possible, you should store all data and final analytical outputs in Amazon S3, and final code in GitHub to facilitate collaboration. Data stored in Amazon S3 can be seamlessly integrated with other AWS services such as Amazon Athena and Amazon Glue. Working with Amazon S3 buckets Types of buckets Amazon S3 buckets are separated into two categories on the Analytical Platform. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 120}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2d7b775a3ffa93798e1b6b8c0daa4e36'}>,\n",
       " <Document: {'content': 'Warehouse data sources Warehouse data sources are used to store data that is accessed by code you run yourself, for example, in RStudio or JupyterLab. You can create warehouse data sources yourself and can provide access to other users you need to collaborate with. Webapp data sources Webapp data sources are used to store data that is accessed by code run by the Analytical Platform, for example by deployed apps or by Airflow pipelines. You cannot create webapp data sources yourself – you must ask the Analytical Platform team to create one on your behalf. If you request that a webapp data source is created when setting up a new app, the app will automatically be given read-only access. You will also be given admin access to the bucket and can provide access to other users you need to collaborate with. The Data Engineering team also manage some buckets that are not shown in the control panel and that are not available to standard users. These buckets are used to store incoming raw data, which may be processed or fed into curated data pipelines. For more information, contact the Data Engineering team on the #ask-data-engineering Slack channel. You can view the data sources you have access to in the control panel. Create a new warehouse data source You can only create new warehouse data sources in the Analytical Platform control panel. You cannot create new buckets directly in the Amazon S3 console. To create a new warehouse data source: Go to the Analytical Platform [control panel](https://controlpanel.services.analytical-platform.service.justice.gov.uk/. Select the Warehouse data tab. Select Create new warehouse data source . Enter a name for the warehouse data source – this must be prefixed with ‘alpha-’. Select Create data source . When you create a new warehouse data source, only you will initially have access. As an admin of the data source, you will be able to add and remove other users from the data access group as required. Further information on managing data access groups can be found here . Data access levels Every bucket has three data access levels: Read only Read/write Admin – this provides read/write access and allows the user to add and remove other users from the bucket’s data access group Path specific access As well as choosing an access level, you can also restrict a user’s access to specific paths in a bucket by entering each path on a new line in the ‘Paths’ textarea field when adding the user to a data access group, taking care not to leave an empty new line after the last path. For example: /folder-one\\n/folder-two This would give the user access to only /folder-one and /folder-two in the bucket and nothing else. If you leave this field blank, the user will be able to access everything in the bucket. Request access to a bucket To gain access to a bucket (warehouse data source or webapp data source), you must be added to the relevant data access group. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 121}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4950c361b4de74192756a7a093703597'}>,\n",
       " <Document: {'content': 'If you know an admin of the bucket you require access to, you should ask them to add you to the data access group. If you do not know any of the admins of the bucket you require access to, you can find a list of the GitHub usernames of all bucket admins on the Warehouse Data page of Control Panel (scroll down the page), or contact the Analytical Platform team via Slack , GitHub or email ( analytical_platform@digital.justice.gov.uk ). If all bucket admins are unavailable (e.g. have left the MoJ), the  Analytical Platform team will be able to grant you access to the datasource if the request is approved by your line manager. When requesting access to a bucket, you should specify the name of the bucket and the level of access you require. You should only request access to data that you have a genuine business need to access and should only request the lowest level of access required for you to complete your work. You may be required to demonstrate the business need for you to access a bucket if requested by a bucket admin or an information asset owner (IAO). Manage access to a bucket Bucket admins can manage access to warehouse data sources and webapp data sources in the Analytical Platform control panel . You cannot manage access to buckets directly in the Amazon S3 console. To manage access to a data source: Go to the Analytical Platform control panel . Select the Warehouse data tab or the Webapp data tab, as relevant. Select the name of the data source you want to manage. To add a new user to the data access group: Type the user’s GitHub username into the input field labelled Grant access to this data to other users . Select the user from the drop-down list. Select the required data access level. Either leave the ‘Paths’ field blank or enter a list of paths to provide restricted access, as described in the section above. Select Grant access . To edit the access level of a user: Select Edit access level next to the name of the user. Select required data access level. Either leave the ‘Paths’ field blank or enter a list of paths to provide restricted access, as described in the section above. Select Save . To remove a user from the data access group: Select Edit access level next to the name of the user. Select Revoke access . Interacting with Amazon S3 via the Analytical Platform You can upload files to Amazon S3 from your local computer or download files from Amazon S3 to your local computer using below tools Amazon S3 console RStudio JupyterLab When uploading files to Amazon S3, you should ensure that you follow all necessary information governance procedures. In particular, you must complete a data movement form when moving any data onto the Analytical Platform. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 122}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f333ad7ece49a1643ade4b9daa7a1b10'}>,\n",
       " <Document: {'content': 'Downloading the data from Amazon S3 to your local machine is also considered as data movement and therefore needs to be managed as such in accordance with the necessary information governance procedures, particularly for Personal Identifiable Information. Your options This section presents a comparison of the various tools available for accessing Amazon S3 on each platform; further details on setup and usage are given below. AWS Console The AWS S3 Console is a browser-based GUI tool. You can use the Amazon S3 console to view an overview of an object. The object overview in the console provides all the essential information for an object in one place. For further details, see the guide further down the page. RStudio There are two main options for interacting with files stored in AWS S3 buckets on the Analytical Platform via RStudio: Rs3tools and botor . Either of these options works well on the Analytical Platform, and you should pick whichever best suits your use-case. Rs3tools is an R-native community-developed MoJ project which consists of a set of helper tools to access Amazon S3 buckets on the Analytical Platform. The installation process for botor takes longer as it requires a Python environment ( botor is a wrapper around Python’s boto3 library). However, it contains a larger range of functionality. Generally, we recommend using Rs3tools unless there is a specific need for the additional functionality in botor . You may also see mentions of another tool, s3tools . s3tools is now deprecated and has been replaced by Rs3tools .More information is available in this ADR Record Most of the original functionality is available via Rs3tools , so this is a good replacement if you are looking to update older code that relied on the s3tools package.If you need the additional functionality available in botor , a guide to migration is available here . In addition, an RStudio plugin, s3browser is available if you only want to browse your files. For further details, see the sections below on Rs3tools , botor and s3browser . JupyterLab The main options for interacting with files stored in AWS S3 buckets on the Analytical Platform via JupyterLab are : Reading files : pandas , mojap-arrow-pd-parser Downloading / Uploading files : boto3 Installation and usage Amazon S3 Console You can use the Amazon S3 Console to upload/download files from/to your local computer (for example, personal or shared storage on DOM1 or Quantum) only. To upload files using the Amazon S3 Console: Log in to the AWS Management Console using your Analytical Platform account. Select Services from the menu bar. Select S3 from the drop down menu. Select the bucket and folder you want to upload files to. Select Upload . Select Add files or drag and drop the files you want to upload. Select Upload . Downloading a file using the Amazon S3 Console follows a similar process: Follow steps 1-3 from the list above. Navigate to the bucket and select the file you want to download. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 123}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3463f010a9f90f1ad5c90caef9b94160'}>,\n",
       " <Document: {'content': 'Select Download or Download as as appropriate. You can also directly navigate to a bucket in the AWS S3 Console by selecting Open on AWS in the Analytical Platform Control Panel. RStudio Rs3tools To install Rs3tools follow the guidance on their homepage . To upload files using Rs3Tools Writing files to S3 Rs3tools :: write_file_to_s3 ( \"my_downloaded_file.csv\" , \"alpha-everyone/delete/my_downloaded_file.csv\" , overwrite = TRUE ) # if file already exists, you recieve an error. overwrite=True enables it to overwrite the file Writing a dataframe to S3 in csv format Rs3tools :: write_df_to_csv_in_s3 ( dataframe_name , \"alpha-everyone/delete/iris.csv\" , overwrite = TRUE ) Downloading a file from S3 using Rs3Tools Rs3tools :: download_file_from_s3 ( \"alpha-everyone/s3tools_tests/iris_base.csv\" , \"my_downloaded_file.csv\" , overwrite = TRUE ) botor You will need to use the package manager renv to install botor .\\nTo get started with renv , see our guidance on the RStudio package management page . Then, go ahead with the botor installation (this is slightly different from the guidance on botor ‘s website as we use the renv package manager): renv :: use_python () ## at the prompt, choose to use python3 renv :: install ( \\'reticulate\\' ) Restart the session (Ctrl+Alt+F10 on a Windows machine). And then: reticulate :: py_install ( \\'boto3\\' ) renv :: install ( \\'botor\\' ) botor contains two functions for downloading or reading files from Amazon S3: s3_upload_file s3_write For example, to write a dataframe to csv, run the following code: library ( botor ) s3_write ( your_df , write.csv , \"s3://your_bucket/your_key.csv\" ) To read files, use one of the following: s3_download_file s3_read And use as follows: library ( botor ) your_df <- s3_read ( read.csv , \"s3://your_bucket/your_key.csv\" ) You can find out more about how to use these and other functions in the Migrating to botor appendix, the botor documentation or by using the help operator in RStudio (for example, ?botor::s3_write ). s3browser You can install s3browser by running the following code: install.packages(\\'remotes\\')\\nlibrary(remotes)\\nremotes::install_github(\\'moj-analytical-services/s3browser\\') To open the browser, run: s3browser::file_explorer_s3() You can find out more about how to use s3browser on GitHub . JupyterLab You can read/write directly from s3 using pandas . However, to get the best representation of the column types in the resulting Pandas dataframe(s), you may wish to use mojap-arrow-pd-parser . mojap-arrow-pd-parser mojap-arrow-pd-parser provides easy csv, jsonl and parquet file readers. To install in terminal: pip install arrow-pd-parser To read/write a csv file from s3: from arrow_pd_parser import reader , writer # Specifying the reader Both reader statements are equivalent and call the same readers under the hood df1 = reader . read ( \"s3://bucket_name/data/all_types.csv\" , file_format = \"csv\" ) df2 = reader . csv . read ( \"s3://bucket_name/data/all_types.csv\" ) # You can also pass the reader args to the reader as kwargs df3 = reader . csv . read ( \"s3://bucket_name/data/all_types.csv\" , nrows = 2 ) # The writer API has the same functionality writer . write ( df1 , file_format = \"parquet\" ) writer . parquet . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 124}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a1abfa38189d4e922bb15ba00f4c379a'}>,\n",
       " <Document: {'content': 'write ( df1 ) mojap-arrow-pd-parser infers the file type from the extension, so for example reader.read(\"s3://bucket_name/file.parquet\") would read a parquet file without need for specifying the file type. The package also has a lot of other functionality including specifying data types when reading (or writing). More details can be found in the package README . pandas You can use any of the pandas read functions (for example, read_csv or read_json ) to download data directly from Amazon S3. This requires that you have installed the pandas and s3fs packages. To install these, run the following code in a terminal: python -m pip install pandas s3fs As an example, to read a CSV, you should run the following code: import pandas as pd\\npd.read_csv(\\'s3://bucket_name/key\\') Here, you should substitute bucket_name with the name of the bucket and key with the path of the object in Amazon S3. boto3 You can also download or read objects using the boto3 package. You can install boto3 by running the following code in a terminal: pip install boto3 To download a file from Amazon S3, you should use the following code: import boto3 s3 = boto3 . resource ( \\'s3\\' ) s3 . Object ( \\'bucket_name\\' , \\'key\\' ). download_file ( \\'local_path\\' ) If you receive an ImportError , try restarting your kernel, so that Python recognises your boto3 installation. Here, you should substitute \\'bucket_name\\' with the name of the bucket, \\'key\\' with the path of the object in Amazon S3 and local_path with the local path where you would like to save the downloaded file. To upload a file to Amazon S3, you should use the following code: #Upload sample contents to s3 s3 = boto3 . client ( \\'s3\\' ) data = b \\'This is the content of the file uploaded from python boto3\\' file_name = \\'your_file_name.txt\\' response = s3 . put_object ( Bucket = your_bucket_name , Body = data , Key = file_name ) print ( \\'AWS response code for uploading file is \\' + str ( response [ \\'ResponseMetadata\\' ][ \\'HTTPStatusCode\\' ])) You can find more information in the package documentation . AWS Data Wrangler You can also use AWS Wrangler to work with data stored in Amazon S3. More information can be found in the product documentation . This page was last reviewed on 22 June 2022.\\n\\nIt needs to be reviewed again on 22 September 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 22 September 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAmazon S3 - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 125}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'df27ae7023db9443c2165c044a6e0d96'}>,\n",
       " <Document: {'content': 'Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 126}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '77b329a04cdaaec1738007ecfa242975'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Amazon S3 What is Amazon S3? Amazon S3 is a web-based cloud storage platform. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 127}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd8987c7066b75bd938d680b487085194'}>,\n",
       " <Document: {'content': 'It is one of the primary file storage locations on the Analytical Platform, alongside individual users’ home directories. You should use your home directory to store working copies of code and analytical outputs. Where possible, you should store all data and final analytical outputs in Amazon S3, and final code in GitHub to facilitate collaboration. Data stored in Amazon S3 can be seamlessly integrated with other AWS services such as Amazon Athena and Amazon Glue. Working with Amazon S3 buckets Types of buckets Amazon S3 buckets are separated into two categories on the Analytical Platform. Warehouse data sources Warehouse data sources are used to store data that is accessed by code you run yourself, for example, in RStudio or JupyterLab. You can create warehouse data sources yourself and can provide access to other users you need to collaborate with. Webapp data sources Webapp data sources are used to store data that is accessed by code run by the Analytical Platform, for example by deployed apps or by Airflow pipelines. You cannot create webapp data sources yourself – you must ask the Analytical Platform team to create one on your behalf. If you request that a webapp data source is created when setting up a new app, the app will automatically be given read-only access. You will also be given admin access to the bucket and can provide access to other users you need to collaborate with. The Data Engineering team also manage some buckets that are not shown in the control panel and that are not available to standard users. These buckets are used to store incoming raw data, which may be processed or fed into curated data pipelines. For more information, contact the Data Engineering team on the #ask-data-engineering Slack channel. You can view the data sources you have access to in the control panel. Create a new warehouse data source You can only create new warehouse data sources in the Analytical Platform control panel. You cannot create new buckets directly in the Amazon S3 console. To create a new warehouse data source: Go to the Analytical Platform [control panel](https://controlpanel.services.analytical-platform.service.justice.gov.uk/. Select the Warehouse data tab. Select Create new warehouse data source . Enter a name for the warehouse data source – this must be prefixed with ‘alpha-’. Select Create data source . When you create a new warehouse data source, only you will initially have access. As an admin of the data source, you will be able to add and remove other users from the data access group as required. Further information on managing data access groups can be found here . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 128}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8a1096864d6074dc423c7bcad27ee480'}>,\n",
       " <Document: {'content': 'Data access levels Every bucket has three data access levels: Read only Read/write Admin – this provides read/write access and allows the user to add and remove other users from the bucket’s data access group Path specific access As well as choosing an access level, you can also restrict a user’s access to specific paths in a bucket by entering each path on a new line in the ‘Paths’ textarea field when adding the user to a data access group, taking care not to leave an empty new line after the last path. For example: /folder-one\\n/folder-two This would give the user access to only /folder-one and /folder-two in the bucket and nothing else. If you leave this field blank, the user will be able to access everything in the bucket. Request access to a bucket To gain access to a bucket (warehouse data source or webapp data source), you must be added to the relevant data access group. If you know an admin of the bucket you require access to, you should ask them to add you to the data access group. If you do not know any of the admins of the bucket you require access to, you can find a list of the GitHub usernames of all bucket admins on the Warehouse Data page of Control Panel (scroll down the page), or contact the Analytical Platform team via Slack , GitHub or email ( analytical_platform@digital.justice.gov.uk ). If all bucket admins are unavailable (e.g. have left the MoJ), the  Analytical Platform team will be able to grant you access to the datasource if the request is approved by your line manager. When requesting access to a bucket, you should specify the name of the bucket and the level of access you require. You should only request access to data that you have a genuine business need to access and should only request the lowest level of access required for you to complete your work. You may be required to demonstrate the business need for you to access a bucket if requested by a bucket admin or an information asset owner (IAO). Manage access to a bucket Bucket admins can manage access to warehouse data sources and webapp data sources in the Analytical Platform control panel . You cannot manage access to buckets directly in the Amazon S3 console. To manage access to a data source: Go to the Analytical Platform control panel . Select the Warehouse data tab or the Webapp data tab, as relevant. Select the name of the data source you want to manage. To add a new user to the data access group: Type the user’s GitHub username into the input field labelled Grant access to this data to other users . Select the user from the drop-down list. Select the required data access level. Either leave the ‘Paths’ field blank or enter a list of paths to provide restricted access, as described in the section above. Select Grant access . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 129}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eb0472237d2997672d2df631ac298f2d'}>,\n",
       " <Document: {'content': 'To edit the access level of a user: Select Edit access level next to the name of the user. Select required data access level. Either leave the ‘Paths’ field blank or enter a list of paths to provide restricted access, as described in the section above. Select Save . To remove a user from the data access group: Select Edit access level next to the name of the user. Select Revoke access . Interacting with Amazon S3 via the Analytical Platform You can upload files to Amazon S3 from your local computer or download files from Amazon S3 to your local computer using below tools Amazon S3 console RStudio JupyterLab When uploading files to Amazon S3, you should ensure that you follow all necessary information governance procedures. In particular, you must complete a data movement form when moving any data onto the Analytical Platform. Downloading the data from Amazon S3 to your local machine is also considered as data movement and therefore needs to be managed as such in accordance with the necessary information governance procedures, particularly for Personal Identifiable Information. Your options This section presents a comparison of the various tools available for accessing Amazon S3 on each platform; further details on setup and usage are given below. AWS Console The AWS S3 Console is a browser-based GUI tool. You can use the Amazon S3 console to view an overview of an object. The object overview in the console provides all the essential information for an object in one place. For further details, see the guide further down the page. RStudio There are two main options for interacting with files stored in AWS S3 buckets on the Analytical Platform via RStudio: Rs3tools and botor . Either of these options works well on the Analytical Platform, and you should pick whichever best suits your use-case. Rs3tools is an R-native community-developed MoJ project which consists of a set of helper tools to access Amazon S3 buckets on the Analytical Platform. The installation process for botor takes longer as it requires a Python environment ( botor is a wrapper around Python’s boto3 library). However, it contains a larger range of functionality. Generally, we recommend using Rs3tools unless there is a specific need for the additional functionality in botor . You may also see mentions of another tool, s3tools . s3tools is now deprecated and has been replaced by Rs3tools .More information is available in this ADR Record Most of the original functionality is available via Rs3tools , so this is a good replacement if you are looking to update older code that relied on the s3tools package.If you need the additional functionality available in botor , a guide to migration is available here . In addition, an RStudio plugin, s3browser is available if you only want to browse your files. For further details, see the sections below on Rs3tools , botor and s3browser . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 130}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '900c0c06885545a7f1feb8d8358272b7'}>,\n",
       " <Document: {'content': 'JupyterLab The main options for interacting with files stored in AWS S3 buckets on the Analytical Platform via JupyterLab are : Reading files : pandas , mojap-arrow-pd-parser Downloading / Uploading files : boto3 Installation and usage Amazon S3 Console You can use the Amazon S3 Console to upload/download files from/to your local computer (for example, personal or shared storage on DOM1 or Quantum) only. To upload files using the Amazon S3 Console: Log in to the AWS Management Console using your Analytical Platform account. Select Services from the menu bar. Select S3 from the drop down menu. Select the bucket and folder you want to upload files to. Select Upload . Select Add files or drag and drop the files you want to upload. Select Upload . Downloading a file using the Amazon S3 Console follows a similar process: Follow steps 1-3 from the list above. Navigate to the bucket and select the file you want to download. Select Download or Download as as appropriate. You can also directly navigate to a bucket in the AWS S3 Console by selecting Open on AWS in the Analytical Platform Control Panel. RStudio Rs3tools To install Rs3tools follow the guidance on their homepage . To upload files using Rs3Tools Writing files to S3 Rs3tools :: write_file_to_s3 ( \"my_downloaded_file.csv\" , \"alpha-everyone/delete/my_downloaded_file.csv\" , overwrite = TRUE ) # if file already exists, you recieve an error. overwrite=True enables it to overwrite the file Writing a dataframe to S3 in csv format Rs3tools :: write_df_to_csv_in_s3 ( dataframe_name , \"alpha-everyone/delete/iris.csv\" , overwrite = TRUE ) Downloading a file from S3 using Rs3Tools Rs3tools :: download_file_from_s3 ( \"alpha-everyone/s3tools_tests/iris_base.csv\" , \"my_downloaded_file.csv\" , overwrite = TRUE ) botor You will need to use the package manager renv to install botor .\\nTo get started with renv , see our guidance on the RStudio package management page . Then, go ahead with the botor installation (this is slightly different from the guidance on botor ‘s website as we use the renv package manager): renv :: use_python () ## at the prompt, choose to use python3 renv :: install ( \\'reticulate\\' ) Restart the session (Ctrl+Alt+F10 on a Windows machine). And then: reticulate :: py_install ( \\'boto3\\' ) renv :: install ( \\'botor\\' ) botor contains two functions for downloading or reading files from Amazon S3: s3_upload_file s3_write For example, to write a dataframe to csv, run the following code: library ( botor ) s3_write ( your_df , write.csv , \"s3://your_bucket/your_key.csv\" ) To read files, use one of the following: s3_download_file s3_read And use as follows: library ( botor ) your_df <- s3_read ( read.csv , \"s3://your_bucket/your_key.csv\" ) You can find out more about how to use these and other functions in the Migrating to botor appendix, the botor documentation or by using the help operator in RStudio (for example, ?botor::s3_write ). ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 131}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c7f0c2b5aa9fc3715f0204c060f2ebf9'}>,\n",
       " <Document: {'content': 's3browser You can install s3browser by running the following code: install.packages(\\'remotes\\')\\nlibrary(remotes)\\nremotes::install_github(\\'moj-analytical-services/s3browser\\') To open the browser, run: s3browser::file_explorer_s3() You can find out more about how to use s3browser on GitHub . JupyterLab You can read/write directly from s3 using pandas . However, to get the best representation of the column types in the resulting Pandas dataframe(s), you may wish to use mojap-arrow-pd-parser . mojap-arrow-pd-parser mojap-arrow-pd-parser provides easy csv, jsonl and parquet file readers. To install in terminal: pip install arrow-pd-parser To read/write a csv file from s3: from arrow_pd_parser import reader , writer # Specifying the reader Both reader statements are equivalent and call the same readers under the hood df1 = reader . read ( \"s3://bucket_name/data/all_types.csv\" , file_format = \"csv\" ) df2 = reader . csv . read ( \"s3://bucket_name/data/all_types.csv\" ) # You can also pass the reader args to the reader as kwargs df3 = reader . csv . read ( \"s3://bucket_name/data/all_types.csv\" , nrows = 2 ) # The writer API has the same functionality writer . write ( df1 , file_format = \"parquet\" ) writer . parquet . write ( df1 ) mojap-arrow-pd-parser infers the file type from the extension, so for example reader.read(\"s3://bucket_name/file.parquet\") would read a parquet file without need for specifying the file type. The package also has a lot of other functionality including specifying data types when reading (or writing). More details can be found in the package README . pandas You can use any of the pandas read functions (for example, read_csv or read_json ) to download data directly from Amazon S3. This requires that you have installed the pandas and s3fs packages. To install these, run the following code in a terminal: python -m pip install pandas s3fs As an example, to read a CSV, you should run the following code: import pandas as pd\\npd.read_csv(\\'s3://bucket_name/key\\') Here, you should substitute bucket_name with the name of the bucket and key with the path of the object in Amazon S3. boto3 You can also download or read objects using the boto3 package. You can install boto3 by running the following code in a terminal: pip install boto3 To download a file from Amazon S3, you should use the following code: import boto3 s3 = boto3 . resource ( \\'s3\\' ) s3 . Object ( \\'bucket_name\\' , \\'key\\' ). download_file ( \\'local_path\\' ) If you receive an ImportError , try restarting your kernel, so that Python recognises your boto3 installation. Here, you should substitute \\'bucket_name\\' with the name of the bucket, \\'key\\' with the path of the object in Amazon S3 and local_path with the local path where you would like to save the downloaded file. To upload a file to Amazon S3, you should use the following code: #Upload sample contents to s3 s3 = boto3 . client ( \\'s3\\' ) data = b \\'This is the content of the file uploaded from python boto3\\' file_name = \\'your_file_name.txt\\' response = s3 . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 132}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b05c65c174e7a9e6cf0795ccc5ff9f9a'}>,\n",
       " <Document: {'content': \"put_object ( Bucket = your_bucket_name , Body = data , Key = file_name ) print ( 'AWS response code for uploading file is ' + str ( response [ 'ResponseMetadata' ][ 'HTTPStatusCode' ])) You can find more information in the package documentation . AWS Data Wrangler You can also use AWS Wrangler to work with data stored in Amazon S3. More information can be found in the product documentation . This page was last reviewed on 22 June 2022.\\n\\nIt needs to be reviewed again on 22 September 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 22 September 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nCurated databases - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 133}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '45104c7af92f646e8578375e810aa9de'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 134}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Curated databases This is guidance contains information on using curated databases on the Analytical Platform. SQL Amazon Athena Querying Athena from the AP Databases Data Discovery Tool This page was last reviewed on 17 August 2021.\\n\\nIt needs to be reviewed again on 17 August 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 17 August 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nSQL - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 135}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4db51bf020b00ec5dbe3bb60362f0e3'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 136}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools SQL quickguide SQL (pronounced ‘S-Q-L’ or ‘sequel’) is a programming language used to access and manipulate databases. There are several versions of SQL that share a common framework but can have different syntax and functionality. The version of SQL used by Amazon Athena is based on Presto 0.217 . This guide will cover some basic concepts to get you going, but if you’re looking for something more in depth, take a look at the SQL Training repo here . Database structure A database is a collection of structured data that consists of one or more tables. Tables consist of a number of rows and columns. A single row is called a record, entity or object. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 137}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd12ab558f5acf39a690e698505f873ee'}>,\n",
       " <Document: {'content': \"A single column is called a field or attribute. in SQL, to refer to a table called table_1 in a database called database_1 , we write database_1.table_1 . Databases in Amazon Athena work in a slightly different way to other relational database systems. Databases and tables simply contain metadata that define schemas for underlying source data stored in S3. The metadata tells Athena where the source data is stored and how it is structured. Consequently, when you submit a query to Athena, it runs on the underlying source data. The superposition of a database structure makes it quick and easy to perform such queries. Data types SQL supports several different data types. You can find out about those supported by Presto here . The format and use of most of these data types will be familiar to users of other versions of SQL. The way dates, times and timestamps (datetimes) are specified can be slightly different. DATE Dates are written in the format DATE 'YYYY-MM-DD' . TIME Times are written in the format TIME 'HH:MM:SS.SSS' . You can also specify a timezone using TIME HH:MM:SS.SSS <TIMEZONE> . A list of timezones can be found here . TIMESTAMP Timestamps are the equivalent of datetimes in other versions of SQL. Timestamps are written in the format TIMESTAMP 'YYYY-MM-DD HH:MM:SS.SSS . As for times, you can specify a timezone in the same way. Commands SELECT The SELECT statement is used to retrieve data from one or more tables or views in a database. The following code retrieves column1, column2, ... from a table called table_name in a database called database_name . SELECT column1, column2, ...\\nFROM database_name.table_name; The following code retrieves all columns from a table called table_name in a database called database_name . SELECT *\\nFROM database_name.table_name; SELECT DISTINCT The SELECT DISTINCT statement returns only one copy of each set of duplicate rows. It only looks at values in the selected columns when identifying duplicate rows. The following code retrieves all columns from a table called table_name in a database called database_name and returns only one copy of each set of duplicate rows. SELECT DISTINCT *\\nFROM database_name.table_name; If there are no duplicate rows, SELECT is equivalent to SELECT DISTINCT . AS The AS operator is used give aliases, or temporary names, to tables and columns. Table aliases are used to: make code more concise and easy to follow when working with tables with long names; and differentiate between fields with the same name in two or more different tables. Column aliases are used to: give columns more readable names in the result set; and assign names to computed columns. If an alias contains spaces, it must be contained within double quotation marks or square brackets. WHERE The WHERE clause is used to filter records based on a condition. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 138}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd243e8fec487371b8764877d5b8d8f0d'}>,\n",
       " <Document: {'content': 'Operators can be used to specify a condition: Operator Description = Equal to <> Not equal to > Greater than < Less than >= Greater than or equal to <= Less than or equal to [NOT] IN (Not) in a specified list IS [NOT] NULL (Not) null BETWEEN Between two values, inclusive of the start and end values IS [NOT] DISTINCT FROM See here IN In a list of the form (value1, value2, ...) You can filter records based on more than one condition using the boolean operators AND and OR . Conditions can also be negated using the NOT operator. LIMIT The LIMIT clause restricts the number of rows in the result set. The rows in the result set will be arbitrary unless the query contains an ORDER BY clause. The following code retrieves all columns from a table called table_name in a database called database_name but restricts the output to 10 rows. SELECT *\\nFROM database_name.table_name\\nLIMIT 10; ORDER BY The ORDER BY statement is used to sort the outputs of a query. You can sort using multiple columns by separating the names of the columns with a comma. For example, ORDER BY column1, column2, column3 would sort a query output first by column1 , then by column2 and finally by column3 . By default, ORDER BY sorts in ascending order. To sort in descending order, insert the keyword DESC after the column name. You can also explicitly sort in ascending order by inserting the keyword ASC after the column name. For example, ORDER BY column1 ASC, column2 DESC would sort a query output first by column1 in ascending order and then by column2 in descending order. SELECT column1,\\ncolumn2,\\ncolumn3\\nFROM database_name.table_name\\nORDER BY column1 ASC, column2 DESC; GROUP BY The GROUP BY statement is used to group the results of a query output and is often used with summary functions: MAX MIN SUM COUNT AVG JOIN There are five different types of join: [INNER] JOIN returns all records that have matching values in both tables to be joined. LEFT [OUTER] JOIN returns all records from the left table and records from the right table with matching values. RIGHT [OUTER] JOIN returns all records from the right table and records from the left table with matching values. FULL [OUTER] JOIN returns all records from both tables regardless of whether they have matching values or not. CROSS JOIN returns the cartesian product of both tables, i.e., each record from the left table is matched with each record from the right table. All of these join statements, apart from CROSS JOIN require you to specify the column(s) on which to join using the ON statement. As an example, the following code selects all columns from table_1 (which has the alias t1 ) and joins them to all columns from table_2 (which has the alias t2 ) where column1 in table_1 matches column1 in table_2 : SELECT t1. ,\\nt2. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 139}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b3cc89f2612a699d69cdc1242d8bf9c5'}>,\n",
       " <Document: {'content': 'FROM database_name.table_1 t1\\nINNER JOIN database_name.table_2 t2 ON t1.column1 = t2.column1; CREATE VIEW A view is the output of a stored query that can be accessed by a name in a similar way to a table. It may be useful to create a view if you often run the same query. When you select a view, the underlying query is rerun, so you will always see an output based on the most up-to-date data. To create a view, we use the CREATE [OR REPLACE] VIEW AS statement. For example, the following code creates a view called view_1 that selects all columns from a table called table_name in a database called database_name : CREATE OR REPLACE VIEW view_1 AS\\nSELECT *\\nFROM database_name.table_name; Here, OR REPLACE updates the view if it already exists and creates it if not. If OR REPLACE is not included but the view already exists, the query will result in an error. DROP VIEW To delete a view, we use the DROP VIEW statement. For example, the following code deletes a view called view_1 from a database called database_1 : DROP VIEW database_1.view_1; Including IF EXISTS supresses an error if the view does not exist. CREATE TABLE To create, edit or delete tables in Athena you need to have additional read/write permissions. If you require access to this functionality, please contact the Analytical Platform team. Creating a table from existing data To create a table in a database from existing data, we use the CREATE TABLE [IF NOT EXISTS] statement. The following code creates a new table called table_1 that selects two columns from a table called table_name in a database called database_name : CREATE TABLE IF NOT EXISTS table_1 AS\\nSELECT column1, column2\\nFROM database_name.table_name; Including IF NOT EXISTS ensures that we only create a table when one does not already exist. If we did not include this but a table did already exist, the query would result in an error. This table will be stored permanently in the database and will not be updated, even if the underlying data used in the CREATE TABLE query changes. Creating a new table To create a new table, we also use the CREATE TABLE statement. Here we must specify the names of the columns in the new table and their data types. INSERT INTO To insert data into a table we use the INSERT INTO statement. We can insert a single line at a time, multiple lines at a time or can insert data using a query on existing tables. ALTER TABLE We can add, remove and modify columns in an existing table using the ALTER TABLE statement. DROP TABLE To delete a table, we use the DROP TABLE [IF EXISTS] statement. For example, the following code deletes a table called table_1 from a database called database_1 : DROP TABLE IF EXISTS database_1.table_1; Including IF EXISTS supresses an error if the table does not exist. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 140}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1e3704c4c46705214b9e19a7fd7825ad'}>,\n",
       " <Document: {'content': 'You should always check carefully before deleting a table and should never delete a table that you have not created yourself. Functions SQL supports hundreds of functions, a full list of which can be found in the Presto documentation . This page was last reviewed on 17 January 2020.\\n\\nIt needs to be reviewed again on 17 January 2021\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 17 January 2021\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nSQL - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 141}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bfc421cc9d9f82a8e50a0623e5520ca8'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 142}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools SQL quickguide SQL (pronounced ‘S-Q-L’ or ‘sequel’) is a programming language used to access and manipulate databases. There are several versions of SQL that share a common framework but can have different syntax and functionality. The version of SQL used by Amazon Athena is based on Presto 0.217 . This guide will cover some basic concepts to get you going, but if you’re looking for something more in depth, take a look at the SQL Training repo here . Database structure A database is a collection of structured data that consists of one or more tables. Tables consist of a number of rows and columns. A single row is called a record, entity or object. A single column is called a field or attribute. in SQL, to refer to a table called table_1 in a database called database_1 , we write database_1.table_1 . Databases in Amazon Athena work in a slightly different way to other relational database systems. Databases and tables simply contain metadata that define schemas for underlying source data stored in S3. The metadata tells Athena where the source data is stored and how it is structured. Consequently, when you submit a query to Athena, it runs on the underlying source data. The superposition of a database structure makes it quick and easy to perform such queries. Data types SQL supports several different data types. You can find out about those supported by Presto here . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 143}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3f607c27eaa85e229d81bfef8ce07a03'}>,\n",
       " <Document: {'content': \"The format and use of most of these data types will be familiar to users of other versions of SQL. The way dates, times and timestamps (datetimes) are specified can be slightly different. DATE Dates are written in the format DATE 'YYYY-MM-DD' . TIME Times are written in the format TIME 'HH:MM:SS.SSS' . You can also specify a timezone using TIME HH:MM:SS.SSS <TIMEZONE> . A list of timezones can be found here . TIMESTAMP Timestamps are the equivalent of datetimes in other versions of SQL. Timestamps are written in the format TIMESTAMP 'YYYY-MM-DD HH:MM:SS.SSS . As for times, you can specify a timezone in the same way. Commands SELECT The SELECT statement is used to retrieve data from one or more tables or views in a database. The following code retrieves column1, column2, ... from a table called table_name in a database called database_name . SELECT column1, column2, ...\\nFROM database_name.table_name; The following code retrieves all columns from a table called table_name in a database called database_name . SELECT *\\nFROM database_name.table_name; SELECT DISTINCT The SELECT DISTINCT statement returns only one copy of each set of duplicate rows. It only looks at values in the selected columns when identifying duplicate rows. The following code retrieves all columns from a table called table_name in a database called database_name and returns only one copy of each set of duplicate rows. SELECT DISTINCT *\\nFROM database_name.table_name; If there are no duplicate rows, SELECT is equivalent to SELECT DISTINCT . AS The AS operator is used give aliases, or temporary names, to tables and columns. Table aliases are used to: make code more concise and easy to follow when working with tables with long names; and differentiate between fields with the same name in two or more different tables. Column aliases are used to: give columns more readable names in the result set; and assign names to computed columns. If an alias contains spaces, it must be contained within double quotation marks or square brackets. WHERE The WHERE clause is used to filter records based on a condition. Operators can be used to specify a condition: Operator Description = Equal to <> Not equal to > Greater than < Less than >= Greater than or equal to <= Less than or equal to [NOT] IN (Not) in a specified list IS [NOT] NULL (Not) null BETWEEN Between two values, inclusive of the start and end values IS [NOT] DISTINCT FROM See here IN In a list of the form (value1, value2, ...) You can filter records based on more than one condition using the boolean operators AND and OR . Conditions can also be negated using the NOT operator. LIMIT The LIMIT clause restricts the number of rows in the result set. The rows in the result set will be arbitrary unless the query contains an ORDER BY clause. The following code retrieves all columns from a table called table_name in a database called database_name but restricts the output to 10 rows. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 144}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9c6bb292e0028ab7b6f022ae2e15ecf6'}>,\n",
       " <Document: {'content': 'SELECT *\\nFROM database_name.table_name\\nLIMIT 10; ORDER BY The ORDER BY statement is used to sort the outputs of a query. You can sort using multiple columns by separating the names of the columns with a comma. For example, ORDER BY column1, column2, column3 would sort a query output first by column1 , then by column2 and finally by column3 . By default, ORDER BY sorts in ascending order. To sort in descending order, insert the keyword DESC after the column name. You can also explicitly sort in ascending order by inserting the keyword ASC after the column name. For example, ORDER BY column1 ASC, column2 DESC would sort a query output first by column1 in ascending order and then by column2 in descending order. SELECT column1,\\ncolumn2,\\ncolumn3\\nFROM database_name.table_name\\nORDER BY column1 ASC, column2 DESC; GROUP BY The GROUP BY statement is used to group the results of a query output and is often used with summary functions: MAX MIN SUM COUNT AVG JOIN There are five different types of join: [INNER] JOIN returns all records that have matching values in both tables to be joined. LEFT [OUTER] JOIN returns all records from the left table and records from the right table with matching values. RIGHT [OUTER] JOIN returns all records from the right table and records from the left table with matching values. FULL [OUTER] JOIN returns all records from both tables regardless of whether they have matching values or not. CROSS JOIN returns the cartesian product of both tables, i.e., each record from the left table is matched with each record from the right table. All of these join statements, apart from CROSS JOIN require you to specify the column(s) on which to join using the ON statement. As an example, the following code selects all columns from table_1 (which has the alias t1 ) and joins them to all columns from table_2 (which has the alias t2 ) where column1 in table_1 matches column1 in table_2 : SELECT t1. ,\\nt2. FROM database_name.table_1 t1\\nINNER JOIN database_name.table_2 t2 ON t1.column1 = t2.column1; CREATE VIEW A view is the output of a stored query that can be accessed by a name in a similar way to a table. It may be useful to create a view if you often run the same query. When you select a view, the underlying query is rerun, so you will always see an output based on the most up-to-date data. To create a view, we use the CREATE [OR REPLACE] VIEW AS statement. For example, the following code creates a view called view_1 that selects all columns from a table called table_name in a database called database_name : CREATE OR REPLACE VIEW view_1 AS\\nSELECT *\\nFROM database_name.table_name; Here, OR REPLACE updates the view if it already exists and creates it if not. If OR REPLACE is not included but the view already exists, the query will result in an error. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 145}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '90655c142225fd47b831bbe25017fa5b'}>,\n",
       " <Document: {'content': 'DROP VIEW To delete a view, we use the DROP VIEW statement. For example, the following code deletes a view called view_1 from a database called database_1 : DROP VIEW database_1.view_1; Including IF EXISTS supresses an error if the view does not exist. CREATE TABLE To create, edit or delete tables in Athena you need to have additional read/write permissions. If you require access to this functionality, please contact the Analytical Platform team. Creating a table from existing data To create a table in a database from existing data, we use the CREATE TABLE [IF NOT EXISTS] statement. The following code creates a new table called table_1 that selects two columns from a table called table_name in a database called database_name : CREATE TABLE IF NOT EXISTS table_1 AS\\nSELECT column1, column2\\nFROM database_name.table_name; Including IF NOT EXISTS ensures that we only create a table when one does not already exist. If we did not include this but a table did already exist, the query would result in an error. This table will be stored permanently in the database and will not be updated, even if the underlying data used in the CREATE TABLE query changes. Creating a new table To create a new table, we also use the CREATE TABLE statement. Here we must specify the names of the columns in the new table and their data types. INSERT INTO To insert data into a table we use the INSERT INTO statement. We can insert a single line at a time, multiple lines at a time or can insert data using a query on existing tables. ALTER TABLE We can add, remove and modify columns in an existing table using the ALTER TABLE statement. DROP TABLE To delete a table, we use the DROP TABLE [IF EXISTS] statement. For example, the following code deletes a table called table_1 from a database called database_1 : DROP TABLE IF EXISTS database_1.table_1; Including IF EXISTS supresses an error if the table does not exist. You should always check carefully before deleting a table and should never delete a table that you have not created yourself. Functions SQL supports hundreds of functions, a full list of which can be found in the Presto documentation . This page was last reviewed on 17 January 2020.\\n\\nIt needs to be reviewed again on 17 January 2021\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 17 January 2021\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nSQL - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 146}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4d57e9c737d46d16d26571af28fffb32'}>,\n",
       " <Document: {'content': 'Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 147}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '357c63352e7956c43921829a81ea261'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools SQL quickguide SQL (pronounced ‘S-Q-L’ or ‘sequel’) is a programming language used to access and manipulate databases. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 148}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cf6fe4989994129615e83f1d31a95d3e'}>,\n",
       " <Document: {'content': \"There are several versions of SQL that share a common framework but can have different syntax and functionality. The version of SQL used by Amazon Athena is based on Presto 0.217 . This guide will cover some basic concepts to get you going, but if you’re looking for something more in depth, take a look at the SQL Training repo here . Database structure A database is a collection of structured data that consists of one or more tables. Tables consist of a number of rows and columns. A single row is called a record, entity or object. A single column is called a field or attribute. in SQL, to refer to a table called table_1 in a database called database_1 , we write database_1.table_1 . Databases in Amazon Athena work in a slightly different way to other relational database systems. Databases and tables simply contain metadata that define schemas for underlying source data stored in S3. The metadata tells Athena where the source data is stored and how it is structured. Consequently, when you submit a query to Athena, it runs on the underlying source data. The superposition of a database structure makes it quick and easy to perform such queries. Data types SQL supports several different data types. You can find out about those supported by Presto here . The format and use of most of these data types will be familiar to users of other versions of SQL. The way dates, times and timestamps (datetimes) are specified can be slightly different. DATE Dates are written in the format DATE 'YYYY-MM-DD' . TIME Times are written in the format TIME 'HH:MM:SS.SSS' . You can also specify a timezone using TIME HH:MM:SS.SSS <TIMEZONE> . A list of timezones can be found here . TIMESTAMP Timestamps are the equivalent of datetimes in other versions of SQL. Timestamps are written in the format TIMESTAMP 'YYYY-MM-DD HH:MM:SS.SSS . As for times, you can specify a timezone in the same way. Commands SELECT The SELECT statement is used to retrieve data from one or more tables or views in a database. The following code retrieves column1, column2, ... from a table called table_name in a database called database_name . SELECT column1, column2, ...\\nFROM database_name.table_name; The following code retrieves all columns from a table called table_name in a database called database_name . SELECT *\\nFROM database_name.table_name; SELECT DISTINCT The SELECT DISTINCT statement returns only one copy of each set of duplicate rows. It only looks at values in the selected columns when identifying duplicate rows. The following code retrieves all columns from a table called table_name in a database called database_name and returns only one copy of each set of duplicate rows. SELECT DISTINCT *\\nFROM database_name.table_name; If there are no duplicate rows, SELECT is equivalent to SELECT DISTINCT . AS The AS operator is used give aliases, or temporary names, to tables and columns. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 149}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7899257c6e2dbad55cfebc768a40d1c4'}>,\n",
       " <Document: {'content': 'Table aliases are used to: make code more concise and easy to follow when working with tables with long names; and differentiate between fields with the same name in two or more different tables. Column aliases are used to: give columns more readable names in the result set; and assign names to computed columns. If an alias contains spaces, it must be contained within double quotation marks or square brackets. WHERE The WHERE clause is used to filter records based on a condition. Operators can be used to specify a condition: Operator Description = Equal to <> Not equal to > Greater than < Less than >= Greater than or equal to <= Less than or equal to [NOT] IN (Not) in a specified list IS [NOT] NULL (Not) null BETWEEN Between two values, inclusive of the start and end values IS [NOT] DISTINCT FROM See here IN In a list of the form (value1, value2, ...) You can filter records based on more than one condition using the boolean operators AND and OR . Conditions can also be negated using the NOT operator. LIMIT The LIMIT clause restricts the number of rows in the result set. The rows in the result set will be arbitrary unless the query contains an ORDER BY clause. The following code retrieves all columns from a table called table_name in a database called database_name but restricts the output to 10 rows. SELECT *\\nFROM database_name.table_name\\nLIMIT 10; ORDER BY The ORDER BY statement is used to sort the outputs of a query. You can sort using multiple columns by separating the names of the columns with a comma. For example, ORDER BY column1, column2, column3 would sort a query output first by column1 , then by column2 and finally by column3 . By default, ORDER BY sorts in ascending order. To sort in descending order, insert the keyword DESC after the column name. You can also explicitly sort in ascending order by inserting the keyword ASC after the column name. For example, ORDER BY column1 ASC, column2 DESC would sort a query output first by column1 in ascending order and then by column2 in descending order. SELECT column1,\\ncolumn2,\\ncolumn3\\nFROM database_name.table_name\\nORDER BY column1 ASC, column2 DESC; GROUP BY The GROUP BY statement is used to group the results of a query output and is often used with summary functions: MAX MIN SUM COUNT AVG JOIN There are five different types of join: [INNER] JOIN returns all records that have matching values in both tables to be joined. LEFT [OUTER] JOIN returns all records from the left table and records from the right table with matching values. RIGHT [OUTER] JOIN returns all records from the right table and records from the left table with matching values. FULL [OUTER] JOIN returns all records from both tables regardless of whether they have matching values or not. CROSS JOIN returns the cartesian product of both tables, i.e.', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 150}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f0e20af751d4eac317d78580eb0743d1'}>,\n",
       " <Document: {'content': ', each record from the left table is matched with each record from the right table. All of these join statements, apart from CROSS JOIN require you to specify the column(s) on which to join using the ON statement. As an example, the following code selects all columns from table_1 (which has the alias t1 ) and joins them to all columns from table_2 (which has the alias t2 ) where column1 in table_1 matches column1 in table_2 : SELECT t1. ,\\nt2. FROM database_name.table_1 t1\\nINNER JOIN database_name.table_2 t2 ON t1.column1 = t2.column1; CREATE VIEW A view is the output of a stored query that can be accessed by a name in a similar way to a table. It may be useful to create a view if you often run the same query. When you select a view, the underlying query is rerun, so you will always see an output based on the most up-to-date data. To create a view, we use the CREATE [OR REPLACE] VIEW AS statement. For example, the following code creates a view called view_1 that selects all columns from a table called table_name in a database called database_name : CREATE OR REPLACE VIEW view_1 AS\\nSELECT *\\nFROM database_name.table_name; Here, OR REPLACE updates the view if it already exists and creates it if not. If OR REPLACE is not included but the view already exists, the query will result in an error. DROP VIEW To delete a view, we use the DROP VIEW statement. For example, the following code deletes a view called view_1 from a database called database_1 : DROP VIEW database_1.view_1; Including IF EXISTS supresses an error if the view does not exist. CREATE TABLE To create, edit or delete tables in Athena you need to have additional read/write permissions. If you require access to this functionality, please contact the Analytical Platform team. Creating a table from existing data To create a table in a database from existing data, we use the CREATE TABLE [IF NOT EXISTS] statement. The following code creates a new table called table_1 that selects two columns from a table called table_name in a database called database_name : CREATE TABLE IF NOT EXISTS table_1 AS\\nSELECT column1, column2\\nFROM database_name.table_name; Including IF NOT EXISTS ensures that we only create a table when one does not already exist. If we did not include this but a table did already exist, the query would result in an error. This table will be stored permanently in the database and will not be updated, even if the underlying data used in the CREATE TABLE query changes. Creating a new table To create a new table, we also use the CREATE TABLE statement. Here we must specify the names of the columns in the new table and their data types. INSERT INTO To insert data into a table we use the INSERT INTO statement. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 151}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c55b9a322f979be4377c67b1e6e0f117'}>,\n",
       " <Document: {'content': 'We can insert a single line at a time, multiple lines at a time or can insert data using a query on existing tables. ALTER TABLE We can add, remove and modify columns in an existing table using the ALTER TABLE statement. DROP TABLE To delete a table, we use the DROP TABLE [IF EXISTS] statement. For example, the following code deletes a table called table_1 from a database called database_1 : DROP TABLE IF EXISTS database_1.table_1; Including IF EXISTS supresses an error if the table does not exist. You should always check carefully before deleting a table and should never delete a table that you have not created yourself. Functions SQL supports hundreds of functions, a full list of which can be found in the Presto documentation . This page was last reviewed on 17 January 2020.\\n\\nIt needs to be reviewed again on 17 January 2021\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 17 January 2021\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nSQL - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 152}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '319b75be01e93c1fd848e798d21470ae'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 153}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools SQL quickguide SQL (pronounced ‘S-Q-L’ or ‘sequel’) is a programming language used to access and manipulate databases. There are several versions of SQL that share a common framework but can have different syntax and functionality. The version of SQL used by Amazon Athena is based on Presto 0.217 . This guide will cover some basic concepts to get you going, but if you’re looking for something more in depth, take a look at the SQL Training repo here . Database structure A database is a collection of structured data that consists of one or more tables. Tables consist of a number of rows and columns. A single row is called a record, entity or object. A single column is called a field or attribute. in SQL, to refer to a table called table_1 in a database called database_1 , we write database_1.table_1 . Databases in Amazon Athena work in a slightly different way to other relational database systems. Databases and tables simply contain metadata that define schemas for underlying source data stored in S3. The metadata tells Athena where the source data is stored and how it is structured. Consequently, when you submit a query to Athena, it runs on the underlying source data. The superposition of a database structure makes it quick and easy to perform such queries. Data types SQL supports several different data types. You can find out about those supported by Presto here . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 154}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3f607c27eaa85e229d81bfef8ce07a03'}>,\n",
       " <Document: {'content': \"The format and use of most of these data types will be familiar to users of other versions of SQL. The way dates, times and timestamps (datetimes) are specified can be slightly different. DATE Dates are written in the format DATE 'YYYY-MM-DD' . TIME Times are written in the format TIME 'HH:MM:SS.SSS' . You can also specify a timezone using TIME HH:MM:SS.SSS <TIMEZONE> . A list of timezones can be found here . TIMESTAMP Timestamps are the equivalent of datetimes in other versions of SQL. Timestamps are written in the format TIMESTAMP 'YYYY-MM-DD HH:MM:SS.SSS . As for times, you can specify a timezone in the same way. Commands SELECT The SELECT statement is used to retrieve data from one or more tables or views in a database. The following code retrieves column1, column2, ... from a table called table_name in a database called database_name . SELECT column1, column2, ...\\nFROM database_name.table_name; The following code retrieves all columns from a table called table_name in a database called database_name . SELECT *\\nFROM database_name.table_name; SELECT DISTINCT The SELECT DISTINCT statement returns only one copy of each set of duplicate rows. It only looks at values in the selected columns when identifying duplicate rows. The following code retrieves all columns from a table called table_name in a database called database_name and returns only one copy of each set of duplicate rows. SELECT DISTINCT *\\nFROM database_name.table_name; If there are no duplicate rows, SELECT is equivalent to SELECT DISTINCT . AS The AS operator is used give aliases, or temporary names, to tables and columns. Table aliases are used to: make code more concise and easy to follow when working with tables with long names; and differentiate between fields with the same name in two or more different tables. Column aliases are used to: give columns more readable names in the result set; and assign names to computed columns. If an alias contains spaces, it must be contained within double quotation marks or square brackets. WHERE The WHERE clause is used to filter records based on a condition. Operators can be used to specify a condition: Operator Description = Equal to <> Not equal to > Greater than < Less than >= Greater than or equal to <= Less than or equal to [NOT] IN (Not) in a specified list IS [NOT] NULL (Not) null BETWEEN Between two values, inclusive of the start and end values IS [NOT] DISTINCT FROM See here IN In a list of the form (value1, value2, ...) You can filter records based on more than one condition using the boolean operators AND and OR . Conditions can also be negated using the NOT operator. LIMIT The LIMIT clause restricts the number of rows in the result set. The rows in the result set will be arbitrary unless the query contains an ORDER BY clause. The following code retrieves all columns from a table called table_name in a database called database_name but restricts the output to 10 rows. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 155}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9c6bb292e0028ab7b6f022ae2e15ecf6'}>,\n",
       " <Document: {'content': 'SELECT *\\nFROM database_name.table_name\\nLIMIT 10; ORDER BY The ORDER BY statement is used to sort the outputs of a query. You can sort using multiple columns by separating the names of the columns with a comma. For example, ORDER BY column1, column2, column3 would sort a query output first by column1 , then by column2 and finally by column3 . By default, ORDER BY sorts in ascending order. To sort in descending order, insert the keyword DESC after the column name. You can also explicitly sort in ascending order by inserting the keyword ASC after the column name. For example, ORDER BY column1 ASC, column2 DESC would sort a query output first by column1 in ascending order and then by column2 in descending order. SELECT column1,\\ncolumn2,\\ncolumn3\\nFROM database_name.table_name\\nORDER BY column1 ASC, column2 DESC; GROUP BY The GROUP BY statement is used to group the results of a query output and is often used with summary functions: MAX MIN SUM COUNT AVG JOIN There are five different types of join: [INNER] JOIN returns all records that have matching values in both tables to be joined. LEFT [OUTER] JOIN returns all records from the left table and records from the right table with matching values. RIGHT [OUTER] JOIN returns all records from the right table and records from the left table with matching values. FULL [OUTER] JOIN returns all records from both tables regardless of whether they have matching values or not. CROSS JOIN returns the cartesian product of both tables, i.e., each record from the left table is matched with each record from the right table. All of these join statements, apart from CROSS JOIN require you to specify the column(s) on which to join using the ON statement. As an example, the following code selects all columns from table_1 (which has the alias t1 ) and joins them to all columns from table_2 (which has the alias t2 ) where column1 in table_1 matches column1 in table_2 : SELECT t1. ,\\nt2. FROM database_name.table_1 t1\\nINNER JOIN database_name.table_2 t2 ON t1.column1 = t2.column1; CREATE VIEW A view is the output of a stored query that can be accessed by a name in a similar way to a table. It may be useful to create a view if you often run the same query. When you select a view, the underlying query is rerun, so you will always see an output based on the most up-to-date data. To create a view, we use the CREATE [OR REPLACE] VIEW AS statement. For example, the following code creates a view called view_1 that selects all columns from a table called table_name in a database called database_name : CREATE OR REPLACE VIEW view_1 AS\\nSELECT *\\nFROM database_name.table_name; Here, OR REPLACE updates the view if it already exists and creates it if not. If OR REPLACE is not included but the view already exists, the query will result in an error. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 156}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '90655c142225fd47b831bbe25017fa5b'}>,\n",
       " <Document: {'content': 'DROP VIEW To delete a view, we use the DROP VIEW statement. For example, the following code deletes a view called view_1 from a database called database_1 : DROP VIEW database_1.view_1; Including IF EXISTS supresses an error if the view does not exist. CREATE TABLE To create, edit or delete tables in Athena you need to have additional read/write permissions. If you require access to this functionality, please contact the Analytical Platform team. Creating a table from existing data To create a table in a database from existing data, we use the CREATE TABLE [IF NOT EXISTS] statement. The following code creates a new table called table_1 that selects two columns from a table called table_name in a database called database_name : CREATE TABLE IF NOT EXISTS table_1 AS\\nSELECT column1, column2\\nFROM database_name.table_name; Including IF NOT EXISTS ensures that we only create a table when one does not already exist. If we did not include this but a table did already exist, the query would result in an error. This table will be stored permanently in the database and will not be updated, even if the underlying data used in the CREATE TABLE query changes. Creating a new table To create a new table, we also use the CREATE TABLE statement. Here we must specify the names of the columns in the new table and their data types. INSERT INTO To insert data into a table we use the INSERT INTO statement. We can insert a single line at a time, multiple lines at a time or can insert data using a query on existing tables. ALTER TABLE We can add, remove and modify columns in an existing table using the ALTER TABLE statement. DROP TABLE To delete a table, we use the DROP TABLE [IF EXISTS] statement. For example, the following code deletes a table called table_1 from a database called database_1 : DROP TABLE IF EXISTS database_1.table_1; Including IF EXISTS supresses an error if the table does not exist. You should always check carefully before deleting a table and should never delete a table that you have not created yourself. Functions SQL supports hundreds of functions, a full list of which can be found in the Presto documentation . This page was last reviewed on 17 January 2020.\\n\\nIt needs to be reviewed again on 17 January 2021\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 17 January 2021\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nSQL - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 157}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4d57e9c737d46d16d26571af28fffb32'}>,\n",
       " <Document: {'content': 'Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 158}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '357c63352e7956c43921829a81ea261'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools SQL quickguide SQL (pronounced ‘S-Q-L’ or ‘sequel’) is a programming language used to access and manipulate databases. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 159}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cf6fe4989994129615e83f1d31a95d3e'}>,\n",
       " <Document: {'content': \"There are several versions of SQL that share a common framework but can have different syntax and functionality. The version of SQL used by Amazon Athena is based on Presto 0.217 . This guide will cover some basic concepts to get you going, but if you’re looking for something more in depth, take a look at the SQL Training repo here . Database structure A database is a collection of structured data that consists of one or more tables. Tables consist of a number of rows and columns. A single row is called a record, entity or object. A single column is called a field or attribute. in SQL, to refer to a table called table_1 in a database called database_1 , we write database_1.table_1 . Databases in Amazon Athena work in a slightly different way to other relational database systems. Databases and tables simply contain metadata that define schemas for underlying source data stored in S3. The metadata tells Athena where the source data is stored and how it is structured. Consequently, when you submit a query to Athena, it runs on the underlying source data. The superposition of a database structure makes it quick and easy to perform such queries. Data types SQL supports several different data types. You can find out about those supported by Presto here . The format and use of most of these data types will be familiar to users of other versions of SQL. The way dates, times and timestamps (datetimes) are specified can be slightly different. DATE Dates are written in the format DATE 'YYYY-MM-DD' . TIME Times are written in the format TIME 'HH:MM:SS.SSS' . You can also specify a timezone using TIME HH:MM:SS.SSS <TIMEZONE> . A list of timezones can be found here . TIMESTAMP Timestamps are the equivalent of datetimes in other versions of SQL. Timestamps are written in the format TIMESTAMP 'YYYY-MM-DD HH:MM:SS.SSS . As for times, you can specify a timezone in the same way. Commands SELECT The SELECT statement is used to retrieve data from one or more tables or views in a database. The following code retrieves column1, column2, ... from a table called table_name in a database called database_name . SELECT column1, column2, ...\\nFROM database_name.table_name; The following code retrieves all columns from a table called table_name in a database called database_name . SELECT *\\nFROM database_name.table_name; SELECT DISTINCT The SELECT DISTINCT statement returns only one copy of each set of duplicate rows. It only looks at values in the selected columns when identifying duplicate rows. The following code retrieves all columns from a table called table_name in a database called database_name and returns only one copy of each set of duplicate rows. SELECT DISTINCT *\\nFROM database_name.table_name; If there are no duplicate rows, SELECT is equivalent to SELECT DISTINCT . AS The AS operator is used give aliases, or temporary names, to tables and columns. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 160}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7899257c6e2dbad55cfebc768a40d1c4'}>,\n",
       " <Document: {'content': 'Table aliases are used to: make code more concise and easy to follow when working with tables with long names; and differentiate between fields with the same name in two or more different tables. Column aliases are used to: give columns more readable names in the result set; and assign names to computed columns. If an alias contains spaces, it must be contained within double quotation marks or square brackets. WHERE The WHERE clause is used to filter records based on a condition. Operators can be used to specify a condition: Operator Description = Equal to <> Not equal to > Greater than < Less than >= Greater than or equal to <= Less than or equal to [NOT] IN (Not) in a specified list IS [NOT] NULL (Not) null BETWEEN Between two values, inclusive of the start and end values IS [NOT] DISTINCT FROM See here IN In a list of the form (value1, value2, ...) You can filter records based on more than one condition using the boolean operators AND and OR . Conditions can also be negated using the NOT operator. LIMIT The LIMIT clause restricts the number of rows in the result set. The rows in the result set will be arbitrary unless the query contains an ORDER BY clause. The following code retrieves all columns from a table called table_name in a database called database_name but restricts the output to 10 rows. SELECT *\\nFROM database_name.table_name\\nLIMIT 10; ORDER BY The ORDER BY statement is used to sort the outputs of a query. You can sort using multiple columns by separating the names of the columns with a comma. For example, ORDER BY column1, column2, column3 would sort a query output first by column1 , then by column2 and finally by column3 . By default, ORDER BY sorts in ascending order. To sort in descending order, insert the keyword DESC after the column name. You can also explicitly sort in ascending order by inserting the keyword ASC after the column name. For example, ORDER BY column1 ASC, column2 DESC would sort a query output first by column1 in ascending order and then by column2 in descending order. SELECT column1,\\ncolumn2,\\ncolumn3\\nFROM database_name.table_name\\nORDER BY column1 ASC, column2 DESC; GROUP BY The GROUP BY statement is used to group the results of a query output and is often used with summary functions: MAX MIN SUM COUNT AVG JOIN There are five different types of join: [INNER] JOIN returns all records that have matching values in both tables to be joined. LEFT [OUTER] JOIN returns all records from the left table and records from the right table with matching values. RIGHT [OUTER] JOIN returns all records from the right table and records from the left table with matching values. FULL [OUTER] JOIN returns all records from both tables regardless of whether they have matching values or not. CROSS JOIN returns the cartesian product of both tables, i.e.', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 161}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f0e20af751d4eac317d78580eb0743d1'}>,\n",
       " <Document: {'content': ', each record from the left table is matched with each record from the right table. All of these join statements, apart from CROSS JOIN require you to specify the column(s) on which to join using the ON statement. As an example, the following code selects all columns from table_1 (which has the alias t1 ) and joins them to all columns from table_2 (which has the alias t2 ) where column1 in table_1 matches column1 in table_2 : SELECT t1. ,\\nt2. FROM database_name.table_1 t1\\nINNER JOIN database_name.table_2 t2 ON t1.column1 = t2.column1; CREATE VIEW A view is the output of a stored query that can be accessed by a name in a similar way to a table. It may be useful to create a view if you often run the same query. When you select a view, the underlying query is rerun, so you will always see an output based on the most up-to-date data. To create a view, we use the CREATE [OR REPLACE] VIEW AS statement. For example, the following code creates a view called view_1 that selects all columns from a table called table_name in a database called database_name : CREATE OR REPLACE VIEW view_1 AS\\nSELECT *\\nFROM database_name.table_name; Here, OR REPLACE updates the view if it already exists and creates it if not. If OR REPLACE is not included but the view already exists, the query will result in an error. DROP VIEW To delete a view, we use the DROP VIEW statement. For example, the following code deletes a view called view_1 from a database called database_1 : DROP VIEW database_1.view_1; Including IF EXISTS supresses an error if the view does not exist. CREATE TABLE To create, edit or delete tables in Athena you need to have additional read/write permissions. If you require access to this functionality, please contact the Analytical Platform team. Creating a table from existing data To create a table in a database from existing data, we use the CREATE TABLE [IF NOT EXISTS] statement. The following code creates a new table called table_1 that selects two columns from a table called table_name in a database called database_name : CREATE TABLE IF NOT EXISTS table_1 AS\\nSELECT column1, column2\\nFROM database_name.table_name; Including IF NOT EXISTS ensures that we only create a table when one does not already exist. If we did not include this but a table did already exist, the query would result in an error. This table will be stored permanently in the database and will not be updated, even if the underlying data used in the CREATE TABLE query changes. Creating a new table To create a new table, we also use the CREATE TABLE statement. Here we must specify the names of the columns in the new table and their data types. INSERT INTO To insert data into a table we use the INSERT INTO statement. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 162}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c55b9a322f979be4377c67b1e6e0f117'}>,\n",
       " <Document: {'content': 'We can insert a single line at a time, multiple lines at a time or can insert data using a query on existing tables. ALTER TABLE We can add, remove and modify columns in an existing table using the ALTER TABLE statement. DROP TABLE To delete a table, we use the DROP TABLE [IF EXISTS] statement. For example, the following code deletes a table called table_1 from a database called database_1 : DROP TABLE IF EXISTS database_1.table_1; Including IF EXISTS supresses an error if the table does not exist. You should always check carefully before deleting a table and should never delete a table that you have not created yourself. Functions SQL supports hundreds of functions, a full list of which can be found in the Presto documentation . This page was last reviewed on 17 January 2020.\\n\\nIt needs to be reviewed again on 17 January 2021\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 17 January 2021\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAmazon Athena - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 163}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cfc176daee056826bfcaed8f65795876'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 164}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Amazon Athena Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds. Source: AWS Accessing Amazon Athena You can access the AWS console at aws.services.analytical-platform.service.justice.gov.uk . You may need to login with GitHub and go through two-factor authentication. To access Athena, select Services , then Athena . This will bring you to the Athena query editor. Here, you can: access and create databases and tables write, run and save queries view and download query outputs to your local computer Athena Access Issue Some users may find when first running a query in the Athena editor, that they get an error similar to: Access denied when writing output to url: s3://<bucket_name>/<file_name.csv>.\\nPlease ensure you are allowed to access the S3 bucket.\\nIf you are encrypting query results with KMS key, please ensure you are allowed to access your KMS key A fix for this error can be found here . Previewing tables In many cases, it may be useful to preview a table to get a better understanding of its structure and contents. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 165}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a3fc07f724226446e4cdc1800d62c3eb'}>,\n",
       " <Document: {'content': 'To preview a table, select a database using the dropdown menu and find the object you want to preview either directly from the list or using the Filter table and views… search bar. You can view the name and type of each column in a table by selecting the blue arrow to the left of the object name. To run a query to preview a table, select the three dots (⋮) to the right of the object name and select Preview table . This will run the following query that selects all columns from the table and returns 10 rows from the output: SELECT *\\nFROM database_name.table_name\\nLIMIT 10; Working with tables You can create, update and delete tables using the code in the SQL section, however, you must also specify the storage format and location of the table in S3. You can also use Rstudio, JupyterLab and the Athena UI. In particular, the Athena UI allows you to create tables directly from data stored in S3 or by using the AWS Glue Crawler. This guidance does not cover use of the AWS Glue Crawler. Using RStudio The Analytical platform hosts a number of analytical coding environments. For those experienced in R , you can query Athena using the RStudio tool. To execute Athena queries, you can use the data engineering team maintained package dbtools . This package uses the Python package pydbtools under the hood and works alongside user IAM policies on the platform. It is also significantly faster than using database drivers provided by Amazon. Follow the setup guidance to get started. The quickstart guidance here provides detailed examples for creating, querying and deleting tables . You can also use Rdbtools . This is an analytical platform community maintained package which has some additional functionality that dbtools does not. It can be used with the understanding that if the package requires fixing or updating, it is the responsibility of those using the package to do so. Using JupyterLab Another analytical tool available on the platform is JupyterLab, which you can use to query Athena data via Python scripts. To do this, install the pydbtools package. This is a wrapper for awswrangler that which presets/defines some of the input parameters to the athena module functions to align with our platform setup. You can perform advanced tasks such as utilising temporary tables, creating and deleting . See the quickstart guide for more details. Using the Athena UI Create a table Create a table using drop-down menus Selecting Create table in the database window brings up a menu list with the following options: Create table From S3 bucket data from AWS Glue Crawler SQL templates CREATE TABLE CREATE TABLE AS SELECT Selecting CREATE TABLE or CREATE TABLE AS SELECT generates an example query that you can edit to create a new table. These example queries are of the same form as those described in the previous section. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 166}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1d3c95be092190dee7834d724ff9341d'}>,\n",
       " <Document: {'content': \"Selecting from S3 bucket data will open a new window that guides you through four steps to create a new table from data in S3: Select which database to store your table in, input a name for your table and input the S3 path to the data from which you want to make the table. The S3 path should be of the form s3://bucket/folder/ . Select the format of the input data. You may also have to complete additional fields depending on the format of the input data. Input the name and data type of each column in the table. When adding a large number of columns, it may be easier to use the Bulk add columns option. Select whether you want to partition the data in the table. This step is optional. Create a table using SQL code If using code to create a table in Athena, you must also specify the storage format and location of the table in S3. If creating a table from existing data, you can specify this information using the WITH statement. CREATE TABLE IF NOT EXISTS database_1.table_1\\nWITH (format = 'format', external_location = 'location') AS\\nSELECT *\\nFROM database_name.table_name; Here, format can be any of the following: ORC PARQUET AVRO JSON TEXTFILE If format is not specified, PARQUET is used by default. Additionally, location is the S3 path where you would like to store the table, for example, s3://alpha-everyone . There are several other parameters that you can specify. Information on these parameters can be found in the Athena documentation . If you are creating a new table, you can specify the storage format using the STORED AS statement and the storage location using the LOCATION statement. CREATE TABLE IF NOT EXISTS table_1 AS (\\ncolumn_name1 column_type1,\\ncolumn_name2 column_type2,\\n...\\n)\\nSTORED AS 'format'\\nLOCATION 'location'; Here, format and location are the same as above. Delete a table To delete a table using the Athena UI, select the three dots (⋮) next to the name of the table you want to delete and select Delete table . Run a query To create and run a new query: Select the plus (+) tab above the editor window. Write your code in the editor window (or copy and paste from another editor). Select Run query . Progress information, including the estimated time elapsed will be displayed in the results window while the query is being processed. Once the query has been completed, the output will be displayed in the results window. Download query outputs When you have run a query, you can download the output to your local computer as a CSV file. To download the output, select the page icon above the results table. SQL resources You might find the SQL Training repository useful. This training is for using SQL (Athena) with the Analytical Platform. SQL (pronounced ‘S-Q-L’ or ‘sequel’) is a programming language used to access and manipulate databases. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 167}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6ea65edcf3bff0aeb65d8717fbf8779c'}>,\n",
       " <Document: {'content': 'There are several versions of SQL that share a common framework but can have different syntax and functionality. The version of SQL used by Amazon Athena is based on Presto 0.217 . This page was last reviewed on 19 August 2021.\\n\\nIt needs to be reviewed again on 19 August 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 19 August 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAmazon Athena - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 168}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '90101a9e1e437c4bc895d441cbb1e6b2'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 169}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Amazon Athena Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds. Source: AWS Accessing Amazon Athena You can access the AWS console at aws.services.analytical-platform.service.justice.gov.uk . You may need to login with GitHub and go through two-factor authentication. To access Athena, select Services , then Athena . This will bring you to the Athena query editor. Here, you can: access and create databases and tables write, run and save queries view and download query outputs to your local computer Athena Access Issue Some users may find when first running a query in the Athena editor, that they get an error similar to: Access denied when writing output to url: s3://<bucket_name>/<file_name.csv>.\\nPlease ensure you are allowed to access the S3 bucket.\\nIf you are encrypting query results with KMS key, please ensure you are allowed to access your KMS key A fix for this error can be found here . Previewing tables In many cases, it may be useful to preview a table to get a better understanding of its structure and contents. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 170}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a3fc07f724226446e4cdc1800d62c3eb'}>,\n",
       " <Document: {'content': 'To preview a table, select a database using the dropdown menu and find the object you want to preview either directly from the list or using the Filter table and views… search bar. You can view the name and type of each column in a table by selecting the blue arrow to the left of the object name. To run a query to preview a table, select the three dots (⋮) to the right of the object name and select Preview table . This will run the following query that selects all columns from the table and returns 10 rows from the output: SELECT *\\nFROM database_name.table_name\\nLIMIT 10; Working with tables You can create, update and delete tables using the code in the SQL section, however, you must also specify the storage format and location of the table in S3. You can also use Rstudio, JupyterLab and the Athena UI. In particular, the Athena UI allows you to create tables directly from data stored in S3 or by using the AWS Glue Crawler. This guidance does not cover use of the AWS Glue Crawler. Using RStudio The Analytical platform hosts a number of analytical coding environments. For those experienced in R , you can query Athena using the RStudio tool. To execute Athena queries, you can use the data engineering team maintained package dbtools . This package uses the Python package pydbtools under the hood and works alongside user IAM policies on the platform. It is also significantly faster than using database drivers provided by Amazon. Follow the setup guidance to get started. The quickstart guidance here provides detailed examples for creating, querying and deleting tables . You can also use Rdbtools . This is an analytical platform community maintained package which has some additional functionality that dbtools does not. It can be used with the understanding that if the package requires fixing or updating, it is the responsibility of those using the package to do so. Using JupyterLab Another analytical tool available on the platform is JupyterLab, which you can use to query Athena data via Python scripts. To do this, install the pydbtools package. This is a wrapper for awswrangler that which presets/defines some of the input parameters to the athena module functions to align with our platform setup. You can perform advanced tasks such as utilising temporary tables, creating and deleting . See the quickstart guide for more details. Using the Athena UI Create a table Create a table using drop-down menus Selecting Create table in the database window brings up a menu list with the following options: Create table From S3 bucket data from AWS Glue Crawler SQL templates CREATE TABLE CREATE TABLE AS SELECT Selecting CREATE TABLE or CREATE TABLE AS SELECT generates an example query that you can edit to create a new table. These example queries are of the same form as those described in the previous section. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 171}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1d3c95be092190dee7834d724ff9341d'}>,\n",
       " <Document: {'content': \"Selecting from S3 bucket data will open a new window that guides you through four steps to create a new table from data in S3: Select which database to store your table in, input a name for your table and input the S3 path to the data from which you want to make the table. The S3 path should be of the form s3://bucket/folder/ . Select the format of the input data. You may also have to complete additional fields depending on the format of the input data. Input the name and data type of each column in the table. When adding a large number of columns, it may be easier to use the Bulk add columns option. Select whether you want to partition the data in the table. This step is optional. Create a table using SQL code If using code to create a table in Athena, you must also specify the storage format and location of the table in S3. If creating a table from existing data, you can specify this information using the WITH statement. CREATE TABLE IF NOT EXISTS database_1.table_1\\nWITH (format = 'format', external_location = 'location') AS\\nSELECT *\\nFROM database_name.table_name; Here, format can be any of the following: ORC PARQUET AVRO JSON TEXTFILE If format is not specified, PARQUET is used by default. Additionally, location is the S3 path where you would like to store the table, for example, s3://alpha-everyone . There are several other parameters that you can specify. Information on these parameters can be found in the Athena documentation . If you are creating a new table, you can specify the storage format using the STORED AS statement and the storage location using the LOCATION statement. CREATE TABLE IF NOT EXISTS table_1 AS (\\ncolumn_name1 column_type1,\\ncolumn_name2 column_type2,\\n...\\n)\\nSTORED AS 'format'\\nLOCATION 'location'; Here, format and location are the same as above. Delete a table To delete a table using the Athena UI, select the three dots (⋮) next to the name of the table you want to delete and select Delete table . Run a query To create and run a new query: Select the plus (+) tab above the editor window. Write your code in the editor window (or copy and paste from another editor). Select Run query . Progress information, including the estimated time elapsed will be displayed in the results window while the query is being processed. Once the query has been completed, the output will be displayed in the results window. Download query outputs When you have run a query, you can download the output to your local computer as a CSV file. To download the output, select the page icon above the results table. SQL resources You might find the SQL Training repository useful. This training is for using SQL (Athena) with the Analytical Platform. SQL (pronounced ‘S-Q-L’ or ‘sequel’) is a programming language used to access and manipulate databases. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 172}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6ea65edcf3bff0aeb65d8717fbf8779c'}>,\n",
       " <Document: {'content': 'There are several versions of SQL that share a common framework but can have different syntax and functionality. The version of SQL used by Amazon Athena is based on Presto 0.217 . This page was last reviewed on 19 August 2021.\\n\\nIt needs to be reviewed again on 19 August 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 19 August 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAmazon Athena - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 173}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '90101a9e1e437c4bc895d441cbb1e6b2'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 174}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Amazon Athena Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds. Source: AWS Accessing Amazon Athena You can access the AWS console at aws.services.analytical-platform.service.justice.gov.uk . You may need to login with GitHub and go through two-factor authentication. To access Athena, select Services , then Athena . This will bring you to the Athena query editor. Here, you can: access and create databases and tables write, run and save queries view and download query outputs to your local computer Athena Access Issue Some users may find when first running a query in the Athena editor, that they get an error similar to: Access denied when writing output to url: s3://<bucket_name>/<file_name.csv>.\\nPlease ensure you are allowed to access the S3 bucket.\\nIf you are encrypting query results with KMS key, please ensure you are allowed to access your KMS key A fix for this error can be found here . Previewing tables In many cases, it may be useful to preview a table to get a better understanding of its structure and contents. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 175}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a3fc07f724226446e4cdc1800d62c3eb'}>,\n",
       " <Document: {'content': 'To preview a table, select a database using the dropdown menu and find the object you want to preview either directly from the list or using the Filter table and views… search bar. You can view the name and type of each column in a table by selecting the blue arrow to the left of the object name. To run a query to preview a table, select the three dots (⋮) to the right of the object name and select Preview table . This will run the following query that selects all columns from the table and returns 10 rows from the output: SELECT *\\nFROM database_name.table_name\\nLIMIT 10; Working with tables You can create, update and delete tables using the code in the SQL section, however, you must also specify the storage format and location of the table in S3. You can also use Rstudio, JupyterLab and the Athena UI. In particular, the Athena UI allows you to create tables directly from data stored in S3 or by using the AWS Glue Crawler. This guidance does not cover use of the AWS Glue Crawler. Using RStudio The Analytical platform hosts a number of analytical coding environments. For those experienced in R , you can query Athena using the RStudio tool. To execute Athena queries, you can use the data engineering team maintained package dbtools . This package uses the Python package pydbtools under the hood and works alongside user IAM policies on the platform. It is also significantly faster than using database drivers provided by Amazon. Follow the setup guidance to get started. The quickstart guidance here provides detailed examples for creating, querying and deleting tables . You can also use Rdbtools . This is an analytical platform community maintained package which has some additional functionality that dbtools does not. It can be used with the understanding that if the package requires fixing or updating, it is the responsibility of those using the package to do so. Using JupyterLab Another analytical tool available on the platform is JupyterLab, which you can use to query Athena data via Python scripts. To do this, install the pydbtools package. This is a wrapper for awswrangler that which presets/defines some of the input parameters to the athena module functions to align with our platform setup. You can perform advanced tasks such as utilising temporary tables, creating and deleting . See the quickstart guide for more details. Using the Athena UI Create a table Create a table using drop-down menus Selecting Create table in the database window brings up a menu list with the following options: Create table From S3 bucket data from AWS Glue Crawler SQL templates CREATE TABLE CREATE TABLE AS SELECT Selecting CREATE TABLE or CREATE TABLE AS SELECT generates an example query that you can edit to create a new table. These example queries are of the same form as those described in the previous section. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 176}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1d3c95be092190dee7834d724ff9341d'}>,\n",
       " <Document: {'content': \"Selecting from S3 bucket data will open a new window that guides you through four steps to create a new table from data in S3: Select which database to store your table in, input a name for your table and input the S3 path to the data from which you want to make the table. The S3 path should be of the form s3://bucket/folder/ . Select the format of the input data. You may also have to complete additional fields depending on the format of the input data. Input the name and data type of each column in the table. When adding a large number of columns, it may be easier to use the Bulk add columns option. Select whether you want to partition the data in the table. This step is optional. Create a table using SQL code If using code to create a table in Athena, you must also specify the storage format and location of the table in S3. If creating a table from existing data, you can specify this information using the WITH statement. CREATE TABLE IF NOT EXISTS database_1.table_1\\nWITH (format = 'format', external_location = 'location') AS\\nSELECT *\\nFROM database_name.table_name; Here, format can be any of the following: ORC PARQUET AVRO JSON TEXTFILE If format is not specified, PARQUET is used by default. Additionally, location is the S3 path where you would like to store the table, for example, s3://alpha-everyone . There are several other parameters that you can specify. Information on these parameters can be found in the Athena documentation . If you are creating a new table, you can specify the storage format using the STORED AS statement and the storage location using the LOCATION statement. CREATE TABLE IF NOT EXISTS table_1 AS (\\ncolumn_name1 column_type1,\\ncolumn_name2 column_type2,\\n...\\n)\\nSTORED AS 'format'\\nLOCATION 'location'; Here, format and location are the same as above. Delete a table To delete a table using the Athena UI, select the three dots (⋮) next to the name of the table you want to delete and select Delete table . Run a query To create and run a new query: Select the plus (+) tab above the editor window. Write your code in the editor window (or copy and paste from another editor). Select Run query . Progress information, including the estimated time elapsed will be displayed in the results window while the query is being processed. Once the query has been completed, the output will be displayed in the results window. Download query outputs When you have run a query, you can download the output to your local computer as a CSV file. To download the output, select the page icon above the results table. SQL resources You might find the SQL Training repository useful. This training is for using SQL (Athena) with the Analytical Platform. SQL (pronounced ‘S-Q-L’ or ‘sequel’) is a programming language used to access and manipulate databases. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 177}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6ea65edcf3bff0aeb65d8717fbf8779c'}>,\n",
       " <Document: {'content': 'There are several versions of SQL that share a common framework but can have different syntax and functionality. The version of SQL used by Amazon Athena is based on Presto 0.217 . This page was last reviewed on 19 August 2021.\\n\\nIt needs to be reviewed again on 19 August 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 19 August 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAmazon Athena - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 178}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '90101a9e1e437c4bc895d441cbb1e6b2'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 179}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Amazon Athena Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds. Source: AWS Accessing Amazon Athena You can access the AWS console at aws.services.analytical-platform.service.justice.gov.uk . You may need to login with GitHub and go through two-factor authentication. To access Athena, select Services , then Athena . This will bring you to the Athena query editor. Here, you can: access and create databases and tables write, run and save queries view and download query outputs to your local computer Athena Access Issue Some users may find when first running a query in the Athena editor, that they get an error similar to: Access denied when writing output to url: s3://<bucket_name>/<file_name.csv>.\\nPlease ensure you are allowed to access the S3 bucket.\\nIf you are encrypting query results with KMS key, please ensure you are allowed to access your KMS key A fix for this error can be found here . Previewing tables In many cases, it may be useful to preview a table to get a better understanding of its structure and contents. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 180}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a3fc07f724226446e4cdc1800d62c3eb'}>,\n",
       " <Document: {'content': 'To preview a table, select a database using the dropdown menu and find the object you want to preview either directly from the list or using the Filter table and views… search bar. You can view the name and type of each column in a table by selecting the blue arrow to the left of the object name. To run a query to preview a table, select the three dots (⋮) to the right of the object name and select Preview table . This will run the following query that selects all columns from the table and returns 10 rows from the output: SELECT *\\nFROM database_name.table_name\\nLIMIT 10; Working with tables You can create, update and delete tables using the code in the SQL section, however, you must also specify the storage format and location of the table in S3. You can also use Rstudio, JupyterLab and the Athena UI. In particular, the Athena UI allows you to create tables directly from data stored in S3 or by using the AWS Glue Crawler. This guidance does not cover use of the AWS Glue Crawler. Using RStudio The Analytical platform hosts a number of analytical coding environments. For those experienced in R , you can query Athena using the RStudio tool. To execute Athena queries, you can use the data engineering team maintained package dbtools . This package uses the Python package pydbtools under the hood and works alongside user IAM policies on the platform. It is also significantly faster than using database drivers provided by Amazon. Follow the setup guidance to get started. The quickstart guidance here provides detailed examples for creating, querying and deleting tables . You can also use Rdbtools . This is an analytical platform community maintained package which has some additional functionality that dbtools does not. It can be used with the understanding that if the package requires fixing or updating, it is the responsibility of those using the package to do so. Using JupyterLab Another analytical tool available on the platform is JupyterLab, which you can use to query Athena data via Python scripts. To do this, install the pydbtools package. This is a wrapper for awswrangler that which presets/defines some of the input parameters to the athena module functions to align with our platform setup. You can perform advanced tasks such as utilising temporary tables, creating and deleting . See the quickstart guide for more details. Using the Athena UI Create a table Create a table using drop-down menus Selecting Create table in the database window brings up a menu list with the following options: Create table From S3 bucket data from AWS Glue Crawler SQL templates CREATE TABLE CREATE TABLE AS SELECT Selecting CREATE TABLE or CREATE TABLE AS SELECT generates an example query that you can edit to create a new table. These example queries are of the same form as those described in the previous section. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 181}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1d3c95be092190dee7834d724ff9341d'}>,\n",
       " <Document: {'content': \"Selecting from S3 bucket data will open a new window that guides you through four steps to create a new table from data in S3: Select which database to store your table in, input a name for your table and input the S3 path to the data from which you want to make the table. The S3 path should be of the form s3://bucket/folder/ . Select the format of the input data. You may also have to complete additional fields depending on the format of the input data. Input the name and data type of each column in the table. When adding a large number of columns, it may be easier to use the Bulk add columns option. Select whether you want to partition the data in the table. This step is optional. Create a table using SQL code If using code to create a table in Athena, you must also specify the storage format and location of the table in S3. If creating a table from existing data, you can specify this information using the WITH statement. CREATE TABLE IF NOT EXISTS database_1.table_1\\nWITH (format = 'format', external_location = 'location') AS\\nSELECT *\\nFROM database_name.table_name; Here, format can be any of the following: ORC PARQUET AVRO JSON TEXTFILE If format is not specified, PARQUET is used by default. Additionally, location is the S3 path where you would like to store the table, for example, s3://alpha-everyone . There are several other parameters that you can specify. Information on these parameters can be found in the Athena documentation . If you are creating a new table, you can specify the storage format using the STORED AS statement and the storage location using the LOCATION statement. CREATE TABLE IF NOT EXISTS table_1 AS (\\ncolumn_name1 column_type1,\\ncolumn_name2 column_type2,\\n...\\n)\\nSTORED AS 'format'\\nLOCATION 'location'; Here, format and location are the same as above. Delete a table To delete a table using the Athena UI, select the three dots (⋮) next to the name of the table you want to delete and select Delete table . Run a query To create and run a new query: Select the plus (+) tab above the editor window. Write your code in the editor window (or copy and paste from another editor). Select Run query . Progress information, including the estimated time elapsed will be displayed in the results window while the query is being processed. Once the query has been completed, the output will be displayed in the results window. Download query outputs When you have run a query, you can download the output to your local computer as a CSV file. To download the output, select the page icon above the results table. SQL resources You might find the SQL Training repository useful. This training is for using SQL (Athena) with the Analytical Platform. SQL (pronounced ‘S-Q-L’ or ‘sequel’) is a programming language used to access and manipulate databases. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 182}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6ea65edcf3bff0aeb65d8717fbf8779c'}>,\n",
       " <Document: {'content': 'There are several versions of SQL that share a common framework but can have different syntax and functionality. The version of SQL used by Amazon Athena is based on Presto 0.217 . This page was last reviewed on 19 August 2021.\\n\\nIt needs to be reviewed again on 19 August 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 19 August 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAmazon Athena - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 183}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '90101a9e1e437c4bc895d441cbb1e6b2'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 184}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Amazon Athena Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds. Source: AWS Accessing Amazon Athena You can access the AWS console at aws.services.analytical-platform.service.justice.gov.uk . You may need to login with GitHub and go through two-factor authentication. To access Athena, select Services , then Athena . This will bring you to the Athena query editor. Here, you can: access and create databases and tables write, run and save queries view and download query outputs to your local computer Athena Access Issue Some users may find when first running a query in the Athena editor, that they get an error similar to: Access denied when writing output to url: s3://<bucket_name>/<file_name.csv>.\\nPlease ensure you are allowed to access the S3 bucket.\\nIf you are encrypting query results with KMS key, please ensure you are allowed to access your KMS key A fix for this error can be found here . Previewing tables In many cases, it may be useful to preview a table to get a better understanding of its structure and contents. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 185}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a3fc07f724226446e4cdc1800d62c3eb'}>,\n",
       " <Document: {'content': 'To preview a table, select a database using the dropdown menu and find the object you want to preview either directly from the list or using the Filter table and views… search bar. You can view the name and type of each column in a table by selecting the blue arrow to the left of the object name. To run a query to preview a table, select the three dots (⋮) to the right of the object name and select Preview table . This will run the following query that selects all columns from the table and returns 10 rows from the output: SELECT *\\nFROM database_name.table_name\\nLIMIT 10; Working with tables You can create, update and delete tables using the code in the SQL section, however, you must also specify the storage format and location of the table in S3. You can also use Rstudio, JupyterLab and the Athena UI. In particular, the Athena UI allows you to create tables directly from data stored in S3 or by using the AWS Glue Crawler. This guidance does not cover use of the AWS Glue Crawler. Using RStudio The Analytical platform hosts a number of analytical coding environments. For those experienced in R , you can query Athena using the RStudio tool. To execute Athena queries, you can use the data engineering team maintained package dbtools . This package uses the Python package pydbtools under the hood and works alongside user IAM policies on the platform. It is also significantly faster than using database drivers provided by Amazon. Follow the setup guidance to get started. The quickstart guidance here provides detailed examples for creating, querying and deleting tables . You can also use Rdbtools . This is an analytical platform community maintained package which has some additional functionality that dbtools does not. It can be used with the understanding that if the package requires fixing or updating, it is the responsibility of those using the package to do so. Using JupyterLab Another analytical tool available on the platform is JupyterLab, which you can use to query Athena data via Python scripts. To do this, install the pydbtools package. This is a wrapper for awswrangler that which presets/defines some of the input parameters to the athena module functions to align with our platform setup. You can perform advanced tasks such as utilising temporary tables, creating and deleting . See the quickstart guide for more details. Using the Athena UI Create a table Create a table using drop-down menus Selecting Create table in the database window brings up a menu list with the following options: Create table From S3 bucket data from AWS Glue Crawler SQL templates CREATE TABLE CREATE TABLE AS SELECT Selecting CREATE TABLE or CREATE TABLE AS SELECT generates an example query that you can edit to create a new table. These example queries are of the same form as those described in the previous section. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 186}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1d3c95be092190dee7834d724ff9341d'}>,\n",
       " <Document: {'content': \"Selecting from S3 bucket data will open a new window that guides you through four steps to create a new table from data in S3: Select which database to store your table in, input a name for your table and input the S3 path to the data from which you want to make the table. The S3 path should be of the form s3://bucket/folder/ . Select the format of the input data. You may also have to complete additional fields depending on the format of the input data. Input the name and data type of each column in the table. When adding a large number of columns, it may be easier to use the Bulk add columns option. Select whether you want to partition the data in the table. This step is optional. Create a table using SQL code If using code to create a table in Athena, you must also specify the storage format and location of the table in S3. If creating a table from existing data, you can specify this information using the WITH statement. CREATE TABLE IF NOT EXISTS database_1.table_1\\nWITH (format = 'format', external_location = 'location') AS\\nSELECT *\\nFROM database_name.table_name; Here, format can be any of the following: ORC PARQUET AVRO JSON TEXTFILE If format is not specified, PARQUET is used by default. Additionally, location is the S3 path where you would like to store the table, for example, s3://alpha-everyone . There are several other parameters that you can specify. Information on these parameters can be found in the Athena documentation . If you are creating a new table, you can specify the storage format using the STORED AS statement and the storage location using the LOCATION statement. CREATE TABLE IF NOT EXISTS table_1 AS (\\ncolumn_name1 column_type1,\\ncolumn_name2 column_type2,\\n...\\n)\\nSTORED AS 'format'\\nLOCATION 'location'; Here, format and location are the same as above. Delete a table To delete a table using the Athena UI, select the three dots (⋮) next to the name of the table you want to delete and select Delete table . Run a query To create and run a new query: Select the plus (+) tab above the editor window. Write your code in the editor window (or copy and paste from another editor). Select Run query . Progress information, including the estimated time elapsed will be displayed in the results window while the query is being processed. Once the query has been completed, the output will be displayed in the results window. Download query outputs When you have run a query, you can download the output to your local computer as a CSV file. To download the output, select the page icon above the results table. SQL resources You might find the SQL Training repository useful. This training is for using SQL (Athena) with the Analytical Platform. SQL (pronounced ‘S-Q-L’ or ‘sequel’) is a programming language used to access and manipulate databases. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 187}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6ea65edcf3bff0aeb65d8717fbf8779c'}>,\n",
       " <Document: {'content': 'There are several versions of SQL that share a common framework but can have different syntax and functionality. The version of SQL used by Amazon Athena is based on Presto 0.217 . This page was last reviewed on 19 August 2021.\\n\\nIt needs to be reviewed again on 19 August 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 19 August 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAmazon Athena - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 188}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '90101a9e1e437c4bc895d441cbb1e6b2'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 189}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Amazon Athena Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds. Source: AWS Accessing Amazon Athena You can access the AWS console at aws.services.analytical-platform.service.justice.gov.uk . You may need to login with GitHub and go through two-factor authentication. To access Athena, select Services , then Athena . This will bring you to the Athena query editor. Here, you can: access and create databases and tables write, run and save queries view and download query outputs to your local computer Athena Access Issue Some users may find when first running a query in the Athena editor, that they get an error similar to: Access denied when writing output to url: s3://<bucket_name>/<file_name.csv>.\\nPlease ensure you are allowed to access the S3 bucket.\\nIf you are encrypting query results with KMS key, please ensure you are allowed to access your KMS key A fix for this error can be found here . Previewing tables In many cases, it may be useful to preview a table to get a better understanding of its structure and contents. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 190}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a3fc07f724226446e4cdc1800d62c3eb'}>,\n",
       " <Document: {'content': 'To preview a table, select a database using the dropdown menu and find the object you want to preview either directly from the list or using the Filter table and views… search bar. You can view the name and type of each column in a table by selecting the blue arrow to the left of the object name. To run a query to preview a table, select the three dots (⋮) to the right of the object name and select Preview table . This will run the following query that selects all columns from the table and returns 10 rows from the output: SELECT *\\nFROM database_name.table_name\\nLIMIT 10; Working with tables You can create, update and delete tables using the code in the SQL section, however, you must also specify the storage format and location of the table in S3. You can also use Rstudio, JupyterLab and the Athena UI. In particular, the Athena UI allows you to create tables directly from data stored in S3 or by using the AWS Glue Crawler. This guidance does not cover use of the AWS Glue Crawler. Using RStudio The Analytical platform hosts a number of analytical coding environments. For those experienced in R , you can query Athena using the RStudio tool. To execute Athena queries, you can use the data engineering team maintained package dbtools . This package uses the Python package pydbtools under the hood and works alongside user IAM policies on the platform. It is also significantly faster than using database drivers provided by Amazon. Follow the setup guidance to get started. The quickstart guidance here provides detailed examples for creating, querying and deleting tables . You can also use Rdbtools . This is an analytical platform community maintained package which has some additional functionality that dbtools does not. It can be used with the understanding that if the package requires fixing or updating, it is the responsibility of those using the package to do so. Using JupyterLab Another analytical tool available on the platform is JupyterLab, which you can use to query Athena data via Python scripts. To do this, install the pydbtools package. This is a wrapper for awswrangler that which presets/defines some of the input parameters to the athena module functions to align with our platform setup. You can perform advanced tasks such as utilising temporary tables, creating and deleting . See the quickstart guide for more details. Using the Athena UI Create a table Create a table using drop-down menus Selecting Create table in the database window brings up a menu list with the following options: Create table From S3 bucket data from AWS Glue Crawler SQL templates CREATE TABLE CREATE TABLE AS SELECT Selecting CREATE TABLE or CREATE TABLE AS SELECT generates an example query that you can edit to create a new table. These example queries are of the same form as those described in the previous section. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 191}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1d3c95be092190dee7834d724ff9341d'}>,\n",
       " <Document: {'content': \"Selecting from S3 bucket data will open a new window that guides you through four steps to create a new table from data in S3: Select which database to store your table in, input a name for your table and input the S3 path to the data from which you want to make the table. The S3 path should be of the form s3://bucket/folder/ . Select the format of the input data. You may also have to complete additional fields depending on the format of the input data. Input the name and data type of each column in the table. When adding a large number of columns, it may be easier to use the Bulk add columns option. Select whether you want to partition the data in the table. This step is optional. Create a table using SQL code If using code to create a table in Athena, you must also specify the storage format and location of the table in S3. If creating a table from existing data, you can specify this information using the WITH statement. CREATE TABLE IF NOT EXISTS database_1.table_1\\nWITH (format = 'format', external_location = 'location') AS\\nSELECT *\\nFROM database_name.table_name; Here, format can be any of the following: ORC PARQUET AVRO JSON TEXTFILE If format is not specified, PARQUET is used by default. Additionally, location is the S3 path where you would like to store the table, for example, s3://alpha-everyone . There are several other parameters that you can specify. Information on these parameters can be found in the Athena documentation . If you are creating a new table, you can specify the storage format using the STORED AS statement and the storage location using the LOCATION statement. CREATE TABLE IF NOT EXISTS table_1 AS (\\ncolumn_name1 column_type1,\\ncolumn_name2 column_type2,\\n...\\n)\\nSTORED AS 'format'\\nLOCATION 'location'; Here, format and location are the same as above. Delete a table To delete a table using the Athena UI, select the three dots (⋮) next to the name of the table you want to delete and select Delete table . Run a query To create and run a new query: Select the plus (+) tab above the editor window. Write your code in the editor window (or copy and paste from another editor). Select Run query . Progress information, including the estimated time elapsed will be displayed in the results window while the query is being processed. Once the query has been completed, the output will be displayed in the results window. Download query outputs When you have run a query, you can download the output to your local computer as a CSV file. To download the output, select the page icon above the results table. SQL resources You might find the SQL Training repository useful. This training is for using SQL (Athena) with the Analytical Platform. SQL (pronounced ‘S-Q-L’ or ‘sequel’) is a programming language used to access and manipulate databases. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 192}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6ea65edcf3bff0aeb65d8717fbf8779c'}>,\n",
       " <Document: {'content': 'There are several versions of SQL that share a common framework but can have different syntax and functionality. The version of SQL used by Amazon Athena is based on Presto 0.217 . This page was last reviewed on 19 August 2021.\\n\\nIt needs to be reviewed again on 19 August 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 19 August 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAmazon Athena - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 193}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '90101a9e1e437c4bc895d441cbb1e6b2'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 194}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Amazon Athena Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds. Source: AWS Accessing Amazon Athena You can access the AWS console at aws.services.analytical-platform.service.justice.gov.uk . You may need to login with GitHub and go through two-factor authentication. To access Athena, select Services , then Athena . This will bring you to the Athena query editor. Here, you can: access and create databases and tables write, run and save queries view and download query outputs to your local computer Athena Access Issue Some users may find when first running a query in the Athena editor, that they get an error similar to: Access denied when writing output to url: s3://<bucket_name>/<file_name.csv>.\\nPlease ensure you are allowed to access the S3 bucket.\\nIf you are encrypting query results with KMS key, please ensure you are allowed to access your KMS key A fix for this error can be found here . Previewing tables In many cases, it may be useful to preview a table to get a better understanding of its structure and contents. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 195}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a3fc07f724226446e4cdc1800d62c3eb'}>,\n",
       " <Document: {'content': 'To preview a table, select a database using the dropdown menu and find the object you want to preview either directly from the list or using the Filter table and views… search bar. You can view the name and type of each column in a table by selecting the blue arrow to the left of the object name. To run a query to preview a table, select the three dots (⋮) to the right of the object name and select Preview table . This will run the following query that selects all columns from the table and returns 10 rows from the output: SELECT *\\nFROM database_name.table_name\\nLIMIT 10; Working with tables You can create, update and delete tables using the code in the SQL section, however, you must also specify the storage format and location of the table in S3. You can also use Rstudio, JupyterLab and the Athena UI. In particular, the Athena UI allows you to create tables directly from data stored in S3 or by using the AWS Glue Crawler. This guidance does not cover use of the AWS Glue Crawler. Using RStudio The Analytical platform hosts a number of analytical coding environments. For those experienced in R , you can query Athena using the RStudio tool. To execute Athena queries, you can use the data engineering team maintained package dbtools . This package uses the Python package pydbtools under the hood and works alongside user IAM policies on the platform. It is also significantly faster than using database drivers provided by Amazon. Follow the setup guidance to get started. The quickstart guidance here provides detailed examples for creating, querying and deleting tables . You can also use Rdbtools . This is an analytical platform community maintained package which has some additional functionality that dbtools does not. It can be used with the understanding that if the package requires fixing or updating, it is the responsibility of those using the package to do so. Using JupyterLab Another analytical tool available on the platform is JupyterLab, which you can use to query Athena data via Python scripts. To do this, install the pydbtools package. This is a wrapper for awswrangler that which presets/defines some of the input parameters to the athena module functions to align with our platform setup. You can perform advanced tasks such as utilising temporary tables, creating and deleting . See the quickstart guide for more details. Using the Athena UI Create a table Create a table using drop-down menus Selecting Create table in the database window brings up a menu list with the following options: Create table From S3 bucket data from AWS Glue Crawler SQL templates CREATE TABLE CREATE TABLE AS SELECT Selecting CREATE TABLE or CREATE TABLE AS SELECT generates an example query that you can edit to create a new table. These example queries are of the same form as those described in the previous section. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 196}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1d3c95be092190dee7834d724ff9341d'}>,\n",
       " <Document: {'content': \"Selecting from S3 bucket data will open a new window that guides you through four steps to create a new table from data in S3: Select which database to store your table in, input a name for your table and input the S3 path to the data from which you want to make the table. The S3 path should be of the form s3://bucket/folder/ . Select the format of the input data. You may also have to complete additional fields depending on the format of the input data. Input the name and data type of each column in the table. When adding a large number of columns, it may be easier to use the Bulk add columns option. Select whether you want to partition the data in the table. This step is optional. Create a table using SQL code If using code to create a table in Athena, you must also specify the storage format and location of the table in S3. If creating a table from existing data, you can specify this information using the WITH statement. CREATE TABLE IF NOT EXISTS database_1.table_1\\nWITH (format = 'format', external_location = 'location') AS\\nSELECT *\\nFROM database_name.table_name; Here, format can be any of the following: ORC PARQUET AVRO JSON TEXTFILE If format is not specified, PARQUET is used by default. Additionally, location is the S3 path where you would like to store the table, for example, s3://alpha-everyone . There are several other parameters that you can specify. Information on these parameters can be found in the Athena documentation . If you are creating a new table, you can specify the storage format using the STORED AS statement and the storage location using the LOCATION statement. CREATE TABLE IF NOT EXISTS table_1 AS (\\ncolumn_name1 column_type1,\\ncolumn_name2 column_type2,\\n...\\n)\\nSTORED AS 'format'\\nLOCATION 'location'; Here, format and location are the same as above. Delete a table To delete a table using the Athena UI, select the three dots (⋮) next to the name of the table you want to delete and select Delete table . Run a query To create and run a new query: Select the plus (+) tab above the editor window. Write your code in the editor window (or copy and paste from another editor). Select Run query . Progress information, including the estimated time elapsed will be displayed in the results window while the query is being processed. Once the query has been completed, the output will be displayed in the results window. Download query outputs When you have run a query, you can download the output to your local computer as a CSV file. To download the output, select the page icon above the results table. SQL resources You might find the SQL Training repository useful. This training is for using SQL (Athena) with the Analytical Platform. SQL (pronounced ‘S-Q-L’ or ‘sequel’) is a programming language used to access and manipulate databases. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 197}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6ea65edcf3bff0aeb65d8717fbf8779c'}>,\n",
       " <Document: {'content': 'There are several versions of SQL that share a common framework but can have different syntax and functionality. The version of SQL used by Amazon Athena is based on Presto 0.217 . This page was last reviewed on 19 August 2021.\\n\\nIt needs to be reviewed again on 19 August 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 19 August 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\ndbtools - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 198}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'aee2e22aa596b93072800ecf65c11656'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 199}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Querying databases from the AP The Data Engineering Team have developed some simple packages to allow you to query our databases using R and Python on the Analytical Platform. pydbtools is a python module, while dbtools is an R package that uses pydbtools under the hood. You can use these packages to execute SQL queries on Athena databases, bringing the results into your environment as a DataFrame to do further analysis. While these are the only “officially” supported packages, there is also the community built Rdbtools which has additional functionality. It can be used with the understanding that if the package requires fixing or updating, it is the responsibility of those using the package to do so. R - dbtools The README in the dbtools repository gives details on how to install and use the package with R. This package is maintained by the data engineering team. R - Rdbtools The README provides details. This package is maintained by the analytical platform user community. Python - pydbtools The README in the pydbtools repository gives details on how to install and use the package with Python. This package is maintained by the data engineering team. This page was last reviewed on 5 May 2021.\\n\\nIt needs to be reviewed again on 5 May 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 5 May 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 200}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3c811dc35733a60c79763f6227ac3883'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\ndbtools - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 201}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c6cf31773b6883b0f610ba316acef335'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 202}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Querying databases from the AP The Data Engineering Team have developed some simple packages to allow you to query our databases using R and Python on the Analytical Platform. pydbtools is a python module, while dbtools is an R package that uses pydbtools under the hood. You can use these packages to execute SQL queries on Athena databases, bringing the results into your environment as a DataFrame to do further analysis. While these are the only “officially” supported packages, there is also the community built Rdbtools which has additional functionality. It can be used with the understanding that if the package requires fixing or updating, it is the responsibility of those using the package to do so. R - dbtools The README in the dbtools repository gives details on how to install and use the package with R. This package is maintained by the data engineering team. R - Rdbtools The README provides details. This package is maintained by the analytical platform user community. Python - pydbtools The README in the pydbtools repository gives details on how to install and use the package with Python. This package is maintained by the data engineering team. This page was last reviewed on 5 May 2021.\\n\\nIt needs to be reviewed again on 5 May 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 5 May 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 203}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3c811dc35733a60c79763f6227ac3883'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\ndbtools - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 204}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c6cf31773b6883b0f610ba316acef335'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 205}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Querying databases from the AP The Data Engineering Team have developed some simple packages to allow you to query our databases using R and Python on the Analytical Platform. pydbtools is a python module, while dbtools is an R package that uses pydbtools under the hood. You can use these packages to execute SQL queries on Athena databases, bringing the results into your environment as a DataFrame to do further analysis. While these are the only “officially” supported packages, there is also the community built Rdbtools which has additional functionality. It can be used with the understanding that if the package requires fixing or updating, it is the responsibility of those using the package to do so. R - dbtools The README in the dbtools repository gives details on how to install and use the package with R. This package is maintained by the data engineering team. R - Rdbtools The README provides details. This package is maintained by the analytical platform user community. Python - pydbtools The README in the pydbtools repository gives details on how to install and use the package with Python. This package is maintained by the data engineering team. This page was last reviewed on 5 May 2021.\\n\\nIt needs to be reviewed again on 5 May 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 5 May 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 206}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3c811dc35733a60c79763f6227ac3883'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\ndbtools - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 207}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c6cf31773b6883b0f610ba316acef335'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 208}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Querying databases from the AP The Data Engineering Team have developed some simple packages to allow you to query our databases using R and Python on the Analytical Platform. pydbtools is a python module, while dbtools is an R package that uses pydbtools under the hood. You can use these packages to execute SQL queries on Athena databases, bringing the results into your environment as a DataFrame to do further analysis. While these are the only “officially” supported packages, there is also the community built Rdbtools which has additional functionality. It can be used with the understanding that if the package requires fixing or updating, it is the responsibility of those using the package to do so. R - dbtools The README in the dbtools repository gives details on how to install and use the package with R. This package is maintained by the data engineering team. R - Rdbtools The README provides details. This package is maintained by the analytical platform user community. Python - pydbtools The README in the pydbtools repository gives details on how to install and use the package with Python. This package is maintained by the data engineering team. This page was last reviewed on 5 May 2021.\\n\\nIt needs to be reviewed again on 5 May 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 5 May 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 209}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3c811dc35733a60c79763f6227ac3883'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDatabases - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 210}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1390a15cd7fe7ec210fb8e564cb9f7e3'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 211}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Databases The data engineering team curate and maintain databases on the Analytical Platform that can be made accessible to users. All of our current databases are deployed and accessible using Amazon Athena. AP users can query these databases via the Amazon Athena SQL workbench (see the “Accessing Amazon Athena” section of the docs on how to access it). Databases can also be queried in Python using pydbtools . They can be queried in R using dbtools or Rdbtools , though note the latter is maintained by the analytical platform user community and is therefore not supported by the data engineering team. To see what databases are available and how to request access, see the README in the data-engineering-database-access repository on GitHub. For each database, take careful note of any guidance documents to ensure you understand how to use the data correctly. Most curated databases on the AP use one of two data models. Snapshot database models attach specific points in time to each record. Queries should specify the same snapshot literal for all data. Temporal database models attach a start and end timestamp to each record indicating the period of its validity with respect to the source database. Note, you must be a member of the moj-analytical-services GitHub organisation to access the repository. To find out about the metadata of the curated databases (without making a database access request), you can use the data discovery tool . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 212}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '10ae03c061179aa1f258725e6920ab'}>,\n",
       " <Document: {'content': \"User-maintained databases In addition to curated data sources provided by the Data Engineering team, Platform users may wish to use the same infrastructure to create their own databases. While this requires slightly more maintenance, it gives you access to vastly more processing power for manipulating data than that available through R and Python on the Platform. Databases are created by writing data in a well-defined schema to an S3 location, and providing a register of the metadata to an AWS service. Access is controlled using the permissions of the S3 location of the underlying data. There is guidance on helper tools for creating your own Amazon Athena database here . Using mojap_*_timestamp filters In order to enable reproducible data analysis, some curated data sources on the Analytical Platform employ a temporal record-keeping system that preserves prior states of the data. To do this it simply labels every record from the source system with the following columns. These columns are created on the Platform - they do not exist in the source databases: mojap_start_datetime - The time at which extraction code found this record and loaded it on to the Platform. mojap_end_datetime - The time at which extraction code found this record to have been edited and replaced it with a newer version on the Platform. mojap_current_record or mojap_latest_record - A boolean label for whether this record (on the Platform) is the most recent version. mojap_end_datetime has the same value as the mojap_start_datetime of the record that succeeded it. Where a record is the most recent version, mojap_end_datetime takes a placeholder value of 2999-01-01 . These times are UTC/GMT all year round (as is the timezone on all Platform instances). In this way, as records are edited in the source database, they are recorded on the Platform without overwriting historical versions of the record, enabling analysis to be run reproducibly even as records are modified in the source systems. Note that the Platform typically extracts data from source systems shortly after midnight each day. Therefore it will only record table states found at that time - record states in source systems are only uploaded if they were present during an extraction. The Platform does not record every edit to a source database. All queries on curated Platform data must filter for the correct temporal table state. Otherwise you will duplicate records. For Platform curated data sources the unique key of a record becomes {source_db_uk AND (mojap_start_datetime OR mojap_end_datetime)} . In almost all cases, you should use a specific timestamp in a filter of the form: ... mojap_start_datetime <= timestamp'YYYY-MM-DD'\\nAND mojap_end_datetime > timestamp'YYYY-MM-DD' Don’t be tempted to use the SQL BETWEEN construct - it is inclusive of both timestamps and therefore could fail to return the correct results. mojap_current_record or mojap_latest_record should only be used for keeping scheduled data products up-to-date, as they do not facilitate reproducible analysis. All three of the temporal record columns are strictly not-null in the Platform data model. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 213}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1ed6ea96057f6ed95a699065b50c35dc'}>,\n",
       " <Document: {'content': \"Joining temporal-schema tables When joining tables and using mojap_ _datetime columns you must be careful whether you filter the query using the WHERE clause, or the JOIN condition.\\nNote that in any (in)equality test, SQL engines will evaluate NULL values as False . This means when filtering on any table join other than inner joins, there is a possibility of unintentionally dropping the unmatched records due to filtering for a mojap_*_datetime table state in the wrong place. See below for some specimen examples of how to apply these filters correctly. In the following, read <X.state_filter> as shorthand for X.mojap_start_datetime <= timestamp'YYYY-MM-DD' AND X.mojap_start_datetime > timestamp'YYYY-MM-DD' . If you are producing dynamic data products and need live data, read X.mojap_current_record or X.mojap_latest_record - whichever is used in the data source. Subquery solution The most fail-proof solution (albeit ugly) is to use subqueries to explicitly filter the tables before they are invoked in the main query. SELECT <…>\\nFROM (select * from table_A where <state_filter>) AS A\\n<any> JOIN (select * from table_B where <state_filter>) AS B\\non A.key = B.key For complex queries it might be worth checking to see if this has a performance penalty compared to the alternative solutions below. LEFT JOIN The preserved ‘left side’ table can be filtered in the WHERE clause, but the other table must be filtered in the ON condition. SELECT <…>\\nFROM table_A AS A\\nLEFT JOIN table_B AS B\\nON A.key = B.key\\nAND <B.state_filter>\\nWHERE <A.state_filter> Alternatively, you can explicitly handle the NULL pitfall. If you are unfamiliar with the difference in how SQL engines handle a WHERE and ON filter, comparing these two options might help you understand what is happening. This query is also easier to edit to return only the unmatched records from table A. SELECT <…>\\nFROM table_A AS A\\nLEFT JOIN table_B AS B\\nON A.key = B.key\\nWHERE <A.state_filter>\\nAND (<B.state_filter> OR B.mojap_start_datetime is null) RIGHT JOIN Just like the left join, but with the tables reversed (obviously). SELECT <…>\\nFROM table_A AS A\\nRIGHT JOIN table_B AS B\\nON A.key = B.key\\nAND <A.state_filter>\\nWHERE <B.state_filter> -- Alternatively\\nSELECT <…>\\nFROM table_A AS A\\nRIGHT JOIN table_B AS B\\nON A.key = B.key\\nWHERE <B.state_filter>\\nAND (<A.state_filter> OR A.mojap_start_datetime is null) INNER JOIN Because no unmatched records are included in the results of an inner join, it does not matter where the state filter is. Note that by putting both state filters into the WHERE clause, any join type results in an inner join, due to unmatched records being dropped. SELECT <…>\\nFROM table_A AS A\\nINNER JOIN table_B AS B\\nON A.key = B.key\\nWHERE <A.state_filter>\\nAND <B.state_filter> -- Equivalently\\nSELECT <…>\\nFROM table_A AS A\\nINNER JOIN table_B AS B\\nON A.key = B.key\\nAND <A.state_filter>\\nAND <B.state_filter> *SQL engines can often be configured to treat NULL as a literal value, but it is unusual for this to be useful. This page was last reviewed on 17 January 2020.\\n\\n\", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 214}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '207f14c7a7ff98076c9437f7d1842f9b'}>,\n",
       " <Document: {'content': 'It needs to be reviewed again on 17 January 2021\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 17 January 2021\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDatabases - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 215}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '170e2e6a05a48b1b9987e0778e68b04d'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 216}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Databases The data engineering team curate and maintain databases on the Analytical Platform that can be made accessible to users. All of our current databases are deployed and accessible using Amazon Athena. AP users can query these databases via the Amazon Athena SQL workbench (see the “Accessing Amazon Athena” section of the docs on how to access it). Databases can also be queried in Python using pydbtools . They can be queried in R using dbtools or Rdbtools , though note the latter is maintained by the analytical platform user community and is therefore not supported by the data engineering team. To see what databases are available and how to request access, see the README in the data-engineering-database-access repository on GitHub. For each database, take careful note of any guidance documents to ensure you understand how to use the data correctly. Most curated databases on the AP use one of two data models. Snapshot database models attach specific points in time to each record. Queries should specify the same snapshot literal for all data. Temporal database models attach a start and end timestamp to each record indicating the period of its validity with respect to the source database. Note, you must be a member of the moj-analytical-services GitHub organisation to access the repository. To find out about the metadata of the curated databases (without making a database access request), you can use the data discovery tool . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 217}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '10ae03c061179aa1f258725e6920ab'}>,\n",
       " <Document: {'content': \"User-maintained databases In addition to curated data sources provided by the Data Engineering team, Platform users may wish to use the same infrastructure to create their own databases. While this requires slightly more maintenance, it gives you access to vastly more processing power for manipulating data than that available through R and Python on the Platform. Databases are created by writing data in a well-defined schema to an S3 location, and providing a register of the metadata to an AWS service. Access is controlled using the permissions of the S3 location of the underlying data. There is guidance on helper tools for creating your own Amazon Athena database here . Using mojap_*_timestamp filters In order to enable reproducible data analysis, some curated data sources on the Analytical Platform employ a temporal record-keeping system that preserves prior states of the data. To do this it simply labels every record from the source system with the following columns. These columns are created on the Platform - they do not exist in the source databases: mojap_start_datetime - The time at which extraction code found this record and loaded it on to the Platform. mojap_end_datetime - The time at which extraction code found this record to have been edited and replaced it with a newer version on the Platform. mojap_current_record or mojap_latest_record - A boolean label for whether this record (on the Platform) is the most recent version. mojap_end_datetime has the same value as the mojap_start_datetime of the record that succeeded it. Where a record is the most recent version, mojap_end_datetime takes a placeholder value of 2999-01-01 . These times are UTC/GMT all year round (as is the timezone on all Platform instances). In this way, as records are edited in the source database, they are recorded on the Platform without overwriting historical versions of the record, enabling analysis to be run reproducibly even as records are modified in the source systems. Note that the Platform typically extracts data from source systems shortly after midnight each day. Therefore it will only record table states found at that time - record states in source systems are only uploaded if they were present during an extraction. The Platform does not record every edit to a source database. All queries on curated Platform data must filter for the correct temporal table state. Otherwise you will duplicate records. For Platform curated data sources the unique key of a record becomes {source_db_uk AND (mojap_start_datetime OR mojap_end_datetime)} . In almost all cases, you should use a specific timestamp in a filter of the form: ... mojap_start_datetime <= timestamp'YYYY-MM-DD'\\nAND mojap_end_datetime > timestamp'YYYY-MM-DD' Don’t be tempted to use the SQL BETWEEN construct - it is inclusive of both timestamps and therefore could fail to return the correct results. mojap_current_record or mojap_latest_record should only be used for keeping scheduled data products up-to-date, as they do not facilitate reproducible analysis. All three of the temporal record columns are strictly not-null in the Platform data model. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 218}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1ed6ea96057f6ed95a699065b50c35dc'}>,\n",
       " <Document: {'content': \"Joining temporal-schema tables When joining tables and using mojap_ _datetime columns you must be careful whether you filter the query using the WHERE clause, or the JOIN condition.\\nNote that in any (in)equality test, SQL engines will evaluate NULL values as False . This means when filtering on any table join other than inner joins, there is a possibility of unintentionally dropping the unmatched records due to filtering for a mojap_*_datetime table state in the wrong place. See below for some specimen examples of how to apply these filters correctly. In the following, read <X.state_filter> as shorthand for X.mojap_start_datetime <= timestamp'YYYY-MM-DD' AND X.mojap_start_datetime > timestamp'YYYY-MM-DD' . If you are producing dynamic data products and need live data, read X.mojap_current_record or X.mojap_latest_record - whichever is used in the data source. Subquery solution The most fail-proof solution (albeit ugly) is to use subqueries to explicitly filter the tables before they are invoked in the main query. SELECT <…>\\nFROM (select * from table_A where <state_filter>) AS A\\n<any> JOIN (select * from table_B where <state_filter>) AS B\\non A.key = B.key For complex queries it might be worth checking to see if this has a performance penalty compared to the alternative solutions below. LEFT JOIN The preserved ‘left side’ table can be filtered in the WHERE clause, but the other table must be filtered in the ON condition. SELECT <…>\\nFROM table_A AS A\\nLEFT JOIN table_B AS B\\nON A.key = B.key\\nAND <B.state_filter>\\nWHERE <A.state_filter> Alternatively, you can explicitly handle the NULL pitfall. If you are unfamiliar with the difference in how SQL engines handle a WHERE and ON filter, comparing these two options might help you understand what is happening. This query is also easier to edit to return only the unmatched records from table A. SELECT <…>\\nFROM table_A AS A\\nLEFT JOIN table_B AS B\\nON A.key = B.key\\nWHERE <A.state_filter>\\nAND (<B.state_filter> OR B.mojap_start_datetime is null) RIGHT JOIN Just like the left join, but with the tables reversed (obviously). SELECT <…>\\nFROM table_A AS A\\nRIGHT JOIN table_B AS B\\nON A.key = B.key\\nAND <A.state_filter>\\nWHERE <B.state_filter> -- Alternatively\\nSELECT <…>\\nFROM table_A AS A\\nRIGHT JOIN table_B AS B\\nON A.key = B.key\\nWHERE <B.state_filter>\\nAND (<A.state_filter> OR A.mojap_start_datetime is null) INNER JOIN Because no unmatched records are included in the results of an inner join, it does not matter where the state filter is. Note that by putting both state filters into the WHERE clause, any join type results in an inner join, due to unmatched records being dropped. SELECT <…>\\nFROM table_A AS A\\nINNER JOIN table_B AS B\\nON A.key = B.key\\nWHERE <A.state_filter>\\nAND <B.state_filter> -- Equivalently\\nSELECT <…>\\nFROM table_A AS A\\nINNER JOIN table_B AS B\\nON A.key = B.key\\nAND <A.state_filter>\\nAND <B.state_filter> *SQL engines can often be configured to treat NULL as a literal value, but it is unusual for this to be useful. This page was last reviewed on 17 January 2020.\\n\\n\", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 219}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '207f14c7a7ff98076c9437f7d1842f9b'}>,\n",
       " <Document: {'content': 'It needs to be reviewed again on 17 January 2021\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 17 January 2021\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDatabases - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 220}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '170e2e6a05a48b1b9987e0778e68b04d'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 221}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Databases The data engineering team curate and maintain databases on the Analytical Platform that can be made accessible to users. All of our current databases are deployed and accessible using Amazon Athena. AP users can query these databases via the Amazon Athena SQL workbench (see the “Accessing Amazon Athena” section of the docs on how to access it). Databases can also be queried in Python using pydbtools . They can be queried in R using dbtools or Rdbtools , though note the latter is maintained by the analytical platform user community and is therefore not supported by the data engineering team. To see what databases are available and how to request access, see the README in the data-engineering-database-access repository on GitHub. For each database, take careful note of any guidance documents to ensure you understand how to use the data correctly. Most curated databases on the AP use one of two data models. Snapshot database models attach specific points in time to each record. Queries should specify the same snapshot literal for all data. Temporal database models attach a start and end timestamp to each record indicating the period of its validity with respect to the source database. Note, you must be a member of the moj-analytical-services GitHub organisation to access the repository. To find out about the metadata of the curated databases (without making a database access request), you can use the data discovery tool . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 222}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '10ae03c061179aa1f258725e6920ab'}>,\n",
       " <Document: {'content': \"User-maintained databases In addition to curated data sources provided by the Data Engineering team, Platform users may wish to use the same infrastructure to create their own databases. While this requires slightly more maintenance, it gives you access to vastly more processing power for manipulating data than that available through R and Python on the Platform. Databases are created by writing data in a well-defined schema to an S3 location, and providing a register of the metadata to an AWS service. Access is controlled using the permissions of the S3 location of the underlying data. There is guidance on helper tools for creating your own Amazon Athena database here . Using mojap_*_timestamp filters In order to enable reproducible data analysis, some curated data sources on the Analytical Platform employ a temporal record-keeping system that preserves prior states of the data. To do this it simply labels every record from the source system with the following columns. These columns are created on the Platform - they do not exist in the source databases: mojap_start_datetime - The time at which extraction code found this record and loaded it on to the Platform. mojap_end_datetime - The time at which extraction code found this record to have been edited and replaced it with a newer version on the Platform. mojap_current_record or mojap_latest_record - A boolean label for whether this record (on the Platform) is the most recent version. mojap_end_datetime has the same value as the mojap_start_datetime of the record that succeeded it. Where a record is the most recent version, mojap_end_datetime takes a placeholder value of 2999-01-01 . These times are UTC/GMT all year round (as is the timezone on all Platform instances). In this way, as records are edited in the source database, they are recorded on the Platform without overwriting historical versions of the record, enabling analysis to be run reproducibly even as records are modified in the source systems. Note that the Platform typically extracts data from source systems shortly after midnight each day. Therefore it will only record table states found at that time - record states in source systems are only uploaded if they were present during an extraction. The Platform does not record every edit to a source database. All queries on curated Platform data must filter for the correct temporal table state. Otherwise you will duplicate records. For Platform curated data sources the unique key of a record becomes {source_db_uk AND (mojap_start_datetime OR mojap_end_datetime)} . In almost all cases, you should use a specific timestamp in a filter of the form: ... mojap_start_datetime <= timestamp'YYYY-MM-DD'\\nAND mojap_end_datetime > timestamp'YYYY-MM-DD' Don’t be tempted to use the SQL BETWEEN construct - it is inclusive of both timestamps and therefore could fail to return the correct results. mojap_current_record or mojap_latest_record should only be used for keeping scheduled data products up-to-date, as they do not facilitate reproducible analysis. All three of the temporal record columns are strictly not-null in the Platform data model. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 223}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1ed6ea96057f6ed95a699065b50c35dc'}>,\n",
       " <Document: {'content': \"Joining temporal-schema tables When joining tables and using mojap_ _datetime columns you must be careful whether you filter the query using the WHERE clause, or the JOIN condition.\\nNote that in any (in)equality test, SQL engines will evaluate NULL values as False . This means when filtering on any table join other than inner joins, there is a possibility of unintentionally dropping the unmatched records due to filtering for a mojap_*_datetime table state in the wrong place. See below for some specimen examples of how to apply these filters correctly. In the following, read <X.state_filter> as shorthand for X.mojap_start_datetime <= timestamp'YYYY-MM-DD' AND X.mojap_start_datetime > timestamp'YYYY-MM-DD' . If you are producing dynamic data products and need live data, read X.mojap_current_record or X.mojap_latest_record - whichever is used in the data source. Subquery solution The most fail-proof solution (albeit ugly) is to use subqueries to explicitly filter the tables before they are invoked in the main query. SELECT <…>\\nFROM (select * from table_A where <state_filter>) AS A\\n<any> JOIN (select * from table_B where <state_filter>) AS B\\non A.key = B.key For complex queries it might be worth checking to see if this has a performance penalty compared to the alternative solutions below. LEFT JOIN The preserved ‘left side’ table can be filtered in the WHERE clause, but the other table must be filtered in the ON condition. SELECT <…>\\nFROM table_A AS A\\nLEFT JOIN table_B AS B\\nON A.key = B.key\\nAND <B.state_filter>\\nWHERE <A.state_filter> Alternatively, you can explicitly handle the NULL pitfall. If you are unfamiliar with the difference in how SQL engines handle a WHERE and ON filter, comparing these two options might help you understand what is happening. This query is also easier to edit to return only the unmatched records from table A. SELECT <…>\\nFROM table_A AS A\\nLEFT JOIN table_B AS B\\nON A.key = B.key\\nWHERE <A.state_filter>\\nAND (<B.state_filter> OR B.mojap_start_datetime is null) RIGHT JOIN Just like the left join, but with the tables reversed (obviously). SELECT <…>\\nFROM table_A AS A\\nRIGHT JOIN table_B AS B\\nON A.key = B.key\\nAND <A.state_filter>\\nWHERE <B.state_filter> -- Alternatively\\nSELECT <…>\\nFROM table_A AS A\\nRIGHT JOIN table_B AS B\\nON A.key = B.key\\nWHERE <B.state_filter>\\nAND (<A.state_filter> OR A.mojap_start_datetime is null) INNER JOIN Because no unmatched records are included in the results of an inner join, it does not matter where the state filter is. Note that by putting both state filters into the WHERE clause, any join type results in an inner join, due to unmatched records being dropped. SELECT <…>\\nFROM table_A AS A\\nINNER JOIN table_B AS B\\nON A.key = B.key\\nWHERE <A.state_filter>\\nAND <B.state_filter> -- Equivalently\\nSELECT <…>\\nFROM table_A AS A\\nINNER JOIN table_B AS B\\nON A.key = B.key\\nAND <A.state_filter>\\nAND <B.state_filter> *SQL engines can often be configured to treat NULL as a literal value, but it is unusual for this to be useful. This page was last reviewed on 17 January 2020.\\n\\n\", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 224}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '207f14c7a7ff98076c9437f7d1842f9b'}>,\n",
       " <Document: {'content': 'It needs to be reviewed again on 17 January 2021\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 17 January 2021\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDatabases - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 225}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '170e2e6a05a48b1b9987e0778e68b04d'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 226}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Databases The data engineering team curate and maintain databases on the Analytical Platform that can be made accessible to users. All of our current databases are deployed and accessible using Amazon Athena. AP users can query these databases via the Amazon Athena SQL workbench (see the “Accessing Amazon Athena” section of the docs on how to access it). Databases can also be queried in Python using pydbtools . They can be queried in R using dbtools or Rdbtools , though note the latter is maintained by the analytical platform user community and is therefore not supported by the data engineering team. To see what databases are available and how to request access, see the README in the data-engineering-database-access repository on GitHub. For each database, take careful note of any guidance documents to ensure you understand how to use the data correctly. Most curated databases on the AP use one of two data models. Snapshot database models attach specific points in time to each record. Queries should specify the same snapshot literal for all data. Temporal database models attach a start and end timestamp to each record indicating the period of its validity with respect to the source database. Note, you must be a member of the moj-analytical-services GitHub organisation to access the repository. To find out about the metadata of the curated databases (without making a database access request), you can use the data discovery tool . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 227}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '10ae03c061179aa1f258725e6920ab'}>,\n",
       " <Document: {'content': \"User-maintained databases In addition to curated data sources provided by the Data Engineering team, Platform users may wish to use the same infrastructure to create their own databases. While this requires slightly more maintenance, it gives you access to vastly more processing power for manipulating data than that available through R and Python on the Platform. Databases are created by writing data in a well-defined schema to an S3 location, and providing a register of the metadata to an AWS service. Access is controlled using the permissions of the S3 location of the underlying data. There is guidance on helper tools for creating your own Amazon Athena database here . Using mojap_*_timestamp filters In order to enable reproducible data analysis, some curated data sources on the Analytical Platform employ a temporal record-keeping system that preserves prior states of the data. To do this it simply labels every record from the source system with the following columns. These columns are created on the Platform - they do not exist in the source databases: mojap_start_datetime - The time at which extraction code found this record and loaded it on to the Platform. mojap_end_datetime - The time at which extraction code found this record to have been edited and replaced it with a newer version on the Platform. mojap_current_record or mojap_latest_record - A boolean label for whether this record (on the Platform) is the most recent version. mojap_end_datetime has the same value as the mojap_start_datetime of the record that succeeded it. Where a record is the most recent version, mojap_end_datetime takes a placeholder value of 2999-01-01 . These times are UTC/GMT all year round (as is the timezone on all Platform instances). In this way, as records are edited in the source database, they are recorded on the Platform without overwriting historical versions of the record, enabling analysis to be run reproducibly even as records are modified in the source systems. Note that the Platform typically extracts data from source systems shortly after midnight each day. Therefore it will only record table states found at that time - record states in source systems are only uploaded if they were present during an extraction. The Platform does not record every edit to a source database. All queries on curated Platform data must filter for the correct temporal table state. Otherwise you will duplicate records. For Platform curated data sources the unique key of a record becomes {source_db_uk AND (mojap_start_datetime OR mojap_end_datetime)} . In almost all cases, you should use a specific timestamp in a filter of the form: ... mojap_start_datetime <= timestamp'YYYY-MM-DD'\\nAND mojap_end_datetime > timestamp'YYYY-MM-DD' Don’t be tempted to use the SQL BETWEEN construct - it is inclusive of both timestamps and therefore could fail to return the correct results. mojap_current_record or mojap_latest_record should only be used for keeping scheduled data products up-to-date, as they do not facilitate reproducible analysis. All three of the temporal record columns are strictly not-null in the Platform data model. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 228}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1ed6ea96057f6ed95a699065b50c35dc'}>,\n",
       " <Document: {'content': \"Joining temporal-schema tables When joining tables and using mojap_ _datetime columns you must be careful whether you filter the query using the WHERE clause, or the JOIN condition.\\nNote that in any (in)equality test, SQL engines will evaluate NULL values as False . This means when filtering on any table join other than inner joins, there is a possibility of unintentionally dropping the unmatched records due to filtering for a mojap_*_datetime table state in the wrong place. See below for some specimen examples of how to apply these filters correctly. In the following, read <X.state_filter> as shorthand for X.mojap_start_datetime <= timestamp'YYYY-MM-DD' AND X.mojap_start_datetime > timestamp'YYYY-MM-DD' . If you are producing dynamic data products and need live data, read X.mojap_current_record or X.mojap_latest_record - whichever is used in the data source. Subquery solution The most fail-proof solution (albeit ugly) is to use subqueries to explicitly filter the tables before they are invoked in the main query. SELECT <…>\\nFROM (select * from table_A where <state_filter>) AS A\\n<any> JOIN (select * from table_B where <state_filter>) AS B\\non A.key = B.key For complex queries it might be worth checking to see if this has a performance penalty compared to the alternative solutions below. LEFT JOIN The preserved ‘left side’ table can be filtered in the WHERE clause, but the other table must be filtered in the ON condition. SELECT <…>\\nFROM table_A AS A\\nLEFT JOIN table_B AS B\\nON A.key = B.key\\nAND <B.state_filter>\\nWHERE <A.state_filter> Alternatively, you can explicitly handle the NULL pitfall. If you are unfamiliar with the difference in how SQL engines handle a WHERE and ON filter, comparing these two options might help you understand what is happening. This query is also easier to edit to return only the unmatched records from table A. SELECT <…>\\nFROM table_A AS A\\nLEFT JOIN table_B AS B\\nON A.key = B.key\\nWHERE <A.state_filter>\\nAND (<B.state_filter> OR B.mojap_start_datetime is null) RIGHT JOIN Just like the left join, but with the tables reversed (obviously). SELECT <…>\\nFROM table_A AS A\\nRIGHT JOIN table_B AS B\\nON A.key = B.key\\nAND <A.state_filter>\\nWHERE <B.state_filter> -- Alternatively\\nSELECT <…>\\nFROM table_A AS A\\nRIGHT JOIN table_B AS B\\nON A.key = B.key\\nWHERE <B.state_filter>\\nAND (<A.state_filter> OR A.mojap_start_datetime is null) INNER JOIN Because no unmatched records are included in the results of an inner join, it does not matter where the state filter is. Note that by putting both state filters into the WHERE clause, any join type results in an inner join, due to unmatched records being dropped. SELECT <…>\\nFROM table_A AS A\\nINNER JOIN table_B AS B\\nON A.key = B.key\\nWHERE <A.state_filter>\\nAND <B.state_filter> -- Equivalently\\nSELECT <…>\\nFROM table_A AS A\\nINNER JOIN table_B AS B\\nON A.key = B.key\\nAND <A.state_filter>\\nAND <B.state_filter> *SQL engines can often be configured to treat NULL as a literal value, but it is unusual for this to be useful. This page was last reviewed on 17 January 2020.\\n\\n\", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 229}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '207f14c7a7ff98076c9437f7d1842f9b'}>,\n",
       " <Document: {'content': 'It needs to be reviewed again on 17 January 2021\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 17 January 2021\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDatabases for analysis and apps - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 230}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f9ddde58314e873688fe6544521481f4'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 231}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Using databases and data for apps This section covers how and when to use different tools to query our databases on the Analytical Platform and what to consider when giving data to a deployed app. This section assumes you are getting data from databases that are already created on the Analytical Platform. If you need to upload data to the platform see the sections on S3 and information governance . Guidance on using our databases for analysis We use pydbtools (for Python) and dbtools (for R) on the Analytical Platform to query our databases. You can find out more information about them in the dbtools section . Both these tools use an SQL Engine called Athena to query large datasets in S3 just like a normal relational database system. When you want to manipulate or query the databases on the Analytical Platform consider the following: A good general rule is first to use SQL (via dbtools or pydbtools ) for aggregation, joins, and filtering of your data, and then to bring the resulting table into RStudio or JupyterLab for more nuanced analytical calculations and transforms. If you just need to aggregate, join or filter your data and not do anything special with it then it might be worth considering doing all of your transforms using SQL with dbtools or pydbtools . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 232}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '92b00761012ec2e4e061b68bcc4e17d6'}>,\n",
       " <Document: {'content': 'Performing joins, aggregates, and filters with SQL via dbtools or pydbtools is likely to be much faster than doing this within RStudio or JupyterLab environment using e.g. dplyr or pandas . When you read the data into RStudio/JupyterLab it will store it in the memory (RAM) of your environment. Currently environments have 12GB of memory, which is easily sufficient for most needs. If your data is too large for this capacity it will break the environment, and crash. If in doubt about best practices we have Slack channels for R, Python, SQL, etc where you can ask. Guidance on using databases / data for deployed apps This section of the guidance covers best practice for using data on the platform for deployed apps. If you want to know how to set up an app bucket go to the deploy an R shiny app section of the guidance. What to consider when giving an app access to data Apps are typically accessed by users without Analytical Platform accounts, so we have to be more careful about security. To this end we like to restrict how much data apps can access, in order to prevent unwanted data leaks in the unlikely event of an app being compromised. In order to describe how this affects things when we give apps access to databases, we need to briefly touch on ‘roles’ within the Analytical Platform. When you access things on the Analytical Platform, you do so by taking on your ‘role’, which lists the set of things that you are permitted to do. This is how you interact with RStudio, Jupyter, Airflow, S3 buckets, etc. Since apps also need permissions to access things, they also have roles. When app users are interacting with an app, it will be carrying out tasks using its own role. In order to minimise the damage that an app could do in the unlikely event of it being compromised, we provide apps with much more restricted roles than those given to users. What this means is: Apps cannot access (read/write) S3 buckets created by the AP users (instead they have their own ‘app buckets’ prefixed with alpha-app- ) Apps cannot access curated databases created by the Data Engineering Team Both apps and users can access (read/write) app buckets (permissions to do this are given via the Control Panel). If you create an app that needs specific data from our databases then you need to consider the following practices: Data minimisation When writing data to your app bucket you should only ever give the app the minimal data that it needs. For example, if your app presents analytical results at an aggregated level, you should put just the aggregated data into the app bucket, even if the underlying analysis that creates these results requires personal or low level data. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 233}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'efcfd8d2e202263643df971c728953e5'}>,\n",
       " <Document: {'content': 'You should never put data in the app bucket that you do not want users to see (even if the code behind the app itself has no method to expose that data to the user via the app). Writing data to an app bucket There are multiple ways you can do this and it is up to you to determine how best to do it. You could manually run some code from your RStudio or JupyterLab environment that writes the necessary data to the app bucket whenever it needs updating You could write an Airflow script that writes data from a database to the app bucket. When updating data for Shiny apps you often need to restart the R session so it refreshes the cached data. You can ask the AP team to create a cron job to automate this. Creating a database for your app Note this is only recommended for users who understand how to create/manage databases on Athena. See other parts of guidance on these tools for more details on how to use them. In most cases you will not need to do this. Apps typically do not require this volume of data (see “Data minimisation”). However, if you feel like you do need to create an Athena SQL database because your app requires the processing power from Athena or the data you are reading in is too large to be cached then you will need to do the following: Create an Athena database that will be used for your app (the data will still sit in the app bucket but it will also need a Glue schema created in order for Athena to query it). Message on the data-engineer slack channel to ask them to add this database for your app. Explain why the Athena database is needed for the app. Your app role will then be given access to only read the specific database for your app and no other databases. This again restricts what an app can access, compared to the wider access given to AP users. If you need to update the app database you can follow the same tips / routes to writing data in S3 in the “writing data to an app bucket” section . If exposing your app to SQL you need to be careful with SQL injection . You should avoid things like f-strings or inserting values into SQL strings. There are tools that deal with this like sqlalchemy . On top of this we also restrict apps to only have read access to the Glue Catalogue. This page was last reviewed on 18 May 2021.\\n\\nIt needs to be reviewed again on 18 May 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 18 May 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 234}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5d8016164ac067ce4aa2fbf93f87f14c'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDatabases for analysis and apps - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 235}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e6ead9d6e41daa6eb8712747918c4319'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 236}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Using databases and data for apps This section covers how and when to use different tools to query our databases on the Analytical Platform and what to consider when giving data to a deployed app. This section assumes you are getting data from databases that are already created on the Analytical Platform. If you need to upload data to the platform see the sections on S3 and information governance . Guidance on using our databases for analysis We use pydbtools (for Python) and dbtools (for R) on the Analytical Platform to query our databases. You can find out more information about them in the dbtools section . Both these tools use an SQL Engine called Athena to query large datasets in S3 just like a normal relational database system. When you want to manipulate or query the databases on the Analytical Platform consider the following: A good general rule is first to use SQL (via dbtools or pydbtools ) for aggregation, joins, and filtering of your data, and then to bring the resulting table into RStudio or JupyterLab for more nuanced analytical calculations and transforms. If you just need to aggregate, join or filter your data and not do anything special with it then it might be worth considering doing all of your transforms using SQL with dbtools or pydbtools . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 237}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '92b00761012ec2e4e061b68bcc4e17d6'}>,\n",
       " <Document: {'content': 'Performing joins, aggregates, and filters with SQL via dbtools or pydbtools is likely to be much faster than doing this within RStudio or JupyterLab environment using e.g. dplyr or pandas . When you read the data into RStudio/JupyterLab it will store it in the memory (RAM) of your environment. Currently environments have 12GB of memory, which is easily sufficient for most needs. If your data is too large for this capacity it will break the environment, and crash. If in doubt about best practices we have Slack channels for R, Python, SQL, etc where you can ask. Guidance on using databases / data for deployed apps This section of the guidance covers best practice for using data on the platform for deployed apps. If you want to know how to set up an app bucket go to the deploy an R shiny app section of the guidance. What to consider when giving an app access to data Apps are typically accessed by users without Analytical Platform accounts, so we have to be more careful about security. To this end we like to restrict how much data apps can access, in order to prevent unwanted data leaks in the unlikely event of an app being compromised. In order to describe how this affects things when we give apps access to databases, we need to briefly touch on ‘roles’ within the Analytical Platform. When you access things on the Analytical Platform, you do so by taking on your ‘role’, which lists the set of things that you are permitted to do. This is how you interact with RStudio, Jupyter, Airflow, S3 buckets, etc. Since apps also need permissions to access things, they also have roles. When app users are interacting with an app, it will be carrying out tasks using its own role. In order to minimise the damage that an app could do in the unlikely event of it being compromised, we provide apps with much more restricted roles than those given to users. What this means is: Apps cannot access (read/write) S3 buckets created by the AP users (instead they have their own ‘app buckets’ prefixed with alpha-app- ) Apps cannot access curated databases created by the Data Engineering Team Both apps and users can access (read/write) app buckets (permissions to do this are given via the Control Panel). If you create an app that needs specific data from our databases then you need to consider the following practices: Data minimisation When writing data to your app bucket you should only ever give the app the minimal data that it needs. For example, if your app presents analytical results at an aggregated level, you should put just the aggregated data into the app bucket, even if the underlying analysis that creates these results requires personal or low level data. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 238}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'efcfd8d2e202263643df971c728953e5'}>,\n",
       " <Document: {'content': 'You should never put data in the app bucket that you do not want users to see (even if the code behind the app itself has no method to expose that data to the user via the app). Writing data to an app bucket There are multiple ways you can do this and it is up to you to determine how best to do it. You could manually run some code from your RStudio or JupyterLab environment that writes the necessary data to the app bucket whenever it needs updating You could write an Airflow script that writes data from a database to the app bucket. When updating data for Shiny apps you often need to restart the R session so it refreshes the cached data. You can ask the AP team to create a cron job to automate this. Creating a database for your app Note this is only recommended for users who understand how to create/manage databases on Athena. See other parts of guidance on these tools for more details on how to use them. In most cases you will not need to do this. Apps typically do not require this volume of data (see “Data minimisation”). However, if you feel like you do need to create an Athena SQL database because your app requires the processing power from Athena or the data you are reading in is too large to be cached then you will need to do the following: Create an Athena database that will be used for your app (the data will still sit in the app bucket but it will also need a Glue schema created in order for Athena to query it). Message on the data-engineer slack channel to ask them to add this database for your app. Explain why the Athena database is needed for the app. Your app role will then be given access to only read the specific database for your app and no other databases. This again restricts what an app can access, compared to the wider access given to AP users. If you need to update the app database you can follow the same tips / routes to writing data in S3 in the “writing data to an app bucket” section . If exposing your app to SQL you need to be careful with SQL injection . You should avoid things like f-strings or inserting values into SQL strings. There are tools that deal with this like sqlalchemy . On top of this we also restrict apps to only have read access to the Glue Catalogue. This page was last reviewed on 18 May 2021.\\n\\nIt needs to be reviewed again on 18 May 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 18 May 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 239}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5d8016164ac067ce4aa2fbf93f87f14c'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDatabases for analysis and apps - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 240}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e6ead9d6e41daa6eb8712747918c4319'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 241}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Using databases and data for apps This section covers how and when to use different tools to query our databases on the Analytical Platform and what to consider when giving data to a deployed app. This section assumes you are getting data from databases that are already created on the Analytical Platform. If you need to upload data to the platform see the sections on S3 and information governance . Guidance on using our databases for analysis We use pydbtools (for Python) and dbtools (for R) on the Analytical Platform to query our databases. You can find out more information about them in the dbtools section . Both these tools use an SQL Engine called Athena to query large datasets in S3 just like a normal relational database system. When you want to manipulate or query the databases on the Analytical Platform consider the following: A good general rule is first to use SQL (via dbtools or pydbtools ) for aggregation, joins, and filtering of your data, and then to bring the resulting table into RStudio or JupyterLab for more nuanced analytical calculations and transforms. If you just need to aggregate, join or filter your data and not do anything special with it then it might be worth considering doing all of your transforms using SQL with dbtools or pydbtools . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 242}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '92b00761012ec2e4e061b68bcc4e17d6'}>,\n",
       " <Document: {'content': 'Performing joins, aggregates, and filters with SQL via dbtools or pydbtools is likely to be much faster than doing this within RStudio or JupyterLab environment using e.g. dplyr or pandas . When you read the data into RStudio/JupyterLab it will store it in the memory (RAM) of your environment. Currently environments have 12GB of memory, which is easily sufficient for most needs. If your data is too large for this capacity it will break the environment, and crash. If in doubt about best practices we have Slack channels for R, Python, SQL, etc where you can ask. Guidance on using databases / data for deployed apps This section of the guidance covers best practice for using data on the platform for deployed apps. If you want to know how to set up an app bucket go to the deploy an R shiny app section of the guidance. What to consider when giving an app access to data Apps are typically accessed by users without Analytical Platform accounts, so we have to be more careful about security. To this end we like to restrict how much data apps can access, in order to prevent unwanted data leaks in the unlikely event of an app being compromised. In order to describe how this affects things when we give apps access to databases, we need to briefly touch on ‘roles’ within the Analytical Platform. When you access things on the Analytical Platform, you do so by taking on your ‘role’, which lists the set of things that you are permitted to do. This is how you interact with RStudio, Jupyter, Airflow, S3 buckets, etc. Since apps also need permissions to access things, they also have roles. When app users are interacting with an app, it will be carrying out tasks using its own role. In order to minimise the damage that an app could do in the unlikely event of it being compromised, we provide apps with much more restricted roles than those given to users. What this means is: Apps cannot access (read/write) S3 buckets created by the AP users (instead they have their own ‘app buckets’ prefixed with alpha-app- ) Apps cannot access curated databases created by the Data Engineering Team Both apps and users can access (read/write) app buckets (permissions to do this are given via the Control Panel). If you create an app that needs specific data from our databases then you need to consider the following practices: Data minimisation When writing data to your app bucket you should only ever give the app the minimal data that it needs. For example, if your app presents analytical results at an aggregated level, you should put just the aggregated data into the app bucket, even if the underlying analysis that creates these results requires personal or low level data. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 243}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'efcfd8d2e202263643df971c728953e5'}>,\n",
       " <Document: {'content': 'You should never put data in the app bucket that you do not want users to see (even if the code behind the app itself has no method to expose that data to the user via the app). Writing data to an app bucket There are multiple ways you can do this and it is up to you to determine how best to do it. You could manually run some code from your RStudio or JupyterLab environment that writes the necessary data to the app bucket whenever it needs updating You could write an Airflow script that writes data from a database to the app bucket. When updating data for Shiny apps you often need to restart the R session so it refreshes the cached data. You can ask the AP team to create a cron job to automate this. Creating a database for your app Note this is only recommended for users who understand how to create/manage databases on Athena. See other parts of guidance on these tools for more details on how to use them. In most cases you will not need to do this. Apps typically do not require this volume of data (see “Data minimisation”). However, if you feel like you do need to create an Athena SQL database because your app requires the processing power from Athena or the data you are reading in is too large to be cached then you will need to do the following: Create an Athena database that will be used for your app (the data will still sit in the app bucket but it will also need a Glue schema created in order for Athena to query it). Message on the data-engineer slack channel to ask them to add this database for your app. Explain why the Athena database is needed for the app. Your app role will then be given access to only read the specific database for your app and no other databases. This again restricts what an app can access, compared to the wider access given to AP users. If you need to update the app database you can follow the same tips / routes to writing data in S3 in the “writing data to an app bucket” section . If exposing your app to SQL you need to be careful with SQL injection . You should avoid things like f-strings or inserting values into SQL strings. There are tools that deal with this like sqlalchemy . On top of this we also restrict apps to only have read access to the Glue Catalogue. This page was last reviewed on 18 May 2021.\\n\\nIt needs to be reviewed again on 18 May 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 18 May 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 244}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5d8016164ac067ce4aa2fbf93f87f14c'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDatabase Documentation - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 245}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '31ae1707a66d12677763357c3bc16fd2'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 246}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data Discovery and Documentation Documentation on curated databases is contained in the data discovery tool . Access to the tool is now governed via GitHub; Analytical Platform users have access by default. Basic metadata is provided automatically by data engineers. Documentation is provided by users, so please do add what you know! Instructions on how to add your material are contained within in the app. This page was last reviewed on 10 March 2023.\\n\\nIt needs to be reviewed again on 10 March 2024\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 10 March 2024\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nExporting data - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 247}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5f1c5a140d38ece802ee61f5b172b87a'}>,\n",
       " <Document: {'content': 'How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 248}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7c4cd5826a95361bc83ec8203d47a614'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Exporting data to other platforms You may want to send work you’ve done on the Analytical Platform to another Ministry of Justice digital platform. For example, you might need to regularly send data to the HMPPS Performance Hub or to a web app on Cloud Platform. If the destination platform uses Amazon Web Services, we can automate this sort of transfer. Ask the data engineering team on the #ask-data-engineering Slack channel . You’re responsible for making sure this is permitted in your data protection impact assessment, and for arranging permission to move the data. If you’re not sure if the destination platform uses Amazon Web Services, ask one of the developers of the service you’re trying to send data to. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 249}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7fa0c0cb16882a90a8dcbe990ed4333b'}>,\n",
       " <Document: {'content': 'If the destination uses another cloud provider (such as Microsoft Azure), a transfer might still be possible, but it will be harder. Still ask the data engineering team , so we know about the need, but be warned that we might not be able to prioritise the work. This page was last reviewed on 14 July 2022.\\n\\nIt needs to be reviewed again on 14 July 2023\\nby the page owner #data_engineering . This page was set to be reviewed before 14 July 2023\\nby the page owner #data_engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nTools - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 250}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd4e46b47497c31ba6a66459f04195348'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 251}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Tools and services The Analytical (AP) provides a range of tools, services and packages. This page describes the core tools and services that comprise the platform, as well as additional packages you can use to perform data analysis. Note that we only provides support for third-party tools and services for features directly involving the Analytical Platform, such as bespoke configurations. For any other support with third-party tools and services, see the vendor’s documentation; we have provided links where possible. Core tools and services Airflow A tool for scheduling and monitoring workflows. Control panel Main entry point to the Analytical Platform. Allows you to configure tools and view their status. Create a Derived Table A tool for creating persistent derived tables in Athena. RStudio Development environment for writing R code and R Shiny apps. For more information, see the RStudio documentation . JupyterLab Development environment for writing Python code. For more information, see the JupyterLab documentation . Data Discovery Allows you to browse the databases that are available on the Analytical Platform. Data Uploader Web application for uploading data (.csv, .json, .jsonl) to the Analytical Platform in a standardised way. Data Extractor Extracts data from applications, services or microservices to the Analytical Platform in a standardised way. GitHub Online hosting platform for git. Git is a distributed version control system that allows you to track changes in files, while GitHub hosts the Analytical Platform’s code. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 252}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fbe7b715b731439db295095c456423b2'}>,\n",
       " <Document: {'content': 'Register my data Moves data from microservices into the Analytical Platform’s curated databases in a standardised way. Python packages The Data Engineering team maintain Python packages that help with data manipulation. The following are the packages we consider the most useful for doing so: athena_tools Provides a simple way to create small persisting ad hoc databases. Currently in Alpha. dataengineeringutils3 Collection of useful utilities for interacting with AWS. mojap-arrow-pd-parser Ensures type conformance when reading with arrow or pandas. mojap-aws-tools-demo Contains helpful guides on how to use the Python packages listed in this section. You can also ask for help with these in the #ask-data-engineering Slack channel on the Justice Digital workspace . mojap-metadata Defined metadata that interacts with other packages (including arrow-pd-parser) to ensure type conformance, as well as schema converters. pydbtools Queries MoJAP athena databases with features such as temp table creation. R packages The following native R packages remove the need for using Python in R projects. dbtools Allows you to access databases from the Analytical Platform. The Data Engineering team maintains this package. Rdbtools Allows you to access Athena databases from the Analytical Platform. The Analytical Platform community maintain this package. Rs3tools Allows you to access AWS S3 from the Analytical Platform, which is mainly compatible with the legacy package s3tools . The Analytical Platform community maintain this package. This page was last reviewed on 8 December 2022.\\n\\nIt needs to be reviewed again on 8 December 2023\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 8 December 2023\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nCreate a Derived Table - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 253}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'be92ee8a56cfaececb344b519cc95251'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 254}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Create a Derived Table ⚠️ This service is in beta ⚠️ Create a Derived Table is a tool for creating persistent derived tables in Athena. Read on to find out more or get in touch at the #ask-data-modelling channel. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 255}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a2be658022afa20a067e98e08aada777'}>,\n",
       " <Document: {'content': 'Overview What is Create a Derived Table Getting Started Database Access RStudio Set Up Collaborating with Git Planning Models Data Modelling Concepts Project Structure Creating Models Models Source and Ref Functions Seeds Tests Macros Linting Linting YAML files Linting SQL files Deploying Models Deploying to Dev Scheduling to Prod Further documentation Troubleshooting Quick Reference dbt-athena Upgrade Guidance Resources Learn more about dbt in the docs Check out Discourse for commonly asked questions and answers Join the chat on Slack for live discussions and support Find dbt events near you Check out the blog for the latest news on dbt’s development and best practices License Unless stated otherwise, the codebase is released under the MIT License . This covers both the codebase and any sample code in the documentation. The documentation is © Crown copyright and available under the terms of the Open Government 3.0 licence. This page was last reviewed on 15 September 2022.\\n\\nIt needs to be reviewed again on 15 September 2023\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 15 September 2023\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nWhat is Create a Derived Table - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 256}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '17ca702253264ef64a82410125bb3ad0'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 257}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools What is Create a Derived Table? Create a Derived Table is a service that brings dbt, Git, and data access controls together to allow you to deploy tables derived from data available on the Analytical Platform; straight to Athena, in a reproducible way, and with scheduled table updates. All you’ll need to do is submit the SQL to derive your tables along with a few configuration files in a GitHub PR. dbt is at the core of Create a Derived Table and it’s packed full of features for transforming data using SQL so you’ll need to get familiar with certain bits of dbt syntax. To learn more about dbt, take a look at their documentation . Some of the basics about working with dbt are covered below, but you can also sign up and work through dbt’s own training courses for free. The training uses their own user interface instead of Create a Derived Table but it’s still relevant here. We’re still in beta so we’d love to get some of you using Create a Derived Table to get your feedback to help guide data modelling best practice in the Ministry of Justice and make sure we can continue to improve the user experience. This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 258}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '862f7cc278eb9698caad9ac6d00a0bea'}>,\n",
       " <Document: {'content': 'This might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDatabase Access - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 259}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dde237e69a91422b34d065cb6fc8cb07'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 260}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Database Access Standard database access The first thing you’ll need to work with Create a Derived Table is an Analytical Platform account with standard database access. If you don’t have that already, follow the guidance on how to make changes to the standard database access project file in the Data Engineering Database Access repo. Your Data Engineering Database Access project access file As well as standard datbase access, you’ll need a project access file that’s specific to your (or your team’s) work. This will give you access to the source databases used to derive tables as well as the derived tables themselves. Access to derived tables is granted at the domain level, this means you will need to know which domain you will be creating derived tables in before you start working. Your project file should include the source databases and domain(s) that you’ll use to derive your tables from, and the domain(s) that your tables will go in. A list of already available domains can be found in the database access resources for Create a Derived Table . If you don’t know already, you can learn how to set up a project access file by following the guidance on access to curated databases in the Data Engineering Database Access repo. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 261}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '58dbd88f14e4f6fbc3c50e83d80277d7'}>,\n",
       " <Document: {'content': 'A typical project access file might look like: project_id: Analytical Team 1 Derived Tables\\n\\nreview_date: 2022-10-20\\napprover_position: >=G7 of Analytical Team 1 Derived Tables\\napprover_email: approver.name@justice.gov.uk\\nalternative_approver_emails:\\n- alternative.approver-name@justice.gov.uk\\n\\nresources:\\n- create_a_derived_table/domain_a\\n- create_a_derived_table/domain_b\\n- source_database_a/full\\n- source_database_b/full\\n\\nusers:\\n- alpha_user_name_a\\n- alpha_user_name_b\\n- alpha_user_name_c\\n\\nbusiness_case: >\\nTo create derived tables for Analytical Team 1. This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDatabase Access - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 262}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b5d432529c7c4fd42c875da812cd9f74'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 263}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Database Access Standard database access The first thing you’ll need to work with Create a Derived Table is an Analytical Platform account with standard database access. If you don’t have that already, follow the guidance on how to make changes to the standard database access project file in the Data Engineering Database Access repo. Your Data Engineering Database Access project access file As well as standard datbase access, you’ll need a project access file that’s specific to your (or your team’s) work. This will give you access to the source databases used to derive tables as well as the derived tables themselves. Access to derived tables is granted at the domain level, this means you will need to know which domain you will be creating derived tables in before you start working. Your project file should include the source databases and domain(s) that you’ll use to derive your tables from, and the domain(s) that your tables will go in. A list of already available domains can be found in the database access resources for Create a Derived Table . If you don’t know already, you can learn how to set up a project access file by following the guidance on access to curated databases in the Data Engineering Database Access repo. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 264}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '58dbd88f14e4f6fbc3c50e83d80277d7'}>,\n",
       " <Document: {'content': 'A typical project access file might look like: project_id: Analytical Team 1 Derived Tables\\n\\nreview_date: 2022-10-20\\napprover_position: >=G7 of Analytical Team 1 Derived Tables\\napprover_email: approver.name@justice.gov.uk\\nalternative_approver_emails:\\n- alternative.approver-name@justice.gov.uk\\n\\nresources:\\n- create_a_derived_table/domain_a\\n- create_a_derived_table/domain_b\\n- source_database_a/full\\n- source_database_b/full\\n\\nusers:\\n- alpha_user_name_a\\n- alpha_user_name_b\\n- alpha_user_name_c\\n\\nbusiness_case: >\\nTo create derived tables for Analytical Team 1. This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDatabase Access - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 265}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b5d432529c7c4fd42c875da812cd9f74'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 266}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Database Access Standard database access The first thing you’ll need to work with Create a Derived Table is an Analytical Platform account with standard database access. If you don’t have that already, follow the guidance on how to make changes to the standard database access project file in the Data Engineering Database Access repo. Your Data Engineering Database Access project access file As well as standard datbase access, you’ll need a project access file that’s specific to your (or your team’s) work. This will give you access to the source databases used to derive tables as well as the derived tables themselves. Access to derived tables is granted at the domain level, this means you will need to know which domain you will be creating derived tables in before you start working. Your project file should include the source databases and domain(s) that you’ll use to derive your tables from, and the domain(s) that your tables will go in. A list of already available domains can be found in the database access resources for Create a Derived Table . If you don’t know already, you can learn how to set up a project access file by following the guidance on access to curated databases in the Data Engineering Database Access repo. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 267}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '58dbd88f14e4f6fbc3c50e83d80277d7'}>,\n",
       " <Document: {'content': 'A typical project access file might look like: project_id: Analytical Team 1 Derived Tables\\n\\nreview_date: 2022-10-20\\napprover_position: >=G7 of Analytical Team 1 Derived Tables\\napprover_email: approver.name@justice.gov.uk\\nalternative_approver_emails:\\n- alternative.approver-name@justice.gov.uk\\n\\nresources:\\n- create_a_derived_table/domain_a\\n- create_a_derived_table/domain_b\\n- source_database_a/full\\n- source_database_b/full\\n\\nusers:\\n- alpha_user_name_a\\n- alpha_user_name_b\\n- alpha_user_name_c\\n\\nbusiness_case: >\\nTo create derived tables for Analytical Team 1. This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nRstudio Set Up - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 268}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2d924630c0e1bbf440bb001bf321f2a8'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 269}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Rstudio Set Up You’ll need an interactive development environment (IDE) to interact with the repository and write your SQL and YAML code, and a Python virtual environment for dbt to run in. The following sections will show you how to set that up. It’s worth noting at this point that you’ll just be using RStudio as an IDE to interact with the repository (and git), write SQL and YAML code, and to run dbt commands from the terminal in a Python virtual environment. There is no R programming going on. We’re currently not planning to get Create a Derived Table up and running with JupyterLab, as the RStudio IDE is sufficient. Clone the repository using the RStudio GUI Select file > New Project... > Version Control > Git and paste the following into the text fields and then select which directory you would like to clone the repository to. Repository URL: git@github.com:moj-analytical-services/create-a-derived-table.git Project directory name: create-a-derived-table Clone the repository using the terminal cd to the directory where you’d like to clone the repository to and then run the following command: git clone git@github.com:moj-analytical-services/create-a-derived-table.git Setting up a Python virtual environment Python versions 3.7, 3.8, 3.9, 3.10 and 3.11 are compatible with dbt-core v1.5.0. In the terminal, cd into the root of the repository ( create-a-derived-table ). You can check you’re in the correct directory by runnnig pwd . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 270}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5054d215672ab8e1484002654c37187f'}>,\n",
       " <Document: {'content': 'Once you’ve done that run: python3 -m venv venv source venv/bin/activate pip install --upgrade pip pip install -r requirements.txt To install the lint libraries, run: pip install -r requirements-lint.txt Set the following environment variable in your Bash profile: echo \"export DBT_PROFILES_DIR=../.dbt/\" >> ~/.bash_profile Then source your Bash profile by running: source ~/.bash_profile You’ll need to be in the dbt project to run dbt commands. This is the mojap_derived_tables directory: cd mojap_derived_tables Then to check an active connection, run: dbt debug If that’s successful, to install dbt packages, run: dbt deps Congratulations, you’re now ready to get dbt-ing! Show indent guides in RStudio In RStudio you can display vertical guidelines to help you keep track of indetations in your code; this is very helpful in YAML files. In the RStudio IDE go to Tools → Global Options → Code → Display and check the box for Show indent guides . You will also want to set your tab width to 2 spaces, to do this go to Tools → Global Options → Code → Editing and check the box for Insert spaces for Tab and set Tab width to 2 . This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nRstudio Set Up - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 271}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '95919d7a2581a367c5ce3acf19fcd4a3'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 272}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Rstudio Set Up You’ll need an interactive development environment (IDE) to interact with the repository and write your SQL and YAML code, and a Python virtual environment for dbt to run in. The following sections will show you how to set that up. It’s worth noting at this point that you’ll just be using RStudio as an IDE to interact with the repository (and git), write SQL and YAML code, and to run dbt commands from the terminal in a Python virtual environment. There is no R programming going on. We’re currently not planning to get Create a Derived Table up and running with JupyterLab, as the RStudio IDE is sufficient. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 273}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1e2d582df734475767db2573e096b03d'}>,\n",
       " <Document: {'content': 'Clone the repository using the RStudio GUI Select file > New Project... > Version Control > Git and paste the following into the text fields and then select which directory you would like to clone the repository to. Repository URL: git@github.com:moj-analytical-services/create-a-derived-table.git Project directory name: create-a-derived-table Clone the repository using the terminal cd to the directory where you’d like to clone the repository to and then run the following command: git clone git@github.com:moj-analytical-services/create-a-derived-table.git Setting up a Python virtual environment Python versions 3.7, 3.8, 3.9, 3.10 and 3.11 are compatible with dbt-core v1.5.0. In the terminal, cd into the root of the repository ( create-a-derived-table ). You can check you’re in the correct directory by runnnig pwd . Once you’ve done that run: python3 -m venv venv source venv/bin/activate pip install --upgrade pip pip install -r requirements.txt To install the lint libraries, run: pip install -r requirements-lint.txt Set the following environment variable in your Bash profile: echo \"export DBT_PROFILES_DIR=../.dbt/\" >> ~/.bash_profile Then source your Bash profile by running: source ~/.bash_profile You’ll need to be in the dbt project to run dbt commands. This is the mojap_derived_tables directory: cd mojap_derived_tables Then to check an active connection, run: dbt debug If that’s successful, to install dbt packages, run: dbt deps Congratulations, you’re now ready to get dbt-ing! Show indent guides in RStudio In RStudio you can display vertical guidelines to help you keep track of indetations in your code; this is very helpful in YAML files. In the RStudio IDE go to Tools → Global Options → Code → Display and check the box for Show indent guides . You will also want to set your tab width to 2 spaces, to do this go to Tools → Global Options → Code → Editing and check the box for Insert spaces for Tab and set Tab width to 2 . This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nRstudio Set Up - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 274}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6c1eec0eb606344a52476837bc9bd641'}>,\n",
       " <Document: {'content': 'I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 275}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '165700989970d698eee8f1a9a15a7bc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Rstudio Set Up You’ll need an interactive development environment (IDE) to interact with the repository and write your SQL and YAML code, and a Python virtual environment for dbt to run in. The following sections will show you how to set that up. It’s worth noting at this point that you’ll just be using RStudio as an IDE to interact with the repository (and git), write SQL and YAML code, and to run dbt commands from the terminal in a Python virtual environment. There is no R programming going on. We’re currently not planning to get Create a Derived Table up and running with JupyterLab, as the RStudio IDE is sufficient. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 276}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1e2d582df734475767db2573e096b03d'}>,\n",
       " <Document: {'content': 'Clone the repository using the RStudio GUI Select file > New Project... > Version Control > Git and paste the following into the text fields and then select which directory you would like to clone the repository to. Repository URL: git@github.com:moj-analytical-services/create-a-derived-table.git Project directory name: create-a-derived-table Clone the repository using the terminal cd to the directory where you’d like to clone the repository to and then run the following command: git clone git@github.com:moj-analytical-services/create-a-derived-table.git Setting up a Python virtual environment Python versions 3.7, 3.8, 3.9, 3.10 and 3.11 are compatible with dbt-core v1.5.0. In the terminal, cd into the root of the repository ( create-a-derived-table ). You can check you’re in the correct directory by runnnig pwd . Once you’ve done that run: python3 -m venv venv source venv/bin/activate pip install --upgrade pip pip install -r requirements.txt To install the lint libraries, run: pip install -r requirements-lint.txt Set the following environment variable in your Bash profile: echo \"export DBT_PROFILES_DIR=../.dbt/\" >> ~/.bash_profile Then source your Bash profile by running: source ~/.bash_profile You’ll need to be in the dbt project to run dbt commands. This is the mojap_derived_tables directory: cd mojap_derived_tables Then to check an active connection, run: dbt debug If that’s successful, to install dbt packages, run: dbt deps Congratulations, you’re now ready to get dbt-ing! Show indent guides in RStudio In RStudio you can display vertical guidelines to help you keep track of indetations in your code; this is very helpful in YAML files. In the RStudio IDE go to Tools → Global Options → Code → Display and check the box for Show indent guides . You will also want to set your tab width to 2 spaces, to do this go to Tools → Global Options → Code → Editing and check the box for Insert spaces for Tab and set Tab width to 2 . This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nRstudio Set Up - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 277}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6c1eec0eb606344a52476837bc9bd641'}>,\n",
       " <Document: {'content': 'I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 278}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '165700989970d698eee8f1a9a15a7bc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Rstudio Set Up You’ll need an interactive development environment (IDE) to interact with the repository and write your SQL and YAML code, and a Python virtual environment for dbt to run in. The following sections will show you how to set that up. It’s worth noting at this point that you’ll just be using RStudio as an IDE to interact with the repository (and git), write SQL and YAML code, and to run dbt commands from the terminal in a Python virtual environment. There is no R programming going on. We’re currently not planning to get Create a Derived Table up and running with JupyterLab, as the RStudio IDE is sufficient. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 279}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1e2d582df734475767db2573e096b03d'}>,\n",
       " <Document: {'content': 'Clone the repository using the RStudio GUI Select file > New Project... > Version Control > Git and paste the following into the text fields and then select which directory you would like to clone the repository to. Repository URL: git@github.com:moj-analytical-services/create-a-derived-table.git Project directory name: create-a-derived-table Clone the repository using the terminal cd to the directory where you’d like to clone the repository to and then run the following command: git clone git@github.com:moj-analytical-services/create-a-derived-table.git Setting up a Python virtual environment Python versions 3.7, 3.8, 3.9, 3.10 and 3.11 are compatible with dbt-core v1.5.0. In the terminal, cd into the root of the repository ( create-a-derived-table ). You can check you’re in the correct directory by runnnig pwd . Once you’ve done that run: python3 -m venv venv source venv/bin/activate pip install --upgrade pip pip install -r requirements.txt To install the lint libraries, run: pip install -r requirements-lint.txt Set the following environment variable in your Bash profile: echo \"export DBT_PROFILES_DIR=../.dbt/\" >> ~/.bash_profile Then source your Bash profile by running: source ~/.bash_profile You’ll need to be in the dbt project to run dbt commands. This is the mojap_derived_tables directory: cd mojap_derived_tables Then to check an active connection, run: dbt debug If that’s successful, to install dbt packages, run: dbt deps Congratulations, you’re now ready to get dbt-ing! Show indent guides in RStudio In RStudio you can display vertical guidelines to help you keep track of indetations in your code; this is very helpful in YAML files. In the RStudio IDE go to Tools → Global Options → Code → Display and check the box for Show indent guides . You will also want to set your tab width to 2 spaces, to do this go to Tools → Global Options → Code → Editing and check the box for Insert spaces for Tab and set Tab width to 2 . This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nRstudio Set Up - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 280}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6c1eec0eb606344a52476837bc9bd641'}>,\n",
       " <Document: {'content': 'I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 281}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '165700989970d698eee8f1a9a15a7bc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Rstudio Set Up You’ll need an interactive development environment (IDE) to interact with the repository and write your SQL and YAML code, and a Python virtual environment for dbt to run in. The following sections will show you how to set that up. It’s worth noting at this point that you’ll just be using RStudio as an IDE to interact with the repository (and git), write SQL and YAML code, and to run dbt commands from the terminal in a Python virtual environment. There is no R programming going on. We’re currently not planning to get Create a Derived Table up and running with JupyterLab, as the RStudio IDE is sufficient. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 282}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1e2d582df734475767db2573e096b03d'}>,\n",
       " <Document: {'content': 'Clone the repository using the RStudio GUI Select file > New Project... > Version Control > Git and paste the following into the text fields and then select which directory you would like to clone the repository to. Repository URL: git@github.com:moj-analytical-services/create-a-derived-table.git Project directory name: create-a-derived-table Clone the repository using the terminal cd to the directory where you’d like to clone the repository to and then run the following command: git clone git@github.com:moj-analytical-services/create-a-derived-table.git Setting up a Python virtual environment Python versions 3.7, 3.8, 3.9, 3.10 and 3.11 are compatible with dbt-core v1.5.0. In the terminal, cd into the root of the repository ( create-a-derived-table ). You can check you’re in the correct directory by runnnig pwd . Once you’ve done that run: python3 -m venv venv source venv/bin/activate pip install --upgrade pip pip install -r requirements.txt To install the lint libraries, run: pip install -r requirements-lint.txt Set the following environment variable in your Bash profile: echo \"export DBT_PROFILES_DIR=../.dbt/\" >> ~/.bash_profile Then source your Bash profile by running: source ~/.bash_profile You’ll need to be in the dbt project to run dbt commands. This is the mojap_derived_tables directory: cd mojap_derived_tables Then to check an active connection, run: dbt debug If that’s successful, to install dbt packages, run: dbt deps Congratulations, you’re now ready to get dbt-ing! Show indent guides in RStudio In RStudio you can display vertical guidelines to help you keep track of indetations in your code; this is very helpful in YAML files. In the RStudio IDE go to Tools → Global Options → Code → Display and check the box for Show indent guides . You will also want to set your tab width to 2 spaces, to do this go to Tools → Global Options → Code → Editing and check the box for Insert spaces for Tab and set Tab width to 2 . This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nCollaborating with Git - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 283}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '65ed5c724b74f1fafd8fa4f708784652'}>,\n",
       " <Document: {'content': 'I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 284}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '165700989970d698eee8f1a9a15a7bc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Collaborating with Git If you’re working on a data model as part of a team you can take inspiration from the Gitflow Workflow to effectively collaborate and develop your data model. The following is a recommendation, so feel free to use another approach if there’s one that suits your team better. Start with a main branch for your team. Because there is already a main branch in the repository, we suggest you create a branch from the main branch and name it something like project-name-main or team-name-main . Next Analyst 1 is working on table_a , while Analyst 2 is working on table_b . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 285}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9bd2b1ebc070beb7e5243aefa0dd5773'}>,\n",
       " <Document: {'content': 'Analyst 1 should create a branch off project-name-main called a1/model-a-development , prefixing the branch name with their initials and a slash a1/ . Analyst 2 should create a branch off project-name-main called a2/model-b-development , prefixing the branch name with their initials and a slash a2/ . When they have completed the development of their respective models, each analyst should raise a pull request for their respective branches and set the base branch to project-name-main . The analysts, or someone else in the team, can then review the pull request and quality assure the work. When a pull request has been merged into project-name-main , keep other feature branches like a2/model-b-development up to date by merging project-name-main into it. You can do this using the terminal or in the pull request itself where you will see a button at the bottom of the pull request page that says Update branch . If you do use the pull request to update the branch, you’ll need to run git pull locally. When the data model is complete and all changes have been merged into project-name-main , you can request review from the data modelling team who will check it over before it gets merged into main . Creating branches First make sure you’re on the main branch by running the following in terminal: git status If you’re not, run: git checkout main git pull Next create your project-name-main branch (don’t forget to update the project name in the command) by running: git checkout -b <project-name-main> From the project-name-main branch, create your development branch by running: git checkout -b <a1/model-a-development> replacing a1 with your initals. Updating your branch with main When working on your models it is likely that your branch will get out of date with the main branch. To update you branch with the latest changes from main open a terminal and run the following: Check your working tree, commit/push any changes if required git status Switch to the main branch and collect the latest changes, if any git switch main\\ngit fetch\\ngit pull Switch back to your branch and merge in the changes from main git switch <your_branch>\\ngit merge main -m \"update branch with main\" At this point you may have merge conflicts that need to be resolved; please see GitHub resolve merge conflicts . If required, ask for help on the #ask-data-modelling slack channel. This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 286}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd660cb6d47e452360f1f0e778bd5de3e'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nCollaborating with Git - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 287}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2ef5e9acd4ba605c604baf3657dae913'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 288}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Collaborating with Git If you’re working on a data model as part of a team you can take inspiration from the Gitflow Workflow to effectively collaborate and develop your data model. The following is a recommendation, so feel free to use another approach if there’s one that suits your team better. Start with a main branch for your team. Because there is already a main branch in the repository, we suggest you create a branch from the main branch and name it something like project-name-main or team-name-main . Next Analyst 1 is working on table_a , while Analyst 2 is working on table_b . Analyst 1 should create a branch off project-name-main called a1/model-a-development , prefixing the branch name with their initials and a slash a1/ . Analyst 2 should create a branch off project-name-main called a2/model-b-development , prefixing the branch name with their initials and a slash a2/ . When they have completed the development of their respective models, each analyst should raise a pull request for their respective branches and set the base branch to project-name-main . The analysts, or someone else in the team, can then review the pull request and quality assure the work. When a pull request has been merged into project-name-main , keep other feature branches like a2/model-b-development up to date by merging project-name-main into it. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 289}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1d9576fae856d1ac6325bf63cd39c676'}>,\n",
       " <Document: {'content': 'You can do this using the terminal or in the pull request itself where you will see a button at the bottom of the pull request page that says Update branch . If you do use the pull request to update the branch, you’ll need to run git pull locally. When the data model is complete and all changes have been merged into project-name-main , you can request review from the data modelling team who will check it over before it gets merged into main . Creating branches First make sure you’re on the main branch by running the following in terminal: git status If you’re not, run: git checkout main git pull Next create your project-name-main branch (don’t forget to update the project name in the command) by running: git checkout -b <project-name-main> From the project-name-main branch, create your development branch by running: git checkout -b <a1/model-a-development> replacing a1 with your initals. Updating your branch with main When working on your models it is likely that your branch will get out of date with the main branch. To update you branch with the latest changes from main open a terminal and run the following: Check your working tree, commit/push any changes if required git status Switch to the main branch and collect the latest changes, if any git switch main\\ngit fetch\\ngit pull Switch back to your branch and merge in the changes from main git switch <your_branch>\\ngit merge main -m \"update branch with main\" At this point you may have merge conflicts that need to be resolved; please see GitHub resolve merge conflicts . If required, ask for help on the #ask-data-modelling slack channel. This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nCollaborating with Git - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 290}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6295cc56d69babf987c7237ab260ee4'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 291}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Collaborating with Git If you’re working on a data model as part of a team you can take inspiration from the Gitflow Workflow to effectively collaborate and develop your data model. The following is a recommendation, so feel free to use another approach if there’s one that suits your team better. Start with a main branch for your team. Because there is already a main branch in the repository, we suggest you create a branch from the main branch and name it something like project-name-main or team-name-main . Next Analyst 1 is working on table_a , while Analyst 2 is working on table_b . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 292}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9bd2b1ebc070beb7e5243aefa0dd5773'}>,\n",
       " <Document: {'content': 'Analyst 1 should create a branch off project-name-main called a1/model-a-development , prefixing the branch name with their initials and a slash a1/ . Analyst 2 should create a branch off project-name-main called a2/model-b-development , prefixing the branch name with their initials and a slash a2/ . When they have completed the development of their respective models, each analyst should raise a pull request for their respective branches and set the base branch to project-name-main . The analysts, or someone else in the team, can then review the pull request and quality assure the work. When a pull request has been merged into project-name-main , keep other feature branches like a2/model-b-development up to date by merging project-name-main into it. You can do this using the terminal or in the pull request itself where you will see a button at the bottom of the pull request page that says Update branch . If you do use the pull request to update the branch, you’ll need to run git pull locally. When the data model is complete and all changes have been merged into project-name-main , you can request review from the data modelling team who will check it over before it gets merged into main . Creating branches First make sure you’re on the main branch by running the following in terminal: git status If you’re not, run: git checkout main git pull Next create your project-name-main branch (don’t forget to update the project name in the command) by running: git checkout -b <project-name-main> From the project-name-main branch, create your development branch by running: git checkout -b <a1/model-a-development> replacing a1 with your initals. Updating your branch with main When working on your models it is likely that your branch will get out of date with the main branch. To update you branch with the latest changes from main open a terminal and run the following: Check your working tree, commit/push any changes if required git status Switch to the main branch and collect the latest changes, if any git switch main\\ngit fetch\\ngit pull Switch back to your branch and merge in the changes from main git switch <your_branch>\\ngit merge main -m \"update branch with main\" At this point you may have merge conflicts that need to be resolved; please see GitHub resolve merge conflicts . If required, ask for help on the #ask-data-modelling slack channel. This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 293}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd660cb6d47e452360f1f0e778bd5de3e'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData Modelling Concepts - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 294}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8240b31dc9b05773c1e32efe6552a754'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 295}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data Modelling Concepts - placeholder ⚠️ This service is in beta ⚠️ This page is intended to give users a brief introduction to Data Modelling concepts and why we are using dbt as the backend for create-a-derived-table . Please post suggestions to improve this document in our slack channel #ask-data-modelling , or edit and raise a PR. This page was last reviewed on 15 September 2022.\\n\\nIt needs to be reviewed again on 15 September 2023\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 15 September 2023\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nProject Structure - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 296}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e57d03d29547e96a3729eb255d99dc7b'}>,\n",
       " <Document: {'content': 'How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 297}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eeb37422cf651c74b0399aaf97d1c5ae'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Project Structure The Create a Derived Table service should be used for creating tables with well defined use cases, like to serve a performance metric, publication, or MI report. This is because the dbt project ( mojap_derived_tables ) is a shared space and so knowing where to put your work in the broader structure of the project is key. That’s not to say you can’t explore and experiment with dbt within the dbt project, there’s a development envionment where you can try things out without having to worry about making a mess of things. More on that later. Domains The primary consideration relating to project structure is understanding which domain the table you want to create belongs to. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 298}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '481c744022cb1e2dcaa6f40f1774a2b2'}>,\n",
       " <Document: {'content': 'In Create a Derived Table a domain should correspond to some service area or core business concept and is used to logically group databases. Domains are not mutually exclusive so the same concepts can exist in different domains. A domain may be ‘people’ relating HR and corporate, or ‘risk’ relating to a justice system service user’s safety, but it could be more or less granular if appropriate. Databases The secondary consideration is whether the tables you are creating belong in an existing database, if they do, then this step is easy. If you need to create a new database then you’ll need to decide which domain to put it in. It’s also possible to define a database across multiple domains. For example, a number of tables within your database might sit within ‘domain a’ while the rest sit in ‘domain b’. This approach has the benefit of keeping all tables logically grouped within one database but will result in access to those tables being limited by the domain. Standard directory structure and naming conventions The following is an example of how a team might build a data model whilst adhearing to the standard dbt project directory structure required to work with Create a Derived Table. The Prison Safety and Security team have created a database called prison_safety_and_security in the security domain. From the mojap_derived_tables dbt project, the hierarchy of directories must follow models -> domain -> database . The directory structure after this is arbitrary and can be chosen to suit your needs. However, we do recommend that you arrange your models into data marts and suffix your models with descriptions (this will be discussed in more detail). Directory and file names should only use snake case, as in, no-hyphens-like-this , just_underscores_like_this . Models ( .sql files) must be named by the database and table they relate to separated by double underscores, i.e., <database_name>__<table_name>.sql . This is because all models in the models directory must have a unique name. Suffixes should be added that describe each model’s purpose. Fact and dimension models should be suffixed with _fct and _dim respectively. Intermediate models should be suffixed with the main transformation being applied in past tense, e.g., _grouped or _left_joined . You may want to arrange staging models into a specific staging database, or within a single database for all your tables. Staging models should be suffixed with _stg unless built into a specific staging database. Fact, dimension, and staging models must have their own property file that has the same filename as the model they define properties for. Intermediate models should have a single configuration file named properties.yaml . Seed property files must have the same filename as the seed they define properties for. If you have ideas about how you would like to structure your data model, please get in touch as we’d love to hear from you to help guide best practice principles. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 299}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd15b67db636052331f3a401d32bf74be'}>,\n",
       " <Document: {'content': '├── mojap_derived_tables\\n├── dbt_project.yml\\n└── models\\n├── sources\\n│   ├── nomis.yaml\\n│   ├── oasys_prod.yaml\\n│   ├── delius_prod.yaml\\n│   ├── xhibit_v1.yaml\\n│   ...\\n├── security  # domain\\n│   ├── prison_safety_and_security  # database\\n│   │   ├── marts\\n│   │   │   ├── intermediate\\n│   │   │   │   ├── prison_safety_and_security_ inc_ids_grouped.sql  # intermediate table\\n│   │   │   │   ├── prison_safety_and_security questions_filtered.sql  # intermediate table\\n│   │   │   │   ├── prison_safety_and_security question_set_joined.sql  # intermediate table\\n│   │   │   │   └── properties.yaml  # intermediate tables property file\\n│   │   │   └── question_answers  # arbitrary grouping\\n│   │   │       ├── prison_safety_and_security dates_dim.sql  # dimension table\\n│   │   │       ├── prison_safety_and_security dates_dim.yaml  # table property file\\n│   │   │       ├── prison_safety_and_security incidents_fct.sql  # fact table\\n│   │   │       ├── prison_safety_and_security incidents_fct.yaml  # table property file\\n│   │   │       ├── prison_safety_and_security question_answer_fct.sql  # fact table\\n│   │   │       └── prison_safety_and_security question_answer_fct.yaml  # table property file\\n│   │   └── staging\\n│   │       ├── prison_safety_and_security nomis_mod_stg.md  # markdown to be rendered in documentation\\n│   │       ├── prison_safety_and_security nomis_mod_stg.sql  # staging table\\n│   │       └── prison_safety_and_security _nomis_mod_stg.yaml  # table property file Data modelling Data modelling is hard, so if the considerations about domains, databases, or data model structures aren’t clear, reach out to the data modelling team and we’ll do our best to help you out. This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nProject Structure - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 300}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8ba2aff2bb659101f66dd156ccd2d5f8'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 301}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Project Structure The Create a Derived Table service should be used for creating tables with well defined use cases, like to serve a performance metric, publication, or MI report. This is because the dbt project ( mojap_derived_tables ) is a shared space and so knowing where to put your work in the broader structure of the project is key. That’s not to say you can’t explore and experiment with dbt within the dbt project, there’s a development envionment where you can try things out without having to worry about making a mess of things. More on that later. Domains The primary consideration relating to project structure is understanding which domain the table you want to create belongs to. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 302}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '481c744022cb1e2dcaa6f40f1774a2b2'}>,\n",
       " <Document: {'content': 'In Create a Derived Table a domain should correspond to some service area or core business concept and is used to logically group databases. Domains are not mutually exclusive so the same concepts can exist in different domains. A domain may be ‘people’ relating HR and corporate, or ‘risk’ relating to a justice system service user’s safety, but it could be more or less granular if appropriate. Databases The secondary consideration is whether the tables you are creating belong in an existing database, if they do, then this step is easy. If you need to create a new database then you’ll need to decide which domain to put it in. It’s also possible to define a database across multiple domains. For example, a number of tables within your database might sit within ‘domain a’ while the rest sit in ‘domain b’. This approach has the benefit of keeping all tables logically grouped within one database but will result in access to those tables being limited by the domain. Standard directory structure and naming conventions The following is an example of how a team might build a data model whilst adhearing to the standard dbt project directory structure required to work with Create a Derived Table. The Prison Safety and Security team have created a database called prison_safety_and_security in the security domain. From the mojap_derived_tables dbt project, the hierarchy of directories must follow models -> domain -> database . The directory structure after this is arbitrary and can be chosen to suit your needs. However, we do recommend that you arrange your models into data marts and suffix your models with descriptions (this will be discussed in more detail). Directory and file names should only use snake case, as in, no-hyphens-like-this , just_underscores_like_this . Models ( .sql files) must be named by the database and table they relate to separated by double underscores, i.e., <database_name>__<table_name>.sql . This is because all models in the models directory must have a unique name. Suffixes should be added that describe each model’s purpose. Fact and dimension models should be suffixed with _fct and _dim respectively. Intermediate models should be suffixed with the main transformation being applied in past tense, e.g., _grouped or _left_joined . You may want to arrange staging models into a specific staging database, or within a single database for all your tables. Staging models should be suffixed with _stg unless built into a specific staging database. Fact, dimension, and staging models must have their own property file that has the same filename as the model they define properties for. Intermediate models should have a single configuration file named properties.yaml . Seed property files must have the same filename as the seed they define properties for. If you have ideas about how you would like to structure your data model, please get in touch as we’d love to hear from you to help guide best practice principles. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 303}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd15b67db636052331f3a401d32bf74be'}>,\n",
       " <Document: {'content': '├── mojap_derived_tables\\n├── dbt_project.yml\\n└── models\\n├── sources\\n│   ├── nomis.yaml\\n│   ├── oasys_prod.yaml\\n│   ├── delius_prod.yaml\\n│   ├── xhibit_v1.yaml\\n│   ...\\n├── security  # domain\\n│   ├── prison_safety_and_security  # database\\n│   │   ├── marts\\n│   │   │   ├── intermediate\\n│   │   │   │   ├── prison_safety_and_security_ inc_ids_grouped.sql  # intermediate table\\n│   │   │   │   ├── prison_safety_and_security questions_filtered.sql  # intermediate table\\n│   │   │   │   ├── prison_safety_and_security question_set_joined.sql  # intermediate table\\n│   │   │   │   └── properties.yaml  # intermediate tables property file\\n│   │   │   └── question_answers  # arbitrary grouping\\n│   │   │       ├── prison_safety_and_security dates_dim.sql  # dimension table\\n│   │   │       ├── prison_safety_and_security dates_dim.yaml  # table property file\\n│   │   │       ├── prison_safety_and_security incidents_fct.sql  # fact table\\n│   │   │       ├── prison_safety_and_security incidents_fct.yaml  # table property file\\n│   │   │       ├── prison_safety_and_security question_answer_fct.sql  # fact table\\n│   │   │       └── prison_safety_and_security question_answer_fct.yaml  # table property file\\n│   │   └── staging\\n│   │       ├── prison_safety_and_security nomis_mod_stg.md  # markdown to be rendered in documentation\\n│   │       ├── prison_safety_and_security nomis_mod_stg.sql  # staging table\\n│   │       └── prison_safety_and_security _nomis_mod_stg.yaml  # table property file Data modelling Data modelling is hard, so if the considerations about domains, databases, or data model structures aren’t clear, reach out to the data modelling team and we’ll do our best to help you out. This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nProject Structure - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 304}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8ba2aff2bb659101f66dd156ccd2d5f8'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 305}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Project Structure The Create a Derived Table service should be used for creating tables with well defined use cases, like to serve a performance metric, publication, or MI report. This is because the dbt project ( mojap_derived_tables ) is a shared space and so knowing where to put your work in the broader structure of the project is key. That’s not to say you can’t explore and experiment with dbt within the dbt project, there’s a development envionment where you can try things out without having to worry about making a mess of things. More on that later. Domains The primary consideration relating to project structure is understanding which domain the table you want to create belongs to. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 306}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '481c744022cb1e2dcaa6f40f1774a2b2'}>,\n",
       " <Document: {'content': 'In Create a Derived Table a domain should correspond to some service area or core business concept and is used to logically group databases. Domains are not mutually exclusive so the same concepts can exist in different domains. A domain may be ‘people’ relating HR and corporate, or ‘risk’ relating to a justice system service user’s safety, but it could be more or less granular if appropriate. Databases The secondary consideration is whether the tables you are creating belong in an existing database, if they do, then this step is easy. If you need to create a new database then you’ll need to decide which domain to put it in. It’s also possible to define a database across multiple domains. For example, a number of tables within your database might sit within ‘domain a’ while the rest sit in ‘domain b’. This approach has the benefit of keeping all tables logically grouped within one database but will result in access to those tables being limited by the domain. Standard directory structure and naming conventions The following is an example of how a team might build a data model whilst adhearing to the standard dbt project directory structure required to work with Create a Derived Table. The Prison Safety and Security team have created a database called prison_safety_and_security in the security domain. From the mojap_derived_tables dbt project, the hierarchy of directories must follow models -> domain -> database . The directory structure after this is arbitrary and can be chosen to suit your needs. However, we do recommend that you arrange your models into data marts and suffix your models with descriptions (this will be discussed in more detail). Directory and file names should only use snake case, as in, no-hyphens-like-this , just_underscores_like_this . Models ( .sql files) must be named by the database and table they relate to separated by double underscores, i.e., <database_name>__<table_name>.sql . This is because all models in the models directory must have a unique name. Suffixes should be added that describe each model’s purpose. Fact and dimension models should be suffixed with _fct and _dim respectively. Intermediate models should be suffixed with the main transformation being applied in past tense, e.g., _grouped or _left_joined . You may want to arrange staging models into a specific staging database, or within a single database for all your tables. Staging models should be suffixed with _stg unless built into a specific staging database. Fact, dimension, and staging models must have their own property file that has the same filename as the model they define properties for. Intermediate models should have a single configuration file named properties.yaml . Seed property files must have the same filename as the seed they define properties for. If you have ideas about how you would like to structure your data model, please get in touch as we’d love to hear from you to help guide best practice principles. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 307}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd15b67db636052331f3a401d32bf74be'}>,\n",
       " <Document: {'content': '├── mojap_derived_tables\\n├── dbt_project.yml\\n└── models\\n├── sources\\n│   ├── nomis.yaml\\n│   ├── oasys_prod.yaml\\n│   ├── delius_prod.yaml\\n│   ├── xhibit_v1.yaml\\n│   ...\\n├── security  # domain\\n│   ├── prison_safety_and_security  # database\\n│   │   ├── marts\\n│   │   │   ├── intermediate\\n│   │   │   │   ├── prison_safety_and_security_ inc_ids_grouped.sql  # intermediate table\\n│   │   │   │   ├── prison_safety_and_security questions_filtered.sql  # intermediate table\\n│   │   │   │   ├── prison_safety_and_security question_set_joined.sql  # intermediate table\\n│   │   │   │   └── properties.yaml  # intermediate tables property file\\n│   │   │   └── question_answers  # arbitrary grouping\\n│   │   │       ├── prison_safety_and_security dates_dim.sql  # dimension table\\n│   │   │       ├── prison_safety_and_security dates_dim.yaml  # table property file\\n│   │   │       ├── prison_safety_and_security incidents_fct.sql  # fact table\\n│   │   │       ├── prison_safety_and_security incidents_fct.yaml  # table property file\\n│   │   │       ├── prison_safety_and_security question_answer_fct.sql  # fact table\\n│   │   │       └── prison_safety_and_security question_answer_fct.yaml  # table property file\\n│   │   └── staging\\n│   │       ├── prison_safety_and_security nomis_mod_stg.md  # markdown to be rendered in documentation\\n│   │       ├── prison_safety_and_security nomis_mod_stg.sql  # staging table\\n│   │       └── prison_safety_and_security _nomis_mod_stg.yaml  # table property file Data modelling Data modelling is hard, so if the considerations about domains, databases, or data model structures aren’t clear, reach out to the data modelling team and we’ll do our best to help you out. This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nProject Structure - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 308}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8ba2aff2bb659101f66dd156ccd2d5f8'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 309}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Project Structure The Create a Derived Table service should be used for creating tables with well defined use cases, like to serve a performance metric, publication, or MI report. This is because the dbt project ( mojap_derived_tables ) is a shared space and so knowing where to put your work in the broader structure of the project is key. That’s not to say you can’t explore and experiment with dbt within the dbt project, there’s a development envionment where you can try things out without having to worry about making a mess of things. More on that later. Domains The primary consideration relating to project structure is understanding which domain the table you want to create belongs to. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 310}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '481c744022cb1e2dcaa6f40f1774a2b2'}>,\n",
       " <Document: {'content': 'In Create a Derived Table a domain should correspond to some service area or core business concept and is used to logically group databases. Domains are not mutually exclusive so the same concepts can exist in different domains. A domain may be ‘people’ relating HR and corporate, or ‘risk’ relating to a justice system service user’s safety, but it could be more or less granular if appropriate. Databases The secondary consideration is whether the tables you are creating belong in an existing database, if they do, then this step is easy. If you need to create a new database then you’ll need to decide which domain to put it in. It’s also possible to define a database across multiple domains. For example, a number of tables within your database might sit within ‘domain a’ while the rest sit in ‘domain b’. This approach has the benefit of keeping all tables logically grouped within one database but will result in access to those tables being limited by the domain. Standard directory structure and naming conventions The following is an example of how a team might build a data model whilst adhearing to the standard dbt project directory structure required to work with Create a Derived Table. The Prison Safety and Security team have created a database called prison_safety_and_security in the security domain. From the mojap_derived_tables dbt project, the hierarchy of directories must follow models -> domain -> database . The directory structure after this is arbitrary and can be chosen to suit your needs. However, we do recommend that you arrange your models into data marts and suffix your models with descriptions (this will be discussed in more detail). Directory and file names should only use snake case, as in, no-hyphens-like-this , just_underscores_like_this . Models ( .sql files) must be named by the database and table they relate to separated by double underscores, i.e., <database_name>__<table_name>.sql . This is because all models in the models directory must have a unique name. Suffixes should be added that describe each model’s purpose. Fact and dimension models should be suffixed with _fct and _dim respectively. Intermediate models should be suffixed with the main transformation being applied in past tense, e.g., _grouped or _left_joined . You may want to arrange staging models into a specific staging database, or within a single database for all your tables. Staging models should be suffixed with _stg unless built into a specific staging database. Fact, dimension, and staging models must have their own property file that has the same filename as the model they define properties for. Intermediate models should have a single configuration file named properties.yaml . Seed property files must have the same filename as the seed they define properties for. If you have ideas about how you would like to structure your data model, please get in touch as we’d love to hear from you to help guide best practice principles. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 311}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd15b67db636052331f3a401d32bf74be'}>,\n",
       " <Document: {'content': '├── mojap_derived_tables\\n├── dbt_project.yml\\n└── models\\n├── sources\\n│   ├── nomis.yaml\\n│   ├── oasys_prod.yaml\\n│   ├── delius_prod.yaml\\n│   ├── xhibit_v1.yaml\\n│   ...\\n├── security  # domain\\n│   ├── prison_safety_and_security  # database\\n│   │   ├── marts\\n│   │   │   ├── intermediate\\n│   │   │   │   ├── prison_safety_and_security_ inc_ids_grouped.sql  # intermediate table\\n│   │   │   │   ├── prison_safety_and_security questions_filtered.sql  # intermediate table\\n│   │   │   │   ├── prison_safety_and_security question_set_joined.sql  # intermediate table\\n│   │   │   │   └── properties.yaml  # intermediate tables property file\\n│   │   │   └── question_answers  # arbitrary grouping\\n│   │   │       ├── prison_safety_and_security dates_dim.sql  # dimension table\\n│   │   │       ├── prison_safety_and_security dates_dim.yaml  # table property file\\n│   │   │       ├── prison_safety_and_security incidents_fct.sql  # fact table\\n│   │   │       ├── prison_safety_and_security incidents_fct.yaml  # table property file\\n│   │   │       ├── prison_safety_and_security question_answer_fct.sql  # fact table\\n│   │   │       └── prison_safety_and_security question_answer_fct.yaml  # table property file\\n│   │   └── staging\\n│   │       ├── prison_safety_and_security nomis_mod_stg.md  # markdown to be rendered in documentation\\n│   │       ├── prison_safety_and_security nomis_mod_stg.sql  # staging table\\n│   │       └── prison_safety_and_security _nomis_mod_stg.yaml  # table property file Data modelling Data modelling is hard, so if the considerations about domains, databases, or data model structures aren’t clear, reach out to the data modelling team and we’ll do our best to help you out. This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nProject Structure - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 312}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8ba2aff2bb659101f66dd156ccd2d5f8'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 313}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Project Structure The Create a Derived Table service should be used for creating tables with well defined use cases, like to serve a performance metric, publication, or MI report. This is because the dbt project ( mojap_derived_tables ) is a shared space and so knowing where to put your work in the broader structure of the project is key. That’s not to say you can’t explore and experiment with dbt within the dbt project, there’s a development envionment where you can try things out without having to worry about making a mess of things. More on that later. Domains The primary consideration relating to project structure is understanding which domain the table you want to create belongs to. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 314}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '481c744022cb1e2dcaa6f40f1774a2b2'}>,\n",
       " <Document: {'content': 'In Create a Derived Table a domain should correspond to some service area or core business concept and is used to logically group databases. Domains are not mutually exclusive so the same concepts can exist in different domains. A domain may be ‘people’ relating HR and corporate, or ‘risk’ relating to a justice system service user’s safety, but it could be more or less granular if appropriate. Databases The secondary consideration is whether the tables you are creating belong in an existing database, if they do, then this step is easy. If you need to create a new database then you’ll need to decide which domain to put it in. It’s also possible to define a database across multiple domains. For example, a number of tables within your database might sit within ‘domain a’ while the rest sit in ‘domain b’. This approach has the benefit of keeping all tables logically grouped within one database but will result in access to those tables being limited by the domain. Standard directory structure and naming conventions The following is an example of how a team might build a data model whilst adhearing to the standard dbt project directory structure required to work with Create a Derived Table. The Prison Safety and Security team have created a database called prison_safety_and_security in the security domain. From the mojap_derived_tables dbt project, the hierarchy of directories must follow models -> domain -> database . The directory structure after this is arbitrary and can be chosen to suit your needs. However, we do recommend that you arrange your models into data marts and suffix your models with descriptions (this will be discussed in more detail). Directory and file names should only use snake case, as in, no-hyphens-like-this , just_underscores_like_this . Models ( .sql files) must be named by the database and table they relate to separated by double underscores, i.e., <database_name>__<table_name>.sql . This is because all models in the models directory must have a unique name. Suffixes should be added that describe each model’s purpose. Fact and dimension models should be suffixed with _fct and _dim respectively. Intermediate models should be suffixed with the main transformation being applied in past tense, e.g., _grouped or _left_joined . You may want to arrange staging models into a specific staging database, or within a single database for all your tables. Staging models should be suffixed with _stg unless built into a specific staging database. Fact, dimension, and staging models must have their own property file that has the same filename as the model they define properties for. Intermediate models should have a single configuration file named properties.yaml . Seed property files must have the same filename as the seed they define properties for. If you have ideas about how you would like to structure your data model, please get in touch as we’d love to hear from you to help guide best practice principles. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 315}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd15b67db636052331f3a401d32bf74be'}>,\n",
       " <Document: {'content': '├── mojap_derived_tables\\n├── dbt_project.yml\\n└── models\\n├── sources\\n│   ├── nomis.yaml\\n│   ├── oasys_prod.yaml\\n│   ├── delius_prod.yaml\\n│   ├── xhibit_v1.yaml\\n│   ...\\n├── security  # domain\\n│   ├── prison_safety_and_security  # database\\n│   │   ├── marts\\n│   │   │   ├── intermediate\\n│   │   │   │   ├── prison_safety_and_security_ inc_ids_grouped.sql  # intermediate table\\n│   │   │   │   ├── prison_safety_and_security questions_filtered.sql  # intermediate table\\n│   │   │   │   ├── prison_safety_and_security question_set_joined.sql  # intermediate table\\n│   │   │   │   └── properties.yaml  # intermediate tables property file\\n│   │   │   └── question_answers  # arbitrary grouping\\n│   │   │       ├── prison_safety_and_security dates_dim.sql  # dimension table\\n│   │   │       ├── prison_safety_and_security dates_dim.yaml  # table property file\\n│   │   │       ├── prison_safety_and_security incidents_fct.sql  # fact table\\n│   │   │       ├── prison_safety_and_security incidents_fct.yaml  # table property file\\n│   │   │       ├── prison_safety_and_security question_answer_fct.sql  # fact table\\n│   │   │       └── prison_safety_and_security question_answer_fct.yaml  # table property file\\n│   │   └── staging\\n│   │       ├── prison_safety_and_security nomis_mod_stg.md  # markdown to be rendered in documentation\\n│   │       ├── prison_safety_and_security nomis_mod_stg.sql  # staging table\\n│   │       └── prison_safety_and_security _nomis_mod_stg.yaml  # table property file Data modelling Data modelling is hard, so if the considerations about domains, databases, or data model structures aren’t clear, reach out to the data modelling team and we’ll do our best to help you out. This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nModels - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 316}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '54bad73e8c28d1b50d509f0716340c7a'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 317}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Models What is a model? A model is a select statement. Models are defined in .sql files and each .sql file contains one model/ select statement. The term ‘model’ is almost synonymous with ‘table’ and for the most part can be used interchangeably and thought of the same thing. The term ‘model’ is used because a model can be materialised in different ways; it can be ephemeral, a view, or indeed a table. More on materialisations later. From here on the term ‘model’ will be used instead of ‘table’. Model properties Resources in your project — models, seeds, tests, and the rest — can have a number of declared properties. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 318}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'efc79548fb85c6d4db0c8dddd0c155be'}>,\n",
       " <Document: {'content': \"Resources can also define configurations, which are a special kind of property that bring extra abilities. What’s the distinction? Properties are declared for resources one-by-one in .yaml files. Configs can be defined there, nested under a config property. They can also be set one-by-one via a config() macro directly in model ( .sql ) files, and for many resources at once in dbt_project.yml . Because configs can be set in multiple places, they are also applied hierarchically. An individual resource might inherit or override configs set elsewhere. A rule of thumb: properties declare things about your project resources; configs go the extra step of telling dbt how to build those resources in Athena. This is generally true, but not always, so it’s always good to check! For example, you can use resource properties to: Describe models, snapshots, seed files, and their columns Assert “truths” about a model, in the form of tests, e.g. “this id column is unique” Define pointers to existing tables that contain raw data, in the form of sources, and assert the expected “freshness” of this raw data Define official downstream uses of your data models, in the form of exposures Whereas you can use configurations to: Change how a model will be materialised (table, view, incremental, etc) Overwrite where model or seed data will be written to Declare whether a resource should persist its descriptions as comments in the database Apply tags and “meta” properties Where can I define configs? Configure a whole directory of models, seeds, tests, etc. from the dbt_project.yml file, under the corresponding resource key ( models: , seeds: , tests: , etc). In the example below the materialized: table configuration has been applied to the entire mojap_derived_tables project. The sentences/ and question_answers/ directories have schedule tags configured for all models in those respective directories. ⚠️ Only add configurations to your own work! ⚠️ models:\\nmojap_derived_tables:\\n+materialized: table prison:\\nprison_safety_and_security:\\nmarts:\\nsentences:\\n+tags: monthly\\n\\nquestion_answers:\\n+tags: weekly Configure an individual model, seed, or test using a config property in a .yaml property file. This is the preferred method for applying configurations because it groups the configurations with defined properties for a given model, etc. and provides good visibility of what’s being applied. The below example applies the incremental materialisation and partitioned by configuration to the question_answer_fct model. version: 2 models:\\n- name: prison_safety_and_security__question_answer_fct\\ndescription: The question and answer fact table.\\nconfig:\\nmaterialized: incremental\\nincremental_strategy: append\\npartitioned_by: ['snapshot_date']\\n+column_types:\\ncolumn_1: varchar(5)\\ncolumn_2: int If for some reason it is not possible or reasonable to apply a configuration in a property file, you can use a config() Jinja macro within a model or test SQL file. The following example shows how the same configuration above can be applied in a model or test file. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 319}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f5528220d948da47fb2b09c7c388406d'}>,\n",
       " <Document: {'content': \"{{\\nconfig(\\nmaterialized='incremental'\\nincremental_strategy='append'\\npartitioned_by=['snapshot_date']\\n)\\n}} Config inheritance Configurations are prioritised in order of specificity, which is generally the inverse of the order above: an in-file config() block takes precedence over properties defied in a .yaml property file, which takes precedence over a configuration defined in the dbt_project.yml file. (Note that generic tests work a little differently when it comes to specificity. See dbt’s documentation on test configs .) Materialisations Materialisations are strategies for persisting dbt models in a warehouse. There are four types of materializations built into dbt. They are: table view ⚠️ not currently supported ⚠️ incremental ephemeral This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nModels - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 320}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '96dfb0f8408858063d61aea697b9cec2'}>,\n",
       " <Document: {'content': 'Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 321}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e8ca0924c3a77c45321578c4e5b38fd'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Models What is a model? A model is a select statement. Models are defined in .sql files and each .sql file contains one model/ select statement. The term ‘model’ is almost synonymous with ‘table’ and for the most part can be used interchangeably and thought of the same thing. The term ‘model’ is used because a model can be materialised in different ways; it can be ephemeral, a view, or indeed a table. More on materialisations later. From here on the term ‘model’ will be used instead of ‘table’. Model properties Resources in your project — models, seeds, tests, and the rest — can have a number of declared properties. Resources can also define configurations, which are a special kind of property that bring extra abilities. What’s the distinction? Properties are declared for resources one-by-one in .yaml files. Configs can be defined there, nested under a config property. They can also be set one-by-one via a config() macro directly in model ( .sql ) files, and for many resources at once in dbt_project.yml . Because configs can be set in multiple places, they are also applied hierarchically. An individual resource might inherit or override configs set elsewhere. A rule of thumb: properties declare things about your project resources; configs go the extra step of telling dbt how to build those resources in Athena. This is generally true, but not always, so it’s always good to check! ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 322}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4f01b3692d88fd19955ed18fe09f3122'}>,\n",
       " <Document: {'content': \"For example, you can use resource properties to: Describe models, snapshots, seed files, and their columns Assert “truths” about a model, in the form of tests, e.g. “this id column is unique” Define pointers to existing tables that contain raw data, in the form of sources, and assert the expected “freshness” of this raw data Define official downstream uses of your data models, in the form of exposures Whereas you can use configurations to: Change how a model will be materialised (table, view, incremental, etc) Overwrite where model or seed data will be written to Declare whether a resource should persist its descriptions as comments in the database Apply tags and “meta” properties Where can I define configs? Configure a whole directory of models, seeds, tests, etc. from the dbt_project.yml file, under the corresponding resource key ( models: , seeds: , tests: , etc). In the example below the materialized: table configuration has been applied to the entire mojap_derived_tables project. The sentences/ and question_answers/ directories have schedule tags configured for all models in those respective directories. ⚠️ Only add configurations to your own work! ⚠️ models:\\nmojap_derived_tables:\\n+materialized: table prison:\\nprison_safety_and_security:\\nmarts:\\nsentences:\\n+tags: monthly\\n\\nquestion_answers:\\n+tags: weekly Configure an individual model, seed, or test using a config property in a .yaml property file. This is the preferred method for applying configurations because it groups the configurations with defined properties for a given model, etc. and provides good visibility of what’s being applied. The below example applies the incremental materialisation and partitioned by configuration to the question_answer_fct model. version: 2 models:\\n- name: prison_safety_and_security__question_answer_fct\\ndescription: The question and answer fact table.\\nconfig:\\nmaterialized: incremental\\nincremental_strategy: append\\npartitioned_by: ['snapshot_date']\\n+column_types:\\ncolumn_1: varchar(5)\\ncolumn_2: int If for some reason it is not possible or reasonable to apply a configuration in a property file, you can use a config() Jinja macro within a model or test SQL file. The following example shows how the same configuration above can be applied in a model or test file. {{\\nconfig(\\nmaterialized='incremental'\\nincremental_strategy='append'\\npartitioned_by=['snapshot_date']\\n)\\n}} Config inheritance Configurations are prioritised in order of specificity, which is generally the inverse of the order above: an in-file config() block takes precedence over properties defied in a .yaml property file, which takes precedence over a configuration defined in the dbt_project.yml file. (Note that generic tests work a little differently when it comes to specificity. See dbt’s documentation on test configs .) Materialisations Materialisations are strategies for persisting dbt models in a warehouse. There are four types of materializations built into dbt. They are: table view ⚠️ not currently supported ⚠️ incremental ephemeral This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 323}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7f2ab42ac9b913f400bb5928dc5e43cb'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nModels - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 324}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4b20642c9db5c4734dc455423c3c5f5e'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 325}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Models What is a model? A model is a select statement. Models are defined in .sql files and each .sql file contains one model/ select statement. The term ‘model’ is almost synonymous with ‘table’ and for the most part can be used interchangeably and thought of the same thing. The term ‘model’ is used because a model can be materialised in different ways; it can be ephemeral, a view, or indeed a table. More on materialisations later. From here on the term ‘model’ will be used instead of ‘table’. Model properties Resources in your project — models, seeds, tests, and the rest — can have a number of declared properties. Resources can also define configurations, which are a special kind of property that bring extra abilities. What’s the distinction? Properties are declared for resources one-by-one in .yaml files. Configs can be defined there, nested under a config property. They can also be set one-by-one via a config() macro directly in model ( .sql ) files, and for many resources at once in dbt_project.yml . Because configs can be set in multiple places, they are also applied hierarchically. An individual resource might inherit or override configs set elsewhere. A rule of thumb: properties declare things about your project resources; configs go the extra step of telling dbt how to build those resources in Athena. This is generally true, but not always, so it’s always good to check! ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 326}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4f01b3692d88fd19955ed18fe09f3122'}>,\n",
       " <Document: {'content': \"For example, you can use resource properties to: Describe models, snapshots, seed files, and their columns Assert “truths” about a model, in the form of tests, e.g. “this id column is unique” Define pointers to existing tables that contain raw data, in the form of sources, and assert the expected “freshness” of this raw data Define official downstream uses of your data models, in the form of exposures Whereas you can use configurations to: Change how a model will be materialised (table, view, incremental, etc) Overwrite where model or seed data will be written to Declare whether a resource should persist its descriptions as comments in the database Apply tags and “meta” properties Where can I define configs? Configure a whole directory of models, seeds, tests, etc. from the dbt_project.yml file, under the corresponding resource key ( models: , seeds: , tests: , etc). In the example below the materialized: table configuration has been applied to the entire mojap_derived_tables project. The sentences/ and question_answers/ directories have schedule tags configured for all models in those respective directories. ⚠️ Only add configurations to your own work! ⚠️ models:\\nmojap_derived_tables:\\n+materialized: table prison:\\nprison_safety_and_security:\\nmarts:\\nsentences:\\n+tags: monthly\\n\\nquestion_answers:\\n+tags: weekly Configure an individual model, seed, or test using a config property in a .yaml property file. This is the preferred method for applying configurations because it groups the configurations with defined properties for a given model, etc. and provides good visibility of what’s being applied. The below example applies the incremental materialisation and partitioned by configuration to the question_answer_fct model. version: 2 models:\\n- name: prison_safety_and_security__question_answer_fct\\ndescription: The question and answer fact table.\\nconfig:\\nmaterialized: incremental\\nincremental_strategy: append\\npartitioned_by: ['snapshot_date']\\n+column_types:\\ncolumn_1: varchar(5)\\ncolumn_2: int If for some reason it is not possible or reasonable to apply a configuration in a property file, you can use a config() Jinja macro within a model or test SQL file. The following example shows how the same configuration above can be applied in a model or test file. {{\\nconfig(\\nmaterialized='incremental'\\nincremental_strategy='append'\\npartitioned_by=['snapshot_date']\\n)\\n}} Config inheritance Configurations are prioritised in order of specificity, which is generally the inverse of the order above: an in-file config() block takes precedence over properties defied in a .yaml property file, which takes precedence over a configuration defined in the dbt_project.yml file. (Note that generic tests work a little differently when it comes to specificity. See dbt’s documentation on test configs .) Materialisations Materialisations are strategies for persisting dbt models in a warehouse. There are four types of materializations built into dbt. They are: table view ⚠️ not currently supported ⚠️ incremental ephemeral This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 327}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7f2ab42ac9b913f400bb5928dc5e43cb'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nModels - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 328}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4b20642c9db5c4734dc455423c3c5f5e'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 329}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Models What is a model? A model is a select statement. Models are defined in .sql files and each .sql file contains one model/ select statement. The term ‘model’ is almost synonymous with ‘table’ and for the most part can be used interchangeably and thought of the same thing. The term ‘model’ is used because a model can be materialised in different ways; it can be ephemeral, a view, or indeed a table. More on materialisations later. From here on the term ‘model’ will be used instead of ‘table’. Model properties Resources in your project — models, seeds, tests, and the rest — can have a number of declared properties. Resources can also define configurations, which are a special kind of property that bring extra abilities. What’s the distinction? Properties are declared for resources one-by-one in .yaml files. Configs can be defined there, nested under a config property. They can also be set one-by-one via a config() macro directly in model ( .sql ) files, and for many resources at once in dbt_project.yml . Because configs can be set in multiple places, they are also applied hierarchically. An individual resource might inherit or override configs set elsewhere. A rule of thumb: properties declare things about your project resources; configs go the extra step of telling dbt how to build those resources in Athena. This is generally true, but not always, so it’s always good to check! ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 330}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4f01b3692d88fd19955ed18fe09f3122'}>,\n",
       " <Document: {'content': \"For example, you can use resource properties to: Describe models, snapshots, seed files, and their columns Assert “truths” about a model, in the form of tests, e.g. “this id column is unique” Define pointers to existing tables that contain raw data, in the form of sources, and assert the expected “freshness” of this raw data Define official downstream uses of your data models, in the form of exposures Whereas you can use configurations to: Change how a model will be materialised (table, view, incremental, etc) Overwrite where model or seed data will be written to Declare whether a resource should persist its descriptions as comments in the database Apply tags and “meta” properties Where can I define configs? Configure a whole directory of models, seeds, tests, etc. from the dbt_project.yml file, under the corresponding resource key ( models: , seeds: , tests: , etc). In the example below the materialized: table configuration has been applied to the entire mojap_derived_tables project. The sentences/ and question_answers/ directories have schedule tags configured for all models in those respective directories. ⚠️ Only add configurations to your own work! ⚠️ models:\\nmojap_derived_tables:\\n+materialized: table prison:\\nprison_safety_and_security:\\nmarts:\\nsentences:\\n+tags: monthly\\n\\nquestion_answers:\\n+tags: weekly Configure an individual model, seed, or test using a config property in a .yaml property file. This is the preferred method for applying configurations because it groups the configurations with defined properties for a given model, etc. and provides good visibility of what’s being applied. The below example applies the incremental materialisation and partitioned by configuration to the question_answer_fct model. version: 2 models:\\n- name: prison_safety_and_security__question_answer_fct\\ndescription: The question and answer fact table.\\nconfig:\\nmaterialized: incremental\\nincremental_strategy: append\\npartitioned_by: ['snapshot_date']\\n+column_types:\\ncolumn_1: varchar(5)\\ncolumn_2: int If for some reason it is not possible or reasonable to apply a configuration in a property file, you can use a config() Jinja macro within a model or test SQL file. The following example shows how the same configuration above can be applied in a model or test file. {{\\nconfig(\\nmaterialized='incremental'\\nincremental_strategy='append'\\npartitioned_by=['snapshot_date']\\n)\\n}} Config inheritance Configurations are prioritised in order of specificity, which is generally the inverse of the order above: an in-file config() block takes precedence over properties defied in a .yaml property file, which takes precedence over a configuration defined in the dbt_project.yml file. (Note that generic tests work a little differently when it comes to specificity. See dbt’s documentation on test configs .) Materialisations Materialisations are strategies for persisting dbt models in a warehouse. There are four types of materializations built into dbt. They are: table view ⚠️ not currently supported ⚠️ incremental ephemeral This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 331}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7f2ab42ac9b913f400bb5928dc5e43cb'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nModels - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 332}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4b20642c9db5c4734dc455423c3c5f5e'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 333}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Models What is a model? A model is a select statement. Models are defined in .sql files and each .sql file contains one model/ select statement. The term ‘model’ is almost synonymous with ‘table’ and for the most part can be used interchangeably and thought of the same thing. The term ‘model’ is used because a model can be materialised in different ways; it can be ephemeral, a view, or indeed a table. More on materialisations later. From here on the term ‘model’ will be used instead of ‘table’. Model properties Resources in your project — models, seeds, tests, and the rest — can have a number of declared properties. Resources can also define configurations, which are a special kind of property that bring extra abilities. What’s the distinction? Properties are declared for resources one-by-one in .yaml files. Configs can be defined there, nested under a config property. They can also be set one-by-one via a config() macro directly in model ( .sql ) files, and for many resources at once in dbt_project.yml . Because configs can be set in multiple places, they are also applied hierarchically. An individual resource might inherit or override configs set elsewhere. A rule of thumb: properties declare things about your project resources; configs go the extra step of telling dbt how to build those resources in Athena. This is generally true, but not always, so it’s always good to check! ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 334}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4f01b3692d88fd19955ed18fe09f3122'}>,\n",
       " <Document: {'content': \"For example, you can use resource properties to: Describe models, snapshots, seed files, and their columns Assert “truths” about a model, in the form of tests, e.g. “this id column is unique” Define pointers to existing tables that contain raw data, in the form of sources, and assert the expected “freshness” of this raw data Define official downstream uses of your data models, in the form of exposures Whereas you can use configurations to: Change how a model will be materialised (table, view, incremental, etc) Overwrite where model or seed data will be written to Declare whether a resource should persist its descriptions as comments in the database Apply tags and “meta” properties Where can I define configs? Configure a whole directory of models, seeds, tests, etc. from the dbt_project.yml file, under the corresponding resource key ( models: , seeds: , tests: , etc). In the example below the materialized: table configuration has been applied to the entire mojap_derived_tables project. The sentences/ and question_answers/ directories have schedule tags configured for all models in those respective directories. ⚠️ Only add configurations to your own work! ⚠️ models:\\nmojap_derived_tables:\\n+materialized: table prison:\\nprison_safety_and_security:\\nmarts:\\nsentences:\\n+tags: monthly\\n\\nquestion_answers:\\n+tags: weekly Configure an individual model, seed, or test using a config property in a .yaml property file. This is the preferred method for applying configurations because it groups the configurations with defined properties for a given model, etc. and provides good visibility of what’s being applied. The below example applies the incremental materialisation and partitioned by configuration to the question_answer_fct model. version: 2 models:\\n- name: prison_safety_and_security__question_answer_fct\\ndescription: The question and answer fact table.\\nconfig:\\nmaterialized: incremental\\nincremental_strategy: append\\npartitioned_by: ['snapshot_date']\\n+column_types:\\ncolumn_1: varchar(5)\\ncolumn_2: int If for some reason it is not possible or reasonable to apply a configuration in a property file, you can use a config() Jinja macro within a model or test SQL file. The following example shows how the same configuration above can be applied in a model or test file. {{\\nconfig(\\nmaterialized='incremental'\\nincremental_strategy='append'\\npartitioned_by=['snapshot_date']\\n)\\n}} Config inheritance Configurations are prioritised in order of specificity, which is generally the inverse of the order above: an in-file config() block takes precedence over properties defied in a .yaml property file, which takes precedence over a configuration defined in the dbt_project.yml file. (Note that generic tests work a little differently when it comes to specificity. See dbt’s documentation on test configs .) Materialisations Materialisations are strategies for persisting dbt models in a warehouse. There are four types of materializations built into dbt. They are: table view ⚠️ not currently supported ⚠️ incremental ephemeral This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 335}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7f2ab42ac9b913f400bb5928dc5e43cb'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nModels - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 336}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4b20642c9db5c4734dc455423c3c5f5e'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 337}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Models What is a model? A model is a select statement. Models are defined in .sql files and each .sql file contains one model/ select statement. The term ‘model’ is almost synonymous with ‘table’ and for the most part can be used interchangeably and thought of the same thing. The term ‘model’ is used because a model can be materialised in different ways; it can be ephemeral, a view, or indeed a table. More on materialisations later. From here on the term ‘model’ will be used instead of ‘table’. Model properties Resources in your project — models, seeds, tests, and the rest — can have a number of declared properties. Resources can also define configurations, which are a special kind of property that bring extra abilities. What’s the distinction? Properties are declared for resources one-by-one in .yaml files. Configs can be defined there, nested under a config property. They can also be set one-by-one via a config() macro directly in model ( .sql ) files, and for many resources at once in dbt_project.yml . Because configs can be set in multiple places, they are also applied hierarchically. An individual resource might inherit or override configs set elsewhere. A rule of thumb: properties declare things about your project resources; configs go the extra step of telling dbt how to build those resources in Athena. This is generally true, but not always, so it’s always good to check! ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 338}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4f01b3692d88fd19955ed18fe09f3122'}>,\n",
       " <Document: {'content': \"For example, you can use resource properties to: Describe models, snapshots, seed files, and their columns Assert “truths” about a model, in the form of tests, e.g. “this id column is unique” Define pointers to existing tables that contain raw data, in the form of sources, and assert the expected “freshness” of this raw data Define official downstream uses of your data models, in the form of exposures Whereas you can use configurations to: Change how a model will be materialised (table, view, incremental, etc) Overwrite where model or seed data will be written to Declare whether a resource should persist its descriptions as comments in the database Apply tags and “meta” properties Where can I define configs? Configure a whole directory of models, seeds, tests, etc. from the dbt_project.yml file, under the corresponding resource key ( models: , seeds: , tests: , etc). In the example below the materialized: table configuration has been applied to the entire mojap_derived_tables project. The sentences/ and question_answers/ directories have schedule tags configured for all models in those respective directories. ⚠️ Only add configurations to your own work! ⚠️ models:\\nmojap_derived_tables:\\n+materialized: table prison:\\nprison_safety_and_security:\\nmarts:\\nsentences:\\n+tags: monthly\\n\\nquestion_answers:\\n+tags: weekly Configure an individual model, seed, or test using a config property in a .yaml property file. This is the preferred method for applying configurations because it groups the configurations with defined properties for a given model, etc. and provides good visibility of what’s being applied. The below example applies the incremental materialisation and partitioned by configuration to the question_answer_fct model. version: 2 models:\\n- name: prison_safety_and_security__question_answer_fct\\ndescription: The question and answer fact table.\\nconfig:\\nmaterialized: incremental\\nincremental_strategy: append\\npartitioned_by: ['snapshot_date']\\n+column_types:\\ncolumn_1: varchar(5)\\ncolumn_2: int If for some reason it is not possible or reasonable to apply a configuration in a property file, you can use a config() Jinja macro within a model or test SQL file. The following example shows how the same configuration above can be applied in a model or test file. {{\\nconfig(\\nmaterialized='incremental'\\nincremental_strategy='append'\\npartitioned_by=['snapshot_date']\\n)\\n}} Config inheritance Configurations are prioritised in order of specificity, which is generally the inverse of the order above: an in-file config() block takes precedence over properties defied in a .yaml property file, which takes precedence over a configuration defined in the dbt_project.yml file. (Note that generic tests work a little differently when it comes to specificity. See dbt’s documentation on test configs .) Materialisations Materialisations are strategies for persisting dbt models in a warehouse. There are four types of materializations built into dbt. They are: table view ⚠️ not currently supported ⚠️ incremental ephemeral This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 339}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7f2ab42ac9b913f400bb5928dc5e43cb'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nSource and Ref Functions - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 340}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2f61b46cbcbfb29d5aebae0aa3d6813f'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 341}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Source and Ref Functions Sources Sources are descriptions of the databases and tables already in Analytical Platform. With those tables defined as sources in dbt, it is then possible to select from source tables in your models using the source() function which helps define the lineage of your data. To see which sources have been defined, look in the ./mojap_derived_tables/models/sources/ directory.\\nBelow is an example of using the source() function to select from the contact table in the delius_prod database: model_a.sql select * from {{ source(\"delius_prod\", \"contact\") }} limit 10 Adding a new source If a database is not defined as a source it must be added. Please follow the instructions below: Create a new branch off main . Add the source database name exactly as it appears in AWS Athena to the list in scripts/source_database_name.txt . Ensure it is in alphabetical order for readability. Commit and push the changes, then raise a pull request. The update-source workflow is scheduled to run weekly. When run it generates the YAML code for all sources listed, creating any new files and updating existing ones. Then it raises a PR to merge these changes into main . Once this PR is merged the source is available. If you need a source urgently please follow the the steps above and then contact the Data Modelling team at #ask-data-modelling . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 342}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f6a297a5628a67b9de586b625eea9b6d'}>,\n",
       " <Document: {'content': 'The ref function The most important function in dbt is ref() ; it’s impossible to build even moderately complex models without it. ref() is how you reference one model within another as typically models are built to be ‘stacked’ on top of one another. These references between models are then used to automatically build the dependency graph. Here is how this looks in practice: model_b.sql select contact_id from {{ ref(\"model_a\") }}\\nwhere mojap_current_record=true This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nSource and Ref Functions - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 343}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '83d2bc12d3bfc7e00e772224f5a36aa'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 344}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Source and Ref Functions Sources Sources are descriptions of the databases and tables already in Analytical Platform. With those tables defined as sources in dbt, it is then possible to select from source tables in your models using the source() function which helps define the lineage of your data. To see which sources have been defined, look in the ./mojap_derived_tables/models/sources/ directory.\\nBelow is an example of using the source() function to select from the contact table in the delius_prod database: model_a.sql select * from {{ source(\"delius_prod\", \"contact\") }} limit 10 Adding a new source If a database is not defined as a source it must be added. Please follow the instructions below: Create a new branch off main . Add the source database name exactly as it appears in AWS Athena to the list in scripts/source_database_name.txt . Ensure it is in alphabetical order for readability. Commit and push the changes, then raise a pull request. The update-source workflow is scheduled to run weekly. When run it generates the YAML code for all sources listed, creating any new files and updating existing ones. Then it raises a PR to merge these changes into main . Once this PR is merged the source is available. If you need a source urgently please follow the the steps above and then contact the Data Modelling team at #ask-data-modelling . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 345}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f6a297a5628a67b9de586b625eea9b6d'}>,\n",
       " <Document: {'content': 'The ref function The most important function in dbt is ref() ; it’s impossible to build even moderately complex models without it. ref() is how you reference one model within another as typically models are built to be ‘stacked’ on top of one another. These references between models are then used to automatically build the dependency graph. Here is how this looks in practice: model_b.sql select contact_id from {{ ref(\"model_a\") }}\\nwhere mojap_current_record=true This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nSource and Ref Functions - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 346}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '83d2bc12d3bfc7e00e772224f5a36aa'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 347}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Source and Ref Functions Sources Sources are descriptions of the databases and tables already in Analytical Platform. With those tables defined as sources in dbt, it is then possible to select from source tables in your models using the source() function which helps define the lineage of your data. To see which sources have been defined, look in the ./mojap_derived_tables/models/sources/ directory.\\nBelow is an example of using the source() function to select from the contact table in the delius_prod database: model_a.sql select * from {{ source(\"delius_prod\", \"contact\") }} limit 10 Adding a new source If a database is not defined as a source it must be added. Please follow the instructions below: Create a new branch off main . Add the source database name exactly as it appears in AWS Athena to the list in scripts/source_database_name.txt . Ensure it is in alphabetical order for readability. Commit and push the changes, then raise a pull request. The update-source workflow is scheduled to run weekly. When run it generates the YAML code for all sources listed, creating any new files and updating existing ones. Then it raises a PR to merge these changes into main . Once this PR is merged the source is available. If you need a source urgently please follow the the steps above and then contact the Data Modelling team at #ask-data-modelling . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 348}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f6a297a5628a67b9de586b625eea9b6d'}>,\n",
       " <Document: {'content': 'The ref function The most important function in dbt is ref() ; it’s impossible to build even moderately complex models without it. ref() is how you reference one model within another as typically models are built to be ‘stacked’ on top of one another. These references between models are then used to automatically build the dependency graph. Here is how this looks in practice: model_b.sql select contact_id from {{ ref(\"model_a\") }}\\nwhere mojap_current_record=true This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nSource and Ref Functions - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 349}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '83d2bc12d3bfc7e00e772224f5a36aa'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 350}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Source and Ref Functions Sources Sources are descriptions of the databases and tables already in Analytical Platform. With those tables defined as sources in dbt, it is then possible to select from source tables in your models using the source() function which helps define the lineage of your data. To see which sources have been defined, look in the ./mojap_derived_tables/models/sources/ directory.\\nBelow is an example of using the source() function to select from the contact table in the delius_prod database: model_a.sql select * from {{ source(\"delius_prod\", \"contact\") }} limit 10 Adding a new source If a database is not defined as a source it must be added. Please follow the instructions below: Create a new branch off main . Add the source database name exactly as it appears in AWS Athena to the list in scripts/source_database_name.txt . Ensure it is in alphabetical order for readability. Commit and push the changes, then raise a pull request. The update-source workflow is scheduled to run weekly. When run it generates the YAML code for all sources listed, creating any new files and updating existing ones. Then it raises a PR to merge these changes into main . Once this PR is merged the source is available. If you need a source urgently please follow the the steps above and then contact the Data Modelling team at #ask-data-modelling . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 351}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f6a297a5628a67b9de586b625eea9b6d'}>,\n",
       " <Document: {'content': 'The ref function The most important function in dbt is ref() ; it’s impossible to build even moderately complex models without it. ref() is how you reference one model within another as typically models are built to be ‘stacked’ on top of one another. These references between models are then used to automatically build the dependency graph. Here is how this looks in practice: model_b.sql select contact_id from {{ ref(\"model_a\") }}\\nwhere mojap_current_record=true This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nSeeds - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 352}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8a7cf434d2cbc1182d7a5497af6a1073'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 353}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Seeds What are seeds? Seeds are lookup tables easily created from a .csv file. Put the .csv in the ./mojap_derived_tables/seeds/ directory and follow the same directory structure requirements and naming conventions as for models. As with marts models, your seeds should have property files that have the same filename as the seed. Seeds can be accessed by anyone with standard database access and so must not contain sensitive data. Generally, seeds shouldn’t contain more than 1000 rows, they don’t contain complex data types, and they don’t change very often. You can deploy a seed with more than 1000 rows, but it’s not reccomended and it will take quite a long time to build. ⚠️ Seeds must not contain sensitive data. ⚠️ This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nSeeds - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 354}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9d74f801677ca960ead7362ffa5f3298'}>,\n",
       " <Document: {'content': 'Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 355}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f5a617e2b8bf3e97f303a94c8b4da304'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Seeds What are seeds? Seeds are lookup tables easily created from a .csv file. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 356}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '55787d3ff2655eba4bb8b921a489ef6e'}>,\n",
       " <Document: {'content': 'Put the .csv in the ./mojap_derived_tables/seeds/ directory and follow the same directory structure requirements and naming conventions as for models. As with marts models, your seeds should have property files that have the same filename as the seed. Seeds can be accessed by anyone with standard database access and so must not contain sensitive data. Generally, seeds shouldn’t contain more than 1000 rows, they don’t contain complex data types, and they don’t change very often. You can deploy a seed with more than 1000 rows, but it’s not reccomended and it will take quite a long time to build. ⚠️ Seeds must not contain sensitive data. ⚠️ This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\ntests - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 357}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e78d17b344ce6ae250c0f7dbdd454993'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 358}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Tests Tests are assertions you make about your models and other resources (e.g. sources, seeds and snapshots). You can do things like test whether a column contains nulls or only unique values, compare row counts between tables, or check all of the records in a child table have a corresponding record in a parent table. dbt ships with some tests you can use but there many more out there in packages like dbt_utils , so do checkout dbt’s package hub for what’s available. For more information on tests, see dbt’s tests documentation . Available tests There is an ecosystem of packages containing helpful macros and tests you can use, see dbt package hub . If you need a package adding to the dbt project, update the packages.yml file then rerun dbt deps . Custom generic tests A test is really just a SQL query that returns rows that meet a certain criteria. If the number of returned rows is not zero, then the test fails. When you know that, you’ll soon realise that it’s not too difficult to write your own tests. Tests you write yourself are called custom generic tests and are writen in special macros; read the dbt guidance on how to write custom generic tests to find out more. There are some requirements specific to Create a Derived Table that you must follow when writing custom generic tests. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 359}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '392fc122da547bb5e99d4feb9de3bcec'}>,\n",
       " <Document: {'content': 'They must live subdirectory named after the team, project, or database they relate to in the ./mojap_derived_tables/tests/generic/ directory and should follow a naming convention where the subdirectory and test name are separated by double underscores, for example team_name__test_name . This is so that ownership of the test is clear and so that the filename is unique. ├── mojap_derived_tables\\n├── dbt_project.yml\\n└── tests\\n└── generic\\n├── prison_safety_and_security  # team name\\n├── prison_safety_and_security__test_name.sql\\n...\\n├── other_project_name  # project name\\n├── other_project_name__test_name.sql\\n...\\n├── other_database_name  # database name\\n├── other_database_name__test_name.sql\\n...\\n... Singular tests Singular tests should not be used and they will not be run in the production environment. Configuring tests Defining tests for your resources is done within the property file for that resource under the tests property. See the example below. version: 2\\n\\nmodels:\\n- name: <model_a>\\ntests:\\n- dbt_utils.equal_rowcount:\\ncompare_model: <model_b>\\ncolumns:\\n- name: <column_a>\\ntests:\\n- not_null\\n- dbt_utils.not_constant This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\ntests - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 360}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6245e179c6f550491687b6bbbe731a0a'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 361}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Tests Tests are assertions you make about your models and other resources (e.g. sources, seeds and snapshots). You can do things like test whether a column contains nulls or only unique values, compare row counts between tables, or check all of the records in a child table have a corresponding record in a parent table. dbt ships with some tests you can use but there many more out there in packages like dbt_utils , so do checkout dbt’s package hub for what’s available. For more information on tests, see dbt’s tests documentation . Available tests There is an ecosystem of packages containing helpful macros and tests you can use, see dbt package hub . If you need a package adding to the dbt project, update the packages.yml file then rerun dbt deps . Custom generic tests A test is really just a SQL query that returns rows that meet a certain criteria. If the number of returned rows is not zero, then the test fails. When you know that, you’ll soon realise that it’s not too difficult to write your own tests. Tests you write yourself are called custom generic tests and are writen in special macros; read the dbt guidance on how to write custom generic tests to find out more. There are some requirements specific to Create a Derived Table that you must follow when writing custom generic tests. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 362}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '392fc122da547bb5e99d4feb9de3bcec'}>,\n",
       " <Document: {'content': 'They must live subdirectory named after the team, project, or database they relate to in the ./mojap_derived_tables/tests/generic/ directory and should follow a naming convention where the subdirectory and test name are separated by double underscores, for example team_name__test_name . This is so that ownership of the test is clear and so that the filename is unique. ├── mojap_derived_tables\\n├── dbt_project.yml\\n└── tests\\n└── generic\\n├── prison_safety_and_security  # team name\\n├── prison_safety_and_security__test_name.sql\\n...\\n├── other_project_name  # project name\\n├── other_project_name__test_name.sql\\n...\\n├── other_database_name  # database name\\n├── other_database_name__test_name.sql\\n...\\n... Singular tests Singular tests should not be used and they will not be run in the production environment. Configuring tests Defining tests for your resources is done within the property file for that resource under the tests property. See the example below. version: 2\\n\\nmodels:\\n- name: <model_a>\\ntests:\\n- dbt_utils.equal_rowcount:\\ncompare_model: <model_b>\\ncolumns:\\n- name: <column_a>\\ntests:\\n- not_null\\n- dbt_utils.not_constant This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\ntests - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 363}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6245e179c6f550491687b6bbbe731a0a'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 364}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Tests Tests are assertions you make about your models and other resources (e.g. sources, seeds and snapshots). You can do things like test whether a column contains nulls or only unique values, compare row counts between tables, or check all of the records in a child table have a corresponding record in a parent table. dbt ships with some tests you can use but there many more out there in packages like dbt_utils , so do checkout dbt’s package hub for what’s available. For more information on tests, see dbt’s tests documentation . Available tests There is an ecosystem of packages containing helpful macros and tests you can use, see dbt package hub . If you need a package adding to the dbt project, update the packages.yml file then rerun dbt deps . Custom generic tests A test is really just a SQL query that returns rows that meet a certain criteria. If the number of returned rows is not zero, then the test fails. When you know that, you’ll soon realise that it’s not too difficult to write your own tests. Tests you write yourself are called custom generic tests and are writen in special macros; read the dbt guidance on how to write custom generic tests to find out more. There are some requirements specific to Create a Derived Table that you must follow when writing custom generic tests. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 365}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '392fc122da547bb5e99d4feb9de3bcec'}>,\n",
       " <Document: {'content': 'They must live subdirectory named after the team, project, or database they relate to in the ./mojap_derived_tables/tests/generic/ directory and should follow a naming convention where the subdirectory and test name are separated by double underscores, for example team_name__test_name . This is so that ownership of the test is clear and so that the filename is unique. ├── mojap_derived_tables\\n├── dbt_project.yml\\n└── tests\\n└── generic\\n├── prison_safety_and_security  # team name\\n├── prison_safety_and_security__test_name.sql\\n...\\n├── other_project_name  # project name\\n├── other_project_name__test_name.sql\\n...\\n├── other_database_name  # database name\\n├── other_database_name__test_name.sql\\n...\\n... Singular tests Singular tests should not be used and they will not be run in the production environment. Configuring tests Defining tests for your resources is done within the property file for that resource under the tests property. See the example below. version: 2\\n\\nmodels:\\n- name: <model_a>\\ntests:\\n- dbt_utils.equal_rowcount:\\ncompare_model: <model_b>\\ncolumns:\\n- name: <column_a>\\ntests:\\n- not_null\\n- dbt_utils.not_constant This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\ntests - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 366}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6245e179c6f550491687b6bbbe731a0a'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 367}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Tests Tests are assertions you make about your models and other resources (e.g. sources, seeds and snapshots). You can do things like test whether a column contains nulls or only unique values, compare row counts between tables, or check all of the records in a child table have a corresponding record in a parent table. dbt ships with some tests you can use but there many more out there in packages like dbt_utils , so do checkout dbt’s package hub for what’s available. For more information on tests, see dbt’s tests documentation . Available tests There is an ecosystem of packages containing helpful macros and tests you can use, see dbt package hub . If you need a package adding to the dbt project, update the packages.yml file then rerun dbt deps . Custom generic tests A test is really just a SQL query that returns rows that meet a certain criteria. If the number of returned rows is not zero, then the test fails. When you know that, you’ll soon realise that it’s not too difficult to write your own tests. Tests you write yourself are called custom generic tests and are writen in special macros; read the dbt guidance on how to write custom generic tests to find out more. There are some requirements specific to Create a Derived Table that you must follow when writing custom generic tests. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 368}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '392fc122da547bb5e99d4feb9de3bcec'}>,\n",
       " <Document: {'content': 'They must live subdirectory named after the team, project, or database they relate to in the ./mojap_derived_tables/tests/generic/ directory and should follow a naming convention where the subdirectory and test name are separated by double underscores, for example team_name__test_name . This is so that ownership of the test is clear and so that the filename is unique. ├── mojap_derived_tables\\n├── dbt_project.yml\\n└── tests\\n└── generic\\n├── prison_safety_and_security  # team name\\n├── prison_safety_and_security__test_name.sql\\n...\\n├── other_project_name  # project name\\n├── other_project_name__test_name.sql\\n...\\n├── other_database_name  # database name\\n├── other_database_name__test_name.sql\\n...\\n... Singular tests Singular tests should not be used and they will not be run in the production environment. Configuring tests Defining tests for your resources is done within the property file for that resource under the tests property. See the example below. version: 2\\n\\nmodels:\\n- name: <model_a>\\ntests:\\n- dbt_utils.equal_rowcount:\\ncompare_model: <model_b>\\ncolumns:\\n- name: <column_a>\\ntests:\\n- not_null\\n- dbt_utils.not_constant This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\ntests - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 369}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6245e179c6f550491687b6bbbe731a0a'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 370}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Tests Tests are assertions you make about your models and other resources (e.g. sources, seeds and snapshots). You can do things like test whether a column contains nulls or only unique values, compare row counts between tables, or check all of the records in a child table have a corresponding record in a parent table. dbt ships with some tests you can use but there many more out there in packages like dbt_utils , so do checkout dbt’s package hub for what’s available. For more information on tests, see dbt’s tests documentation . Available tests There is an ecosystem of packages containing helpful macros and tests you can use, see dbt package hub . If you need a package adding to the dbt project, update the packages.yml file then rerun dbt deps . Custom generic tests A test is really just a SQL query that returns rows that meet a certain criteria. If the number of returned rows is not zero, then the test fails. When you know that, you’ll soon realise that it’s not too difficult to write your own tests. Tests you write yourself are called custom generic tests and are writen in special macros; read the dbt guidance on how to write custom generic tests to find out more. There are some requirements specific to Create a Derived Table that you must follow when writing custom generic tests. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 371}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '392fc122da547bb5e99d4feb9de3bcec'}>,\n",
       " <Document: {'content': 'They must live subdirectory named after the team, project, or database they relate to in the ./mojap_derived_tables/tests/generic/ directory and should follow a naming convention where the subdirectory and test name are separated by double underscores, for example team_name__test_name . This is so that ownership of the test is clear and so that the filename is unique. ├── mojap_derived_tables\\n├── dbt_project.yml\\n└── tests\\n└── generic\\n├── prison_safety_and_security  # team name\\n├── prison_safety_and_security__test_name.sql\\n...\\n├── other_project_name  # project name\\n├── other_project_name__test_name.sql\\n...\\n├── other_database_name  # database name\\n├── other_database_name__test_name.sql\\n...\\n... Singular tests Singular tests should not be used and they will not be run in the production environment. Configuring tests Defining tests for your resources is done within the property file for that resource under the tests property. See the example below. version: 2\\n\\nmodels:\\n- name: <model_a>\\ntests:\\n- dbt_utils.equal_rowcount:\\ncompare_model: <model_b>\\ncolumns:\\n- name: <column_a>\\ntests:\\n- not_null\\n- dbt_utils.not_constant This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nmacros - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 372}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5691b500fbbc521b5f436cd11b609427'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 373}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Macros Macros are just Jinja functions. You can write your own macros which should live at ./mojap_derived_tables/macros/ , see dbt’s docs on Jinja macros . There is an ecosystem of packages containing helpful macros and tests you can use, see dbt package hub . If you need a package adding to the dbt project, update the packages.yml file then rerun dbt deps . This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nLinting YAML Files - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 374}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd9db1c57d031bf6ca58944cd4ed1c932'}>,\n",
       " <Document: {'content': 'How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 375}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7c4cd5826a95361bc83ec8203d47a614'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Linting YAML files Linting is the automated checking of your code for programmatic and stylistic errors performed by running a ‘linter’. The linter will analyse your code and output error messages to the terminal. Linters should be run from the root of the repository, i.e., the create-a-derived-table directory. YAML demands 2 spaces (not 1 tab) for each indentation, it can be frustrating, if you get spaces and tabs muddled (they are not always equivalent) as you can get a lint error but not be able to see it easily. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 376}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '60b59825ab18317c0e1a903163f6a7ca'}>,\n",
       " <Document: {'content': 'We recommend setting up vertical guidlines in RStudio to keep track of your indentations, see above Show indent guides in RStudio , and to use the spacebar to create 2 spaces for indentation. To lint a single YAML file, run: yamllint .../path/to/yaml/file.yaml Or a whole directory of YAML files by running: yamllint .../path/to/yaml/directory/ Folded style > Use > to split code over multiple lines; each newline is interpreted as a space (unless the newline is on an empty line or after a differently indented line) hence description: >\\nthe quick brown fox\\njumped over the lazy dog is interpreted the same as description: the quick brown fox jumped over the lazy dog Literal style | Use | (pipe) to keep newlines, for instance when running subsequent commands in a GitHub action workflow run: |\\npython -m pip install --upgrade pip\\npip install yamllint This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nLinting YAML Files - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 377}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '11a4f87595d54714867c3beb14903d53'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 378}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Linting YAML files Linting is the automated checking of your code for programmatic and stylistic errors performed by running a ‘linter’. The linter will analyse your code and output error messages to the terminal. Linters should be run from the root of the repository, i.e., the create-a-derived-table directory. YAML demands 2 spaces (not 1 tab) for each indentation, it can be frustrating, if you get spaces and tabs muddled (they are not always equivalent) as you can get a lint error but not be able to see it easily. We recommend setting up vertical guidlines in RStudio to keep track of your indentations, see above Show indent guides in RStudio , and to use the spacebar to create 2 spaces for indentation. To lint a single YAML file, run: yamllint .../path/to/yaml/file.yaml Or a whole directory of YAML files by running: yamllint .../path/to/yaml/directory/ Folded style > Use > to split code over multiple lines; each newline is interpreted as a space (unless the newline is on an empty line or after a differently indented line) hence description: >\\nthe quick brown fox\\njumped over the lazy dog is interpreted the same as description: the quick brown fox jumped over the lazy dog Literal style | Use | (pipe) to keep newlines, for instance when running subsequent commands in a GitHub action workflow run: |\\npython -m pip install --upgrade pip\\npip install yamllint This page was last reviewed on 7 August 2023.\\n\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 379}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c46ec6741a705507e2ac45e3ab3e4e87'}>,\n",
       " <Document: {'content': 'It needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nLinting YAML Files - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 380}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e78009b9abd5a6a2569ba31bb23f5811'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 381}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Linting YAML files Linting is the automated checking of your code for programmatic and stylistic errors performed by running a ‘linter’. The linter will analyse your code and output error messages to the terminal. Linters should be run from the root of the repository, i.e., the create-a-derived-table directory. YAML demands 2 spaces (not 1 tab) for each indentation, it can be frustrating, if you get spaces and tabs muddled (they are not always equivalent) as you can get a lint error but not be able to see it easily. We recommend setting up vertical guidlines in RStudio to keep track of your indentations, see above Show indent guides in RStudio , and to use the spacebar to create 2 spaces for indentation. To lint a single YAML file, run: yamllint .../path/to/yaml/file.yaml Or a whole directory of YAML files by running: yamllint .../path/to/yaml/directory/ Folded style > Use > to split code over multiple lines; each newline is interpreted as a space (unless the newline is on an empty line or after a differently indented line) hence description: >\\nthe quick brown fox\\njumped over the lazy dog is interpreted the same as description: the quick brown fox jumped over the lazy dog Literal style | Use | (pipe) to keep newlines, for instance when running subsequent commands in a GitHub action workflow run: |\\npython -m pip install --upgrade pip\\npip install yamllint This page was last reviewed on 7 August 2023.\\n\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 382}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c46ec6741a705507e2ac45e3ab3e4e87'}>,\n",
       " <Document: {'content': 'It needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nQuick Reference - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 383}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '786a45486dd15d0e5ea12e4b84a1a5f3'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 384}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Quick Reference ⚠️ This service is in beta ⚠️ This page is intended to give users who have read through the detailed sections of the create-a-derived-table user guidance a quick reference to refresh their memory. Please post suggestions for improvements in our slack channel #ask-data-modelling , or edit this document and raise a pull request. Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production Tips Glossary Glossary of key words in the context of create-a-derived-table / dbt . _dim : dimension model suffix naming convention. _fct : fact model suffix naming convention. _stg : staging model suffix naming convention. .yaml : preferred YAML file extension (rather than .yml ). create-a-derived-table : MoJ tool to create derived tables and serve them in AWS Athena. dbt : data build tool open source software which create-a-derived-table is built on. dbt-athena : an open source community built dbt adapter to allow dbt to work with AWS Athena. MoJ currently uses its own fork. model : more or less synonymous with table . run_artefacts : files created when models are compiled or run. seed : lookup table. source : any table on the MoJ Analytical Platform not created by create-a-derived-table which may be used as a starting point to derived models. table : tabular data in the usual sense; also the default materialisation type for a model . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 385}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '78bb47ae476080b18966dd62c1808108'}>,\n",
       " <Document: {'content': 'Set up This list comprises everything you need to do and consider to get set up and ready to start building models collaboratively. It is intended as a quick check or reference list and assumes you have read the detailed instructions in each section of the create-a-derived-table user guidance. See Troubleshooting if you have any problems. Read the detailed create-a-derived-table user guidance. Check your use case is appropriate; you may contact the Data Modelling team for advice at #ask-data-modelling . Decide an appropriate domain within create-a-derived-table for your project. Decide on naming conventions for your models in the form database_name__table_name , note separation using __ (“dunder”). Database name must be unique within MoJ. Set up an MoJ Analytical Platform account . Add your alpha_user name to standard_database_access and raise a PR. This grants access to the create_a_derived_table/basic resource, which includes access to the general domain, seeds and run_artefacts . Create a project access file for your project in data-engineering-database-access/project_access . In the project access file under Resources include the create-a-derived-table domains required to write models to , as well as the source databases you will be buildung models from . Full instructions here . If an MoJ Analytical Platform database is not listed as a source in source_database_names.txt then you can add it, see Adding a new source . Set up the RStudio IDE; set up a project and clone the repo into it. See Set up the RStudio working environment for GUI instructions. Using Terminal navigate to where you want the create-a-derived-table project to sit and run git clone git@github.com:moj-analytical-services/create-a-derived-table.git . Navigate to the create-a-derived-table directory in Terminal and set up a Python virtual environment; activate it, upgrade pip, and install requirements. See Setting up a Python virtual environment and Virtual environment set up . Use Github Workflow method to collaborate on a project. Branch off main and create a main branch for your project, project-name-main ; all subsequent developers should branch off project-name-main to create feature branches for this project. When raising a PR ensure you merge into this branch, before merging into main ; the PR summary should read something like “ github-user wants to merge X commits into project-name-main from project-name-feature-branch ”. See also Collaborating with Git and Git commands . You are now ready to start building models collaboratively with create-a-derived-tbale . If you have any problems please check Troubleshooting , or ask at #ask-data-modelling providing context and links if appropriate. Virtual environment set up Clone the create-a-derived-table repository git clone git@github.com: moj-analytical-services/create-a-derived-table.git Ensure you are in the root directory create-a-derived-table . Set up and activate a Python virtual environment python3 -m venv venv\\nsource venv/bin/activate Install requirements: pip install --upgrade pip\\npip install -r requirements.txt\\npip install -r requirements-lint.txt Set up and source the Bash profile echo \"export DBT_PROFILES_DIR=../.dbt/\" >> ~/.bash_profile\\nsource ~/.bash_profile Set up dbt cd mojap_derived_tables\\ndbt debug\\ndbt deps Git commands Some useful bash and git commands. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 386}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1639329b1cd63317fbf5a98bbcfe722'}>,\n",
       " <Document: {'content': 'cd path/to/root navigate to the create-a-derived-table (root) directory of the repository cd .. go up one directory cd ~ go to the top level directory (this may be create-a-derived-table ) ls list directory contents git clone git@github.com:moj-analytical-services/create-a-derived-table.git clone the repo git branch check which of your local branches you are on git branch -a list all local and remote branches git switch main switch to the main branch git fetch fetch the latest content from the remote branch (view only) git pull pull the latest content from the remote, updating the local copy git checkout -b <new-branch> create a new branch off the current branch git checkout -b <new-branch> <from-branch> create a new branch off a specific branch git add path/to/files/ add a direcrtory of files to the staging area git status check status of current branch and see what files are staged git commit -m \"<descriptive message>\" save changes to local repo with a descriptive message git push origin <new-branch> push local changes to the remote, if the branch doesn’t exist it is created, else it is updated git fetch origin <other-branch> fetch latest content from a specific branch git pull origin <other-branch> pull latest content from a specific branch into current branch git merge origin/<other-branch> merge changes from specific branch into current branch dbt commands Remove run artefacts from previous invocations of dbt: dbt clean Compile model code which checks SQL and YAML is syntactically correct. The compiled model files will be saved in the corresponding directory under mojap_derived_tables/target/compiled/ : dbt compile --select models/.../path/to/my/models/ Deploy seeds, models and run tests (note models must be deployed before tests can run successfully): dbt run --select models/.../path/to/my/models/\\ndbt seed --select seeds/.../path/to/my/seeds/\\ndbt test --select models/.../path/to/my/models Build and open a local copy of the dbt docs : dbt docs generate\\ndbt docs serve yamllint commands To lint a single YAML file or a directory of YAML files navigate to the root directory and run: yamllint .../path/to/yaml/file.yaml\\nyamllint .../path/to/yaml/directory/ Validate and reformat YAML using the online YAML Validator tool. Copy/paste code into it and click Go to validate. Note that to pass the YAML linter settings all YAML files must contain a final empty line which may not copy across from the online tool. sqlfluff commands To lint a single SQL file or a directory of SQL files navigate to the root directory and run: sqlfluff lint .../path/to/sql/file.sql\\nsqlfluff lint .../path/to/sql/directory/ To lint and fix a single SQL file or a directory of SQL files run: sqlfluff fix .../path/to/sql/file.sql\\nsqlfluff fix .../path/to/sql/directory/ You are asked to confirm before proceding with the fix as it edits your files. Moving models to production: When you are ready to submit a pull request to merge models into the main branch, please check against the following list: Seamless User Experience: Prepare the models following data modelling good practise, more information here . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 387}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '16f30c6c3ec515e17044fce64b4dc606'}>,\n",
       " <Document: {'content': 'Consider the following factors to enhance the user experience in terms of how models are presented: Should a model be visible in the same database, or be moved to a staging database? Are the names of databases/tables/columns clear to users? How does the model appear in dbt docs ? Readability: Ensure linting checks pass locally on SQL and YAML code so that code layout meets organisation standards for ease of human readability. Comprehensive Testing: Ensure that the development models are accompanied by sufficient tests. There are many tests availble at column and table level, as well as the option to create user defined tests, more information here . Confirm these tests pass consistently. This is a critical validation step to provide model quality assurance to consumers. Successful Deployment in Development: Upon raising a pull request the deploy-dev workflow is triggered. This must complete successfully before any code can be merged into main . You may need to update your branch with the latest from main and resolve any conflicts. Model Scheduling: Determine the most appropriate scheduling for the model deployment that aligns to upstream deployment pipelines. For example, if the upstream sources are only updated weekly, a daily schedule may be unnecessary. Update the dbt_project.yml using the corresponding schedule tag , more information here . Please note, only models with a declared schedule tag are deployed to production. Tips If you define any variables to inject into your model sql files using {{ var(...) }} , they need to be in the dbt_project.yml file. This page was last reviewed on 15 September 2022.\\n\\nIt needs to be reviewed again on 15 September 2023\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 15 September 2023\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nQuick Reference - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 388}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9c5dcb9ae01fe7c2a47363007f43b43c'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 389}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Quick Reference ⚠️ This service is in beta ⚠️ This page is intended to give users who have read through the detailed sections of the create-a-derived-table user guidance a quick reference to refresh their memory. Please post suggestions for improvements in our slack channel #ask-data-modelling , or edit this document and raise a pull request. Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production Tips Glossary Glossary of key words in the context of create-a-derived-table / dbt . _dim : dimension model suffix naming convention. _fct : fact model suffix naming convention. _stg : staging model suffix naming convention. .yaml : preferred YAML file extension (rather than .yml ). ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 390}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '83d1aad399a8193feb826b584ee72ad7'}>,\n",
       " <Document: {'content': 'create-a-derived-table : MoJ tool to create derived tables and serve them in AWS Athena. dbt : data build tool open source software which create-a-derived-table is built on. dbt-athena : an open source community built dbt adapter to allow dbt to work with AWS Athena. MoJ currently uses its own fork. model : more or less synonymous with table . run_artefacts : files created when models are compiled or run. seed : lookup table. source : any table on the MoJ Analytical Platform not created by create-a-derived-table which may be used as a starting point to derived models. table : tabular data in the usual sense; also the default materialisation type for a model . Set up This list comprises everything you need to do and consider to get set up and ready to start building models collaboratively. It is intended as a quick check or reference list and assumes you have read the detailed instructions in each section of the create-a-derived-table user guidance. See Troubleshooting if you have any problems. Read the detailed create-a-derived-table user guidance. Check your use case is appropriate; you may contact the Data Modelling team for advice at #ask-data-modelling . Decide an appropriate domain within create-a-derived-table for your project. Decide on naming conventions for your models in the form database_name__table_name , note separation using __ (“dunder”). Database name must be unique within MoJ. Set up an MoJ Analytical Platform account . Add your alpha_user name to standard_database_access and raise a PR. This grants access to the create_a_derived_table/basic resource, which includes access to the general domain, seeds and run_artefacts . Create a project access file for your project in data-engineering-database-access/project_access . In the project access file under Resources include the create-a-derived-table domains required to write models to , as well as the source databases you will be buildung models from . Full instructions here . If an MoJ Analytical Platform database is not listed as a source in source_database_names.txt then you can add it, see Adding a new source . Set up the RStudio IDE; set up a project and clone the repo into it. See Set up the RStudio working environment for GUI instructions. Using Terminal navigate to where you want the create-a-derived-table project to sit and run git clone git@github.com:moj-analytical-services/create-a-derived-table.git . Navigate to the create-a-derived-table directory in Terminal and set up a Python virtual environment; activate it, upgrade pip, and install requirements. See Setting up a Python virtual environment and Virtual environment set up . Use Github Workflow method to collaborate on a project. Branch off main and create a main branch for your project, project-name-main ; all subsequent developers should branch off project-name-main to create feature branches for this project. When raising a PR ensure you merge into this branch, before merging into main ; the PR summary should read something like “ github-user wants to merge X commits into project-name-main from project-name-feature-branch ”. See also Collaborating with Git and Git commands . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 391}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd3c7dacef99178610f3803a7342a6913'}>,\n",
       " <Document: {'content': 'You are now ready to start building models collaboratively with create-a-derived-tbale . If you have any problems please check Troubleshooting , or ask at #ask-data-modelling providing context and links if appropriate. Virtual environment set up Clone the create-a-derived-table repository git clone git@github.com: moj-analytical-services/create-a-derived-table.git Ensure you are in the root directory create-a-derived-table . Set up and activate a Python virtual environment python3 -m venv venv\\nsource venv/bin/activate Install requirements: pip install --upgrade pip\\npip install -r requirements.txt\\npip install -r requirements-lint.txt Set up and source the Bash profile echo \"export DBT_PROFILES_DIR=../.dbt/\" >> ~/.bash_profile\\nsource ~/.bash_profile Set up dbt cd mojap_derived_tables\\ndbt debug\\ndbt deps Git commands Some useful bash and git commands. cd path/to/root navigate to the create-a-derived-table (root) directory of the repository cd .. go up one directory cd ~ go to the top level directory (this may be create-a-derived-table ) ls list directory contents git clone git@github.com:moj-analytical-services/create-a-derived-table.git clone the repo git branch check which of your local branches you are on git branch -a list all local and remote branches git switch main switch to the main branch git fetch fetch the latest content from the remote branch (view only) git pull pull the latest content from the remote, updating the local copy git checkout -b <new-branch> create a new branch off the current branch git checkout -b <new-branch> <from-branch> create a new branch off a specific branch git add path/to/files/ add a direcrtory of files to the staging area git status check status of current branch and see what files are staged git commit -m \"<descriptive message>\" save changes to local repo with a descriptive message git push origin <new-branch> push local changes to the remote, if the branch doesn’t exist it is created, else it is updated git fetch origin <other-branch> fetch latest content from a specific branch git pull origin <other-branch> pull latest content from a specific branch into current branch git merge origin/<other-branch> merge changes from specific branch into current branch dbt commands Remove run artefacts from previous invocations of dbt: dbt clean Compile model code which checks SQL and YAML is syntactically correct. The compiled model files will be saved in the corresponding directory under mojap_derived_tables/target/compiled/ : dbt compile --select models/.../path/to/my/models/ Deploy seeds, models and run tests (note models must be deployed before tests can run successfully): dbt run --select models/.../path/to/my/models/\\ndbt seed --select seeds/.../path/to/my/seeds/\\ndbt test --select models/.../path/to/my/models Build and open a local copy of the dbt docs : dbt docs generate\\ndbt docs serve yamllint commands To lint a single YAML file or a directory of YAML files navigate to the root directory and run: yamllint .../path/to/yaml/file.yaml\\nyamllint .../path/to/yaml/directory/ Validate and reformat YAML using the online YAML Validator tool. Copy/paste code into it and click Go to validate. Note that to pass the YAML linter settings all YAML files must contain a final empty line which may not copy across from the online tool. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 392}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fc0d9ca920837e7f55d857d29e9d4a45'}>,\n",
       " <Document: {'content': 'sqlfluff commands To lint a single SQL file or a directory of SQL files navigate to the root directory and run: sqlfluff lint .../path/to/sql/file.sql\\nsqlfluff lint .../path/to/sql/directory/ To lint and fix a single SQL file or a directory of SQL files run: sqlfluff fix .../path/to/sql/file.sql\\nsqlfluff fix .../path/to/sql/directory/ You are asked to confirm before proceding with the fix as it edits your files. Moving models to production: When you are ready to submit a pull request to merge models into the main branch, please check against the following list: Seamless User Experience: Prepare the models following data modelling good practise, more information here . Consider the following factors to enhance the user experience in terms of how models are presented: Should a model be visible in the same database, or be moved to a staging database? Are the names of databases/tables/columns clear to users? How does the model appear in dbt docs ? Readability: Ensure linting checks pass locally on SQL and YAML code so that code layout meets organisation standards for ease of human readability. Comprehensive Testing: Ensure that the development models are accompanied by sufficient tests. There are many tests availble at column and table level, as well as the option to create user defined tests, more information here . Confirm these tests pass consistently. This is a critical validation step to provide model quality assurance to consumers. Successful Deployment in Development: Upon raising a pull request the deploy-dev workflow is triggered. This must complete successfully before any code can be merged into main . You may need to update your branch with the latest from main and resolve any conflicts. Model Scheduling: Determine the most appropriate scheduling for the model deployment that aligns to upstream deployment pipelines. For example, if the upstream sources are only updated weekly, a daily schedule may be unnecessary. Update the dbt_project.yml using the corresponding schedule tag , more information here . Please note, only models with a declared schedule tag are deployed to production. Tips If you define any variables to inject into your model sql files using {{ var(...) }} , they need to be in the dbt_project.yml file. This page was last reviewed on 15 September 2022.\\n\\nIt needs to be reviewed again on 15 September 2023\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 15 September 2023\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nQuick Reference - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 393}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'da9e8595b76c2f8749f7e105c84a9674'}>,\n",
       " <Document: {'content': 'Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 394}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9ba4b74f4fa634ec10c04b4f7c578f23'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 395}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7a1bbf0601e5e6586cff17ef311f612'}>,\n",
       " <Document: {'content': 'Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Quick Reference ⚠️ This service is in beta ⚠️ This page is intended to give users who have read through the detailed sections of the create-a-derived-table user guidance a quick reference to refresh their memory. Please post suggestions for improvements in our slack channel #ask-data-modelling , or edit this document and raise a pull request. Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production Tips Glossary Glossary of key words in the context of create-a-derived-table / dbt . _dim : dimension model suffix naming convention. _fct : fact model suffix naming convention. _stg : staging model suffix naming convention. .yaml : preferred YAML file extension (rather than .yml ). create-a-derived-table : MoJ tool to create derived tables and serve them in AWS Athena. dbt : data build tool open source software which create-a-derived-table is built on. dbt-athena : an open source community built dbt adapter to allow dbt to work with AWS Athena. MoJ currently uses its own fork. model : more or less synonymous with table . run_artefacts : files created when models are compiled or run. seed : lookup table. source : any table on the MoJ Analytical Platform not created by create-a-derived-table which may be used as a starting point to derived models. table : tabular data in the usual sense; also the default materialisation type for a model . Set up This list comprises everything you need to do and consider to get set up and ready to start building models collaboratively. It is intended as a quick check or reference list and assumes you have read the detailed instructions in each section of the create-a-derived-table user guidance. See Troubleshooting if you have any problems. Read the detailed create-a-derived-table user guidance. Check your use case is appropriate; you may contact the Data Modelling team for advice at #ask-data-modelling . Decide an appropriate domain within create-a-derived-table for your project. Decide on naming conventions for your models in the form database_name__table_name , note separation using __ (“dunder”). Database name must be unique within MoJ. Set up an MoJ Analytical Platform account . Add your alpha_user name to standard_database_access and raise a PR. This grants access to the create_a_derived_table/basic resource, which includes access to the general domain, seeds and run_artefacts . Create a project access file for your project in data-engineering-database-access/project_access . In the project access file under Resources include the create-a-derived-table domains required to write models to , as well as the source databases you will be buildung models from . Full instructions here . If an MoJ Analytical Platform database is not listed as a source in source_database_names.txt then you can add it, see Adding a new source . Set up the RStudio IDE; set up a project and clone the repo into it. See Set up the RStudio working environment for GUI instructions. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 396}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '17246840c2d95b08ef48504a788d4ce2'}>,\n",
       " <Document: {'content': 'Using Terminal navigate to where you want the create-a-derived-table project to sit and run git clone git@github.com:moj-analytical-services/create-a-derived-table.git . Navigate to the create-a-derived-table directory in Terminal and set up a Python virtual environment; activate it, upgrade pip, and install requirements. See Setting up a Python virtual environment and Virtual environment set up . Use Github Workflow method to collaborate on a project. Branch off main and create a main branch for your project, project-name-main ; all subsequent developers should branch off project-name-main to create feature branches for this project. When raising a PR ensure you merge into this branch, before merging into main ; the PR summary should read something like “ github-user wants to merge X commits into project-name-main from project-name-feature-branch ”. See also Collaborating with Git and Git commands . You are now ready to start building models collaboratively with create-a-derived-tbale . If you have any problems please check Troubleshooting , or ask at #ask-data-modelling providing context and links if appropriate. Virtual environment set up Clone the create-a-derived-table repository git clone git@github.com: moj-analytical-services/create-a-derived-table.git Ensure you are in the root directory create-a-derived-table . Set up and activate a Python virtual environment python3 -m venv venv\\nsource venv/bin/activate Install requirements: pip install --upgrade pip\\npip install -r requirements.txt\\npip install -r requirements-lint.txt Set up and source the Bash profile echo \"export DBT_PROFILES_DIR=../.dbt/\" >> ~/.bash_profile\\nsource ~/.bash_profile Set up dbt cd mojap_derived_tables\\ndbt debug\\ndbt deps Git commands Some useful bash and git commands. cd path/to/root navigate to the create-a-derived-table (root) directory of the repository cd .. go up one directory cd ~ go to the top level directory (this may be create-a-derived-table ) ls list directory contents git clone git@github.com:moj-analytical-services/create-a-derived-table.git clone the repo git branch check which of your local branches you are on git branch -a list all local and remote branches git switch main switch to the main branch git fetch fetch the latest content from the remote branch (view only) git pull pull the latest content from the remote, updating the local copy git checkout -b <new-branch> create a new branch off the current branch git checkout -b <new-branch> <from-branch> create a new branch off a specific branch git add path/to/files/ add a direcrtory of files to the staging area git status check status of current branch and see what files are staged git commit -m \"<descriptive message>\" save changes to local repo with a descriptive message git push origin <new-branch> push local changes to the remote, if the branch doesn’t exist it is created, else it is updated git fetch origin <other-branch> fetch latest content from a specific branch git pull origin <other-branch> pull latest content from a specific branch into current branch git merge origin/<other-branch> merge changes from specific branch into current branch dbt commands Remove run artefacts from previous invocations of dbt: dbt clean Compile model code which checks SQL and YAML is syntactically correct. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 397}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c5d4e3884e805e0f710e5a5c650eedd8'}>,\n",
       " <Document: {'content': 'The compiled model files will be saved in the corresponding directory under mojap_derived_tables/target/compiled/ : dbt compile --select models/.../path/to/my/models/ Deploy seeds, models and run tests (note models must be deployed before tests can run successfully): dbt run --select models/.../path/to/my/models/\\ndbt seed --select seeds/.../path/to/my/seeds/\\ndbt test --select models/.../path/to/my/models Build and open a local copy of the dbt docs : dbt docs generate\\ndbt docs serve yamllint commands To lint a single YAML file or a directory of YAML files navigate to the root directory and run: yamllint .../path/to/yaml/file.yaml\\nyamllint .../path/to/yaml/directory/ Validate and reformat YAML using the online YAML Validator tool. Copy/paste code into it and click Go to validate. Note that to pass the YAML linter settings all YAML files must contain a final empty line which may not copy across from the online tool. sqlfluff commands To lint a single SQL file or a directory of SQL files navigate to the root directory and run: sqlfluff lint .../path/to/sql/file.sql\\nsqlfluff lint .../path/to/sql/directory/ To lint and fix a single SQL file or a directory of SQL files run: sqlfluff fix .../path/to/sql/file.sql\\nsqlfluff fix .../path/to/sql/directory/ You are asked to confirm before proceding with the fix as it edits your files. Moving models to production: When you are ready to submit a pull request to merge models into the main branch, please check against the following list: Seamless User Experience: Prepare the models following data modelling good practise, more information here . Consider the following factors to enhance the user experience in terms of how models are presented: Should a model be visible in the same database, or be moved to a staging database? Are the names of databases/tables/columns clear to users? How does the model appear in dbt docs ? Readability: Ensure linting checks pass locally on SQL and YAML code so that code layout meets organisation standards for ease of human readability. Comprehensive Testing: Ensure that the development models are accompanied by sufficient tests. There are many tests availble at column and table level, as well as the option to create user defined tests, more information here . Confirm these tests pass consistently. This is a critical validation step to provide model quality assurance to consumers. Successful Deployment in Development: Upon raising a pull request the deploy-dev workflow is triggered. This must complete successfully before any code can be merged into main . You may need to update your branch with the latest from main and resolve any conflicts. Model Scheduling: Determine the most appropriate scheduling for the model deployment that aligns to upstream deployment pipelines. For example, if the upstream sources are only updated weekly, a daily schedule may be unnecessary. Update the dbt_project.yml using the corresponding schedule tag , more information here . Please note, only models with a declared schedule tag are deployed to production. Tips If you define any variables to inject into your model sql files using {{ var(...) }} , they need to be in the dbt_project.yml file. This page was last reviewed on 15 September 2022.\\n\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 398}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'af8383e70888944b9138d6d337f2e5fe'}>,\n",
       " <Document: {'content': 'It needs to be reviewed again on 15 September 2023\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 15 September 2023\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nQuick Reference - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 399}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e57bd651a4230a44411fe98c1801e753'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 400}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Quick Reference ⚠️ This service is in beta ⚠️ This page is intended to give users who have read through the detailed sections of the create-a-derived-table user guidance a quick reference to refresh their memory. Please post suggestions for improvements in our slack channel #ask-data-modelling , or edit this document and raise a pull request. Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production Tips Glossary Glossary of key words in the context of create-a-derived-table / dbt . _dim : dimension model suffix naming convention. _fct : fact model suffix naming convention. _stg : staging model suffix naming convention. .yaml : preferred YAML file extension (rather than .yml ). create-a-derived-table : MoJ tool to create derived tables and serve them in AWS Athena. dbt : data build tool open source software which create-a-derived-table is built on. dbt-athena : an open source community built dbt adapter to allow dbt to work with AWS Athena. MoJ currently uses its own fork. model : more or less synonymous with table . run_artefacts : files created when models are compiled or run. seed : lookup table. source : any table on the MoJ Analytical Platform not created by create-a-derived-table which may be used as a starting point to derived models. table : tabular data in the usual sense; also the default materialisation type for a model . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 401}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '78bb47ae476080b18966dd62c1808108'}>,\n",
       " <Document: {'content': 'Set up This list comprises everything you need to do and consider to get set up and ready to start building models collaboratively. It is intended as a quick check or reference list and assumes you have read the detailed instructions in each section of the create-a-derived-table user guidance. See Troubleshooting if you have any problems. Read the detailed create-a-derived-table user guidance. Check your use case is appropriate; you may contact the Data Modelling team for advice at #ask-data-modelling . Decide an appropriate domain within create-a-derived-table for your project. Decide on naming conventions for your models in the form database_name__table_name , note separation using __ (“dunder”). Database name must be unique within MoJ. Set up an MoJ Analytical Platform account . Add your alpha_user name to standard_database_access and raise a PR. This grants access to the create_a_derived_table/basic resource, which includes access to the general domain, seeds and run_artefacts . Create a project access file for your project in data-engineering-database-access/project_access . In the project access file under Resources include the create-a-derived-table domains required to write models to , as well as the source databases you will be buildung models from . Full instructions here . If an MoJ Analytical Platform database is not listed as a source in source_database_names.txt then you can add it, see Adding a new source . Set up the RStudio IDE; set up a project and clone the repo into it. See Set up the RStudio working environment for GUI instructions. Using Terminal navigate to where you want the create-a-derived-table project to sit and run git clone git@github.com:moj-analytical-services/create-a-derived-table.git . Navigate to the create-a-derived-table directory in Terminal and set up a Python virtual environment; activate it, upgrade pip, and install requirements. See Setting up a Python virtual environment and Virtual environment set up . Use Github Workflow method to collaborate on a project. Branch off main and create a main branch for your project, project-name-main ; all subsequent developers should branch off project-name-main to create feature branches for this project. When raising a PR ensure you merge into this branch, before merging into main ; the PR summary should read something like “ github-user wants to merge X commits into project-name-main from project-name-feature-branch ”. See also Collaborating with Git and Git commands . You are now ready to start building models collaboratively with create-a-derived-tbale . If you have any problems please check Troubleshooting , or ask at #ask-data-modelling providing context and links if appropriate. Virtual environment set up Clone the create-a-derived-table repository git clone git@github.com: moj-analytical-services/create-a-derived-table.git Ensure you are in the root directory create-a-derived-table . Set up and activate a Python virtual environment python3 -m venv venv\\nsource venv/bin/activate Install requirements: pip install --upgrade pip\\npip install -r requirements.txt\\npip install -r requirements-lint.txt Set up and source the Bash profile echo \"export DBT_PROFILES_DIR=../.dbt/\" >> ~/.bash_profile\\nsource ~/.bash_profile Set up dbt cd mojap_derived_tables\\ndbt debug\\ndbt deps Git commands Some useful bash and git commands. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 402}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1639329b1cd63317fbf5a98bbcfe722'}>,\n",
       " <Document: {'content': 'cd path/to/root navigate to the create-a-derived-table (root) directory of the repository cd .. go up one directory cd ~ go to the top level directory (this may be create-a-derived-table ) ls list directory contents git clone git@github.com:moj-analytical-services/create-a-derived-table.git clone the repo git branch check which of your local branches you are on git branch -a list all local and remote branches git switch main switch to the main branch git fetch fetch the latest content from the remote branch (view only) git pull pull the latest content from the remote, updating the local copy git checkout -b <new-branch> create a new branch off the current branch git checkout -b <new-branch> <from-branch> create a new branch off a specific branch git add path/to/files/ add a direcrtory of files to the staging area git status check status of current branch and see what files are staged git commit -m \"<descriptive message>\" save changes to local repo with a descriptive message git push origin <new-branch> push local changes to the remote, if the branch doesn’t exist it is created, else it is updated git fetch origin <other-branch> fetch latest content from a specific branch git pull origin <other-branch> pull latest content from a specific branch into current branch git merge origin/<other-branch> merge changes from specific branch into current branch dbt commands Remove run artefacts from previous invocations of dbt: dbt clean Compile model code which checks SQL and YAML is syntactically correct. The compiled model files will be saved in the corresponding directory under mojap_derived_tables/target/compiled/ : dbt compile --select models/.../path/to/my/models/ Deploy seeds, models and run tests (note models must be deployed before tests can run successfully): dbt run --select models/.../path/to/my/models/\\ndbt seed --select seeds/.../path/to/my/seeds/\\ndbt test --select models/.../path/to/my/models Build and open a local copy of the dbt docs : dbt docs generate\\ndbt docs serve yamllint commands To lint a single YAML file or a directory of YAML files navigate to the root directory and run: yamllint .../path/to/yaml/file.yaml\\nyamllint .../path/to/yaml/directory/ Validate and reformat YAML using the online YAML Validator tool. Copy/paste code into it and click Go to validate. Note that to pass the YAML linter settings all YAML files must contain a final empty line which may not copy across from the online tool. sqlfluff commands To lint a single SQL file or a directory of SQL files navigate to the root directory and run: sqlfluff lint .../path/to/sql/file.sql\\nsqlfluff lint .../path/to/sql/directory/ To lint and fix a single SQL file or a directory of SQL files run: sqlfluff fix .../path/to/sql/file.sql\\nsqlfluff fix .../path/to/sql/directory/ You are asked to confirm before proceding with the fix as it edits your files. Moving models to production: When you are ready to submit a pull request to merge models into the main branch, please check against the following list: Seamless User Experience: Prepare the models following data modelling good practise, more information here . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 403}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '16f30c6c3ec515e17044fce64b4dc606'}>,\n",
       " <Document: {'content': 'Consider the following factors to enhance the user experience in terms of how models are presented: Should a model be visible in the same database, or be moved to a staging database? Are the names of databases/tables/columns clear to users? How does the model appear in dbt docs ? Readability: Ensure linting checks pass locally on SQL and YAML code so that code layout meets organisation standards for ease of human readability. Comprehensive Testing: Ensure that the development models are accompanied by sufficient tests. There are many tests availble at column and table level, as well as the option to create user defined tests, more information here . Confirm these tests pass consistently. This is a critical validation step to provide model quality assurance to consumers. Successful Deployment in Development: Upon raising a pull request the deploy-dev workflow is triggered. This must complete successfully before any code can be merged into main . You may need to update your branch with the latest from main and resolve any conflicts. Model Scheduling: Determine the most appropriate scheduling for the model deployment that aligns to upstream deployment pipelines. For example, if the upstream sources are only updated weekly, a daily schedule may be unnecessary. Update the dbt_project.yml using the corresponding schedule tag , more information here . Please note, only models with a declared schedule tag are deployed to production. Tips If you define any variables to inject into your model sql files using {{ var(...) }} , they need to be in the dbt_project.yml file. This page was last reviewed on 15 September 2022.\\n\\nIt needs to be reviewed again on 15 September 2023\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 15 September 2023\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nQuick Reference - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 404}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9c5dcb9ae01fe7c2a47363007f43b43c'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 405}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Quick Reference ⚠️ This service is in beta ⚠️ This page is intended to give users who have read through the detailed sections of the create-a-derived-table user guidance a quick reference to refresh their memory. Please post suggestions for improvements in our slack channel #ask-data-modelling , or edit this document and raise a pull request. Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production Tips Glossary Glossary of key words in the context of create-a-derived-table / dbt . _dim : dimension model suffix naming convention. _fct : fact model suffix naming convention. _stg : staging model suffix naming convention. .yaml : preferred YAML file extension (rather than .yml ). ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 406}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '83d1aad399a8193feb826b584ee72ad7'}>,\n",
       " <Document: {'content': 'create-a-derived-table : MoJ tool to create derived tables and serve them in AWS Athena. dbt : data build tool open source software which create-a-derived-table is built on. dbt-athena : an open source community built dbt adapter to allow dbt to work with AWS Athena. MoJ currently uses its own fork. model : more or less synonymous with table . run_artefacts : files created when models are compiled or run. seed : lookup table. source : any table on the MoJ Analytical Platform not created by create-a-derived-table which may be used as a starting point to derived models. table : tabular data in the usual sense; also the default materialisation type for a model . Set up This list comprises everything you need to do and consider to get set up and ready to start building models collaboratively. It is intended as a quick check or reference list and assumes you have read the detailed instructions in each section of the create-a-derived-table user guidance. See Troubleshooting if you have any problems. Read the detailed create-a-derived-table user guidance. Check your use case is appropriate; you may contact the Data Modelling team for advice at #ask-data-modelling . Decide an appropriate domain within create-a-derived-table for your project. Decide on naming conventions for your models in the form database_name__table_name , note separation using __ (“dunder”). Database name must be unique within MoJ. Set up an MoJ Analytical Platform account . Add your alpha_user name to standard_database_access and raise a PR. This grants access to the create_a_derived_table/basic resource, which includes access to the general domain, seeds and run_artefacts . Create a project access file for your project in data-engineering-database-access/project_access . In the project access file under Resources include the create-a-derived-table domains required to write models to , as well as the source databases you will be buildung models from . Full instructions here . If an MoJ Analytical Platform database is not listed as a source in source_database_names.txt then you can add it, see Adding a new source . Set up the RStudio IDE; set up a project and clone the repo into it. See Set up the RStudio working environment for GUI instructions. Using Terminal navigate to where you want the create-a-derived-table project to sit and run git clone git@github.com:moj-analytical-services/create-a-derived-table.git . Navigate to the create-a-derived-table directory in Terminal and set up a Python virtual environment; activate it, upgrade pip, and install requirements. See Setting up a Python virtual environment and Virtual environment set up . Use Github Workflow method to collaborate on a project. Branch off main and create a main branch for your project, project-name-main ; all subsequent developers should branch off project-name-main to create feature branches for this project. When raising a PR ensure you merge into this branch, before merging into main ; the PR summary should read something like “ github-user wants to merge X commits into project-name-main from project-name-feature-branch ”. See also Collaborating with Git and Git commands . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 407}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd3c7dacef99178610f3803a7342a6913'}>,\n",
       " <Document: {'content': 'You are now ready to start building models collaboratively with create-a-derived-tbale . If you have any problems please check Troubleshooting , or ask at #ask-data-modelling providing context and links if appropriate. Virtual environment set up Clone the create-a-derived-table repository git clone git@github.com: moj-analytical-services/create-a-derived-table.git Ensure you are in the root directory create-a-derived-table . Set up and activate a Python virtual environment python3 -m venv venv\\nsource venv/bin/activate Install requirements: pip install --upgrade pip\\npip install -r requirements.txt\\npip install -r requirements-lint.txt Set up and source the Bash profile echo \"export DBT_PROFILES_DIR=../.dbt/\" >> ~/.bash_profile\\nsource ~/.bash_profile Set up dbt cd mojap_derived_tables\\ndbt debug\\ndbt deps Git commands Some useful bash and git commands. cd path/to/root navigate to the create-a-derived-table (root) directory of the repository cd .. go up one directory cd ~ go to the top level directory (this may be create-a-derived-table ) ls list directory contents git clone git@github.com:moj-analytical-services/create-a-derived-table.git clone the repo git branch check which of your local branches you are on git branch -a list all local and remote branches git switch main switch to the main branch git fetch fetch the latest content from the remote branch (view only) git pull pull the latest content from the remote, updating the local copy git checkout -b <new-branch> create a new branch off the current branch git checkout -b <new-branch> <from-branch> create a new branch off a specific branch git add path/to/files/ add a direcrtory of files to the staging area git status check status of current branch and see what files are staged git commit -m \"<descriptive message>\" save changes to local repo with a descriptive message git push origin <new-branch> push local changes to the remote, if the branch doesn’t exist it is created, else it is updated git fetch origin <other-branch> fetch latest content from a specific branch git pull origin <other-branch> pull latest content from a specific branch into current branch git merge origin/<other-branch> merge changes from specific branch into current branch dbt commands Remove run artefacts from previous invocations of dbt: dbt clean Compile model code which checks SQL and YAML is syntactically correct. The compiled model files will be saved in the corresponding directory under mojap_derived_tables/target/compiled/ : dbt compile --select models/.../path/to/my/models/ Deploy seeds, models and run tests (note models must be deployed before tests can run successfully): dbt run --select models/.../path/to/my/models/\\ndbt seed --select seeds/.../path/to/my/seeds/\\ndbt test --select models/.../path/to/my/models Build and open a local copy of the dbt docs : dbt docs generate\\ndbt docs serve yamllint commands To lint a single YAML file or a directory of YAML files navigate to the root directory and run: yamllint .../path/to/yaml/file.yaml\\nyamllint .../path/to/yaml/directory/ Validate and reformat YAML using the online YAML Validator tool. Copy/paste code into it and click Go to validate. Note that to pass the YAML linter settings all YAML files must contain a final empty line which may not copy across from the online tool. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 408}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fc0d9ca920837e7f55d857d29e9d4a45'}>,\n",
       " <Document: {'content': 'sqlfluff commands To lint a single SQL file or a directory of SQL files navigate to the root directory and run: sqlfluff lint .../path/to/sql/file.sql\\nsqlfluff lint .../path/to/sql/directory/ To lint and fix a single SQL file or a directory of SQL files run: sqlfluff fix .../path/to/sql/file.sql\\nsqlfluff fix .../path/to/sql/directory/ You are asked to confirm before proceding with the fix as it edits your files. Moving models to production: When you are ready to submit a pull request to merge models into the main branch, please check against the following list: Seamless User Experience: Prepare the models following data modelling good practise, more information here . Consider the following factors to enhance the user experience in terms of how models are presented: Should a model be visible in the same database, or be moved to a staging database? Are the names of databases/tables/columns clear to users? How does the model appear in dbt docs ? Readability: Ensure linting checks pass locally on SQL and YAML code so that code layout meets organisation standards for ease of human readability. Comprehensive Testing: Ensure that the development models are accompanied by sufficient tests. There are many tests availble at column and table level, as well as the option to create user defined tests, more information here . Confirm these tests pass consistently. This is a critical validation step to provide model quality assurance to consumers. Successful Deployment in Development: Upon raising a pull request the deploy-dev workflow is triggered. This must complete successfully before any code can be merged into main . You may need to update your branch with the latest from main and resolve any conflicts. Model Scheduling: Determine the most appropriate scheduling for the model deployment that aligns to upstream deployment pipelines. For example, if the upstream sources are only updated weekly, a daily schedule may be unnecessary. Update the dbt_project.yml using the corresponding schedule tag , more information here . Please note, only models with a declared schedule tag are deployed to production. Tips If you define any variables to inject into your model sql files using {{ var(...) }} , they need to be in the dbt_project.yml file. This page was last reviewed on 15 September 2022.\\n\\nIt needs to be reviewed again on 15 September 2023\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 15 September 2023\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nQuick Reference - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 409}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'da9e8595b76c2f8749f7e105c84a9674'}>,\n",
       " <Document: {'content': 'Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 410}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9ba4b74f4fa634ec10c04b4f7c578f23'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 411}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7a1bbf0601e5e6586cff17ef311f612'}>,\n",
       " <Document: {'content': 'Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Quick Reference ⚠️ This service is in beta ⚠️ This page is intended to give users who have read through the detailed sections of the create-a-derived-table user guidance a quick reference to refresh their memory. Please post suggestions for improvements in our slack channel #ask-data-modelling , or edit this document and raise a pull request. Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production Tips Glossary Glossary of key words in the context of create-a-derived-table / dbt . _dim : dimension model suffix naming convention. _fct : fact model suffix naming convention. _stg : staging model suffix naming convention. .yaml : preferred YAML file extension (rather than .yml ). create-a-derived-table : MoJ tool to create derived tables and serve them in AWS Athena. dbt : data build tool open source software which create-a-derived-table is built on. dbt-athena : an open source community built dbt adapter to allow dbt to work with AWS Athena. MoJ currently uses its own fork. model : more or less synonymous with table . run_artefacts : files created when models are compiled or run. seed : lookup table. source : any table on the MoJ Analytical Platform not created by create-a-derived-table which may be used as a starting point to derived models. table : tabular data in the usual sense; also the default materialisation type for a model . Set up This list comprises everything you need to do and consider to get set up and ready to start building models collaboratively. It is intended as a quick check or reference list and assumes you have read the detailed instructions in each section of the create-a-derived-table user guidance. See Troubleshooting if you have any problems. Read the detailed create-a-derived-table user guidance. Check your use case is appropriate; you may contact the Data Modelling team for advice at #ask-data-modelling . Decide an appropriate domain within create-a-derived-table for your project. Decide on naming conventions for your models in the form database_name__table_name , note separation using __ (“dunder”). Database name must be unique within MoJ. Set up an MoJ Analytical Platform account . Add your alpha_user name to standard_database_access and raise a PR. This grants access to the create_a_derived_table/basic resource, which includes access to the general domain, seeds and run_artefacts . Create a project access file for your project in data-engineering-database-access/project_access . In the project access file under Resources include the create-a-derived-table domains required to write models to , as well as the source databases you will be buildung models from . Full instructions here . If an MoJ Analytical Platform database is not listed as a source in source_database_names.txt then you can add it, see Adding a new source . Set up the RStudio IDE; set up a project and clone the repo into it. See Set up the RStudio working environment for GUI instructions. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 412}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '17246840c2d95b08ef48504a788d4ce2'}>,\n",
       " <Document: {'content': 'Using Terminal navigate to where you want the create-a-derived-table project to sit and run git clone git@github.com:moj-analytical-services/create-a-derived-table.git . Navigate to the create-a-derived-table directory in Terminal and set up a Python virtual environment; activate it, upgrade pip, and install requirements. See Setting up a Python virtual environment and Virtual environment set up . Use Github Workflow method to collaborate on a project. Branch off main and create a main branch for your project, project-name-main ; all subsequent developers should branch off project-name-main to create feature branches for this project. When raising a PR ensure you merge into this branch, before merging into main ; the PR summary should read something like “ github-user wants to merge X commits into project-name-main from project-name-feature-branch ”. See also Collaborating with Git and Git commands . You are now ready to start building models collaboratively with create-a-derived-tbale . If you have any problems please check Troubleshooting , or ask at #ask-data-modelling providing context and links if appropriate. Virtual environment set up Clone the create-a-derived-table repository git clone git@github.com: moj-analytical-services/create-a-derived-table.git Ensure you are in the root directory create-a-derived-table . Set up and activate a Python virtual environment python3 -m venv venv\\nsource venv/bin/activate Install requirements: pip install --upgrade pip\\npip install -r requirements.txt\\npip install -r requirements-lint.txt Set up and source the Bash profile echo \"export DBT_PROFILES_DIR=../.dbt/\" >> ~/.bash_profile\\nsource ~/.bash_profile Set up dbt cd mojap_derived_tables\\ndbt debug\\ndbt deps Git commands Some useful bash and git commands. cd path/to/root navigate to the create-a-derived-table (root) directory of the repository cd .. go up one directory cd ~ go to the top level directory (this may be create-a-derived-table ) ls list directory contents git clone git@github.com:moj-analytical-services/create-a-derived-table.git clone the repo git branch check which of your local branches you are on git branch -a list all local and remote branches git switch main switch to the main branch git fetch fetch the latest content from the remote branch (view only) git pull pull the latest content from the remote, updating the local copy git checkout -b <new-branch> create a new branch off the current branch git checkout -b <new-branch> <from-branch> create a new branch off a specific branch git add path/to/files/ add a direcrtory of files to the staging area git status check status of current branch and see what files are staged git commit -m \"<descriptive message>\" save changes to local repo with a descriptive message git push origin <new-branch> push local changes to the remote, if the branch doesn’t exist it is created, else it is updated git fetch origin <other-branch> fetch latest content from a specific branch git pull origin <other-branch> pull latest content from a specific branch into current branch git merge origin/<other-branch> merge changes from specific branch into current branch dbt commands Remove run artefacts from previous invocations of dbt: dbt clean Compile model code which checks SQL and YAML is syntactically correct. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 413}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c5d4e3884e805e0f710e5a5c650eedd8'}>,\n",
       " <Document: {'content': 'The compiled model files will be saved in the corresponding directory under mojap_derived_tables/target/compiled/ : dbt compile --select models/.../path/to/my/models/ Deploy seeds, models and run tests (note models must be deployed before tests can run successfully): dbt run --select models/.../path/to/my/models/\\ndbt seed --select seeds/.../path/to/my/seeds/\\ndbt test --select models/.../path/to/my/models Build and open a local copy of the dbt docs : dbt docs generate\\ndbt docs serve yamllint commands To lint a single YAML file or a directory of YAML files navigate to the root directory and run: yamllint .../path/to/yaml/file.yaml\\nyamllint .../path/to/yaml/directory/ Validate and reformat YAML using the online YAML Validator tool. Copy/paste code into it and click Go to validate. Note that to pass the YAML linter settings all YAML files must contain a final empty line which may not copy across from the online tool. sqlfluff commands To lint a single SQL file or a directory of SQL files navigate to the root directory and run: sqlfluff lint .../path/to/sql/file.sql\\nsqlfluff lint .../path/to/sql/directory/ To lint and fix a single SQL file or a directory of SQL files run: sqlfluff fix .../path/to/sql/file.sql\\nsqlfluff fix .../path/to/sql/directory/ You are asked to confirm before proceding with the fix as it edits your files. Moving models to production: When you are ready to submit a pull request to merge models into the main branch, please check against the following list: Seamless User Experience: Prepare the models following data modelling good practise, more information here . Consider the following factors to enhance the user experience in terms of how models are presented: Should a model be visible in the same database, or be moved to a staging database? Are the names of databases/tables/columns clear to users? How does the model appear in dbt docs ? Readability: Ensure linting checks pass locally on SQL and YAML code so that code layout meets organisation standards for ease of human readability. Comprehensive Testing: Ensure that the development models are accompanied by sufficient tests. There are many tests availble at column and table level, as well as the option to create user defined tests, more information here . Confirm these tests pass consistently. This is a critical validation step to provide model quality assurance to consumers. Successful Deployment in Development: Upon raising a pull request the deploy-dev workflow is triggered. This must complete successfully before any code can be merged into main . You may need to update your branch with the latest from main and resolve any conflicts. Model Scheduling: Determine the most appropriate scheduling for the model deployment that aligns to upstream deployment pipelines. For example, if the upstream sources are only updated weekly, a daily schedule may be unnecessary. Update the dbt_project.yml using the corresponding schedule tag , more information here . Please note, only models with a declared schedule tag are deployed to production. Tips If you define any variables to inject into your model sql files using {{ var(...) }} , they need to be in the dbt_project.yml file. This page was last reviewed on 15 September 2022.\\n\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 414}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'af8383e70888944b9138d6d337f2e5fe'}>,\n",
       " <Document: {'content': 'It needs to be reviewed again on 15 September 2023\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 15 September 2023\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nQuick Reference - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 415}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e57bd651a4230a44411fe98c1801e753'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 416}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Quick Reference ⚠️ This service is in beta ⚠️ This page is intended to give users who have read through the detailed sections of the create-a-derived-table user guidance a quick reference to refresh their memory. Please post suggestions for improvements in our slack channel #ask-data-modelling , or edit this document and raise a pull request. Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production Tips Glossary Glossary of key words in the context of create-a-derived-table / dbt . _dim : dimension model suffix naming convention. _fct : fact model suffix naming convention. _stg : staging model suffix naming convention. .yaml : preferred YAML file extension (rather than .yml ). create-a-derived-table : MoJ tool to create derived tables and serve them in AWS Athena. dbt : data build tool open source software which create-a-derived-table is built on. dbt-athena : an open source community built dbt adapter to allow dbt to work with AWS Athena. MoJ currently uses its own fork. model : more or less synonymous with table . run_artefacts : files created when models are compiled or run. seed : lookup table. source : any table on the MoJ Analytical Platform not created by create-a-derived-table which may be used as a starting point to derived models. table : tabular data in the usual sense; also the default materialisation type for a model . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 417}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '78bb47ae476080b18966dd62c1808108'}>,\n",
       " <Document: {'content': 'Set up This list comprises everything you need to do and consider to get set up and ready to start building models collaboratively. It is intended as a quick check or reference list and assumes you have read the detailed instructions in each section of the create-a-derived-table user guidance. See Troubleshooting if you have any problems. Read the detailed create-a-derived-table user guidance. Check your use case is appropriate; you may contact the Data Modelling team for advice at #ask-data-modelling . Decide an appropriate domain within create-a-derived-table for your project. Decide on naming conventions for your models in the form database_name__table_name , note separation using __ (“dunder”). Database name must be unique within MoJ. Set up an MoJ Analytical Platform account . Add your alpha_user name to standard_database_access and raise a PR. This grants access to the create_a_derived_table/basic resource, which includes access to the general domain, seeds and run_artefacts . Create a project access file for your project in data-engineering-database-access/project_access . In the project access file under Resources include the create-a-derived-table domains required to write models to , as well as the source databases you will be buildung models from . Full instructions here . If an MoJ Analytical Platform database is not listed as a source in source_database_names.txt then you can add it, see Adding a new source . Set up the RStudio IDE; set up a project and clone the repo into it. See Set up the RStudio working environment for GUI instructions. Using Terminal navigate to where you want the create-a-derived-table project to sit and run git clone git@github.com:moj-analytical-services/create-a-derived-table.git . Navigate to the create-a-derived-table directory in Terminal and set up a Python virtual environment; activate it, upgrade pip, and install requirements. See Setting up a Python virtual environment and Virtual environment set up . Use Github Workflow method to collaborate on a project. Branch off main and create a main branch for your project, project-name-main ; all subsequent developers should branch off project-name-main to create feature branches for this project. When raising a PR ensure you merge into this branch, before merging into main ; the PR summary should read something like “ github-user wants to merge X commits into project-name-main from project-name-feature-branch ”. See also Collaborating with Git and Git commands . You are now ready to start building models collaboratively with create-a-derived-tbale . If you have any problems please check Troubleshooting , or ask at #ask-data-modelling providing context and links if appropriate. Virtual environment set up Clone the create-a-derived-table repository git clone git@github.com: moj-analytical-services/create-a-derived-table.git Ensure you are in the root directory create-a-derived-table . Set up and activate a Python virtual environment python3 -m venv venv\\nsource venv/bin/activate Install requirements: pip install --upgrade pip\\npip install -r requirements.txt\\npip install -r requirements-lint.txt Set up and source the Bash profile echo \"export DBT_PROFILES_DIR=../.dbt/\" >> ~/.bash_profile\\nsource ~/.bash_profile Set up dbt cd mojap_derived_tables\\ndbt debug\\ndbt deps Git commands Some useful bash and git commands. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 418}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1639329b1cd63317fbf5a98bbcfe722'}>,\n",
       " <Document: {'content': 'cd path/to/root navigate to the create-a-derived-table (root) directory of the repository cd .. go up one directory cd ~ go to the top level directory (this may be create-a-derived-table ) ls list directory contents git clone git@github.com:moj-analytical-services/create-a-derived-table.git clone the repo git branch check which of your local branches you are on git branch -a list all local and remote branches git switch main switch to the main branch git fetch fetch the latest content from the remote branch (view only) git pull pull the latest content from the remote, updating the local copy git checkout -b <new-branch> create a new branch off the current branch git checkout -b <new-branch> <from-branch> create a new branch off a specific branch git add path/to/files/ add a direcrtory of files to the staging area git status check status of current branch and see what files are staged git commit -m \"<descriptive message>\" save changes to local repo with a descriptive message git push origin <new-branch> push local changes to the remote, if the branch doesn’t exist it is created, else it is updated git fetch origin <other-branch> fetch latest content from a specific branch git pull origin <other-branch> pull latest content from a specific branch into current branch git merge origin/<other-branch> merge changes from specific branch into current branch dbt commands Remove run artefacts from previous invocations of dbt: dbt clean Compile model code which checks SQL and YAML is syntactically correct. The compiled model files will be saved in the corresponding directory under mojap_derived_tables/target/compiled/ : dbt compile --select models/.../path/to/my/models/ Deploy seeds, models and run tests (note models must be deployed before tests can run successfully): dbt run --select models/.../path/to/my/models/\\ndbt seed --select seeds/.../path/to/my/seeds/\\ndbt test --select models/.../path/to/my/models Build and open a local copy of the dbt docs : dbt docs generate\\ndbt docs serve yamllint commands To lint a single YAML file or a directory of YAML files navigate to the root directory and run: yamllint .../path/to/yaml/file.yaml\\nyamllint .../path/to/yaml/directory/ Validate and reformat YAML using the online YAML Validator tool. Copy/paste code into it and click Go to validate. Note that to pass the YAML linter settings all YAML files must contain a final empty line which may not copy across from the online tool. sqlfluff commands To lint a single SQL file or a directory of SQL files navigate to the root directory and run: sqlfluff lint .../path/to/sql/file.sql\\nsqlfluff lint .../path/to/sql/directory/ To lint and fix a single SQL file or a directory of SQL files run: sqlfluff fix .../path/to/sql/file.sql\\nsqlfluff fix .../path/to/sql/directory/ You are asked to confirm before proceding with the fix as it edits your files. Moving models to production: When you are ready to submit a pull request to merge models into the main branch, please check against the following list: Seamless User Experience: Prepare the models following data modelling good practise, more information here . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 419}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '16f30c6c3ec515e17044fce64b4dc606'}>,\n",
       " <Document: {'content': 'Consider the following factors to enhance the user experience in terms of how models are presented: Should a model be visible in the same database, or be moved to a staging database? Are the names of databases/tables/columns clear to users? How does the model appear in dbt docs ? Readability: Ensure linting checks pass locally on SQL and YAML code so that code layout meets organisation standards for ease of human readability. Comprehensive Testing: Ensure that the development models are accompanied by sufficient tests. There are many tests availble at column and table level, as well as the option to create user defined tests, more information here . Confirm these tests pass consistently. This is a critical validation step to provide model quality assurance to consumers. Successful Deployment in Development: Upon raising a pull request the deploy-dev workflow is triggered. This must complete successfully before any code can be merged into main . You may need to update your branch with the latest from main and resolve any conflicts. Model Scheduling: Determine the most appropriate scheduling for the model deployment that aligns to upstream deployment pipelines. For example, if the upstream sources are only updated weekly, a daily schedule may be unnecessary. Update the dbt_project.yml using the corresponding schedule tag , more information here . Please note, only models with a declared schedule tag are deployed to production. Tips If you define any variables to inject into your model sql files using {{ var(...) }} , they need to be in the dbt_project.yml file. This page was last reviewed on 15 September 2022.\\n\\nIt needs to be reviewed again on 15 September 2023\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 15 September 2023\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nQuick Reference - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 420}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9c5dcb9ae01fe7c2a47363007f43b43c'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 421}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Quick Reference ⚠️ This service is in beta ⚠️ This page is intended to give users who have read through the detailed sections of the create-a-derived-table user guidance a quick reference to refresh their memory. Please post suggestions for improvements in our slack channel #ask-data-modelling , or edit this document and raise a pull request. Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production Tips Glossary Glossary of key words in the context of create-a-derived-table / dbt . _dim : dimension model suffix naming convention. _fct : fact model suffix naming convention. _stg : staging model suffix naming convention. .yaml : preferred YAML file extension (rather than .yml ). ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 422}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '83d1aad399a8193feb826b584ee72ad7'}>,\n",
       " <Document: {'content': 'create-a-derived-table : MoJ tool to create derived tables and serve them in AWS Athena. dbt : data build tool open source software which create-a-derived-table is built on. dbt-athena : an open source community built dbt adapter to allow dbt to work with AWS Athena. MoJ currently uses its own fork. model : more or less synonymous with table . run_artefacts : files created when models are compiled or run. seed : lookup table. source : any table on the MoJ Analytical Platform not created by create-a-derived-table which may be used as a starting point to derived models. table : tabular data in the usual sense; also the default materialisation type for a model . Set up This list comprises everything you need to do and consider to get set up and ready to start building models collaboratively. It is intended as a quick check or reference list and assumes you have read the detailed instructions in each section of the create-a-derived-table user guidance. See Troubleshooting if you have any problems. Read the detailed create-a-derived-table user guidance. Check your use case is appropriate; you may contact the Data Modelling team for advice at #ask-data-modelling . Decide an appropriate domain within create-a-derived-table for your project. Decide on naming conventions for your models in the form database_name__table_name , note separation using __ (“dunder”). Database name must be unique within MoJ. Set up an MoJ Analytical Platform account . Add your alpha_user name to standard_database_access and raise a PR. This grants access to the create_a_derived_table/basic resource, which includes access to the general domain, seeds and run_artefacts . Create a project access file for your project in data-engineering-database-access/project_access . In the project access file under Resources include the create-a-derived-table domains required to write models to , as well as the source databases you will be buildung models from . Full instructions here . If an MoJ Analytical Platform database is not listed as a source in source_database_names.txt then you can add it, see Adding a new source . Set up the RStudio IDE; set up a project and clone the repo into it. See Set up the RStudio working environment for GUI instructions. Using Terminal navigate to where you want the create-a-derived-table project to sit and run git clone git@github.com:moj-analytical-services/create-a-derived-table.git . Navigate to the create-a-derived-table directory in Terminal and set up a Python virtual environment; activate it, upgrade pip, and install requirements. See Setting up a Python virtual environment and Virtual environment set up . Use Github Workflow method to collaborate on a project. Branch off main and create a main branch for your project, project-name-main ; all subsequent developers should branch off project-name-main to create feature branches for this project. When raising a PR ensure you merge into this branch, before merging into main ; the PR summary should read something like “ github-user wants to merge X commits into project-name-main from project-name-feature-branch ”. See also Collaborating with Git and Git commands . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 423}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd3c7dacef99178610f3803a7342a6913'}>,\n",
       " <Document: {'content': 'You are now ready to start building models collaboratively with create-a-derived-tbale . If you have any problems please check Troubleshooting , or ask at #ask-data-modelling providing context and links if appropriate. Virtual environment set up Clone the create-a-derived-table repository git clone git@github.com: moj-analytical-services/create-a-derived-table.git Ensure you are in the root directory create-a-derived-table . Set up and activate a Python virtual environment python3 -m venv venv\\nsource venv/bin/activate Install requirements: pip install --upgrade pip\\npip install -r requirements.txt\\npip install -r requirements-lint.txt Set up and source the Bash profile echo \"export DBT_PROFILES_DIR=../.dbt/\" >> ~/.bash_profile\\nsource ~/.bash_profile Set up dbt cd mojap_derived_tables\\ndbt debug\\ndbt deps Git commands Some useful bash and git commands. cd path/to/root navigate to the create-a-derived-table (root) directory of the repository cd .. go up one directory cd ~ go to the top level directory (this may be create-a-derived-table ) ls list directory contents git clone git@github.com:moj-analytical-services/create-a-derived-table.git clone the repo git branch check which of your local branches you are on git branch -a list all local and remote branches git switch main switch to the main branch git fetch fetch the latest content from the remote branch (view only) git pull pull the latest content from the remote, updating the local copy git checkout -b <new-branch> create a new branch off the current branch git checkout -b <new-branch> <from-branch> create a new branch off a specific branch git add path/to/files/ add a direcrtory of files to the staging area git status check status of current branch and see what files are staged git commit -m \"<descriptive message>\" save changes to local repo with a descriptive message git push origin <new-branch> push local changes to the remote, if the branch doesn’t exist it is created, else it is updated git fetch origin <other-branch> fetch latest content from a specific branch git pull origin <other-branch> pull latest content from a specific branch into current branch git merge origin/<other-branch> merge changes from specific branch into current branch dbt commands Remove run artefacts from previous invocations of dbt: dbt clean Compile model code which checks SQL and YAML is syntactically correct. The compiled model files will be saved in the corresponding directory under mojap_derived_tables/target/compiled/ : dbt compile --select models/.../path/to/my/models/ Deploy seeds, models and run tests (note models must be deployed before tests can run successfully): dbt run --select models/.../path/to/my/models/\\ndbt seed --select seeds/.../path/to/my/seeds/\\ndbt test --select models/.../path/to/my/models Build and open a local copy of the dbt docs : dbt docs generate\\ndbt docs serve yamllint commands To lint a single YAML file or a directory of YAML files navigate to the root directory and run: yamllint .../path/to/yaml/file.yaml\\nyamllint .../path/to/yaml/directory/ Validate and reformat YAML using the online YAML Validator tool. Copy/paste code into it and click Go to validate. Note that to pass the YAML linter settings all YAML files must contain a final empty line which may not copy across from the online tool. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 424}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fc0d9ca920837e7f55d857d29e9d4a45'}>,\n",
       " <Document: {'content': 'sqlfluff commands To lint a single SQL file or a directory of SQL files navigate to the root directory and run: sqlfluff lint .../path/to/sql/file.sql\\nsqlfluff lint .../path/to/sql/directory/ To lint and fix a single SQL file or a directory of SQL files run: sqlfluff fix .../path/to/sql/file.sql\\nsqlfluff fix .../path/to/sql/directory/ You are asked to confirm before proceding with the fix as it edits your files. Moving models to production: When you are ready to submit a pull request to merge models into the main branch, please check against the following list: Seamless User Experience: Prepare the models following data modelling good practise, more information here . Consider the following factors to enhance the user experience in terms of how models are presented: Should a model be visible in the same database, or be moved to a staging database? Are the names of databases/tables/columns clear to users? How does the model appear in dbt docs ? Readability: Ensure linting checks pass locally on SQL and YAML code so that code layout meets organisation standards for ease of human readability. Comprehensive Testing: Ensure that the development models are accompanied by sufficient tests. There are many tests availble at column and table level, as well as the option to create user defined tests, more information here . Confirm these tests pass consistently. This is a critical validation step to provide model quality assurance to consumers. Successful Deployment in Development: Upon raising a pull request the deploy-dev workflow is triggered. This must complete successfully before any code can be merged into main . You may need to update your branch with the latest from main and resolve any conflicts. Model Scheduling: Determine the most appropriate scheduling for the model deployment that aligns to upstream deployment pipelines. For example, if the upstream sources are only updated weekly, a daily schedule may be unnecessary. Update the dbt_project.yml using the corresponding schedule tag , more information here . Please note, only models with a declared schedule tag are deployed to production. Tips If you define any variables to inject into your model sql files using {{ var(...) }} , they need to be in the dbt_project.yml file. This page was last reviewed on 15 September 2022.\\n\\nIt needs to be reviewed again on 15 September 2023\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 15 September 2023\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nQuick Reference - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 425}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'da9e8595b76c2f8749f7e105c84a9674'}>,\n",
       " <Document: {'content': 'Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 426}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9ba4b74f4fa634ec10c04b4f7c578f23'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 427}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7a1bbf0601e5e6586cff17ef311f612'}>,\n",
       " <Document: {'content': 'Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Quick Reference ⚠️ This service is in beta ⚠️ This page is intended to give users who have read through the detailed sections of the create-a-derived-table user guidance a quick reference to refresh their memory. Please post suggestions for improvements in our slack channel #ask-data-modelling , or edit this document and raise a pull request. Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production Tips Glossary Glossary of key words in the context of create-a-derived-table / dbt . _dim : dimension model suffix naming convention. _fct : fact model suffix naming convention. _stg : staging model suffix naming convention. .yaml : preferred YAML file extension (rather than .yml ). create-a-derived-table : MoJ tool to create derived tables and serve them in AWS Athena. dbt : data build tool open source software which create-a-derived-table is built on. dbt-athena : an open source community built dbt adapter to allow dbt to work with AWS Athena. MoJ currently uses its own fork. model : more or less synonymous with table . run_artefacts : files created when models are compiled or run. seed : lookup table. source : any table on the MoJ Analytical Platform not created by create-a-derived-table which may be used as a starting point to derived models. table : tabular data in the usual sense; also the default materialisation type for a model . Set up This list comprises everything you need to do and consider to get set up and ready to start building models collaboratively. It is intended as a quick check or reference list and assumes you have read the detailed instructions in each section of the create-a-derived-table user guidance. See Troubleshooting if you have any problems. Read the detailed create-a-derived-table user guidance. Check your use case is appropriate; you may contact the Data Modelling team for advice at #ask-data-modelling . Decide an appropriate domain within create-a-derived-table for your project. Decide on naming conventions for your models in the form database_name__table_name , note separation using __ (“dunder”). Database name must be unique within MoJ. Set up an MoJ Analytical Platform account . Add your alpha_user name to standard_database_access and raise a PR. This grants access to the create_a_derived_table/basic resource, which includes access to the general domain, seeds and run_artefacts . Create a project access file for your project in data-engineering-database-access/project_access . In the project access file under Resources include the create-a-derived-table domains required to write models to , as well as the source databases you will be buildung models from . Full instructions here . If an MoJ Analytical Platform database is not listed as a source in source_database_names.txt then you can add it, see Adding a new source . Set up the RStudio IDE; set up a project and clone the repo into it. See Set up the RStudio working environment for GUI instructions. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 428}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '17246840c2d95b08ef48504a788d4ce2'}>,\n",
       " <Document: {'content': 'Using Terminal navigate to where you want the create-a-derived-table project to sit and run git clone git@github.com:moj-analytical-services/create-a-derived-table.git . Navigate to the create-a-derived-table directory in Terminal and set up a Python virtual environment; activate it, upgrade pip, and install requirements. See Setting up a Python virtual environment and Virtual environment set up . Use Github Workflow method to collaborate on a project. Branch off main and create a main branch for your project, project-name-main ; all subsequent developers should branch off project-name-main to create feature branches for this project. When raising a PR ensure you merge into this branch, before merging into main ; the PR summary should read something like “ github-user wants to merge X commits into project-name-main from project-name-feature-branch ”. See also Collaborating with Git and Git commands . You are now ready to start building models collaboratively with create-a-derived-tbale . If you have any problems please check Troubleshooting , or ask at #ask-data-modelling providing context and links if appropriate. Virtual environment set up Clone the create-a-derived-table repository git clone git@github.com: moj-analytical-services/create-a-derived-table.git Ensure you are in the root directory create-a-derived-table . Set up and activate a Python virtual environment python3 -m venv venv\\nsource venv/bin/activate Install requirements: pip install --upgrade pip\\npip install -r requirements.txt\\npip install -r requirements-lint.txt Set up and source the Bash profile echo \"export DBT_PROFILES_DIR=../.dbt/\" >> ~/.bash_profile\\nsource ~/.bash_profile Set up dbt cd mojap_derived_tables\\ndbt debug\\ndbt deps Git commands Some useful bash and git commands. cd path/to/root navigate to the create-a-derived-table (root) directory of the repository cd .. go up one directory cd ~ go to the top level directory (this may be create-a-derived-table ) ls list directory contents git clone git@github.com:moj-analytical-services/create-a-derived-table.git clone the repo git branch check which of your local branches you are on git branch -a list all local and remote branches git switch main switch to the main branch git fetch fetch the latest content from the remote branch (view only) git pull pull the latest content from the remote, updating the local copy git checkout -b <new-branch> create a new branch off the current branch git checkout -b <new-branch> <from-branch> create a new branch off a specific branch git add path/to/files/ add a direcrtory of files to the staging area git status check status of current branch and see what files are staged git commit -m \"<descriptive message>\" save changes to local repo with a descriptive message git push origin <new-branch> push local changes to the remote, if the branch doesn’t exist it is created, else it is updated git fetch origin <other-branch> fetch latest content from a specific branch git pull origin <other-branch> pull latest content from a specific branch into current branch git merge origin/<other-branch> merge changes from specific branch into current branch dbt commands Remove run artefacts from previous invocations of dbt: dbt clean Compile model code which checks SQL and YAML is syntactically correct. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 429}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c5d4e3884e805e0f710e5a5c650eedd8'}>,\n",
       " <Document: {'content': 'The compiled model files will be saved in the corresponding directory under mojap_derived_tables/target/compiled/ : dbt compile --select models/.../path/to/my/models/ Deploy seeds, models and run tests (note models must be deployed before tests can run successfully): dbt run --select models/.../path/to/my/models/\\ndbt seed --select seeds/.../path/to/my/seeds/\\ndbt test --select models/.../path/to/my/models Build and open a local copy of the dbt docs : dbt docs generate\\ndbt docs serve yamllint commands To lint a single YAML file or a directory of YAML files navigate to the root directory and run: yamllint .../path/to/yaml/file.yaml\\nyamllint .../path/to/yaml/directory/ Validate and reformat YAML using the online YAML Validator tool. Copy/paste code into it and click Go to validate. Note that to pass the YAML linter settings all YAML files must contain a final empty line which may not copy across from the online tool. sqlfluff commands To lint a single SQL file or a directory of SQL files navigate to the root directory and run: sqlfluff lint .../path/to/sql/file.sql\\nsqlfluff lint .../path/to/sql/directory/ To lint and fix a single SQL file or a directory of SQL files run: sqlfluff fix .../path/to/sql/file.sql\\nsqlfluff fix .../path/to/sql/directory/ You are asked to confirm before proceding with the fix as it edits your files. Moving models to production: When you are ready to submit a pull request to merge models into the main branch, please check against the following list: Seamless User Experience: Prepare the models following data modelling good practise, more information here . Consider the following factors to enhance the user experience in terms of how models are presented: Should a model be visible in the same database, or be moved to a staging database? Are the names of databases/tables/columns clear to users? How does the model appear in dbt docs ? Readability: Ensure linting checks pass locally on SQL and YAML code so that code layout meets organisation standards for ease of human readability. Comprehensive Testing: Ensure that the development models are accompanied by sufficient tests. There are many tests availble at column and table level, as well as the option to create user defined tests, more information here . Confirm these tests pass consistently. This is a critical validation step to provide model quality assurance to consumers. Successful Deployment in Development: Upon raising a pull request the deploy-dev workflow is triggered. This must complete successfully before any code can be merged into main . You may need to update your branch with the latest from main and resolve any conflicts. Model Scheduling: Determine the most appropriate scheduling for the model deployment that aligns to upstream deployment pipelines. For example, if the upstream sources are only updated weekly, a daily schedule may be unnecessary. Update the dbt_project.yml using the corresponding schedule tag , more information here . Please note, only models with a declared schedule tag are deployed to production. Tips If you define any variables to inject into your model sql files using {{ var(...) }} , they need to be in the dbt_project.yml file. This page was last reviewed on 15 September 2022.\\n\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 430}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'af8383e70888944b9138d6d337f2e5fe'}>,\n",
       " <Document: {'content': 'It needs to be reviewed again on 15 September 2023\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 15 September 2023\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nQuick Reference - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 431}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e57bd651a4230a44411fe98c1801e753'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 432}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Quick Reference ⚠️ This service is in beta ⚠️ This page is intended to give users who have read through the detailed sections of the create-a-derived-table user guidance a quick reference to refresh their memory. Please post suggestions for improvements in our slack channel #ask-data-modelling , or edit this document and raise a pull request. Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production Tips Glossary Glossary of key words in the context of create-a-derived-table / dbt . _dim : dimension model suffix naming convention. _fct : fact model suffix naming convention. _stg : staging model suffix naming convention. .yaml : preferred YAML file extension (rather than .yml ). create-a-derived-table : MoJ tool to create derived tables and serve them in AWS Athena. dbt : data build tool open source software which create-a-derived-table is built on. dbt-athena : an open source community built dbt adapter to allow dbt to work with AWS Athena. MoJ currently uses its own fork. model : more or less synonymous with table . run_artefacts : files created when models are compiled or run. seed : lookup table. source : any table on the MoJ Analytical Platform not created by create-a-derived-table which may be used as a starting point to derived models. table : tabular data in the usual sense; also the default materialisation type for a model . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 433}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '78bb47ae476080b18966dd62c1808108'}>,\n",
       " <Document: {'content': 'Set up This list comprises everything you need to do and consider to get set up and ready to start building models collaboratively. It is intended as a quick check or reference list and assumes you have read the detailed instructions in each section of the create-a-derived-table user guidance. See Troubleshooting if you have any problems. Read the detailed create-a-derived-table user guidance. Check your use case is appropriate; you may contact the Data Modelling team for advice at #ask-data-modelling . Decide an appropriate domain within create-a-derived-table for your project. Decide on naming conventions for your models in the form database_name__table_name , note separation using __ (“dunder”). Database name must be unique within MoJ. Set up an MoJ Analytical Platform account . Add your alpha_user name to standard_database_access and raise a PR. This grants access to the create_a_derived_table/basic resource, which includes access to the general domain, seeds and run_artefacts . Create a project access file for your project in data-engineering-database-access/project_access . In the project access file under Resources include the create-a-derived-table domains required to write models to , as well as the source databases you will be buildung models from . Full instructions here . If an MoJ Analytical Platform database is not listed as a source in source_database_names.txt then you can add it, see Adding a new source . Set up the RStudio IDE; set up a project and clone the repo into it. See Set up the RStudio working environment for GUI instructions. Using Terminal navigate to where you want the create-a-derived-table project to sit and run git clone git@github.com:moj-analytical-services/create-a-derived-table.git . Navigate to the create-a-derived-table directory in Terminal and set up a Python virtual environment; activate it, upgrade pip, and install requirements. See Setting up a Python virtual environment and Virtual environment set up . Use Github Workflow method to collaborate on a project. Branch off main and create a main branch for your project, project-name-main ; all subsequent developers should branch off project-name-main to create feature branches for this project. When raising a PR ensure you merge into this branch, before merging into main ; the PR summary should read something like “ github-user wants to merge X commits into project-name-main from project-name-feature-branch ”. See also Collaborating with Git and Git commands . You are now ready to start building models collaboratively with create-a-derived-tbale . If you have any problems please check Troubleshooting , or ask at #ask-data-modelling providing context and links if appropriate. Virtual environment set up Clone the create-a-derived-table repository git clone git@github.com: moj-analytical-services/create-a-derived-table.git Ensure you are in the root directory create-a-derived-table . Set up and activate a Python virtual environment python3 -m venv venv\\nsource venv/bin/activate Install requirements: pip install --upgrade pip\\npip install -r requirements.txt\\npip install -r requirements-lint.txt Set up and source the Bash profile echo \"export DBT_PROFILES_DIR=../.dbt/\" >> ~/.bash_profile\\nsource ~/.bash_profile Set up dbt cd mojap_derived_tables\\ndbt debug\\ndbt deps Git commands Some useful bash and git commands. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 434}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1639329b1cd63317fbf5a98bbcfe722'}>,\n",
       " <Document: {'content': 'cd path/to/root navigate to the create-a-derived-table (root) directory of the repository cd .. go up one directory cd ~ go to the top level directory (this may be create-a-derived-table ) ls list directory contents git clone git@github.com:moj-analytical-services/create-a-derived-table.git clone the repo git branch check which of your local branches you are on git branch -a list all local and remote branches git switch main switch to the main branch git fetch fetch the latest content from the remote branch (view only) git pull pull the latest content from the remote, updating the local copy git checkout -b <new-branch> create a new branch off the current branch git checkout -b <new-branch> <from-branch> create a new branch off a specific branch git add path/to/files/ add a direcrtory of files to the staging area git status check status of current branch and see what files are staged git commit -m \"<descriptive message>\" save changes to local repo with a descriptive message git push origin <new-branch> push local changes to the remote, if the branch doesn’t exist it is created, else it is updated git fetch origin <other-branch> fetch latest content from a specific branch git pull origin <other-branch> pull latest content from a specific branch into current branch git merge origin/<other-branch> merge changes from specific branch into current branch dbt commands Remove run artefacts from previous invocations of dbt: dbt clean Compile model code which checks SQL and YAML is syntactically correct. The compiled model files will be saved in the corresponding directory under mojap_derived_tables/target/compiled/ : dbt compile --select models/.../path/to/my/models/ Deploy seeds, models and run tests (note models must be deployed before tests can run successfully): dbt run --select models/.../path/to/my/models/\\ndbt seed --select seeds/.../path/to/my/seeds/\\ndbt test --select models/.../path/to/my/models Build and open a local copy of the dbt docs : dbt docs generate\\ndbt docs serve yamllint commands To lint a single YAML file or a directory of YAML files navigate to the root directory and run: yamllint .../path/to/yaml/file.yaml\\nyamllint .../path/to/yaml/directory/ Validate and reformat YAML using the online YAML Validator tool. Copy/paste code into it and click Go to validate. Note that to pass the YAML linter settings all YAML files must contain a final empty line which may not copy across from the online tool. sqlfluff commands To lint a single SQL file or a directory of SQL files navigate to the root directory and run: sqlfluff lint .../path/to/sql/file.sql\\nsqlfluff lint .../path/to/sql/directory/ To lint and fix a single SQL file or a directory of SQL files run: sqlfluff fix .../path/to/sql/file.sql\\nsqlfluff fix .../path/to/sql/directory/ You are asked to confirm before proceding with the fix as it edits your files. Moving models to production: When you are ready to submit a pull request to merge models into the main branch, please check against the following list: Seamless User Experience: Prepare the models following data modelling good practise, more information here . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 435}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '16f30c6c3ec515e17044fce64b4dc606'}>,\n",
       " <Document: {'content': 'Consider the following factors to enhance the user experience in terms of how models are presented: Should a model be visible in the same database, or be moved to a staging database? Are the names of databases/tables/columns clear to users? How does the model appear in dbt docs ? Readability: Ensure linting checks pass locally on SQL and YAML code so that code layout meets organisation standards for ease of human readability. Comprehensive Testing: Ensure that the development models are accompanied by sufficient tests. There are many tests availble at column and table level, as well as the option to create user defined tests, more information here . Confirm these tests pass consistently. This is a critical validation step to provide model quality assurance to consumers. Successful Deployment in Development: Upon raising a pull request the deploy-dev workflow is triggered. This must complete successfully before any code can be merged into main . You may need to update your branch with the latest from main and resolve any conflicts. Model Scheduling: Determine the most appropriate scheduling for the model deployment that aligns to upstream deployment pipelines. For example, if the upstream sources are only updated weekly, a daily schedule may be unnecessary. Update the dbt_project.yml using the corresponding schedule tag , more information here . Please note, only models with a declared schedule tag are deployed to production. Tips If you define any variables to inject into your model sql files using {{ var(...) }} , they need to be in the dbt_project.yml file. This page was last reviewed on 15 September 2022.\\n\\nIt needs to be reviewed again on 15 September 2023\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 15 September 2023\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nQuick Reference - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 436}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9c5dcb9ae01fe7c2a47363007f43b43c'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 437}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Quick Reference ⚠️ This service is in beta ⚠️ This page is intended to give users who have read through the detailed sections of the create-a-derived-table user guidance a quick reference to refresh their memory. Please post suggestions for improvements in our slack channel #ask-data-modelling , or edit this document and raise a pull request. Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production Tips Glossary Glossary of key words in the context of create-a-derived-table / dbt . _dim : dimension model suffix naming convention. _fct : fact model suffix naming convention. _stg : staging model suffix naming convention. .yaml : preferred YAML file extension (rather than .yml ). ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 438}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '83d1aad399a8193feb826b584ee72ad7'}>,\n",
       " <Document: {'content': 'create-a-derived-table : MoJ tool to create derived tables and serve them in AWS Athena. dbt : data build tool open source software which create-a-derived-table is built on. dbt-athena : an open source community built dbt adapter to allow dbt to work with AWS Athena. MoJ currently uses its own fork. model : more or less synonymous with table . run_artefacts : files created when models are compiled or run. seed : lookup table. source : any table on the MoJ Analytical Platform not created by create-a-derived-table which may be used as a starting point to derived models. table : tabular data in the usual sense; also the default materialisation type for a model . Set up This list comprises everything you need to do and consider to get set up and ready to start building models collaboratively. It is intended as a quick check or reference list and assumes you have read the detailed instructions in each section of the create-a-derived-table user guidance. See Troubleshooting if you have any problems. Read the detailed create-a-derived-table user guidance. Check your use case is appropriate; you may contact the Data Modelling team for advice at #ask-data-modelling . Decide an appropriate domain within create-a-derived-table for your project. Decide on naming conventions for your models in the form database_name__table_name , note separation using __ (“dunder”). Database name must be unique within MoJ. Set up an MoJ Analytical Platform account . Add your alpha_user name to standard_database_access and raise a PR. This grants access to the create_a_derived_table/basic resource, which includes access to the general domain, seeds and run_artefacts . Create a project access file for your project in data-engineering-database-access/project_access . In the project access file under Resources include the create-a-derived-table domains required to write models to , as well as the source databases you will be buildung models from . Full instructions here . If an MoJ Analytical Platform database is not listed as a source in source_database_names.txt then you can add it, see Adding a new source . Set up the RStudio IDE; set up a project and clone the repo into it. See Set up the RStudio working environment for GUI instructions. Using Terminal navigate to where you want the create-a-derived-table project to sit and run git clone git@github.com:moj-analytical-services/create-a-derived-table.git . Navigate to the create-a-derived-table directory in Terminal and set up a Python virtual environment; activate it, upgrade pip, and install requirements. See Setting up a Python virtual environment and Virtual environment set up . Use Github Workflow method to collaborate on a project. Branch off main and create a main branch for your project, project-name-main ; all subsequent developers should branch off project-name-main to create feature branches for this project. When raising a PR ensure you merge into this branch, before merging into main ; the PR summary should read something like “ github-user wants to merge X commits into project-name-main from project-name-feature-branch ”. See also Collaborating with Git and Git commands . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 439}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd3c7dacef99178610f3803a7342a6913'}>,\n",
       " <Document: {'content': 'You are now ready to start building models collaboratively with create-a-derived-tbale . If you have any problems please check Troubleshooting , or ask at #ask-data-modelling providing context and links if appropriate. Virtual environment set up Clone the create-a-derived-table repository git clone git@github.com: moj-analytical-services/create-a-derived-table.git Ensure you are in the root directory create-a-derived-table . Set up and activate a Python virtual environment python3 -m venv venv\\nsource venv/bin/activate Install requirements: pip install --upgrade pip\\npip install -r requirements.txt\\npip install -r requirements-lint.txt Set up and source the Bash profile echo \"export DBT_PROFILES_DIR=../.dbt/\" >> ~/.bash_profile\\nsource ~/.bash_profile Set up dbt cd mojap_derived_tables\\ndbt debug\\ndbt deps Git commands Some useful bash and git commands. cd path/to/root navigate to the create-a-derived-table (root) directory of the repository cd .. go up one directory cd ~ go to the top level directory (this may be create-a-derived-table ) ls list directory contents git clone git@github.com:moj-analytical-services/create-a-derived-table.git clone the repo git branch check which of your local branches you are on git branch -a list all local and remote branches git switch main switch to the main branch git fetch fetch the latest content from the remote branch (view only) git pull pull the latest content from the remote, updating the local copy git checkout -b <new-branch> create a new branch off the current branch git checkout -b <new-branch> <from-branch> create a new branch off a specific branch git add path/to/files/ add a direcrtory of files to the staging area git status check status of current branch and see what files are staged git commit -m \"<descriptive message>\" save changes to local repo with a descriptive message git push origin <new-branch> push local changes to the remote, if the branch doesn’t exist it is created, else it is updated git fetch origin <other-branch> fetch latest content from a specific branch git pull origin <other-branch> pull latest content from a specific branch into current branch git merge origin/<other-branch> merge changes from specific branch into current branch dbt commands Remove run artefacts from previous invocations of dbt: dbt clean Compile model code which checks SQL and YAML is syntactically correct. The compiled model files will be saved in the corresponding directory under mojap_derived_tables/target/compiled/ : dbt compile --select models/.../path/to/my/models/ Deploy seeds, models and run tests (note models must be deployed before tests can run successfully): dbt run --select models/.../path/to/my/models/\\ndbt seed --select seeds/.../path/to/my/seeds/\\ndbt test --select models/.../path/to/my/models Build and open a local copy of the dbt docs : dbt docs generate\\ndbt docs serve yamllint commands To lint a single YAML file or a directory of YAML files navigate to the root directory and run: yamllint .../path/to/yaml/file.yaml\\nyamllint .../path/to/yaml/directory/ Validate and reformat YAML using the online YAML Validator tool. Copy/paste code into it and click Go to validate. Note that to pass the YAML linter settings all YAML files must contain a final empty line which may not copy across from the online tool. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 440}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fc0d9ca920837e7f55d857d29e9d4a45'}>,\n",
       " <Document: {'content': 'sqlfluff commands To lint a single SQL file or a directory of SQL files navigate to the root directory and run: sqlfluff lint .../path/to/sql/file.sql\\nsqlfluff lint .../path/to/sql/directory/ To lint and fix a single SQL file or a directory of SQL files run: sqlfluff fix .../path/to/sql/file.sql\\nsqlfluff fix .../path/to/sql/directory/ You are asked to confirm before proceding with the fix as it edits your files. Moving models to production: When you are ready to submit a pull request to merge models into the main branch, please check against the following list: Seamless User Experience: Prepare the models following data modelling good practise, more information here . Consider the following factors to enhance the user experience in terms of how models are presented: Should a model be visible in the same database, or be moved to a staging database? Are the names of databases/tables/columns clear to users? How does the model appear in dbt docs ? Readability: Ensure linting checks pass locally on SQL and YAML code so that code layout meets organisation standards for ease of human readability. Comprehensive Testing: Ensure that the development models are accompanied by sufficient tests. There are many tests availble at column and table level, as well as the option to create user defined tests, more information here . Confirm these tests pass consistently. This is a critical validation step to provide model quality assurance to consumers. Successful Deployment in Development: Upon raising a pull request the deploy-dev workflow is triggered. This must complete successfully before any code can be merged into main . You may need to update your branch with the latest from main and resolve any conflicts. Model Scheduling: Determine the most appropriate scheduling for the model deployment that aligns to upstream deployment pipelines. For example, if the upstream sources are only updated weekly, a daily schedule may be unnecessary. Update the dbt_project.yml using the corresponding schedule tag , more information here . Please note, only models with a declared schedule tag are deployed to production. Tips If you define any variables to inject into your model sql files using {{ var(...) }} , they need to be in the dbt_project.yml file. This page was last reviewed on 15 September 2022.\\n\\nIt needs to be reviewed again on 15 September 2023\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 15 September 2023\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nLinting SQL Files - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 441}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5b47016bc69a132bcfc22994d375f1f8'}>,\n",
       " <Document: {'content': 'Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 442}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '11c79287126f02da77d5b88b5c5e6b40'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Linting SQL files Linting is the automated checking of your code for programmatic and stylistic errors performed by running a ‘linter’. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 443}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f8b11d8c3d8deec936f2498847dc846c'}>,\n",
       " <Document: {'content': 'The linter will analyse your code and output error messages to the terminal. Linters should be run from the root of the repository, i.e., the create-a-derived-table directory. To lint a single SQL file, run: sqlfluff lint .../path/to/sql/file.sql Or a whole directory of SQL files by running: sqlfluff lint .../path/to/sql/directory/ SQLFluff is a formatter as well as a linter. That means you can use it to edit your SQL files to match the linting rules. To format SQL files using SQLFluff replace lint in the commands above with fix . .sqlfluffignore The .sqlfluffignore file (found in the root of the repository) can be used to list files for SQLFluff to exclude from linting. This is only to be used if files continually fail linting, despite best efforts. To date this has only applied to complex macros. The default Jinja templater used by SQLFluff simply cannot deal with complex macros. The dbt templater should solve this issue, and we intend to trial it in the future. This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nLinting SQL Files - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 444}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fc87f75d4916b5aee394d7e5eae4b42e'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 445}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Linting SQL files Linting is the automated checking of your code for programmatic and stylistic errors performed by running a ‘linter’. The linter will analyse your code and output error messages to the terminal. Linters should be run from the root of the repository, i.e., the create-a-derived-table directory. To lint a single SQL file, run: sqlfluff lint .../path/to/sql/file.sql Or a whole directory of SQL files by running: sqlfluff lint .../path/to/sql/directory/ SQLFluff is a formatter as well as a linter. That means you can use it to edit your SQL files to match the linting rules. To format SQL files using SQLFluff replace lint in the commands above with fix . .sqlfluffignore The .sqlfluffignore file (found in the root of the repository) can be used to list files for SQLFluff to exclude from linting. This is only to be used if files continually fail linting, despite best efforts. To date this has only applied to complex macros. The default Jinja templater used by SQLFluff simply cannot deal with complex macros. The dbt templater should solve this issue, and we intend to trial it in the future. This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 446}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '849d6ea5d103d0eff6f1cf5bfc901af0'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDeploying to Dev - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 447}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6714c5aa0a13e5a90d627dfdc0a89fac'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 448}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Deploying to Dev You can run any dbt command in the terminal in RStudio (JupyterLab coming soon) to deploy models and seeds, or to run tests. When you deploy models and seeds from RStudio the database they are built in to will be suffixed with _dev_dbt and the underlying data that gets generated will be written to the following S3 path: s3://mojap-derived-tables/dev/models/domain_name=domain/database_name=database_dev_dbt/table_name=table/data_file The data in S3 and the Glue catalog entry for dev databases and tables is deleted approximately every 10 days. If you come back after a break and find your tables are missing, just rerun dbt. When you’re ready, raise a pull request to merge your changes into the main branch. When you do this a number of workflows will run including, deploying your changes to dev , and SQL and YAML linting checks. Run artefacts for the dev deployment are exported to S3 and are available for 3 days. To get the S3 path that your run artefacts have been exported to, check the Export run artefacts to S3 output of the Deploy dbt dev workflow. The path will look something like: s3://mojap-derived-tables/dev/run_artefacts/run_time=2000-01-01T00:00:00/ Helpful commands As previously mentioned, the mojap_derived_tables dbt project is a shared space and so when deploying your models and other resources you’ll want to make use of the node selection syntax so you don’t end up running everyone else’s. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 449}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dac86c64937993a360352c9b948bbc78'}>,\n",
       " <Document: {'content': \"Don’t worry if you do, it will just take a long time to run and then the deployed resources will eventually be deleted. You should cancel the execution with ctrl+c or ctrl+z and save yourself some time. To select a single model or seed, add the following to your dbt command: --select database_name_ model_name or a number of models by running: --select database_name model_1_name database_name _model_2_name To select a full directory of models, add the following to your dbt command: --select models/.../path/to/my/models/ As you develop and run your models you’ll generate a lot of logs and run artefacts. This can become unwieldy and messy so it’s helpful to clear them out from time to time. To remove run artefacts from previous invocations of dbt, run: dbt clean To check your SQL and YAML is syntactically correct, run: dbt compile --select models/.../path/to/my/models/ To deploy your models, run: dbt run --select models/.../path/to/my/models/ To deploy your seeds, run: dbt seed --select seeds/.../path/to/my/seeds/ Don’t forget that if your models depend on your seeds, you’ll need to deploy your seeds before your models. To run tests on models with tests defined, run: dbt test --select models/.../path/to/my/models/ Using the + prefix The + prefix is a dbt syntax feature which helps disambiguate between resource paths and configurations in the dbt_project.yml file. If you see it used in the dbt_project.yml file and wonder what it is, read dbt’s guidance on using the + prefix . It is also used to configure properties in a nested dictionary which take a dictionary of values in a model, seed or test config .yaml . For example, use +column_types rather than column_types since what follows are further key and value pairs defining the column names and the required data type. It doesn’t hurt to use + prefix so it is recommended to always do so. version: 2 models:\\n- name: prison_safety_and_security_ question_answer_fct\\ndescription: The question and answer fact table.\\nconfig:\\nmaterialized: incremental\\nincremental_strategy: append\\npartitioned_by: ['snapshot_date']\\n+column_types:\\ncolumn_1: varchar(5)\\ncolumn_2: int\\ncolumn_3: string How to use the incremental materialisation with the append strategy You may want your final derived table to retain previous versions of itself and not be overwritten each time your table is deployed. The following example will detail how you can achieve creating snapshots of the data and partitioning the table by those snapshots. If you had a model producing a final table at models/some_domain/some_database/some_database final_derived_table.sql with a snapshot date column of table_snapshot_date you should have a respective YAML config file saved at models/some_domain/some_database/some_database _final_derived_table.yml looking something like the below (with the names of your model and columns) version: 2 models:\\n- name: some_database__final_derived_table config:\\n# makes the model append any new data to the existing table partitioned\\n# by the list of columns given. Effectively creating snapshots\\nmaterialized: incremental\\nincremental_strategy: append\\npartitioned_by: ['table_snapshot_date']\\ntags: monthly description: what a table. columns:\\n- name: unique_id\\ndescription: a unqiue identifier\\n- name: something_interesting\\ndescription: some good data\\n- name: table_snapshot_date\\ndescription: snapshot date column and also table partition. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 450}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2de0ec7b9423a1bc46eae103a3bc5de0'}>,\n",
       " <Document: {'content': 'One important thing to note is that partition columns need to be the last column in your table. And if you have multiple partition columns they would all need to be the last columns and in the same order in the paritioned_by key list value in your yaml config as they appear in your table. If you have defined the config for your model as above, every time you run dbt run --select ... locally via a command in your terminal, the data from that run will be appended to the previous data in the dev version of your database. If you wanted to test the data is being materialised as intended then run once with an early snapshot date and again with a later snapshot date and inspect your data in athena, with a query like: select table_snapshot_date, count(*)\\nfrom some_database_dev_dbt.final_dervied_table\\ngroup by table_snapshot_date You can also inspect the s3 bucket and folder where your data will be saved. In the case of this example it would be mojap_derived_tables/dev/models/domain_name=some_domain/database_name=some_database/table_name=final_derived_table/ . You’d expect to see a number of timestamped folders each containing a partition of your table’s data (based on how many times you’ve run your models). If you want to run your models and disregard all previous snapshots you should add the flag --full-refresh to dbt run , e.g. dbt run --select models/some_domain/some_database/some_database__final_dervied_table.sql --full-refresh . This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDeploying to Dev - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 451}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c1f56bcdad402bd10a8a6b974287cab0'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 452}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Deploying to Dev You can run any dbt command in the terminal in RStudio (JupyterLab coming soon) to deploy models and seeds, or to run tests. When you deploy models and seeds from RStudio the database they are built in to will be suffixed with _dev_dbt and the underlying data that gets generated will be written to the following S3 path: s3://mojap-derived-tables/dev/models/domain_name=domain/database_name=database_dev_dbt/table_name=table/data_file The data in S3 and the Glue catalog entry for dev databases and tables is deleted approximately every 10 days. If you come back after a break and find your tables are missing, just rerun dbt. When you’re ready, raise a pull request to merge your changes into the main branch. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 453}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd96749293a41a4036879b16d36b3792b'}>,\n",
       " <Document: {'content': \"When you do this a number of workflows will run including, deploying your changes to dev , and SQL and YAML linting checks. Run artefacts for the dev deployment are exported to S3 and are available for 3 days. To get the S3 path that your run artefacts have been exported to, check the Export run artefacts to S3 output of the Deploy dbt dev workflow. The path will look something like: s3://mojap-derived-tables/dev/run_artefacts/run_time=2000-01-01T00:00:00/ Helpful commands As previously mentioned, the mojap_derived_tables dbt project is a shared space and so when deploying your models and other resources you’ll want to make use of the node selection syntax so you don’t end up running everyone else’s. Don’t worry if you do, it will just take a long time to run and then the deployed resources will eventually be deleted. You should cancel the execution with ctrl+c or ctrl+z and save yourself some time. To select a single model or seed, add the following to your dbt command: --select database_name_ model_name or a number of models by running: --select database_name model_1_name database_name _model_2_name To select a full directory of models, add the following to your dbt command: --select models/.../path/to/my/models/ As you develop and run your models you’ll generate a lot of logs and run artefacts. This can become unwieldy and messy so it’s helpful to clear them out from time to time. To remove run artefacts from previous invocations of dbt, run: dbt clean To check your SQL and YAML is syntactically correct, run: dbt compile --select models/.../path/to/my/models/ To deploy your models, run: dbt run --select models/.../path/to/my/models/ To deploy your seeds, run: dbt seed --select seeds/.../path/to/my/seeds/ Don’t forget that if your models depend on your seeds, you’ll need to deploy your seeds before your models. To run tests on models with tests defined, run: dbt test --select models/.../path/to/my/models/ Using the + prefix The + prefix is a dbt syntax feature which helps disambiguate between resource paths and configurations in the dbt_project.yml file. If you see it used in the dbt_project.yml file and wonder what it is, read dbt’s guidance on using the + prefix . It is also used to configure properties in a nested dictionary which take a dictionary of values in a model, seed or test config .yaml . For example, use +column_types rather than column_types since what follows are further key and value pairs defining the column names and the required data type. It doesn’t hurt to use + prefix so it is recommended to always do so. version: 2 models:\\n- name: prison_safety_and_security_ question_answer_fct\\ndescription: The question and answer fact table.\\nconfig:\\nmaterialized: incremental\\nincremental_strategy: append\\npartitioned_by: ['snapshot_date']\\n+column_types:\\ncolumn_1: varchar(5)\\ncolumn_2: int\\ncolumn_3: string How to use the incremental materialisation with the append strategy You may want your final derived table to retain previous versions of itself and not be overwritten each time your table is deployed. The following example will detail how you can achieve creating snapshots of the data and partitioning the table by those snapshots. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 454}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5c397bb151f2197a7f22fc895e493f02'}>,\n",
       " <Document: {'content': \"If you had a model producing a final table at models/some_domain/some_database/some_database final_derived_table.sql with a snapshot date column of table_snapshot_date you should have a respective YAML config file saved at models/some_domain/some_database/some_database _final_derived_table.yml looking something like the below (with the names of your model and columns) version: 2 models:\\n- name: some_database__final_derived_table config:\\n# makes the model append any new data to the existing table partitioned\\n# by the list of columns given. Effectively creating snapshots\\nmaterialized: incremental\\nincremental_strategy: append\\npartitioned_by: ['table_snapshot_date']\\ntags: monthly description: what a table. columns:\\n- name: unique_id\\ndescription: a unqiue identifier\\n- name: something_interesting\\ndescription: some good data\\n- name: table_snapshot_date\\ndescription: snapshot date column and also table partition. One important thing to note is that partition columns need to be the last column in your table. And if you have multiple partition columns they would all need to be the last columns and in the same order in the paritioned_by key list value in your yaml config as they appear in your table. If you have defined the config for your model as above, every time you run dbt run --select ... locally via a command in your terminal, the data from that run will be appended to the previous data in the dev version of your database. If you wanted to test the data is being materialised as intended then run once with an early snapshot date and again with a later snapshot date and inspect your data in athena, with a query like: select table_snapshot_date, count(*)\\nfrom some_database_dev_dbt.final_dervied_table\\ngroup by table_snapshot_date You can also inspect the s3 bucket and folder where your data will be saved. In the case of this example it would be mojap_derived_tables/dev/models/domain_name=some_domain/database_name=some_database/table_name=final_derived_table/ . You’d expect to see a number of timestamped folders each containing a partition of your table’s data (based on how many times you’ve run your models). If you want to run your models and disregard all previous snapshots you should add the flag --full-refresh to dbt run , e.g. dbt run --select models/some_domain/some_database/some_database__final_dervied_table.sql --full-refresh . This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDeploying to Dev - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 455}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '867f7aac33cbf641a5d89a222b8e3e2c'}>,\n",
       " <Document: {'content': 'How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 456}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3985a7b2ef2cd1e07deaed02b255e7f3'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Deploying to Dev You can run any dbt command in the terminal in RStudio (JupyterLab coming soon) to deploy models and seeds, or to run tests. When you deploy models and seeds from RStudio the database they are built in to will be suffixed with _dev_dbt and the underlying data that gets generated will be written to the following S3 path: s3://mojap-derived-tables/dev/models/domain_name=domain/database_name=database_dev_dbt/table_name=table/data_file The data in S3 and the Glue catalog entry for dev databases and tables is deleted approximately every 10 days. If you come back after a break and find your tables are missing, just rerun dbt. When you’re ready, raise a pull request to merge your changes into the main branch. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 457}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd96749293a41a4036879b16d36b3792b'}>,\n",
       " <Document: {'content': \"When you do this a number of workflows will run including, deploying your changes to dev , and SQL and YAML linting checks. Run artefacts for the dev deployment are exported to S3 and are available for 3 days. To get the S3 path that your run artefacts have been exported to, check the Export run artefacts to S3 output of the Deploy dbt dev workflow. The path will look something like: s3://mojap-derived-tables/dev/run_artefacts/run_time=2000-01-01T00:00:00/ Helpful commands As previously mentioned, the mojap_derived_tables dbt project is a shared space and so when deploying your models and other resources you’ll want to make use of the node selection syntax so you don’t end up running everyone else’s. Don’t worry if you do, it will just take a long time to run and then the deployed resources will eventually be deleted. You should cancel the execution with ctrl+c or ctrl+z and save yourself some time. To select a single model or seed, add the following to your dbt command: --select database_name_ model_name or a number of models by running: --select database_name model_1_name database_name _model_2_name To select a full directory of models, add the following to your dbt command: --select models/.../path/to/my/models/ As you develop and run your models you’ll generate a lot of logs and run artefacts. This can become unwieldy and messy so it’s helpful to clear them out from time to time. To remove run artefacts from previous invocations of dbt, run: dbt clean To check your SQL and YAML is syntactically correct, run: dbt compile --select models/.../path/to/my/models/ To deploy your models, run: dbt run --select models/.../path/to/my/models/ To deploy your seeds, run: dbt seed --select seeds/.../path/to/my/seeds/ Don’t forget that if your models depend on your seeds, you’ll need to deploy your seeds before your models. To run tests on models with tests defined, run: dbt test --select models/.../path/to/my/models/ Using the + prefix The + prefix is a dbt syntax feature which helps disambiguate between resource paths and configurations in the dbt_project.yml file. If you see it used in the dbt_project.yml file and wonder what it is, read dbt’s guidance on using the + prefix . It is also used to configure properties in a nested dictionary which take a dictionary of values in a model, seed or test config .yaml . For example, use +column_types rather than column_types since what follows are further key and value pairs defining the column names and the required data type. It doesn’t hurt to use + prefix so it is recommended to always do so. version: 2 models:\\n- name: prison_safety_and_security_ question_answer_fct\\ndescription: The question and answer fact table.\\nconfig:\\nmaterialized: incremental\\nincremental_strategy: append\\npartitioned_by: ['snapshot_date']\\n+column_types:\\ncolumn_1: varchar(5)\\ncolumn_2: int\\ncolumn_3: string How to use the incremental materialisation with the append strategy You may want your final derived table to retain previous versions of itself and not be overwritten each time your table is deployed. The following example will detail how you can achieve creating snapshots of the data and partitioning the table by those snapshots. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 458}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5c397bb151f2197a7f22fc895e493f02'}>,\n",
       " <Document: {'content': \"If you had a model producing a final table at models/some_domain/some_database/some_database final_derived_table.sql with a snapshot date column of table_snapshot_date you should have a respective YAML config file saved at models/some_domain/some_database/some_database _final_derived_table.yml looking something like the below (with the names of your model and columns) version: 2 models:\\n- name: some_database__final_derived_table config:\\n# makes the model append any new data to the existing table partitioned\\n# by the list of columns given. Effectively creating snapshots\\nmaterialized: incremental\\nincremental_strategy: append\\npartitioned_by: ['table_snapshot_date']\\ntags: monthly description: what a table. columns:\\n- name: unique_id\\ndescription: a unqiue identifier\\n- name: something_interesting\\ndescription: some good data\\n- name: table_snapshot_date\\ndescription: snapshot date column and also table partition. One important thing to note is that partition columns need to be the last column in your table. And if you have multiple partition columns they would all need to be the last columns and in the same order in the paritioned_by key list value in your yaml config as they appear in your table. If you have defined the config for your model as above, every time you run dbt run --select ... locally via a command in your terminal, the data from that run will be appended to the previous data in the dev version of your database. If you wanted to test the data is being materialised as intended then run once with an early snapshot date and again with a later snapshot date and inspect your data in athena, with a query like: select table_snapshot_date, count(*)\\nfrom some_database_dev_dbt.final_dervied_table\\ngroup by table_snapshot_date You can also inspect the s3 bucket and folder where your data will be saved. In the case of this example it would be mojap_derived_tables/dev/models/domain_name=some_domain/database_name=some_database/table_name=final_derived_table/ . You’d expect to see a number of timestamped folders each containing a partition of your table’s data (based on how many times you’ve run your models). If you want to run your models and disregard all previous snapshots you should add the flag --full-refresh to dbt run , e.g. dbt run --select models/some_domain/some_database/some_database__final_dervied_table.sql --full-refresh . This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDeploying to Dev - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 459}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '867f7aac33cbf641a5d89a222b8e3e2c'}>,\n",
       " <Document: {'content': 'How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 460}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3985a7b2ef2cd1e07deaed02b255e7f3'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Deploying to Dev You can run any dbt command in the terminal in RStudio (JupyterLab coming soon) to deploy models and seeds, or to run tests. When you deploy models and seeds from RStudio the database they are built in to will be suffixed with _dev_dbt and the underlying data that gets generated will be written to the following S3 path: s3://mojap-derived-tables/dev/models/domain_name=domain/database_name=database_dev_dbt/table_name=table/data_file The data in S3 and the Glue catalog entry for dev databases and tables is deleted approximately every 10 days. If you come back after a break and find your tables are missing, just rerun dbt. When you’re ready, raise a pull request to merge your changes into the main branch. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 461}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd96749293a41a4036879b16d36b3792b'}>,\n",
       " <Document: {'content': \"When you do this a number of workflows will run including, deploying your changes to dev , and SQL and YAML linting checks. Run artefacts for the dev deployment are exported to S3 and are available for 3 days. To get the S3 path that your run artefacts have been exported to, check the Export run artefacts to S3 output of the Deploy dbt dev workflow. The path will look something like: s3://mojap-derived-tables/dev/run_artefacts/run_time=2000-01-01T00:00:00/ Helpful commands As previously mentioned, the mojap_derived_tables dbt project is a shared space and so when deploying your models and other resources you’ll want to make use of the node selection syntax so you don’t end up running everyone else’s. Don’t worry if you do, it will just take a long time to run and then the deployed resources will eventually be deleted. You should cancel the execution with ctrl+c or ctrl+z and save yourself some time. To select a single model or seed, add the following to your dbt command: --select database_name_ model_name or a number of models by running: --select database_name model_1_name database_name _model_2_name To select a full directory of models, add the following to your dbt command: --select models/.../path/to/my/models/ As you develop and run your models you’ll generate a lot of logs and run artefacts. This can become unwieldy and messy so it’s helpful to clear them out from time to time. To remove run artefacts from previous invocations of dbt, run: dbt clean To check your SQL and YAML is syntactically correct, run: dbt compile --select models/.../path/to/my/models/ To deploy your models, run: dbt run --select models/.../path/to/my/models/ To deploy your seeds, run: dbt seed --select seeds/.../path/to/my/seeds/ Don’t forget that if your models depend on your seeds, you’ll need to deploy your seeds before your models. To run tests on models with tests defined, run: dbt test --select models/.../path/to/my/models/ Using the + prefix The + prefix is a dbt syntax feature which helps disambiguate between resource paths and configurations in the dbt_project.yml file. If you see it used in the dbt_project.yml file and wonder what it is, read dbt’s guidance on using the + prefix . It is also used to configure properties in a nested dictionary which take a dictionary of values in a model, seed or test config .yaml . For example, use +column_types rather than column_types since what follows are further key and value pairs defining the column names and the required data type. It doesn’t hurt to use + prefix so it is recommended to always do so. version: 2 models:\\n- name: prison_safety_and_security_ question_answer_fct\\ndescription: The question and answer fact table.\\nconfig:\\nmaterialized: incremental\\nincremental_strategy: append\\npartitioned_by: ['snapshot_date']\\n+column_types:\\ncolumn_1: varchar(5)\\ncolumn_2: int\\ncolumn_3: string How to use the incremental materialisation with the append strategy You may want your final derived table to retain previous versions of itself and not be overwritten each time your table is deployed. The following example will detail how you can achieve creating snapshots of the data and partitioning the table by those snapshots. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 462}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5c397bb151f2197a7f22fc895e493f02'}>,\n",
       " <Document: {'content': \"If you had a model producing a final table at models/some_domain/some_database/some_database final_derived_table.sql with a snapshot date column of table_snapshot_date you should have a respective YAML config file saved at models/some_domain/some_database/some_database _final_derived_table.yml looking something like the below (with the names of your model and columns) version: 2 models:\\n- name: some_database__final_derived_table config:\\n# makes the model append any new data to the existing table partitioned\\n# by the list of columns given. Effectively creating snapshots\\nmaterialized: incremental\\nincremental_strategy: append\\npartitioned_by: ['table_snapshot_date']\\ntags: monthly description: what a table. columns:\\n- name: unique_id\\ndescription: a unqiue identifier\\n- name: something_interesting\\ndescription: some good data\\n- name: table_snapshot_date\\ndescription: snapshot date column and also table partition. One important thing to note is that partition columns need to be the last column in your table. And if you have multiple partition columns they would all need to be the last columns and in the same order in the paritioned_by key list value in your yaml config as they appear in your table. If you have defined the config for your model as above, every time you run dbt run --select ... locally via a command in your terminal, the data from that run will be appended to the previous data in the dev version of your database. If you wanted to test the data is being materialised as intended then run once with an early snapshot date and again with a later snapshot date and inspect your data in athena, with a query like: select table_snapshot_date, count(*)\\nfrom some_database_dev_dbt.final_dervied_table\\ngroup by table_snapshot_date You can also inspect the s3 bucket and folder where your data will be saved. In the case of this example it would be mojap_derived_tables/dev/models/domain_name=some_domain/database_name=some_database/table_name=final_derived_table/ . You’d expect to see a number of timestamped folders each containing a partition of your table’s data (based on how many times you’ve run your models). If you want to run your models and disregard all previous snapshots you should add the flag --full-refresh to dbt run , e.g. dbt run --select models/some_domain/some_database/some_database__final_dervied_table.sql --full-refresh . This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nTroubleshooting - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 463}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f5ae7bca9dd2167bdff5f961c234fa10'}>,\n",
       " <Document: {'content': 'How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 464}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3985a7b2ef2cd1e07deaed02b255e7f3'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Troubleshooting ⚠️ This service is in beta ⚠️ This page is intended to help users self-diagnose errors. Please check here first and then, if necessary, ask in our slack channel #ask-data-modelling , providing context. If you discover new errors and/or solutions please post on slack or edit this document and raise a PR. Contents General troubleshooting tips Delete dev models instructions Troubleshooting list dbt artefacts When dbt runs it generates artefacts. The most useful of these to you will be the logs in the ./mojap_derived_tables/logs/ directory and compiled SQL in the ./mojap_derived_tables/target/ directory. Compiled SQL can be useful when debugging errors as error messages will often refer to the line number in the compiled SQL and not that in your model file. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 465}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dbb7171e7c5cd42e77d14ba4c29b37e6'}>,\n",
       " <Document: {'content': 'General troubleshooting tips When you deploy dev models via the MoJ Analytical Platform logs are created locally in mojap_derived_tables/logs/dbt.log . This file details each step of the run and is useful for debugging. The logs for dev and prod models deployed via GitHub actions are stored temporariliy in the S3 bucket mojap-derived-tables , and are accessible under standard_database_access . dbt clean cleans out the local logs and target folders; it is good  practise to start a session with a clean slate. Under mojap_derived_tables/target there exist compiled and run folders containing a duplicate folder structure as under mojap_derived_tables . Here you can find your SQL code as compiled (with the Jinja rendered) and the DDL/DML run code. You can test each of these in Athena to check your SQL works as expected. Testing your code in Athena will also highlight any read access permission issues. All file names and paths should be lowercase. Delete dev models instructions During development you may need to clear out any dev models you have created from the MoJ Analytical Platform and start afresh. To do this you will need to delete the Glue tables, Glue database and the data in S3, via the AWS Console. ⚠️ Note that anyone with write access to a domain also has permission to delete from that domain, so please exercise caution. ⚠️ Sign in to the AWS Console as an alpha_user (sign in with GitHub as you would for Analytical Platform Control Panel). NB: You may not have permission to access AWS Glue and action steps 1 and 2, if you do great (as this is tidier), if not, please proceed from step 3. AWS Glue → Data Catalog → Tables : Delete the Glue tables from the Glue catalog. AWS Glue → Data Catalog → Databases : Delete the Glue database if necessary; the database may contain someone else’s tables that you don’t want to delete; also if you delete all the tables in a database it will automatically disappear. S3 → mojap-derived-tables bucket → dev/ → models/ : In S3 delete from the lowest level first; objects, tables, database; this makes sure there are no orphaned objects floating about and that you don’t unintentionally delete a database containing someone else’s work as well as your own. Run dbt clean to delete local run artefacts before reattempting to deploy models. Troubleshooting list CSV file won’t upload to GitHub from RStudio You have uploaded a CSV file into your local directory within RStudio on the Analytical Platform and pushed changes to the remote on GitHub, then you notice that the CSV file is not there. This is due to Analytical Platform RStudio settings designed to prevent accidental data exposure. Override instructions . Alternatively, upload via the GitHub GUI. Can’t find profiles.yml error Check you are in the mojap_derived_tables directory before running any dbt command. Can’t find dbt_project.yml error Check you are in the mojap_derived_tables directory before running any dbt command. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 466}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '942066bd31f789b6ae8cef9f7bfd8f6d'}>,\n",
       " <Document: {'content': 'Source access - database does not exist error For create-a-derived-table to be able to use an MoJ Analytical Platform database as a source it must be added to the list of sources . If a requied source is not listed contact #ask-data-modelling and ask for it to be added. Resource access - permission denied error The resource list in your project access config file in data-engineeering-database-access must include both the source databases as referenced in the database access folder and any domains within create-a-derived-table that you wish to have read or write access to. For example resources:\\n- source_database_a/full\\n- create_a_derived_table/domain_a\\n- create_a_derived_table/domain_b Problems with other models - does not exist error you are getting fail errors on dev deployment pointing to models other than those you are working on. this can be due to these models having been successfully deployed to prod and now the dev versions have expired, however the dev tests all still run current fix is to redeploy to dev the models causing the problem run dbt run test locally to check all tests pass before creating a PR. Update - this error should no longer occur; the dev workflow now checks if the models and seeds that a test depends on exist before trying to run them, any that do not are excluded. All tests run in prod . Overwrite error in dev - HIVE_PATH_ALREADY_EXISTS HIVE_PATH_ALREADY_EXISTS: Target directory for table ‘database_name_dev_dbt.table_name’ already exists: s3://mojap-derived-tables/dev/models/domain_name=domain_name/database_name=database_name_dev_dbt/table_name=table_name. Normally when you redeploy a model the model is overwritten (unless you are using incremental strategy). However, sometimes, when developing code, things can get messy and you may need to manually delete the Glue database and tables from the Glue catalogue and the corresponding underlying data in the mojap-derived-tables S3 bucket. See Delete dev models instructions . More info from AWS knowledge centre If you use the external_location parameter in the CTAS query, then be sure to specify an Amazon Simple Storage Service (Amazon S3) location that’s empty. The Amazon S3 location that you use to store the CTAS query results must have no data. When you run your CTAS query, the query checks that the path location or prefix in the Amazon S3 bucket has no data. If the Amazon S3 location already has data, the query doesn’t overwrite the data. May need to delete data at dbt-query-dump You may need to manually clean the data at location ‘s3://dbt-query-dump/tables/...’ before retrying. Athena will not delete data in your account. Note may ; this suggestion is usually unhelpful, and the location suggested may not exist. Check your local logs under create-a-a-derived-table/mojap_derived_tables/logs/dbt.log However, it may help to delete your Glue database and tables from the Glue catalogue and the corresponding underlying data in the mojap-derived-tables S3 bucket, see Delete dev models instructions . Query exhausted resources at this scale factor From StackOverflow, here . Athena is just an EMR cluster with hive and prestodb installed. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 467}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ad0c9338d8586cf9c151fa119bfadf06'}>,\n",
       " <Document: {'content': 'The problem you are facing is: Even if your query is distributed across X number of nodes, the ordering phase must be done by just a single node, the master node in this case. So, you can order as much data as the master node has memory. This may be ameliorated with parallelisation/threading using a macro; example here . Partial parse save file not found Not an error; simply a statement that there is no pre-existing attempt to parse models and a full parse must be done. Database ‘mojap’ does not exist ...\\ncreate schema if not exists database_name_dev_dbt\\n...\\nAthena adapter: Error running SQL: macro create_schema\\n...\\ncreate schema if not exists database_name_dev_dbt\\n...\\nAthena adapter: Error running SQL: macro create_schema\\n...\\nRuntime Error\\nRuntime Error\\nFAILED: SemanticException [Error 10072]: Database does not exist: mojap This error appears to be an issue with dbt-athena failing to create the required database name; mojap is set as the default database name (and does not exist) hence the final error. This may occur when a user attempts an unsupported action; please note that the dbt-athena adapter we are using does not support full dbt functionality, see the dbt-athena repo README . When this issue first occured it seemed to jinx a particular database name, and the code worked fine under a different database name. Is sqlfuff up to date? Analytical Platform --no-verify flag in when uploading more than 5MB CSVs This page was last reviewed on 15 September 2022.\\n\\nIt needs to be reviewed again on 15 September 2023\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 15 September 2023\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nTroubleshooting - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 468}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '19af9ae13a4cedffdd248595f7b36a25'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 469}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Troubleshooting ⚠️ This service is in beta ⚠️ This page is intended to help users self-diagnose errors. Please check here first and then, if necessary, ask in our slack channel #ask-data-modelling , providing context. If you discover new errors and/or solutions please post on slack or edit this document and raise a PR. Contents General troubleshooting tips Delete dev models instructions Troubleshooting list dbt artefacts When dbt runs it generates artefacts. The most useful of these to you will be the logs in the ./mojap_derived_tables/logs/ directory and compiled SQL in the ./mojap_derived_tables/target/ directory. Compiled SQL can be useful when debugging errors as error messages will often refer to the line number in the compiled SQL and not that in your model file. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 470}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dbb7171e7c5cd42e77d14ba4c29b37e6'}>,\n",
       " <Document: {'content': 'General troubleshooting tips When you deploy dev models via the MoJ Analytical Platform logs are created locally in mojap_derived_tables/logs/dbt.log . This file details each step of the run and is useful for debugging. The logs for dev and prod models deployed via GitHub actions are stored temporariliy in the S3 bucket mojap-derived-tables , and are accessible under standard_database_access . dbt clean cleans out the local logs and target folders; it is good  practise to start a session with a clean slate. Under mojap_derived_tables/target there exist compiled and run folders containing a duplicate folder structure as under mojap_derived_tables . Here you can find your SQL code as compiled (with the Jinja rendered) and the DDL/DML run code. You can test each of these in Athena to check your SQL works as expected. Testing your code in Athena will also highlight any read access permission issues. All file names and paths should be lowercase. Delete dev models instructions During development you may need to clear out any dev models you have created from the MoJ Analytical Platform and start afresh. To do this you will need to delete the Glue tables, Glue database and the data in S3, via the AWS Console. ⚠️ Note that anyone with write access to a domain also has permission to delete from that domain, so please exercise caution. ⚠️ Sign in to the AWS Console as an alpha_user (sign in with GitHub as you would for Analytical Platform Control Panel). NB: You may not have permission to access AWS Glue and action steps 1 and 2, if you do great (as this is tidier), if not, please proceed from step 3. AWS Glue → Data Catalog → Tables : Delete the Glue tables from the Glue catalog. AWS Glue → Data Catalog → Databases : Delete the Glue database if necessary; the database may contain someone else’s tables that you don’t want to delete; also if you delete all the tables in a database it will automatically disappear. S3 → mojap-derived-tables bucket → dev/ → models/ : In S3 delete from the lowest level first; objects, tables, database; this makes sure there are no orphaned objects floating about and that you don’t unintentionally delete a database containing someone else’s work as well as your own. Run dbt clean to delete local run artefacts before reattempting to deploy models. Troubleshooting list CSV file won’t upload to GitHub from RStudio You have uploaded a CSV file into your local directory within RStudio on the Analytical Platform and pushed changes to the remote on GitHub, then you notice that the CSV file is not there. This is due to Analytical Platform RStudio settings designed to prevent accidental data exposure. Override instructions . Alternatively, upload via the GitHub GUI. Can’t find profiles.yml error Check you are in the mojap_derived_tables directory before running any dbt command. Can’t find dbt_project.yml error Check you are in the mojap_derived_tables directory before running any dbt command. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 471}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '942066bd31f789b6ae8cef9f7bfd8f6d'}>,\n",
       " <Document: {'content': 'Source access - database does not exist error For create-a-derived-table to be able to use an MoJ Analytical Platform database as a source it must be added to the list of sources . If a requied source is not listed contact #ask-data-modelling and ask for it to be added. Resource access - permission denied error The resource list in your project access config file in data-engineeering-database-access must include both the source databases as referenced in the database access folder and any domains within create-a-derived-table that you wish to have read or write access to. For example resources:\\n- source_database_a/full\\n- create_a_derived_table/domain_a\\n- create_a_derived_table/domain_b Problems with other models - does not exist error you are getting fail errors on dev deployment pointing to models other than those you are working on. this can be due to these models having been successfully deployed to prod and now the dev versions have expired, however the dev tests all still run current fix is to redeploy to dev the models causing the problem run dbt run test locally to check all tests pass before creating a PR. Update - this error should no longer occur; the dev workflow now checks if the models and seeds that a test depends on exist before trying to run them, any that do not are excluded. All tests run in prod . Overwrite error in dev - HIVE_PATH_ALREADY_EXISTS HIVE_PATH_ALREADY_EXISTS: Target directory for table ‘database_name_dev_dbt.table_name’ already exists: s3://mojap-derived-tables/dev/models/domain_name=domain_name/database_name=database_name_dev_dbt/table_name=table_name. Normally when you redeploy a model the model is overwritten (unless you are using incremental strategy). However, sometimes, when developing code, things can get messy and you may need to manually delete the Glue database and tables from the Glue catalogue and the corresponding underlying data in the mojap-derived-tables S3 bucket. See Delete dev models instructions . More info from AWS knowledge centre If you use the external_location parameter in the CTAS query, then be sure to specify an Amazon Simple Storage Service (Amazon S3) location that’s empty. The Amazon S3 location that you use to store the CTAS query results must have no data. When you run your CTAS query, the query checks that the path location or prefix in the Amazon S3 bucket has no data. If the Amazon S3 location already has data, the query doesn’t overwrite the data. May need to delete data at dbt-query-dump You may need to manually clean the data at location ‘s3://dbt-query-dump/tables/...’ before retrying. Athena will not delete data in your account. Note may ; this suggestion is usually unhelpful, and the location suggested may not exist. Check your local logs under create-a-a-derived-table/mojap_derived_tables/logs/dbt.log However, it may help to delete your Glue database and tables from the Glue catalogue and the corresponding underlying data in the mojap-derived-tables S3 bucket, see Delete dev models instructions . Query exhausted resources at this scale factor From StackOverflow, here . Athena is just an EMR cluster with hive and prestodb installed. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 472}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ad0c9338d8586cf9c151fa119bfadf06'}>,\n",
       " <Document: {'content': 'The problem you are facing is: Even if your query is distributed across X number of nodes, the ordering phase must be done by just a single node, the master node in this case. So, you can order as much data as the master node has memory. This may be ameliorated with parallelisation/threading using a macro; example here . Partial parse save file not found Not an error; simply a statement that there is no pre-existing attempt to parse models and a full parse must be done. Database ‘mojap’ does not exist ...\\ncreate schema if not exists database_name_dev_dbt\\n...\\nAthena adapter: Error running SQL: macro create_schema\\n...\\ncreate schema if not exists database_name_dev_dbt\\n...\\nAthena adapter: Error running SQL: macro create_schema\\n...\\nRuntime Error\\nRuntime Error\\nFAILED: SemanticException [Error 10072]: Database does not exist: mojap This error appears to be an issue with dbt-athena failing to create the required database name; mojap is set as the default database name (and does not exist) hence the final error. This may occur when a user attempts an unsupported action; please note that the dbt-athena adapter we are using does not support full dbt functionality, see the dbt-athena repo README . When this issue first occured it seemed to jinx a particular database name, and the code worked fine under a different database name. Is sqlfuff up to date? Analytical Platform --no-verify flag in when uploading more than 5MB CSVs This page was last reviewed on 15 September 2022.\\n\\nIt needs to be reviewed again on 15 September 2023\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 15 September 2023\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nTroubleshooting - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 473}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '19af9ae13a4cedffdd248595f7b36a25'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 474}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Troubleshooting ⚠️ This service is in beta ⚠️ This page is intended to help users self-diagnose errors. Please check here first and then, if necessary, ask in our slack channel #ask-data-modelling , providing context. If you discover new errors and/or solutions please post on slack or edit this document and raise a PR. Contents General troubleshooting tips Delete dev models instructions Troubleshooting list dbt artefacts When dbt runs it generates artefacts. The most useful of these to you will be the logs in the ./mojap_derived_tables/logs/ directory and compiled SQL in the ./mojap_derived_tables/target/ directory. Compiled SQL can be useful when debugging errors as error messages will often refer to the line number in the compiled SQL and not that in your model file. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 475}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dbb7171e7c5cd42e77d14ba4c29b37e6'}>,\n",
       " <Document: {'content': 'General troubleshooting tips When you deploy dev models via the MoJ Analytical Platform logs are created locally in mojap_derived_tables/logs/dbt.log . This file details each step of the run and is useful for debugging. The logs for dev and prod models deployed via GitHub actions are stored temporariliy in the S3 bucket mojap-derived-tables , and are accessible under standard_database_access . dbt clean cleans out the local logs and target folders; it is good  practise to start a session with a clean slate. Under mojap_derived_tables/target there exist compiled and run folders containing a duplicate folder structure as under mojap_derived_tables . Here you can find your SQL code as compiled (with the Jinja rendered) and the DDL/DML run code. You can test each of these in Athena to check your SQL works as expected. Testing your code in Athena will also highlight any read access permission issues. All file names and paths should be lowercase. Delete dev models instructions During development you may need to clear out any dev models you have created from the MoJ Analytical Platform and start afresh. To do this you will need to delete the Glue tables, Glue database and the data in S3, via the AWS Console. ⚠️ Note that anyone with write access to a domain also has permission to delete from that domain, so please exercise caution. ⚠️ Sign in to the AWS Console as an alpha_user (sign in with GitHub as you would for Analytical Platform Control Panel). NB: You may not have permission to access AWS Glue and action steps 1 and 2, if you do great (as this is tidier), if not, please proceed from step 3. AWS Glue → Data Catalog → Tables : Delete the Glue tables from the Glue catalog. AWS Glue → Data Catalog → Databases : Delete the Glue database if necessary; the database may contain someone else’s tables that you don’t want to delete; also if you delete all the tables in a database it will automatically disappear. S3 → mojap-derived-tables bucket → dev/ → models/ : In S3 delete from the lowest level first; objects, tables, database; this makes sure there are no orphaned objects floating about and that you don’t unintentionally delete a database containing someone else’s work as well as your own. Run dbt clean to delete local run artefacts before reattempting to deploy models. Troubleshooting list CSV file won’t upload to GitHub from RStudio You have uploaded a CSV file into your local directory within RStudio on the Analytical Platform and pushed changes to the remote on GitHub, then you notice that the CSV file is not there. This is due to Analytical Platform RStudio settings designed to prevent accidental data exposure. Override instructions . Alternatively, upload via the GitHub GUI. Can’t find profiles.yml error Check you are in the mojap_derived_tables directory before running any dbt command. Can’t find dbt_project.yml error Check you are in the mojap_derived_tables directory before running any dbt command. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 476}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '942066bd31f789b6ae8cef9f7bfd8f6d'}>,\n",
       " <Document: {'content': 'Source access - database does not exist error For create-a-derived-table to be able to use an MoJ Analytical Platform database as a source it must be added to the list of sources . If a requied source is not listed contact #ask-data-modelling and ask for it to be added. Resource access - permission denied error The resource list in your project access config file in data-engineeering-database-access must include both the source databases as referenced in the database access folder and any domains within create-a-derived-table that you wish to have read or write access to. For example resources:\\n- source_database_a/full\\n- create_a_derived_table/domain_a\\n- create_a_derived_table/domain_b Problems with other models - does not exist error you are getting fail errors on dev deployment pointing to models other than those you are working on. this can be due to these models having been successfully deployed to prod and now the dev versions have expired, however the dev tests all still run current fix is to redeploy to dev the models causing the problem run dbt run test locally to check all tests pass before creating a PR. Update - this error should no longer occur; the dev workflow now checks if the models and seeds that a test depends on exist before trying to run them, any that do not are excluded. All tests run in prod . Overwrite error in dev - HIVE_PATH_ALREADY_EXISTS HIVE_PATH_ALREADY_EXISTS: Target directory for table ‘database_name_dev_dbt.table_name’ already exists: s3://mojap-derived-tables/dev/models/domain_name=domain_name/database_name=database_name_dev_dbt/table_name=table_name. Normally when you redeploy a model the model is overwritten (unless you are using incremental strategy). However, sometimes, when developing code, things can get messy and you may need to manually delete the Glue database and tables from the Glue catalogue and the corresponding underlying data in the mojap-derived-tables S3 bucket. See Delete dev models instructions . More info from AWS knowledge centre If you use the external_location parameter in the CTAS query, then be sure to specify an Amazon Simple Storage Service (Amazon S3) location that’s empty. The Amazon S3 location that you use to store the CTAS query results must have no data. When you run your CTAS query, the query checks that the path location or prefix in the Amazon S3 bucket has no data. If the Amazon S3 location already has data, the query doesn’t overwrite the data. May need to delete data at dbt-query-dump You may need to manually clean the data at location ‘s3://dbt-query-dump/tables/...’ before retrying. Athena will not delete data in your account. Note may ; this suggestion is usually unhelpful, and the location suggested may not exist. Check your local logs under create-a-a-derived-table/mojap_derived_tables/logs/dbt.log However, it may help to delete your Glue database and tables from the Glue catalogue and the corresponding underlying data in the mojap-derived-tables S3 bucket, see Delete dev models instructions . Query exhausted resources at this scale factor From StackOverflow, here . Athena is just an EMR cluster with hive and prestodb installed. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 477}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ad0c9338d8586cf9c151fa119bfadf06'}>,\n",
       " <Document: {'content': 'The problem you are facing is: Even if your query is distributed across X number of nodes, the ordering phase must be done by just a single node, the master node in this case. So, you can order as much data as the master node has memory. This may be ameliorated with parallelisation/threading using a macro; example here . Partial parse save file not found Not an error; simply a statement that there is no pre-existing attempt to parse models and a full parse must be done. Database ‘mojap’ does not exist ...\\ncreate schema if not exists database_name_dev_dbt\\n...\\nAthena adapter: Error running SQL: macro create_schema\\n...\\ncreate schema if not exists database_name_dev_dbt\\n...\\nAthena adapter: Error running SQL: macro create_schema\\n...\\nRuntime Error\\nRuntime Error\\nFAILED: SemanticException [Error 10072]: Database does not exist: mojap This error appears to be an issue with dbt-athena failing to create the required database name; mojap is set as the default database name (and does not exist) hence the final error. This may occur when a user attempts an unsupported action; please note that the dbt-athena adapter we are using does not support full dbt functionality, see the dbt-athena repo README . When this issue first occured it seemed to jinx a particular database name, and the code worked fine under a different database name. Is sqlfuff up to date? Analytical Platform --no-verify flag in when uploading more than 5MB CSVs This page was last reviewed on 15 September 2022.\\n\\nIt needs to be reviewed again on 15 September 2023\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 15 September 2023\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nTroubleshooting - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 478}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '19af9ae13a4cedffdd248595f7b36a25'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 479}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Troubleshooting ⚠️ This service is in beta ⚠️ This page is intended to help users self-diagnose errors. Please check here first and then, if necessary, ask in our slack channel #ask-data-modelling , providing context. If you discover new errors and/or solutions please post on slack or edit this document and raise a PR. Contents General troubleshooting tips Delete dev models instructions Troubleshooting list dbt artefacts When dbt runs it generates artefacts. The most useful of these to you will be the logs in the ./mojap_derived_tables/logs/ directory and compiled SQL in the ./mojap_derived_tables/target/ directory. Compiled SQL can be useful when debugging errors as error messages will often refer to the line number in the compiled SQL and not that in your model file. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 480}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dbb7171e7c5cd42e77d14ba4c29b37e6'}>,\n",
       " <Document: {'content': 'General troubleshooting tips When you deploy dev models via the MoJ Analytical Platform logs are created locally in mojap_derived_tables/logs/dbt.log . This file details each step of the run and is useful for debugging. The logs for dev and prod models deployed via GitHub actions are stored temporariliy in the S3 bucket mojap-derived-tables , and are accessible under standard_database_access . dbt clean cleans out the local logs and target folders; it is good  practise to start a session with a clean slate. Under mojap_derived_tables/target there exist compiled and run folders containing a duplicate folder structure as under mojap_derived_tables . Here you can find your SQL code as compiled (with the Jinja rendered) and the DDL/DML run code. You can test each of these in Athena to check your SQL works as expected. Testing your code in Athena will also highlight any read access permission issues. All file names and paths should be lowercase. Delete dev models instructions During development you may need to clear out any dev models you have created from the MoJ Analytical Platform and start afresh. To do this you will need to delete the Glue tables, Glue database and the data in S3, via the AWS Console. ⚠️ Note that anyone with write access to a domain also has permission to delete from that domain, so please exercise caution. ⚠️ Sign in to the AWS Console as an alpha_user (sign in with GitHub as you would for Analytical Platform Control Panel). NB: You may not have permission to access AWS Glue and action steps 1 and 2, if you do great (as this is tidier), if not, please proceed from step 3. AWS Glue → Data Catalog → Tables : Delete the Glue tables from the Glue catalog. AWS Glue → Data Catalog → Databases : Delete the Glue database if necessary; the database may contain someone else’s tables that you don’t want to delete; also if you delete all the tables in a database it will automatically disappear. S3 → mojap-derived-tables bucket → dev/ → models/ : In S3 delete from the lowest level first; objects, tables, database; this makes sure there are no orphaned objects floating about and that you don’t unintentionally delete a database containing someone else’s work as well as your own. Run dbt clean to delete local run artefacts before reattempting to deploy models. Troubleshooting list CSV file won’t upload to GitHub from RStudio You have uploaded a CSV file into your local directory within RStudio on the Analytical Platform and pushed changes to the remote on GitHub, then you notice that the CSV file is not there. This is due to Analytical Platform RStudio settings designed to prevent accidental data exposure. Override instructions . Alternatively, upload via the GitHub GUI. Can’t find profiles.yml error Check you are in the mojap_derived_tables directory before running any dbt command. Can’t find dbt_project.yml error Check you are in the mojap_derived_tables directory before running any dbt command. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 481}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '942066bd31f789b6ae8cef9f7bfd8f6d'}>,\n",
       " <Document: {'content': 'Source access - database does not exist error For create-a-derived-table to be able to use an MoJ Analytical Platform database as a source it must be added to the list of sources . If a requied source is not listed contact #ask-data-modelling and ask for it to be added. Resource access - permission denied error The resource list in your project access config file in data-engineeering-database-access must include both the source databases as referenced in the database access folder and any domains within create-a-derived-table that you wish to have read or write access to. For example resources:\\n- source_database_a/full\\n- create_a_derived_table/domain_a\\n- create_a_derived_table/domain_b Problems with other models - does not exist error you are getting fail errors on dev deployment pointing to models other than those you are working on. this can be due to these models having been successfully deployed to prod and now the dev versions have expired, however the dev tests all still run current fix is to redeploy to dev the models causing the problem run dbt run test locally to check all tests pass before creating a PR. Update - this error should no longer occur; the dev workflow now checks if the models and seeds that a test depends on exist before trying to run them, any that do not are excluded. All tests run in prod . Overwrite error in dev - HIVE_PATH_ALREADY_EXISTS HIVE_PATH_ALREADY_EXISTS: Target directory for table ‘database_name_dev_dbt.table_name’ already exists: s3://mojap-derived-tables/dev/models/domain_name=domain_name/database_name=database_name_dev_dbt/table_name=table_name. Normally when you redeploy a model the model is overwritten (unless you are using incremental strategy). However, sometimes, when developing code, things can get messy and you may need to manually delete the Glue database and tables from the Glue catalogue and the corresponding underlying data in the mojap-derived-tables S3 bucket. See Delete dev models instructions . More info from AWS knowledge centre If you use the external_location parameter in the CTAS query, then be sure to specify an Amazon Simple Storage Service (Amazon S3) location that’s empty. The Amazon S3 location that you use to store the CTAS query results must have no data. When you run your CTAS query, the query checks that the path location or prefix in the Amazon S3 bucket has no data. If the Amazon S3 location already has data, the query doesn’t overwrite the data. May need to delete data at dbt-query-dump You may need to manually clean the data at location ‘s3://dbt-query-dump/tables/...’ before retrying. Athena will not delete data in your account. Note may ; this suggestion is usually unhelpful, and the location suggested may not exist. Check your local logs under create-a-a-derived-table/mojap_derived_tables/logs/dbt.log However, it may help to delete your Glue database and tables from the Glue catalogue and the corresponding underlying data in the mojap-derived-tables S3 bucket, see Delete dev models instructions . Query exhausted resources at this scale factor From StackOverflow, here . Athena is just an EMR cluster with hive and prestodb installed. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 482}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ad0c9338d8586cf9c151fa119bfadf06'}>,\n",
       " <Document: {'content': 'The problem you are facing is: Even if your query is distributed across X number of nodes, the ordering phase must be done by just a single node, the master node in this case. So, you can order as much data as the master node has memory. This may be ameliorated with parallelisation/threading using a macro; example here . Partial parse save file not found Not an error; simply a statement that there is no pre-existing attempt to parse models and a full parse must be done. Database ‘mojap’ does not exist ...\\ncreate schema if not exists database_name_dev_dbt\\n...\\nAthena adapter: Error running SQL: macro create_schema\\n...\\ncreate schema if not exists database_name_dev_dbt\\n...\\nAthena adapter: Error running SQL: macro create_schema\\n...\\nRuntime Error\\nRuntime Error\\nFAILED: SemanticException [Error 10072]: Database does not exist: mojap This error appears to be an issue with dbt-athena failing to create the required database name; mojap is set as the default database name (and does not exist) hence the final error. This may occur when a user attempts an unsupported action; please note that the dbt-athena adapter we are using does not support full dbt functionality, see the dbt-athena repo README . When this issue first occured it seemed to jinx a particular database name, and the code worked fine under a different database name. Is sqlfuff up to date? Analytical Platform --no-verify flag in when uploading more than 5MB CSVs This page was last reviewed on 15 September 2022.\\n\\nIt needs to be reviewed again on 15 September 2023\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 15 September 2023\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nTroubleshooting - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 483}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '19af9ae13a4cedffdd248595f7b36a25'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 484}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Troubleshooting ⚠️ This service is in beta ⚠️ This page is intended to help users self-diagnose errors. Please check here first and then, if necessary, ask in our slack channel #ask-data-modelling , providing context. If you discover new errors and/or solutions please post on slack or edit this document and raise a PR. Contents General troubleshooting tips Delete dev models instructions Troubleshooting list dbt artefacts When dbt runs it generates artefacts. The most useful of these to you will be the logs in the ./mojap_derived_tables/logs/ directory and compiled SQL in the ./mojap_derived_tables/target/ directory. Compiled SQL can be useful when debugging errors as error messages will often refer to the line number in the compiled SQL and not that in your model file. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 485}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dbb7171e7c5cd42e77d14ba4c29b37e6'}>,\n",
       " <Document: {'content': 'General troubleshooting tips When you deploy dev models via the MoJ Analytical Platform logs are created locally in mojap_derived_tables/logs/dbt.log . This file details each step of the run and is useful for debugging. The logs for dev and prod models deployed via GitHub actions are stored temporariliy in the S3 bucket mojap-derived-tables , and are accessible under standard_database_access . dbt clean cleans out the local logs and target folders; it is good  practise to start a session with a clean slate. Under mojap_derived_tables/target there exist compiled and run folders containing a duplicate folder structure as under mojap_derived_tables . Here you can find your SQL code as compiled (with the Jinja rendered) and the DDL/DML run code. You can test each of these in Athena to check your SQL works as expected. Testing your code in Athena will also highlight any read access permission issues. All file names and paths should be lowercase. Delete dev models instructions During development you may need to clear out any dev models you have created from the MoJ Analytical Platform and start afresh. To do this you will need to delete the Glue tables, Glue database and the data in S3, via the AWS Console. ⚠️ Note that anyone with write access to a domain also has permission to delete from that domain, so please exercise caution. ⚠️ Sign in to the AWS Console as an alpha_user (sign in with GitHub as you would for Analytical Platform Control Panel). NB: You may not have permission to access AWS Glue and action steps 1 and 2, if you do great (as this is tidier), if not, please proceed from step 3. AWS Glue → Data Catalog → Tables : Delete the Glue tables from the Glue catalog. AWS Glue → Data Catalog → Databases : Delete the Glue database if necessary; the database may contain someone else’s tables that you don’t want to delete; also if you delete all the tables in a database it will automatically disappear. S3 → mojap-derived-tables bucket → dev/ → models/ : In S3 delete from the lowest level first; objects, tables, database; this makes sure there are no orphaned objects floating about and that you don’t unintentionally delete a database containing someone else’s work as well as your own. Run dbt clean to delete local run artefacts before reattempting to deploy models. Troubleshooting list CSV file won’t upload to GitHub from RStudio You have uploaded a CSV file into your local directory within RStudio on the Analytical Platform and pushed changes to the remote on GitHub, then you notice that the CSV file is not there. This is due to Analytical Platform RStudio settings designed to prevent accidental data exposure. Override instructions . Alternatively, upload via the GitHub GUI. Can’t find profiles.yml error Check you are in the mojap_derived_tables directory before running any dbt command. Can’t find dbt_project.yml error Check you are in the mojap_derived_tables directory before running any dbt command. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 486}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '942066bd31f789b6ae8cef9f7bfd8f6d'}>,\n",
       " <Document: {'content': 'Source access - database does not exist error For create-a-derived-table to be able to use an MoJ Analytical Platform database as a source it must be added to the list of sources . If a requied source is not listed contact #ask-data-modelling and ask for it to be added. Resource access - permission denied error The resource list in your project access config file in data-engineeering-database-access must include both the source databases as referenced in the database access folder and any domains within create-a-derived-table that you wish to have read or write access to. For example resources:\\n- source_database_a/full\\n- create_a_derived_table/domain_a\\n- create_a_derived_table/domain_b Problems with other models - does not exist error you are getting fail errors on dev deployment pointing to models other than those you are working on. this can be due to these models having been successfully deployed to prod and now the dev versions have expired, however the dev tests all still run current fix is to redeploy to dev the models causing the problem run dbt run test locally to check all tests pass before creating a PR. Update - this error should no longer occur; the dev workflow now checks if the models and seeds that a test depends on exist before trying to run them, any that do not are excluded. All tests run in prod . Overwrite error in dev - HIVE_PATH_ALREADY_EXISTS HIVE_PATH_ALREADY_EXISTS: Target directory for table ‘database_name_dev_dbt.table_name’ already exists: s3://mojap-derived-tables/dev/models/domain_name=domain_name/database_name=database_name_dev_dbt/table_name=table_name. Normally when you redeploy a model the model is overwritten (unless you are using incremental strategy). However, sometimes, when developing code, things can get messy and you may need to manually delete the Glue database and tables from the Glue catalogue and the corresponding underlying data in the mojap-derived-tables S3 bucket. See Delete dev models instructions . More info from AWS knowledge centre If you use the external_location parameter in the CTAS query, then be sure to specify an Amazon Simple Storage Service (Amazon S3) location that’s empty. The Amazon S3 location that you use to store the CTAS query results must have no data. When you run your CTAS query, the query checks that the path location or prefix in the Amazon S3 bucket has no data. If the Amazon S3 location already has data, the query doesn’t overwrite the data. May need to delete data at dbt-query-dump You may need to manually clean the data at location ‘s3://dbt-query-dump/tables/...’ before retrying. Athena will not delete data in your account. Note may ; this suggestion is usually unhelpful, and the location suggested may not exist. Check your local logs under create-a-a-derived-table/mojap_derived_tables/logs/dbt.log However, it may help to delete your Glue database and tables from the Glue catalogue and the corresponding underlying data in the mojap-derived-tables S3 bucket, see Delete dev models instructions . Query exhausted resources at this scale factor From StackOverflow, here . Athena is just an EMR cluster with hive and prestodb installed. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 487}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ad0c9338d8586cf9c151fa119bfadf06'}>,\n",
       " <Document: {'content': 'The problem you are facing is: Even if your query is distributed across X number of nodes, the ordering phase must be done by just a single node, the master node in this case. So, you can order as much data as the master node has memory. This may be ameliorated with parallelisation/threading using a macro; example here . Partial parse save file not found Not an error; simply a statement that there is no pre-existing attempt to parse models and a full parse must be done. Database ‘mojap’ does not exist ...\\ncreate schema if not exists database_name_dev_dbt\\n...\\nAthena adapter: Error running SQL: macro create_schema\\n...\\ncreate schema if not exists database_name_dev_dbt\\n...\\nAthena adapter: Error running SQL: macro create_schema\\n...\\nRuntime Error\\nRuntime Error\\nFAILED: SemanticException [Error 10072]: Database does not exist: mojap This error appears to be an issue with dbt-athena failing to create the required database name; mojap is set as the default database name (and does not exist) hence the final error. This may occur when a user attempts an unsupported action; please note that the dbt-athena adapter we are using does not support full dbt functionality, see the dbt-athena repo README . When this issue first occured it seemed to jinx a particular database name, and the code worked fine under a different database name. Is sqlfuff up to date? Analytical Platform --no-verify flag in when uploading more than 5MB CSVs This page was last reviewed on 15 September 2022.\\n\\nIt needs to be reviewed again on 15 September 2023\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 15 September 2023\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nTroubleshooting - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 488}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '19af9ae13a4cedffdd248595f7b36a25'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 489}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Troubleshooting ⚠️ This service is in beta ⚠️ This page is intended to help users self-diagnose errors. Please check here first and then, if necessary, ask in our slack channel #ask-data-modelling , providing context. If you discover new errors and/or solutions please post on slack or edit this document and raise a PR. Contents General troubleshooting tips Delete dev models instructions Troubleshooting list dbt artefacts When dbt runs it generates artefacts. The most useful of these to you will be the logs in the ./mojap_derived_tables/logs/ directory and compiled SQL in the ./mojap_derived_tables/target/ directory. Compiled SQL can be useful when debugging errors as error messages will often refer to the line number in the compiled SQL and not that in your model file. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 490}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dbb7171e7c5cd42e77d14ba4c29b37e6'}>,\n",
       " <Document: {'content': 'General troubleshooting tips When you deploy dev models via the MoJ Analytical Platform logs are created locally in mojap_derived_tables/logs/dbt.log . This file details each step of the run and is useful for debugging. The logs for dev and prod models deployed via GitHub actions are stored temporariliy in the S3 bucket mojap-derived-tables , and are accessible under standard_database_access . dbt clean cleans out the local logs and target folders; it is good  practise to start a session with a clean slate. Under mojap_derived_tables/target there exist compiled and run folders containing a duplicate folder structure as under mojap_derived_tables . Here you can find your SQL code as compiled (with the Jinja rendered) and the DDL/DML run code. You can test each of these in Athena to check your SQL works as expected. Testing your code in Athena will also highlight any read access permission issues. All file names and paths should be lowercase. Delete dev models instructions During development you may need to clear out any dev models you have created from the MoJ Analytical Platform and start afresh. To do this you will need to delete the Glue tables, Glue database and the data in S3, via the AWS Console. ⚠️ Note that anyone with write access to a domain also has permission to delete from that domain, so please exercise caution. ⚠️ Sign in to the AWS Console as an alpha_user (sign in with GitHub as you would for Analytical Platform Control Panel). NB: You may not have permission to access AWS Glue and action steps 1 and 2, if you do great (as this is tidier), if not, please proceed from step 3. AWS Glue → Data Catalog → Tables : Delete the Glue tables from the Glue catalog. AWS Glue → Data Catalog → Databases : Delete the Glue database if necessary; the database may contain someone else’s tables that you don’t want to delete; also if you delete all the tables in a database it will automatically disappear. S3 → mojap-derived-tables bucket → dev/ → models/ : In S3 delete from the lowest level first; objects, tables, database; this makes sure there are no orphaned objects floating about and that you don’t unintentionally delete a database containing someone else’s work as well as your own. Run dbt clean to delete local run artefacts before reattempting to deploy models. Troubleshooting list CSV file won’t upload to GitHub from RStudio You have uploaded a CSV file into your local directory within RStudio on the Analytical Platform and pushed changes to the remote on GitHub, then you notice that the CSV file is not there. This is due to Analytical Platform RStudio settings designed to prevent accidental data exposure. Override instructions . Alternatively, upload via the GitHub GUI. Can’t find profiles.yml error Check you are in the mojap_derived_tables directory before running any dbt command. Can’t find dbt_project.yml error Check you are in the mojap_derived_tables directory before running any dbt command. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 491}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '942066bd31f789b6ae8cef9f7bfd8f6d'}>,\n",
       " <Document: {'content': 'Source access - database does not exist error For create-a-derived-table to be able to use an MoJ Analytical Platform database as a source it must be added to the list of sources . If a requied source is not listed contact #ask-data-modelling and ask for it to be added. Resource access - permission denied error The resource list in your project access config file in data-engineeering-database-access must include both the source databases as referenced in the database access folder and any domains within create-a-derived-table that you wish to have read or write access to. For example resources:\\n- source_database_a/full\\n- create_a_derived_table/domain_a\\n- create_a_derived_table/domain_b Problems with other models - does not exist error you are getting fail errors on dev deployment pointing to models other than those you are working on. this can be due to these models having been successfully deployed to prod and now the dev versions have expired, however the dev tests all still run current fix is to redeploy to dev the models causing the problem run dbt run test locally to check all tests pass before creating a PR. Update - this error should no longer occur; the dev workflow now checks if the models and seeds that a test depends on exist before trying to run them, any that do not are excluded. All tests run in prod . Overwrite error in dev - HIVE_PATH_ALREADY_EXISTS HIVE_PATH_ALREADY_EXISTS: Target directory for table ‘database_name_dev_dbt.table_name’ already exists: s3://mojap-derived-tables/dev/models/domain_name=domain_name/database_name=database_name_dev_dbt/table_name=table_name. Normally when you redeploy a model the model is overwritten (unless you are using incremental strategy). However, sometimes, when developing code, things can get messy and you may need to manually delete the Glue database and tables from the Glue catalogue and the corresponding underlying data in the mojap-derived-tables S3 bucket. See Delete dev models instructions . More info from AWS knowledge centre If you use the external_location parameter in the CTAS query, then be sure to specify an Amazon Simple Storage Service (Amazon S3) location that’s empty. The Amazon S3 location that you use to store the CTAS query results must have no data. When you run your CTAS query, the query checks that the path location or prefix in the Amazon S3 bucket has no data. If the Amazon S3 location already has data, the query doesn’t overwrite the data. May need to delete data at dbt-query-dump You may need to manually clean the data at location ‘s3://dbt-query-dump/tables/...’ before retrying. Athena will not delete data in your account. Note may ; this suggestion is usually unhelpful, and the location suggested may not exist. Check your local logs under create-a-a-derived-table/mojap_derived_tables/logs/dbt.log However, it may help to delete your Glue database and tables from the Glue catalogue and the corresponding underlying data in the mojap-derived-tables S3 bucket, see Delete dev models instructions . Query exhausted resources at this scale factor From StackOverflow, here . Athena is just an EMR cluster with hive and prestodb installed. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 492}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ad0c9338d8586cf9c151fa119bfadf06'}>,\n",
       " <Document: {'content': 'The problem you are facing is: Even if your query is distributed across X number of nodes, the ordering phase must be done by just a single node, the master node in this case. So, you can order as much data as the master node has memory. This may be ameliorated with parallelisation/threading using a macro; example here . Partial parse save file not found Not an error; simply a statement that there is no pre-existing attempt to parse models and a full parse must be done. Database ‘mojap’ does not exist ...\\ncreate schema if not exists database_name_dev_dbt\\n...\\nAthena adapter: Error running SQL: macro create_schema\\n...\\ncreate schema if not exists database_name_dev_dbt\\n...\\nAthena adapter: Error running SQL: macro create_schema\\n...\\nRuntime Error\\nRuntime Error\\nFAILED: SemanticException [Error 10072]: Database does not exist: mojap This error appears to be an issue with dbt-athena failing to create the required database name; mojap is set as the default database name (and does not exist) hence the final error. This may occur when a user attempts an unsupported action; please note that the dbt-athena adapter we are using does not support full dbt functionality, see the dbt-athena repo README . When this issue first occured it seemed to jinx a particular database name, and the code worked fine under a different database name. Is sqlfuff up to date? Analytical Platform --no-verify flag in when uploading more than 5MB CSVs This page was last reviewed on 15 September 2022.\\n\\nIt needs to be reviewed again on 15 September 2023\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 15 September 2023\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nScheduling to Prod - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 493}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5f09bc3b700f746f1b87e49263e91624'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 494}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Scheduling to Prod Prod A model or seed is in ‘prod’ when they are merged into main . The data modelling team will review and approve any changes before a pull request is merged into main . If you have added or modified seeds in your pull request, these will be deployed when the pull request is merged. If you have added or modified models in your pull request, these will be deployed at the next schedule as per the defined schedule tag. Run artefacts for prod deployments are exported to S3 and are available for 3 days. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 495}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd88fd9655c80b8fa03866b8751d471e9'}>,\n",
       " <Document: {'content': 'You can get the S3 path by navigating to the Actions tab in the Create a Derived Table repository and selecting the workflow you want the output of. Then check the Export run artefacts to S3 output for the S3 path which will look something like this: s3://mojap-derived-tables/prod/run_artefacts/run_time=yyyy-mm-dd hh:mm:ss/ Scheduling There are three options for scheduling model updates: daily , weekly , and monthly . The monthly schedule runs on the first Sunday of every month and the weekly schedule runs every Sunday. All schedules run at 3AM. To select a schedule for your model, add the tags configuration to your model’s property file, like this: version: 2 models:\\n- name: <your_model_name>\\nconfig:\\ntags: daily You can configure a directory of models to run on the same schedule by adding the tags configuration to the dbt_project.yml file by finding the models resource key and adding a few lines, like this: models:\\nmojap_derived_tables:  # this line is already in the file\\n+materialized: table  # this line is already in the file\\nsecurity:\\nprison_safety_and_security:\\n+tags: daily\\nstaging:\\n+tags: monthly In the above example, the Prison Safety and Security team have configured their entire database to update daily except for the subdirectory of staging models that will update monthly. This page was last reviewed on 7 August 2023.\\n\\nIt needs to be reviewed again on 7 August 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 7 August 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\ndbt-athena Upgrade Guidance - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 496}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4f1dccc70496d390a2008f97d09c91e6'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 497}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools dbt-athena Upgrade Guidance We are in the process of migrating from our in-house maintained fork of the dbt-athena adapter to the community maintained fork dbt-athena-community recommended by dbt . The guidance below details how you can test your models using the dbt-athena-community adapter. If you have any issues please get in touch via the #ask-data-modelling channel. Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Test set up We have created a branch called DMT-236/dbt-athena-upgrade-main which contains all the latest models, sources, seeds, macros from the main branch (that is everything that exists in prod ) and all the required upgrades. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 498}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e83e33825ec90c3300975ef1fea352a6'}>,\n",
       " <Document: {'content': 'The main upgrades which you need to be aware of are: dbt-athena-community 1.5.0. dbt-core 1.5.0. macro generate_s3_location.sql to support our S3 file path Hive style naming convention. script scripts/insert_external_location_config.py to insert the required external_location configuration at the top of every model .sql file. running sqlfluff with the --ignore=templating option. seeds S3 location has changed (this does not effect any references to seeds ) To set up for testing you will need to checkout this branch, uninstall the old adapater and rerun the requirements files to update your local venv with the correct versions. In Terminal (with your venv active) in the root directory run the following to pull the latest from main , switch to DMT-236/dbt-athena-upgrade-main and update your venv : git switch main\\ngit fetch\\ngit pull\\ngit switch DMT-236/dbt-athena-upgrade-main At this point you can refresh the Git tab in the RStudio Environments panel (top right) to check you have switched to this branch. Next uninstall the old adapter: pip install --upgrade pip\\npip uninstall dbt-athena-adapter Here you will be asked to type “Y” to proceed with the uninstall; do so and continue to install requirements: pip install -r requirements.txt\\npip install -r requirements-lint.txt To check you have the correct set up list your local environment packages with pip list --local and check the list output for dbt-core 1.5.0 , dbt-athena-community 1.5.0 and not dbt-athena-adapter 1.0.1 (or any other version of it). If you still have the latter try to uninstall it again; if both old and new adapters are installed there will be conflicts. Full evironment set up guidance here. Test prod models To test your prod models you need to create your own branch off the DMT-236/dbt-athena-upgrade-main branch, deploy your models in dev , run your dbt tests and lint. You can also manually run equality tests in Athena to compare tables from prod created using the old dbt-athena adapter to the tables you have just created in dev using dbt-athena-community adapter. To explicitly create a new branch off DMT-236/dbt-athena-upgrade-main run the following: git checkout -b <new-branch-name> DMT-236/dbt-athena-upgrade-main All your prod models have the external_location parameter inserted into a config block at the top of each .sql file (see the Insert external_location section for more details). As a consequence all prod models are already deployed into dev by the deploy-dev workflow. However, for robustness, we would still like you to test your prod models by deploying them yourselves; cd into the mojap_derived_tables directory to run dbt commands as usual. If you have any issues due to redeploying please delete your dev models and try again, see Delete dev models instructions . Once you have deployed your models please run your tests and lint. ⚠️ See the section below on SQLFluff linting changes ⚠️ Please keep us up to date with your progress in the #ask-data-modelling channel. When all users are happy that prod models are deploying as expected using the upgrades we will merge the branch DMT-236/dbt-athena-upgrade-main into main . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 499}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '89b7c442a97dd8c5f1beb5f9b408babf'}>,\n",
       " <Document: {'content': \"Test dev models Once you have completed testing of your prod models you may wish to continue testing with your dev models. To do this you will need to create another branch off DMT-236/dbt-athena-upgrade-main ( see instructions above ) and then merge into this from your feature branch. For example, I have some dev models on a branch called my-feature-branch so: git checkout -b new-test-branch DMT-236/dbt-athena-upgrade-main\\ngit fetch origin my-feature-branch\\ngit pull origin my-feature-branch\\ngit merge origin/my-feature-branch This creates a new branch new-test-branch off DMT-236/dbt-athena-upgrade-main branch and then collects the changes from my-feature-branch and merges these in to new-test-branch . After the first command check you are on the new-test-branch before proceeding. ⚠️ WARNING ⚠️ Your dev models will not have the external_location parameter set, which is required to store the output model in the correct location. See instructions in the Insert external_location section to insert the external_location and then cd into the mojap_derived_tables directory to run dbt commands as usual. Once you have deployed your models run tests and lint. See the section below on SQLFluff linting changes . Insert external_location The external_location parameter is set by the macro generate_s3_location which is invoked at run time. This combines information from the schema name with the names from the repo directory structure to create the desired S3 location in the form: s3://<bucket_name>/<env_name>/<table_type>/domain_name=<domain_name>/database_name=db_name/table_name=<tb_name> To make this as painless as possible we have prepared a script insert_external_location_config.py which you can run locally to automatically insert the required line or full config into your .sql files. The script must be run from the root directory and requires user input to determine the path to the files that you wish to run it on. In Terminal, in the root directory run python scripts/insert_external_location_config.py You will be prompted to enter the path to the directory containing the files you wish to apply the script to. Type your input starting with the domain directory name and continuing to whichever subdirectory you require (note there is no tab auto complete available) and hit Enter : Enter path to directory, starting with domain directory: <domain_name>/<database_name> If you are copy/pasting be careful not to introduce leading or trailing spaces. The next prompt shows the full file path you have selected and asks you to confirm by typing “Y” (any other input will exit the program): Path selected: 'mojap_derived_tables/models/<domain_name>/<database_name>' Continue with selected path? Enter Y/n: Y The final prompt shows the number of files selected and their paths and asks you to confirm by typing “Y”: Files selected:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Number of files selected:  3\\nContinue with selected files? Enter Y/n: Y Upon entering “Y” the script starts to scan the files and make the required changes. There are three possibilities: Scanning files... Config block exists but no external location set for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql\\nInserting external_location line into existing config block... External location not set and no config block for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql\\nInserting config block... \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 500}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a46d7e635f40a09f3e9f4dea151e1327'}>,\n",
       " <Document: {'content': 'External location set correctly in config block - nothing to do for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Once inserted the config block will look similar to this, but may include additional parameters that you have set previously: {{ config(\\nexternal_location=generate_s3_location()\\n) }} Note that if there was no existing config block it is inserted at the top of the file, displacing (but not overwriting) any existing comments or code. Files are automatically saved once the changes are applied. SQLFluff linting changes As you may be aware we have had issues with SQLFluff being unable to cope with complex Jinja templating mostly in macros. The new generate_s3_location.sql macro is no exception and is added to the sqlfluffignore file so that it is skipped during linting. However, since all models now reference this macro SQLfluff throws the Undefined jinja template variable error. We cannot add all models to sqlfluffignore , hence to circumvent the perceived error please use the --ignore=templating option when running SQLFluff lint or fix, thus: sqlfluff lint --ignore=templating path/to/files/to/lint sqlfluff fix --ignore=templating path/to/files/to/lint/and/fix Update your branch with the dbt-athena upgrade As mentioned above we have created a branch containing all the upgrades called DMT-236/dbt-athena-upgrade-main . While we are testing we may make changes to the DMT-236/dbt-athena-upgrade-main branch which you will then need to merge into your branches. Whilst on the branch you want to update with the latest from DMT-236/dbt-athena-upgrade-main run: git fetch origin DMT-236/dbt-athena-upgrade-main\\ngit pull origin DMT-236/dbt-athena-upgrade-main\\ngit merge origin/DMT-236/dbt-athena-upgrade-main At this point you may have merge conflicts that need to be resolved; please see GitHub resolve merge conflicts . If required, ask for help on the #ask-data-modelling slack channel. S3 location change for seeds Previously the seed S3 location was split between the dev and prod environments and followed the same Hive style path naming convention as for models. It was not straightforward to preserve this feature with the dbt-athena-community adapter and since seeds change little it was not a priority. The old directory structure for the mojap-derived-tables bucket is as below, with the seed directory appearing under both prod and dev directories. Note the database name is suffixed with _dev_dbt under the dev directory: ├── mojap_derived_tables\\n├── dev/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one_dev_dbt/\\n├── table_name=tb_one\\n...\\n...\\n...\\n├── prod/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one/\\n├── table_name=tb_one\\n...\\n...\\n... The new directory structure has a single seeds directory at the same level as the prod and dev directories. A seed created on a dev run will appear under its database name suffixed with _dev_dbt , as before, but the Hive path naming convention is not upheld. Instead we use the naming option schema_table provided by dbt-athena-community which is simply <database_name>/<table_name> ├── mojap_derived_tables\\n├── dev/\\n├── prod/\\n└── seeds/\\n├── database_one_dev_dbt/\\n├── table_one/\\n...\\n├── database_one/\\n├── table_one/\\n...\\n... The changes to the S3 location should not have any impact on users, unless they have specifically referenced a seed by its S3 location. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 501}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b77956a87aea7e2c263ae62aeeb9c2a4'}>,\n",
       " <Document: {'content': 'References in create-a-derived-table using the ref function will be unaffected as this uses   Athena to reference the Glue Catalogue Registration, in the form <database_name.table_name> . The catalogue registrations will be updated with the new S3 locations automatically. License Unless stated otherwise, the codebase is released under the MIT License . This covers both the codebase and any sample code in the documentation. The documentation is © Crown copyright and available under the terms of the Open Government 3.0 licence. This page was last reviewed on 13 June 2023.\\n\\nIt needs to be reviewed again on 13 June 2024\\nby the page owner #ask-data-modelling . This page was set to be reviewed before 13 June 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\ndbt-athena Upgrade Guidance - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 502}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '13b4d123eade954aaf328fc84b3091a3'}>,\n",
       " <Document: {'content': 'Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 503}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9efc545c7905be5a0aa914883b6669ab'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools dbt-athena Upgrade Guidance We are in the process of migrating from our in-house maintained fork of the dbt-athena adapter to the community maintained fork dbt-athena-community recommended by dbt . The guidance below details how you can test your models using the dbt-athena-community adapter. If you have any issues please get in touch via the #ask-data-modelling channel. Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Test set up We have created a branch called DMT-236/dbt-athena-upgrade-main which contains all the latest models, sources, seeds, macros from the main branch (that is everything that exists in prod ) and all the required upgrades. The main upgrades which you need to be aware of are: dbt-athena-community 1.5.0. dbt-core 1.5.0. macro generate_s3_location.sql to support our S3 file path Hive style naming convention. script scripts/insert_external_location_config.py to insert the required external_location configuration at the top of every model .sql file. running sqlfluff with the --ignore=templating option. seeds S3 location has changed (this does not effect any references to seeds ) To set up for testing you will need to checkout this branch, uninstall the old adapater and rerun the requirements files to update your local venv with the correct versions. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 504}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '597609ab553d2b8d6506564cb965ae34'}>,\n",
       " <Document: {'content': 'In Terminal (with your venv active) in the root directory run the following to pull the latest from main , switch to DMT-236/dbt-athena-upgrade-main and update your venv : git switch main\\ngit fetch\\ngit pull\\ngit switch DMT-236/dbt-athena-upgrade-main At this point you can refresh the Git tab in the RStudio Environments panel (top right) to check you have switched to this branch. Next uninstall the old adapter: pip install --upgrade pip\\npip uninstall dbt-athena-adapter Here you will be asked to type “Y” to proceed with the uninstall; do so and continue to install requirements: pip install -r requirements.txt\\npip install -r requirements-lint.txt To check you have the correct set up list your local environment packages with pip list --local and check the list output for dbt-core 1.5.0 , dbt-athena-community 1.5.0 and not dbt-athena-adapter 1.0.1 (or any other version of it). If you still have the latter try to uninstall it again; if both old and new adapters are installed there will be conflicts. Full evironment set up guidance here. Test prod models To test your prod models you need to create your own branch off the DMT-236/dbt-athena-upgrade-main branch, deploy your models in dev , run your dbt tests and lint. You can also manually run equality tests in Athena to compare tables from prod created using the old dbt-athena adapter to the tables you have just created in dev using dbt-athena-community adapter. To explicitly create a new branch off DMT-236/dbt-athena-upgrade-main run the following: git checkout -b <new-branch-name> DMT-236/dbt-athena-upgrade-main All your prod models have the external_location parameter inserted into a config block at the top of each .sql file (see the Insert external_location section for more details). As a consequence all prod models are already deployed into dev by the deploy-dev workflow. However, for robustness, we would still like you to test your prod models by deploying them yourselves; cd into the mojap_derived_tables directory to run dbt commands as usual. If you have any issues due to redeploying please delete your dev models and try again, see Delete dev models instructions . Once you have deployed your models please run your tests and lint. ⚠️ See the section below on SQLFluff linting changes ⚠️ Please keep us up to date with your progress in the #ask-data-modelling channel. When all users are happy that prod models are deploying as expected using the upgrades we will merge the branch DMT-236/dbt-athena-upgrade-main into main . Test dev models Once you have completed testing of your prod models you may wish to continue testing with your dev models. To do this you will need to create another branch off DMT-236/dbt-athena-upgrade-main ( see instructions above ) and then merge into this from your feature branch. For example, I have some dev models on a branch called my-feature-branch so: git checkout -b new-test-branch DMT-236/dbt-athena-upgrade-main\\ngit fetch origin my-feature-branch\\ngit pull origin my-feature-branch\\ngit merge origin/my-feature-branch This creates a new branch new-test-branch off DMT-236/dbt-athena-upgrade-main branch and then collects the changes from my-feature-branch and merges these in to new-test-branch . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 505}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f969738045645acd7c69b1ef28d578a8'}>,\n",
       " <Document: {'content': \"After the first command check you are on the new-test-branch before proceeding. ⚠️ WARNING ⚠️ Your dev models will not have the external_location parameter set, which is required to store the output model in the correct location. See instructions in the Insert external_location section to insert the external_location and then cd into the mojap_derived_tables directory to run dbt commands as usual. Once you have deployed your models run tests and lint. See the section below on SQLFluff linting changes . Insert external_location The external_location parameter is set by the macro generate_s3_location which is invoked at run time. This combines information from the schema name with the names from the repo directory structure to create the desired S3 location in the form: s3://<bucket_name>/<env_name>/<table_type>/domain_name=<domain_name>/database_name=db_name/table_name=<tb_name> To make this as painless as possible we have prepared a script insert_external_location_config.py which you can run locally to automatically insert the required line or full config into your .sql files. The script must be run from the root directory and requires user input to determine the path to the files that you wish to run it on. In Terminal, in the root directory run python scripts/insert_external_location_config.py You will be prompted to enter the path to the directory containing the files you wish to apply the script to. Type your input starting with the domain directory name and continuing to whichever subdirectory you require (note there is no tab auto complete available) and hit Enter : Enter path to directory, starting with domain directory: <domain_name>/<database_name> If you are copy/pasting be careful not to introduce leading or trailing spaces. The next prompt shows the full file path you have selected and asks you to confirm by typing “Y” (any other input will exit the program): Path selected: 'mojap_derived_tables/models/<domain_name>/<database_name>' Continue with selected path? Enter Y/n: Y The final prompt shows the number of files selected and their paths and asks you to confirm by typing “Y”: Files selected:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Number of files selected:  3\\nContinue with selected files? Enter Y/n: Y Upon entering “Y” the script starts to scan the files and make the required changes. There are three possibilities: Scanning files... Config block exists but no external location set for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql\\nInserting external_location line into existing config block... External location not set and no config block for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql\\nInserting config block... External location set correctly in config block - nothing to do for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Once inserted the config block will look similar to this, but may include additional parameters that you have set previously: {{ config(\\nexternal_location=generate_s3_location()\\n) }} Note that if there was no existing config block it is inserted at the top of the file, displacing (but not overwriting) any existing comments or code. Files are automatically saved once the changes are applied. SQLFluff linting changes As you may be aware we have had issues with SQLFluff being unable to cope with complex Jinja templating mostly in macros. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 506}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c76495430a765e1b9f97637a87fbb983'}>,\n",
       " <Document: {'content': 'The new generate_s3_location.sql macro is no exception and is added to the sqlfluffignore file so that it is skipped during linting. However, since all models now reference this macro SQLfluff throws the Undefined jinja template variable error. We cannot add all models to sqlfluffignore , hence to circumvent the perceived error please use the --ignore=templating option when running SQLFluff lint or fix, thus: sqlfluff lint --ignore=templating path/to/files/to/lint sqlfluff fix --ignore=templating path/to/files/to/lint/and/fix Update your branch with the dbt-athena upgrade As mentioned above we have created a branch containing all the upgrades called DMT-236/dbt-athena-upgrade-main . While we are testing we may make changes to the DMT-236/dbt-athena-upgrade-main branch which you will then need to merge into your branches. Whilst on the branch you want to update with the latest from DMT-236/dbt-athena-upgrade-main run: git fetch origin DMT-236/dbt-athena-upgrade-main\\ngit pull origin DMT-236/dbt-athena-upgrade-main\\ngit merge origin/DMT-236/dbt-athena-upgrade-main At this point you may have merge conflicts that need to be resolved; please see GitHub resolve merge conflicts . If required, ask for help on the #ask-data-modelling slack channel. S3 location change for seeds Previously the seed S3 location was split between the dev and prod environments and followed the same Hive style path naming convention as for models. It was not straightforward to preserve this feature with the dbt-athena-community adapter and since seeds change little it was not a priority. The old directory structure for the mojap-derived-tables bucket is as below, with the seed directory appearing under both prod and dev directories. Note the database name is suffixed with _dev_dbt under the dev directory: ├── mojap_derived_tables\\n├── dev/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one_dev_dbt/\\n├── table_name=tb_one\\n...\\n...\\n...\\n├── prod/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one/\\n├── table_name=tb_one\\n...\\n...\\n... The new directory structure has a single seeds directory at the same level as the prod and dev directories. A seed created on a dev run will appear under its database name suffixed with _dev_dbt , as before, but the Hive path naming convention is not upheld. Instead we use the naming option schema_table provided by dbt-athena-community which is simply <database_name>/<table_name> ├── mojap_derived_tables\\n├── dev/\\n├── prod/\\n└── seeds/\\n├── database_one_dev_dbt/\\n├── table_one/\\n...\\n├── database_one/\\n├── table_one/\\n...\\n... The changes to the S3 location should not have any impact on users, unless they have specifically referenced a seed by its S3 location. References in create-a-derived-table using the ref function will be unaffected as this uses   Athena to reference the Glue Catalogue Registration, in the form <database_name.table_name> . The catalogue registrations will be updated with the new S3 locations automatically. License Unless stated otherwise, the codebase is released under the MIT License . This covers both the codebase and any sample code in the documentation. The documentation is © Crown copyright and available under the terms of the Open Government 3.0 licence. This page was last reviewed on 13 June 2023.\\n\\nIt needs to be reviewed again on 13 June 2024\\nby the page owner #ask-data-modelling . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 507}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5d79242522503657d55580d8ac80fb48'}>,\n",
       " <Document: {'content': 'This page was set to be reviewed before 13 June 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\ndbt-athena Upgrade Guidance - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 508}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eb014f360b6812b4d38a4192e392c3c0'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 509}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools dbt-athena Upgrade Guidance We are in the process of migrating from our in-house maintained fork of the dbt-athena adapter to the community maintained fork dbt-athena-community recommended by dbt . The guidance below details how you can test your models using the dbt-athena-community adapter. If you have any issues please get in touch via the #ask-data-modelling channel. Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Test set up We have created a branch called DMT-236/dbt-athena-upgrade-main which contains all the latest models, sources, seeds, macros from the main branch (that is everything that exists in prod ) and all the required upgrades. The main upgrades which you need to be aware of are: dbt-athena-community 1.5.0. dbt-core 1.5.0. macro generate_s3_location.sql to support our S3 file path Hive style naming convention. script scripts/insert_external_location_config.py to insert the required external_location configuration at the top of every model .sql file. running sqlfluff with the --ignore=templating option. seeds S3 location has changed (this does not effect any references to seeds ) To set up for testing you will need to checkout this branch, uninstall the old adapater and rerun the requirements files to update your local venv with the correct versions. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 510}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '597609ab553d2b8d6506564cb965ae34'}>,\n",
       " <Document: {'content': 'In Terminal (with your venv active) in the root directory run the following to pull the latest from main , switch to DMT-236/dbt-athena-upgrade-main and update your venv : git switch main\\ngit fetch\\ngit pull\\ngit switch DMT-236/dbt-athena-upgrade-main At this point you can refresh the Git tab in the RStudio Environments panel (top right) to check you have switched to this branch. Next uninstall the old adapter: pip install --upgrade pip\\npip uninstall dbt-athena-adapter Here you will be asked to type “Y” to proceed with the uninstall; do so and continue to install requirements: pip install -r requirements.txt\\npip install -r requirements-lint.txt To check you have the correct set up list your local environment packages with pip list --local and check the list output for dbt-core 1.5.0 , dbt-athena-community 1.5.0 and not dbt-athena-adapter 1.0.1 (or any other version of it). If you still have the latter try to uninstall it again; if both old and new adapters are installed there will be conflicts. Full evironment set up guidance here. Test prod models To test your prod models you need to create your own branch off the DMT-236/dbt-athena-upgrade-main branch, deploy your models in dev , run your dbt tests and lint. You can also manually run equality tests in Athena to compare tables from prod created using the old dbt-athena adapter to the tables you have just created in dev using dbt-athena-community adapter. To explicitly create a new branch off DMT-236/dbt-athena-upgrade-main run the following: git checkout -b <new-branch-name> DMT-236/dbt-athena-upgrade-main All your prod models have the external_location parameter inserted into a config block at the top of each .sql file (see the Insert external_location section for more details). As a consequence all prod models are already deployed into dev by the deploy-dev workflow. However, for robustness, we would still like you to test your prod models by deploying them yourselves; cd into the mojap_derived_tables directory to run dbt commands as usual. If you have any issues due to redeploying please delete your dev models and try again, see Delete dev models instructions . Once you have deployed your models please run your tests and lint. ⚠️ See the section below on SQLFluff linting changes ⚠️ Please keep us up to date with your progress in the #ask-data-modelling channel. When all users are happy that prod models are deploying as expected using the upgrades we will merge the branch DMT-236/dbt-athena-upgrade-main into main . Test dev models Once you have completed testing of your prod models you may wish to continue testing with your dev models. To do this you will need to create another branch off DMT-236/dbt-athena-upgrade-main ( see instructions above ) and then merge into this from your feature branch. For example, I have some dev models on a branch called my-feature-branch so: git checkout -b new-test-branch DMT-236/dbt-athena-upgrade-main\\ngit fetch origin my-feature-branch\\ngit pull origin my-feature-branch\\ngit merge origin/my-feature-branch This creates a new branch new-test-branch off DMT-236/dbt-athena-upgrade-main branch and then collects the changes from my-feature-branch and merges these in to new-test-branch . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 511}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f969738045645acd7c69b1ef28d578a8'}>,\n",
       " <Document: {'content': \"After the first command check you are on the new-test-branch before proceeding. ⚠️ WARNING ⚠️ Your dev models will not have the external_location parameter set, which is required to store the output model in the correct location. See instructions in the Insert external_location section to insert the external_location and then cd into the mojap_derived_tables directory to run dbt commands as usual. Once you have deployed your models run tests and lint. See the section below on SQLFluff linting changes . Insert external_location The external_location parameter is set by the macro generate_s3_location which is invoked at run time. This combines information from the schema name with the names from the repo directory structure to create the desired S3 location in the form: s3://<bucket_name>/<env_name>/<table_type>/domain_name=<domain_name>/database_name=db_name/table_name=<tb_name> To make this as painless as possible we have prepared a script insert_external_location_config.py which you can run locally to automatically insert the required line or full config into your .sql files. The script must be run from the root directory and requires user input to determine the path to the files that you wish to run it on. In Terminal, in the root directory run python scripts/insert_external_location_config.py You will be prompted to enter the path to the directory containing the files you wish to apply the script to. Type your input starting with the domain directory name and continuing to whichever subdirectory you require (note there is no tab auto complete available) and hit Enter : Enter path to directory, starting with domain directory: <domain_name>/<database_name> If you are copy/pasting be careful not to introduce leading or trailing spaces. The next prompt shows the full file path you have selected and asks you to confirm by typing “Y” (any other input will exit the program): Path selected: 'mojap_derived_tables/models/<domain_name>/<database_name>' Continue with selected path? Enter Y/n: Y The final prompt shows the number of files selected and their paths and asks you to confirm by typing “Y”: Files selected:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Number of files selected:  3\\nContinue with selected files? Enter Y/n: Y Upon entering “Y” the script starts to scan the files and make the required changes. There are three possibilities: Scanning files... Config block exists but no external location set for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql\\nInserting external_location line into existing config block... External location not set and no config block for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql\\nInserting config block... External location set correctly in config block - nothing to do for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Once inserted the config block will look similar to this, but may include additional parameters that you have set previously: {{ config(\\nexternal_location=generate_s3_location()\\n) }} Note that if there was no existing config block it is inserted at the top of the file, displacing (but not overwriting) any existing comments or code. Files are automatically saved once the changes are applied. SQLFluff linting changes As you may be aware we have had issues with SQLFluff being unable to cope with complex Jinja templating mostly in macros. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 512}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c76495430a765e1b9f97637a87fbb983'}>,\n",
       " <Document: {'content': 'The new generate_s3_location.sql macro is no exception and is added to the sqlfluffignore file so that it is skipped during linting. However, since all models now reference this macro SQLfluff throws the Undefined jinja template variable error. We cannot add all models to sqlfluffignore , hence to circumvent the perceived error please use the --ignore=templating option when running SQLFluff lint or fix, thus: sqlfluff lint --ignore=templating path/to/files/to/lint sqlfluff fix --ignore=templating path/to/files/to/lint/and/fix Update your branch with the dbt-athena upgrade As mentioned above we have created a branch containing all the upgrades called DMT-236/dbt-athena-upgrade-main . While we are testing we may make changes to the DMT-236/dbt-athena-upgrade-main branch which you will then need to merge into your branches. Whilst on the branch you want to update with the latest from DMT-236/dbt-athena-upgrade-main run: git fetch origin DMT-236/dbt-athena-upgrade-main\\ngit pull origin DMT-236/dbt-athena-upgrade-main\\ngit merge origin/DMT-236/dbt-athena-upgrade-main At this point you may have merge conflicts that need to be resolved; please see GitHub resolve merge conflicts . If required, ask for help on the #ask-data-modelling slack channel. S3 location change for seeds Previously the seed S3 location was split between the dev and prod environments and followed the same Hive style path naming convention as for models. It was not straightforward to preserve this feature with the dbt-athena-community adapter and since seeds change little it was not a priority. The old directory structure for the mojap-derived-tables bucket is as below, with the seed directory appearing under both prod and dev directories. Note the database name is suffixed with _dev_dbt under the dev directory: ├── mojap_derived_tables\\n├── dev/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one_dev_dbt/\\n├── table_name=tb_one\\n...\\n...\\n...\\n├── prod/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one/\\n├── table_name=tb_one\\n...\\n...\\n... The new directory structure has a single seeds directory at the same level as the prod and dev directories. A seed created on a dev run will appear under its database name suffixed with _dev_dbt , as before, but the Hive path naming convention is not upheld. Instead we use the naming option schema_table provided by dbt-athena-community which is simply <database_name>/<table_name> ├── mojap_derived_tables\\n├── dev/\\n├── prod/\\n└── seeds/\\n├── database_one_dev_dbt/\\n├── table_one/\\n...\\n├── database_one/\\n├── table_one/\\n...\\n... The changes to the S3 location should not have any impact on users, unless they have specifically referenced a seed by its S3 location. References in create-a-derived-table using the ref function will be unaffected as this uses   Athena to reference the Glue Catalogue Registration, in the form <database_name.table_name> . The catalogue registrations will be updated with the new S3 locations automatically. License Unless stated otherwise, the codebase is released under the MIT License . This covers both the codebase and any sample code in the documentation. The documentation is © Crown copyright and available under the terms of the Open Government 3.0 licence. This page was last reviewed on 13 June 2023.\\n\\nIt needs to be reviewed again on 13 June 2024\\nby the page owner #ask-data-modelling . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 513}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5d79242522503657d55580d8ac80fb48'}>,\n",
       " <Document: {'content': 'This page was set to be reviewed before 13 June 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\ndbt-athena Upgrade Guidance - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 514}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eb014f360b6812b4d38a4192e392c3c0'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 515}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools dbt-athena Upgrade Guidance We are in the process of migrating from our in-house maintained fork of the dbt-athena adapter to the community maintained fork dbt-athena-community recommended by dbt . The guidance below details how you can test your models using the dbt-athena-community adapter. If you have any issues please get in touch via the #ask-data-modelling channel. Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Test set up We have created a branch called DMT-236/dbt-athena-upgrade-main which contains all the latest models, sources, seeds, macros from the main branch (that is everything that exists in prod ) and all the required upgrades. The main upgrades which you need to be aware of are: dbt-athena-community 1.5.0. dbt-core 1.5.0. macro generate_s3_location.sql to support our S3 file path Hive style naming convention. script scripts/insert_external_location_config.py to insert the required external_location configuration at the top of every model .sql file. running sqlfluff with the --ignore=templating option. seeds S3 location has changed (this does not effect any references to seeds ) To set up for testing you will need to checkout this branch, uninstall the old adapater and rerun the requirements files to update your local venv with the correct versions. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 516}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '597609ab553d2b8d6506564cb965ae34'}>,\n",
       " <Document: {'content': 'In Terminal (with your venv active) in the root directory run the following to pull the latest from main , switch to DMT-236/dbt-athena-upgrade-main and update your venv : git switch main\\ngit fetch\\ngit pull\\ngit switch DMT-236/dbt-athena-upgrade-main At this point you can refresh the Git tab in the RStudio Environments panel (top right) to check you have switched to this branch. Next uninstall the old adapter: pip install --upgrade pip\\npip uninstall dbt-athena-adapter Here you will be asked to type “Y” to proceed with the uninstall; do so and continue to install requirements: pip install -r requirements.txt\\npip install -r requirements-lint.txt To check you have the correct set up list your local environment packages with pip list --local and check the list output for dbt-core 1.5.0 , dbt-athena-community 1.5.0 and not dbt-athena-adapter 1.0.1 (or any other version of it). If you still have the latter try to uninstall it again; if both old and new adapters are installed there will be conflicts. Full evironment set up guidance here. Test prod models To test your prod models you need to create your own branch off the DMT-236/dbt-athena-upgrade-main branch, deploy your models in dev , run your dbt tests and lint. You can also manually run equality tests in Athena to compare tables from prod created using the old dbt-athena adapter to the tables you have just created in dev using dbt-athena-community adapter. To explicitly create a new branch off DMT-236/dbt-athena-upgrade-main run the following: git checkout -b <new-branch-name> DMT-236/dbt-athena-upgrade-main All your prod models have the external_location parameter inserted into a config block at the top of each .sql file (see the Insert external_location section for more details). As a consequence all prod models are already deployed into dev by the deploy-dev workflow. However, for robustness, we would still like you to test your prod models by deploying them yourselves; cd into the mojap_derived_tables directory to run dbt commands as usual. If you have any issues due to redeploying please delete your dev models and try again, see Delete dev models instructions . Once you have deployed your models please run your tests and lint. ⚠️ See the section below on SQLFluff linting changes ⚠️ Please keep us up to date with your progress in the #ask-data-modelling channel. When all users are happy that prod models are deploying as expected using the upgrades we will merge the branch DMT-236/dbt-athena-upgrade-main into main . Test dev models Once you have completed testing of your prod models you may wish to continue testing with your dev models. To do this you will need to create another branch off DMT-236/dbt-athena-upgrade-main ( see instructions above ) and then merge into this from your feature branch. For example, I have some dev models on a branch called my-feature-branch so: git checkout -b new-test-branch DMT-236/dbt-athena-upgrade-main\\ngit fetch origin my-feature-branch\\ngit pull origin my-feature-branch\\ngit merge origin/my-feature-branch This creates a new branch new-test-branch off DMT-236/dbt-athena-upgrade-main branch and then collects the changes from my-feature-branch and merges these in to new-test-branch . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 517}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f969738045645acd7c69b1ef28d578a8'}>,\n",
       " <Document: {'content': \"After the first command check you are on the new-test-branch before proceeding. ⚠️ WARNING ⚠️ Your dev models will not have the external_location parameter set, which is required to store the output model in the correct location. See instructions in the Insert external_location section to insert the external_location and then cd into the mojap_derived_tables directory to run dbt commands as usual. Once you have deployed your models run tests and lint. See the section below on SQLFluff linting changes . Insert external_location The external_location parameter is set by the macro generate_s3_location which is invoked at run time. This combines information from the schema name with the names from the repo directory structure to create the desired S3 location in the form: s3://<bucket_name>/<env_name>/<table_type>/domain_name=<domain_name>/database_name=db_name/table_name=<tb_name> To make this as painless as possible we have prepared a script insert_external_location_config.py which you can run locally to automatically insert the required line or full config into your .sql files. The script must be run from the root directory and requires user input to determine the path to the files that you wish to run it on. In Terminal, in the root directory run python scripts/insert_external_location_config.py You will be prompted to enter the path to the directory containing the files you wish to apply the script to. Type your input starting with the domain directory name and continuing to whichever subdirectory you require (note there is no tab auto complete available) and hit Enter : Enter path to directory, starting with domain directory: <domain_name>/<database_name> If you are copy/pasting be careful not to introduce leading or trailing spaces. The next prompt shows the full file path you have selected and asks you to confirm by typing “Y” (any other input will exit the program): Path selected: 'mojap_derived_tables/models/<domain_name>/<database_name>' Continue with selected path? Enter Y/n: Y The final prompt shows the number of files selected and their paths and asks you to confirm by typing “Y”: Files selected:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Number of files selected:  3\\nContinue with selected files? Enter Y/n: Y Upon entering “Y” the script starts to scan the files and make the required changes. There are three possibilities: Scanning files... Config block exists but no external location set for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql\\nInserting external_location line into existing config block... External location not set and no config block for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql\\nInserting config block... External location set correctly in config block - nothing to do for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Once inserted the config block will look similar to this, but may include additional parameters that you have set previously: {{ config(\\nexternal_location=generate_s3_location()\\n) }} Note that if there was no existing config block it is inserted at the top of the file, displacing (but not overwriting) any existing comments or code. Files are automatically saved once the changes are applied. SQLFluff linting changes As you may be aware we have had issues with SQLFluff being unable to cope with complex Jinja templating mostly in macros. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 518}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c76495430a765e1b9f97637a87fbb983'}>,\n",
       " <Document: {'content': 'The new generate_s3_location.sql macro is no exception and is added to the sqlfluffignore file so that it is skipped during linting. However, since all models now reference this macro SQLfluff throws the Undefined jinja template variable error. We cannot add all models to sqlfluffignore , hence to circumvent the perceived error please use the --ignore=templating option when running SQLFluff lint or fix, thus: sqlfluff lint --ignore=templating path/to/files/to/lint sqlfluff fix --ignore=templating path/to/files/to/lint/and/fix Update your branch with the dbt-athena upgrade As mentioned above we have created a branch containing all the upgrades called DMT-236/dbt-athena-upgrade-main . While we are testing we may make changes to the DMT-236/dbt-athena-upgrade-main branch which you will then need to merge into your branches. Whilst on the branch you want to update with the latest from DMT-236/dbt-athena-upgrade-main run: git fetch origin DMT-236/dbt-athena-upgrade-main\\ngit pull origin DMT-236/dbt-athena-upgrade-main\\ngit merge origin/DMT-236/dbt-athena-upgrade-main At this point you may have merge conflicts that need to be resolved; please see GitHub resolve merge conflicts . If required, ask for help on the #ask-data-modelling slack channel. S3 location change for seeds Previously the seed S3 location was split between the dev and prod environments and followed the same Hive style path naming convention as for models. It was not straightforward to preserve this feature with the dbt-athena-community adapter and since seeds change little it was not a priority. The old directory structure for the mojap-derived-tables bucket is as below, with the seed directory appearing under both prod and dev directories. Note the database name is suffixed with _dev_dbt under the dev directory: ├── mojap_derived_tables\\n├── dev/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one_dev_dbt/\\n├── table_name=tb_one\\n...\\n...\\n...\\n├── prod/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one/\\n├── table_name=tb_one\\n...\\n...\\n... The new directory structure has a single seeds directory at the same level as the prod and dev directories. A seed created on a dev run will appear under its database name suffixed with _dev_dbt , as before, but the Hive path naming convention is not upheld. Instead we use the naming option schema_table provided by dbt-athena-community which is simply <database_name>/<table_name> ├── mojap_derived_tables\\n├── dev/\\n├── prod/\\n└── seeds/\\n├── database_one_dev_dbt/\\n├── table_one/\\n...\\n├── database_one/\\n├── table_one/\\n...\\n... The changes to the S3 location should not have any impact on users, unless they have specifically referenced a seed by its S3 location. References in create-a-derived-table using the ref function will be unaffected as this uses   Athena to reference the Glue Catalogue Registration, in the form <database_name.table_name> . The catalogue registrations will be updated with the new S3 locations automatically. License Unless stated otherwise, the codebase is released under the MIT License . This covers both the codebase and any sample code in the documentation. The documentation is © Crown copyright and available under the terms of the Open Government 3.0 licence. This page was last reviewed on 13 June 2023.\\n\\nIt needs to be reviewed again on 13 June 2024\\nby the page owner #ask-data-modelling . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 519}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5d79242522503657d55580d8ac80fb48'}>,\n",
       " <Document: {'content': 'This page was set to be reviewed before 13 June 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\ndbt-athena Upgrade Guidance - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 520}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eb014f360b6812b4d38a4192e392c3c0'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 521}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools dbt-athena Upgrade Guidance We are in the process of migrating from our in-house maintained fork of the dbt-athena adapter to the community maintained fork dbt-athena-community recommended by dbt . The guidance below details how you can test your models using the dbt-athena-community adapter. If you have any issues please get in touch via the #ask-data-modelling channel. Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Test set up We have created a branch called DMT-236/dbt-athena-upgrade-main which contains all the latest models, sources, seeds, macros from the main branch (that is everything that exists in prod ) and all the required upgrades. The main upgrades which you need to be aware of are: dbt-athena-community 1.5.0. dbt-core 1.5.0. macro generate_s3_location.sql to support our S3 file path Hive style naming convention. script scripts/insert_external_location_config.py to insert the required external_location configuration at the top of every model .sql file. running sqlfluff with the --ignore=templating option. seeds S3 location has changed (this does not effect any references to seeds ) To set up for testing you will need to checkout this branch, uninstall the old adapater and rerun the requirements files to update your local venv with the correct versions. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 522}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '597609ab553d2b8d6506564cb965ae34'}>,\n",
       " <Document: {'content': 'In Terminal (with your venv active) in the root directory run the following to pull the latest from main , switch to DMT-236/dbt-athena-upgrade-main and update your venv : git switch main\\ngit fetch\\ngit pull\\ngit switch DMT-236/dbt-athena-upgrade-main At this point you can refresh the Git tab in the RStudio Environments panel (top right) to check you have switched to this branch. Next uninstall the old adapter: pip install --upgrade pip\\npip uninstall dbt-athena-adapter Here you will be asked to type “Y” to proceed with the uninstall; do so and continue to install requirements: pip install -r requirements.txt\\npip install -r requirements-lint.txt To check you have the correct set up list your local environment packages with pip list --local and check the list output for dbt-core 1.5.0 , dbt-athena-community 1.5.0 and not dbt-athena-adapter 1.0.1 (or any other version of it). If you still have the latter try to uninstall it again; if both old and new adapters are installed there will be conflicts. Full evironment set up guidance here. Test prod models To test your prod models you need to create your own branch off the DMT-236/dbt-athena-upgrade-main branch, deploy your models in dev , run your dbt tests and lint. You can also manually run equality tests in Athena to compare tables from prod created using the old dbt-athena adapter to the tables you have just created in dev using dbt-athena-community adapter. To explicitly create a new branch off DMT-236/dbt-athena-upgrade-main run the following: git checkout -b <new-branch-name> DMT-236/dbt-athena-upgrade-main All your prod models have the external_location parameter inserted into a config block at the top of each .sql file (see the Insert external_location section for more details). As a consequence all prod models are already deployed into dev by the deploy-dev workflow. However, for robustness, we would still like you to test your prod models by deploying them yourselves; cd into the mojap_derived_tables directory to run dbt commands as usual. If you have any issues due to redeploying please delete your dev models and try again, see Delete dev models instructions . Once you have deployed your models please run your tests and lint. ⚠️ See the section below on SQLFluff linting changes ⚠️ Please keep us up to date with your progress in the #ask-data-modelling channel. When all users are happy that prod models are deploying as expected using the upgrades we will merge the branch DMT-236/dbt-athena-upgrade-main into main . Test dev models Once you have completed testing of your prod models you may wish to continue testing with your dev models. To do this you will need to create another branch off DMT-236/dbt-athena-upgrade-main ( see instructions above ) and then merge into this from your feature branch. For example, I have some dev models on a branch called my-feature-branch so: git checkout -b new-test-branch DMT-236/dbt-athena-upgrade-main\\ngit fetch origin my-feature-branch\\ngit pull origin my-feature-branch\\ngit merge origin/my-feature-branch This creates a new branch new-test-branch off DMT-236/dbt-athena-upgrade-main branch and then collects the changes from my-feature-branch and merges these in to new-test-branch . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 523}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f969738045645acd7c69b1ef28d578a8'}>,\n",
       " <Document: {'content': \"After the first command check you are on the new-test-branch before proceeding. ⚠️ WARNING ⚠️ Your dev models will not have the external_location parameter set, which is required to store the output model in the correct location. See instructions in the Insert external_location section to insert the external_location and then cd into the mojap_derived_tables directory to run dbt commands as usual. Once you have deployed your models run tests and lint. See the section below on SQLFluff linting changes . Insert external_location The external_location parameter is set by the macro generate_s3_location which is invoked at run time. This combines information from the schema name with the names from the repo directory structure to create the desired S3 location in the form: s3://<bucket_name>/<env_name>/<table_type>/domain_name=<domain_name>/database_name=db_name/table_name=<tb_name> To make this as painless as possible we have prepared a script insert_external_location_config.py which you can run locally to automatically insert the required line or full config into your .sql files. The script must be run from the root directory and requires user input to determine the path to the files that you wish to run it on. In Terminal, in the root directory run python scripts/insert_external_location_config.py You will be prompted to enter the path to the directory containing the files you wish to apply the script to. Type your input starting with the domain directory name and continuing to whichever subdirectory you require (note there is no tab auto complete available) and hit Enter : Enter path to directory, starting with domain directory: <domain_name>/<database_name> If you are copy/pasting be careful not to introduce leading or trailing spaces. The next prompt shows the full file path you have selected and asks you to confirm by typing “Y” (any other input will exit the program): Path selected: 'mojap_derived_tables/models/<domain_name>/<database_name>' Continue with selected path? Enter Y/n: Y The final prompt shows the number of files selected and their paths and asks you to confirm by typing “Y”: Files selected:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Number of files selected:  3\\nContinue with selected files? Enter Y/n: Y Upon entering “Y” the script starts to scan the files and make the required changes. There are three possibilities: Scanning files... Config block exists but no external location set for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql\\nInserting external_location line into existing config block... External location not set and no config block for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql\\nInserting config block... External location set correctly in config block - nothing to do for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Once inserted the config block will look similar to this, but may include additional parameters that you have set previously: {{ config(\\nexternal_location=generate_s3_location()\\n) }} Note that if there was no existing config block it is inserted at the top of the file, displacing (but not overwriting) any existing comments or code. Files are automatically saved once the changes are applied. SQLFluff linting changes As you may be aware we have had issues with SQLFluff being unable to cope with complex Jinja templating mostly in macros. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 524}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c76495430a765e1b9f97637a87fbb983'}>,\n",
       " <Document: {'content': 'The new generate_s3_location.sql macro is no exception and is added to the sqlfluffignore file so that it is skipped during linting. However, since all models now reference this macro SQLfluff throws the Undefined jinja template variable error. We cannot add all models to sqlfluffignore , hence to circumvent the perceived error please use the --ignore=templating option when running SQLFluff lint or fix, thus: sqlfluff lint --ignore=templating path/to/files/to/lint sqlfluff fix --ignore=templating path/to/files/to/lint/and/fix Update your branch with the dbt-athena upgrade As mentioned above we have created a branch containing all the upgrades called DMT-236/dbt-athena-upgrade-main . While we are testing we may make changes to the DMT-236/dbt-athena-upgrade-main branch which you will then need to merge into your branches. Whilst on the branch you want to update with the latest from DMT-236/dbt-athena-upgrade-main run: git fetch origin DMT-236/dbt-athena-upgrade-main\\ngit pull origin DMT-236/dbt-athena-upgrade-main\\ngit merge origin/DMT-236/dbt-athena-upgrade-main At this point you may have merge conflicts that need to be resolved; please see GitHub resolve merge conflicts . If required, ask for help on the #ask-data-modelling slack channel. S3 location change for seeds Previously the seed S3 location was split between the dev and prod environments and followed the same Hive style path naming convention as for models. It was not straightforward to preserve this feature with the dbt-athena-community adapter and since seeds change little it was not a priority. The old directory structure for the mojap-derived-tables bucket is as below, with the seed directory appearing under both prod and dev directories. Note the database name is suffixed with _dev_dbt under the dev directory: ├── mojap_derived_tables\\n├── dev/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one_dev_dbt/\\n├── table_name=tb_one\\n...\\n...\\n...\\n├── prod/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one/\\n├── table_name=tb_one\\n...\\n...\\n... The new directory structure has a single seeds directory at the same level as the prod and dev directories. A seed created on a dev run will appear under its database name suffixed with _dev_dbt , as before, but the Hive path naming convention is not upheld. Instead we use the naming option schema_table provided by dbt-athena-community which is simply <database_name>/<table_name> ├── mojap_derived_tables\\n├── dev/\\n├── prod/\\n└── seeds/\\n├── database_one_dev_dbt/\\n├── table_one/\\n...\\n├── database_one/\\n├── table_one/\\n...\\n... The changes to the S3 location should not have any impact on users, unless they have specifically referenced a seed by its S3 location. References in create-a-derived-table using the ref function will be unaffected as this uses   Athena to reference the Glue Catalogue Registration, in the form <database_name.table_name> . The catalogue registrations will be updated with the new S3 locations automatically. License Unless stated otherwise, the codebase is released under the MIT License . This covers both the codebase and any sample code in the documentation. The documentation is © Crown copyright and available under the terms of the Open Government 3.0 licence. This page was last reviewed on 13 June 2023.\\n\\nIt needs to be reviewed again on 13 June 2024\\nby the page owner #ask-data-modelling . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 525}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5d79242522503657d55580d8ac80fb48'}>,\n",
       " <Document: {'content': 'This page was set to be reviewed before 13 June 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\ndbt-athena Upgrade Guidance - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 526}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eb014f360b6812b4d38a4192e392c3c0'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 527}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools dbt-athena Upgrade Guidance We are in the process of migrating from our in-house maintained fork of the dbt-athena adapter to the community maintained fork dbt-athena-community recommended by dbt . The guidance below details how you can test your models using the dbt-athena-community adapter. If you have any issues please get in touch via the #ask-data-modelling channel. Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Test set up We have created a branch called DMT-236/dbt-athena-upgrade-main which contains all the latest models, sources, seeds, macros from the main branch (that is everything that exists in prod ) and all the required upgrades. The main upgrades which you need to be aware of are: dbt-athena-community 1.5.0. dbt-core 1.5.0. macro generate_s3_location.sql to support our S3 file path Hive style naming convention. script scripts/insert_external_location_config.py to insert the required external_location configuration at the top of every model .sql file. running sqlfluff with the --ignore=templating option. seeds S3 location has changed (this does not effect any references to seeds ) To set up for testing you will need to checkout this branch, uninstall the old adapater and rerun the requirements files to update your local venv with the correct versions. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 528}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '597609ab553d2b8d6506564cb965ae34'}>,\n",
       " <Document: {'content': 'In Terminal (with your venv active) in the root directory run the following to pull the latest from main , switch to DMT-236/dbt-athena-upgrade-main and update your venv : git switch main\\ngit fetch\\ngit pull\\ngit switch DMT-236/dbt-athena-upgrade-main At this point you can refresh the Git tab in the RStudio Environments panel (top right) to check you have switched to this branch. Next uninstall the old adapter: pip install --upgrade pip\\npip uninstall dbt-athena-adapter Here you will be asked to type “Y” to proceed with the uninstall; do so and continue to install requirements: pip install -r requirements.txt\\npip install -r requirements-lint.txt To check you have the correct set up list your local environment packages with pip list --local and check the list output for dbt-core 1.5.0 , dbt-athena-community 1.5.0 and not dbt-athena-adapter 1.0.1 (or any other version of it). If you still have the latter try to uninstall it again; if both old and new adapters are installed there will be conflicts. Full evironment set up guidance here. Test prod models To test your prod models you need to create your own branch off the DMT-236/dbt-athena-upgrade-main branch, deploy your models in dev , run your dbt tests and lint. You can also manually run equality tests in Athena to compare tables from prod created using the old dbt-athena adapter to the tables you have just created in dev using dbt-athena-community adapter. To explicitly create a new branch off DMT-236/dbt-athena-upgrade-main run the following: git checkout -b <new-branch-name> DMT-236/dbt-athena-upgrade-main All your prod models have the external_location parameter inserted into a config block at the top of each .sql file (see the Insert external_location section for more details). As a consequence all prod models are already deployed into dev by the deploy-dev workflow. However, for robustness, we would still like you to test your prod models by deploying them yourselves; cd into the mojap_derived_tables directory to run dbt commands as usual. If you have any issues due to redeploying please delete your dev models and try again, see Delete dev models instructions . Once you have deployed your models please run your tests and lint. ⚠️ See the section below on SQLFluff linting changes ⚠️ Please keep us up to date with your progress in the #ask-data-modelling channel. When all users are happy that prod models are deploying as expected using the upgrades we will merge the branch DMT-236/dbt-athena-upgrade-main into main . Test dev models Once you have completed testing of your prod models you may wish to continue testing with your dev models. To do this you will need to create another branch off DMT-236/dbt-athena-upgrade-main ( see instructions above ) and then merge into this from your feature branch. For example, I have some dev models on a branch called my-feature-branch so: git checkout -b new-test-branch DMT-236/dbt-athena-upgrade-main\\ngit fetch origin my-feature-branch\\ngit pull origin my-feature-branch\\ngit merge origin/my-feature-branch This creates a new branch new-test-branch off DMT-236/dbt-athena-upgrade-main branch and then collects the changes from my-feature-branch and merges these in to new-test-branch . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 529}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f969738045645acd7c69b1ef28d578a8'}>,\n",
       " <Document: {'content': \"After the first command check you are on the new-test-branch before proceeding. ⚠️ WARNING ⚠️ Your dev models will not have the external_location parameter set, which is required to store the output model in the correct location. See instructions in the Insert external_location section to insert the external_location and then cd into the mojap_derived_tables directory to run dbt commands as usual. Once you have deployed your models run tests and lint. See the section below on SQLFluff linting changes . Insert external_location The external_location parameter is set by the macro generate_s3_location which is invoked at run time. This combines information from the schema name with the names from the repo directory structure to create the desired S3 location in the form: s3://<bucket_name>/<env_name>/<table_type>/domain_name=<domain_name>/database_name=db_name/table_name=<tb_name> To make this as painless as possible we have prepared a script insert_external_location_config.py which you can run locally to automatically insert the required line or full config into your .sql files. The script must be run from the root directory and requires user input to determine the path to the files that you wish to run it on. In Terminal, in the root directory run python scripts/insert_external_location_config.py You will be prompted to enter the path to the directory containing the files you wish to apply the script to. Type your input starting with the domain directory name and continuing to whichever subdirectory you require (note there is no tab auto complete available) and hit Enter : Enter path to directory, starting with domain directory: <domain_name>/<database_name> If you are copy/pasting be careful not to introduce leading or trailing spaces. The next prompt shows the full file path you have selected and asks you to confirm by typing “Y” (any other input will exit the program): Path selected: 'mojap_derived_tables/models/<domain_name>/<database_name>' Continue with selected path? Enter Y/n: Y The final prompt shows the number of files selected and their paths and asks you to confirm by typing “Y”: Files selected:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Number of files selected:  3\\nContinue with selected files? Enter Y/n: Y Upon entering “Y” the script starts to scan the files and make the required changes. There are three possibilities: Scanning files... Config block exists but no external location set for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql\\nInserting external_location line into existing config block... External location not set and no config block for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql\\nInserting config block... External location set correctly in config block - nothing to do for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Once inserted the config block will look similar to this, but may include additional parameters that you have set previously: {{ config(\\nexternal_location=generate_s3_location()\\n) }} Note that if there was no existing config block it is inserted at the top of the file, displacing (but not overwriting) any existing comments or code. Files are automatically saved once the changes are applied. SQLFluff linting changes As you may be aware we have had issues with SQLFluff being unable to cope with complex Jinja templating mostly in macros. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 530}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c76495430a765e1b9f97637a87fbb983'}>,\n",
       " <Document: {'content': 'The new generate_s3_location.sql macro is no exception and is added to the sqlfluffignore file so that it is skipped during linting. However, since all models now reference this macro SQLfluff throws the Undefined jinja template variable error. We cannot add all models to sqlfluffignore , hence to circumvent the perceived error please use the --ignore=templating option when running SQLFluff lint or fix, thus: sqlfluff lint --ignore=templating path/to/files/to/lint sqlfluff fix --ignore=templating path/to/files/to/lint/and/fix Update your branch with the dbt-athena upgrade As mentioned above we have created a branch containing all the upgrades called DMT-236/dbt-athena-upgrade-main . While we are testing we may make changes to the DMT-236/dbt-athena-upgrade-main branch which you will then need to merge into your branches. Whilst on the branch you want to update with the latest from DMT-236/dbt-athena-upgrade-main run: git fetch origin DMT-236/dbt-athena-upgrade-main\\ngit pull origin DMT-236/dbt-athena-upgrade-main\\ngit merge origin/DMT-236/dbt-athena-upgrade-main At this point you may have merge conflicts that need to be resolved; please see GitHub resolve merge conflicts . If required, ask for help on the #ask-data-modelling slack channel. S3 location change for seeds Previously the seed S3 location was split between the dev and prod environments and followed the same Hive style path naming convention as for models. It was not straightforward to preserve this feature with the dbt-athena-community adapter and since seeds change little it was not a priority. The old directory structure for the mojap-derived-tables bucket is as below, with the seed directory appearing under both prod and dev directories. Note the database name is suffixed with _dev_dbt under the dev directory: ├── mojap_derived_tables\\n├── dev/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one_dev_dbt/\\n├── table_name=tb_one\\n...\\n...\\n...\\n├── prod/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one/\\n├── table_name=tb_one\\n...\\n...\\n... The new directory structure has a single seeds directory at the same level as the prod and dev directories. A seed created on a dev run will appear under its database name suffixed with _dev_dbt , as before, but the Hive path naming convention is not upheld. Instead we use the naming option schema_table provided by dbt-athena-community which is simply <database_name>/<table_name> ├── mojap_derived_tables\\n├── dev/\\n├── prod/\\n└── seeds/\\n├── database_one_dev_dbt/\\n├── table_one/\\n...\\n├── database_one/\\n├── table_one/\\n...\\n... The changes to the S3 location should not have any impact on users, unless they have specifically referenced a seed by its S3 location. References in create-a-derived-table using the ref function will be unaffected as this uses   Athena to reference the Glue Catalogue Registration, in the form <database_name.table_name> . The catalogue registrations will be updated with the new S3 locations automatically. License Unless stated otherwise, the codebase is released under the MIT License . This covers both the codebase and any sample code in the documentation. The documentation is © Crown copyright and available under the terms of the Open Government 3.0 licence. This page was last reviewed on 13 June 2023.\\n\\nIt needs to be reviewed again on 13 June 2024\\nby the page owner #ask-data-modelling . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 531}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5d79242522503657d55580d8ac80fb48'}>,\n",
       " <Document: {'content': 'This page was set to be reviewed before 13 June 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\ndbt-athena Upgrade Guidance - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 532}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eb014f360b6812b4d38a4192e392c3c0'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 533}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools dbt-athena Upgrade Guidance We are in the process of migrating from our in-house maintained fork of the dbt-athena adapter to the community maintained fork dbt-athena-community recommended by dbt . The guidance below details how you can test your models using the dbt-athena-community adapter. If you have any issues please get in touch via the #ask-data-modelling channel. Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Test set up We have created a branch called DMT-236/dbt-athena-upgrade-main which contains all the latest models, sources, seeds, macros from the main branch (that is everything that exists in prod ) and all the required upgrades. The main upgrades which you need to be aware of are: dbt-athena-community 1.5.0. dbt-core 1.5.0. macro generate_s3_location.sql to support our S3 file path Hive style naming convention. script scripts/insert_external_location_config.py to insert the required external_location configuration at the top of every model .sql file. running sqlfluff with the --ignore=templating option. seeds S3 location has changed (this does not effect any references to seeds ) To set up for testing you will need to checkout this branch, uninstall the old adapater and rerun the requirements files to update your local venv with the correct versions. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 534}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '597609ab553d2b8d6506564cb965ae34'}>,\n",
       " <Document: {'content': 'In Terminal (with your venv active) in the root directory run the following to pull the latest from main , switch to DMT-236/dbt-athena-upgrade-main and update your venv : git switch main\\ngit fetch\\ngit pull\\ngit switch DMT-236/dbt-athena-upgrade-main At this point you can refresh the Git tab in the RStudio Environments panel (top right) to check you have switched to this branch. Next uninstall the old adapter: pip install --upgrade pip\\npip uninstall dbt-athena-adapter Here you will be asked to type “Y” to proceed with the uninstall; do so and continue to install requirements: pip install -r requirements.txt\\npip install -r requirements-lint.txt To check you have the correct set up list your local environment packages with pip list --local and check the list output for dbt-core 1.5.0 , dbt-athena-community 1.5.0 and not dbt-athena-adapter 1.0.1 (or any other version of it). If you still have the latter try to uninstall it again; if both old and new adapters are installed there will be conflicts. Full evironment set up guidance here. Test prod models To test your prod models you need to create your own branch off the DMT-236/dbt-athena-upgrade-main branch, deploy your models in dev , run your dbt tests and lint. You can also manually run equality tests in Athena to compare tables from prod created using the old dbt-athena adapter to the tables you have just created in dev using dbt-athena-community adapter. To explicitly create a new branch off DMT-236/dbt-athena-upgrade-main run the following: git checkout -b <new-branch-name> DMT-236/dbt-athena-upgrade-main All your prod models have the external_location parameter inserted into a config block at the top of each .sql file (see the Insert external_location section for more details). As a consequence all prod models are already deployed into dev by the deploy-dev workflow. However, for robustness, we would still like you to test your prod models by deploying them yourselves; cd into the mojap_derived_tables directory to run dbt commands as usual. If you have any issues due to redeploying please delete your dev models and try again, see Delete dev models instructions . Once you have deployed your models please run your tests and lint. ⚠️ See the section below on SQLFluff linting changes ⚠️ Please keep us up to date with your progress in the #ask-data-modelling channel. When all users are happy that prod models are deploying as expected using the upgrades we will merge the branch DMT-236/dbt-athena-upgrade-main into main . Test dev models Once you have completed testing of your prod models you may wish to continue testing with your dev models. To do this you will need to create another branch off DMT-236/dbt-athena-upgrade-main ( see instructions above ) and then merge into this from your feature branch. For example, I have some dev models on a branch called my-feature-branch so: git checkout -b new-test-branch DMT-236/dbt-athena-upgrade-main\\ngit fetch origin my-feature-branch\\ngit pull origin my-feature-branch\\ngit merge origin/my-feature-branch This creates a new branch new-test-branch off DMT-236/dbt-athena-upgrade-main branch and then collects the changes from my-feature-branch and merges these in to new-test-branch . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 535}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f969738045645acd7c69b1ef28d578a8'}>,\n",
       " <Document: {'content': \"After the first command check you are on the new-test-branch before proceeding. ⚠️ WARNING ⚠️ Your dev models will not have the external_location parameter set, which is required to store the output model in the correct location. See instructions in the Insert external_location section to insert the external_location and then cd into the mojap_derived_tables directory to run dbt commands as usual. Once you have deployed your models run tests and lint. See the section below on SQLFluff linting changes . Insert external_location The external_location parameter is set by the macro generate_s3_location which is invoked at run time. This combines information from the schema name with the names from the repo directory structure to create the desired S3 location in the form: s3://<bucket_name>/<env_name>/<table_type>/domain_name=<domain_name>/database_name=db_name/table_name=<tb_name> To make this as painless as possible we have prepared a script insert_external_location_config.py which you can run locally to automatically insert the required line or full config into your .sql files. The script must be run from the root directory and requires user input to determine the path to the files that you wish to run it on. In Terminal, in the root directory run python scripts/insert_external_location_config.py You will be prompted to enter the path to the directory containing the files you wish to apply the script to. Type your input starting with the domain directory name and continuing to whichever subdirectory you require (note there is no tab auto complete available) and hit Enter : Enter path to directory, starting with domain directory: <domain_name>/<database_name> If you are copy/pasting be careful not to introduce leading or trailing spaces. The next prompt shows the full file path you have selected and asks you to confirm by typing “Y” (any other input will exit the program): Path selected: 'mojap_derived_tables/models/<domain_name>/<database_name>' Continue with selected path? Enter Y/n: Y The final prompt shows the number of files selected and their paths and asks you to confirm by typing “Y”: Files selected:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Number of files selected:  3\\nContinue with selected files? Enter Y/n: Y Upon entering “Y” the script starts to scan the files and make the required changes. There are three possibilities: Scanning files... Config block exists but no external location set for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql\\nInserting external_location line into existing config block... External location not set and no config block for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql\\nInserting config block... External location set correctly in config block - nothing to do for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Once inserted the config block will look similar to this, but may include additional parameters that you have set previously: {{ config(\\nexternal_location=generate_s3_location()\\n) }} Note that if there was no existing config block it is inserted at the top of the file, displacing (but not overwriting) any existing comments or code. Files are automatically saved once the changes are applied. SQLFluff linting changes As you may be aware we have had issues with SQLFluff being unable to cope with complex Jinja templating mostly in macros. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 536}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c76495430a765e1b9f97637a87fbb983'}>,\n",
       " <Document: {'content': 'The new generate_s3_location.sql macro is no exception and is added to the sqlfluffignore file so that it is skipped during linting. However, since all models now reference this macro SQLfluff throws the Undefined jinja template variable error. We cannot add all models to sqlfluffignore , hence to circumvent the perceived error please use the --ignore=templating option when running SQLFluff lint or fix, thus: sqlfluff lint --ignore=templating path/to/files/to/lint sqlfluff fix --ignore=templating path/to/files/to/lint/and/fix Update your branch with the dbt-athena upgrade As mentioned above we have created a branch containing all the upgrades called DMT-236/dbt-athena-upgrade-main . While we are testing we may make changes to the DMT-236/dbt-athena-upgrade-main branch which you will then need to merge into your branches. Whilst on the branch you want to update with the latest from DMT-236/dbt-athena-upgrade-main run: git fetch origin DMT-236/dbt-athena-upgrade-main\\ngit pull origin DMT-236/dbt-athena-upgrade-main\\ngit merge origin/DMT-236/dbt-athena-upgrade-main At this point you may have merge conflicts that need to be resolved; please see GitHub resolve merge conflicts . If required, ask for help on the #ask-data-modelling slack channel. S3 location change for seeds Previously the seed S3 location was split between the dev and prod environments and followed the same Hive style path naming convention as for models. It was not straightforward to preserve this feature with the dbt-athena-community adapter and since seeds change little it was not a priority. The old directory structure for the mojap-derived-tables bucket is as below, with the seed directory appearing under both prod and dev directories. Note the database name is suffixed with _dev_dbt under the dev directory: ├── mojap_derived_tables\\n├── dev/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one_dev_dbt/\\n├── table_name=tb_one\\n...\\n...\\n...\\n├── prod/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one/\\n├── table_name=tb_one\\n...\\n...\\n... The new directory structure has a single seeds directory at the same level as the prod and dev directories. A seed created on a dev run will appear under its database name suffixed with _dev_dbt , as before, but the Hive path naming convention is not upheld. Instead we use the naming option schema_table provided by dbt-athena-community which is simply <database_name>/<table_name> ├── mojap_derived_tables\\n├── dev/\\n├── prod/\\n└── seeds/\\n├── database_one_dev_dbt/\\n├── table_one/\\n...\\n├── database_one/\\n├── table_one/\\n...\\n... The changes to the S3 location should not have any impact on users, unless they have specifically referenced a seed by its S3 location. References in create-a-derived-table using the ref function will be unaffected as this uses   Athena to reference the Glue Catalogue Registration, in the form <database_name.table_name> . The catalogue registrations will be updated with the new S3 locations automatically. License Unless stated otherwise, the codebase is released under the MIT License . This covers both the codebase and any sample code in the documentation. The documentation is © Crown copyright and available under the terms of the Open Government 3.0 licence. This page was last reviewed on 13 June 2023.\\n\\nIt needs to be reviewed again on 13 June 2024\\nby the page owner #ask-data-modelling . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 537}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5d79242522503657d55580d8ac80fb48'}>,\n",
       " <Document: {'content': 'This page was set to be reviewed before 13 June 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\ndbt-athena Upgrade Guidance - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 538}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eb014f360b6812b4d38a4192e392c3c0'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 539}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools dbt-athena Upgrade Guidance We are in the process of migrating from our in-house maintained fork of the dbt-athena adapter to the community maintained fork dbt-athena-community recommended by dbt . The guidance below details how you can test your models using the dbt-athena-community adapter. If you have any issues please get in touch via the #ask-data-modelling channel. Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Test set up We have created a branch called DMT-236/dbt-athena-upgrade-main which contains all the latest models, sources, seeds, macros from the main branch (that is everything that exists in prod ) and all the required upgrades. The main upgrades which you need to be aware of are: dbt-athena-community 1.5.0. dbt-core 1.5.0. macro generate_s3_location.sql to support our S3 file path Hive style naming convention. script scripts/insert_external_location_config.py to insert the required external_location configuration at the top of every model .sql file. running sqlfluff with the --ignore=templating option. seeds S3 location has changed (this does not effect any references to seeds ) To set up for testing you will need to checkout this branch, uninstall the old adapater and rerun the requirements files to update your local venv with the correct versions. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 540}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '597609ab553d2b8d6506564cb965ae34'}>,\n",
       " <Document: {'content': 'In Terminal (with your venv active) in the root directory run the following to pull the latest from main , switch to DMT-236/dbt-athena-upgrade-main and update your venv : git switch main\\ngit fetch\\ngit pull\\ngit switch DMT-236/dbt-athena-upgrade-main At this point you can refresh the Git tab in the RStudio Environments panel (top right) to check you have switched to this branch. Next uninstall the old adapter: pip install --upgrade pip\\npip uninstall dbt-athena-adapter Here you will be asked to type “Y” to proceed with the uninstall; do so and continue to install requirements: pip install -r requirements.txt\\npip install -r requirements-lint.txt To check you have the correct set up list your local environment packages with pip list --local and check the list output for dbt-core 1.5.0 , dbt-athena-community 1.5.0 and not dbt-athena-adapter 1.0.1 (or any other version of it). If you still have the latter try to uninstall it again; if both old and new adapters are installed there will be conflicts. Full evironment set up guidance here. Test prod models To test your prod models you need to create your own branch off the DMT-236/dbt-athena-upgrade-main branch, deploy your models in dev , run your dbt tests and lint. You can also manually run equality tests in Athena to compare tables from prod created using the old dbt-athena adapter to the tables you have just created in dev using dbt-athena-community adapter. To explicitly create a new branch off DMT-236/dbt-athena-upgrade-main run the following: git checkout -b <new-branch-name> DMT-236/dbt-athena-upgrade-main All your prod models have the external_location parameter inserted into a config block at the top of each .sql file (see the Insert external_location section for more details). As a consequence all prod models are already deployed into dev by the deploy-dev workflow. However, for robustness, we would still like you to test your prod models by deploying them yourselves; cd into the mojap_derived_tables directory to run dbt commands as usual. If you have any issues due to redeploying please delete your dev models and try again, see Delete dev models instructions . Once you have deployed your models please run your tests and lint. ⚠️ See the section below on SQLFluff linting changes ⚠️ Please keep us up to date with your progress in the #ask-data-modelling channel. When all users are happy that prod models are deploying as expected using the upgrades we will merge the branch DMT-236/dbt-athena-upgrade-main into main . Test dev models Once you have completed testing of your prod models you may wish to continue testing with your dev models. To do this you will need to create another branch off DMT-236/dbt-athena-upgrade-main ( see instructions above ) and then merge into this from your feature branch. For example, I have some dev models on a branch called my-feature-branch so: git checkout -b new-test-branch DMT-236/dbt-athena-upgrade-main\\ngit fetch origin my-feature-branch\\ngit pull origin my-feature-branch\\ngit merge origin/my-feature-branch This creates a new branch new-test-branch off DMT-236/dbt-athena-upgrade-main branch and then collects the changes from my-feature-branch and merges these in to new-test-branch . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 541}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f969738045645acd7c69b1ef28d578a8'}>,\n",
       " <Document: {'content': \"After the first command check you are on the new-test-branch before proceeding. ⚠️ WARNING ⚠️ Your dev models will not have the external_location parameter set, which is required to store the output model in the correct location. See instructions in the Insert external_location section to insert the external_location and then cd into the mojap_derived_tables directory to run dbt commands as usual. Once you have deployed your models run tests and lint. See the section below on SQLFluff linting changes . Insert external_location The external_location parameter is set by the macro generate_s3_location which is invoked at run time. This combines information from the schema name with the names from the repo directory structure to create the desired S3 location in the form: s3://<bucket_name>/<env_name>/<table_type>/domain_name=<domain_name>/database_name=db_name/table_name=<tb_name> To make this as painless as possible we have prepared a script insert_external_location_config.py which you can run locally to automatically insert the required line or full config into your .sql files. The script must be run from the root directory and requires user input to determine the path to the files that you wish to run it on. In Terminal, in the root directory run python scripts/insert_external_location_config.py You will be prompted to enter the path to the directory containing the files you wish to apply the script to. Type your input starting with the domain directory name and continuing to whichever subdirectory you require (note there is no tab auto complete available) and hit Enter : Enter path to directory, starting with domain directory: <domain_name>/<database_name> If you are copy/pasting be careful not to introduce leading or trailing spaces. The next prompt shows the full file path you have selected and asks you to confirm by typing “Y” (any other input will exit the program): Path selected: 'mojap_derived_tables/models/<domain_name>/<database_name>' Continue with selected path? Enter Y/n: Y The final prompt shows the number of files selected and their paths and asks you to confirm by typing “Y”: Files selected:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Number of files selected:  3\\nContinue with selected files? Enter Y/n: Y Upon entering “Y” the script starts to scan the files and make the required changes. There are three possibilities: Scanning files... Config block exists but no external location set for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql\\nInserting external_location line into existing config block... External location not set and no config block for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql\\nInserting config block... External location set correctly in config block - nothing to do for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Once inserted the config block will look similar to this, but may include additional parameters that you have set previously: {{ config(\\nexternal_location=generate_s3_location()\\n) }} Note that if there was no existing config block it is inserted at the top of the file, displacing (but not overwriting) any existing comments or code. Files are automatically saved once the changes are applied. SQLFluff linting changes As you may be aware we have had issues with SQLFluff being unable to cope with complex Jinja templating mostly in macros. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 542}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c76495430a765e1b9f97637a87fbb983'}>,\n",
       " <Document: {'content': 'The new generate_s3_location.sql macro is no exception and is added to the sqlfluffignore file so that it is skipped during linting. However, since all models now reference this macro SQLfluff throws the Undefined jinja template variable error. We cannot add all models to sqlfluffignore , hence to circumvent the perceived error please use the --ignore=templating option when running SQLFluff lint or fix, thus: sqlfluff lint --ignore=templating path/to/files/to/lint sqlfluff fix --ignore=templating path/to/files/to/lint/and/fix Update your branch with the dbt-athena upgrade As mentioned above we have created a branch containing all the upgrades called DMT-236/dbt-athena-upgrade-main . While we are testing we may make changes to the DMT-236/dbt-athena-upgrade-main branch which you will then need to merge into your branches. Whilst on the branch you want to update with the latest from DMT-236/dbt-athena-upgrade-main run: git fetch origin DMT-236/dbt-athena-upgrade-main\\ngit pull origin DMT-236/dbt-athena-upgrade-main\\ngit merge origin/DMT-236/dbt-athena-upgrade-main At this point you may have merge conflicts that need to be resolved; please see GitHub resolve merge conflicts . If required, ask for help on the #ask-data-modelling slack channel. S3 location change for seeds Previously the seed S3 location was split between the dev and prod environments and followed the same Hive style path naming convention as for models. It was not straightforward to preserve this feature with the dbt-athena-community adapter and since seeds change little it was not a priority. The old directory structure for the mojap-derived-tables bucket is as below, with the seed directory appearing under both prod and dev directories. Note the database name is suffixed with _dev_dbt under the dev directory: ├── mojap_derived_tables\\n├── dev/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one_dev_dbt/\\n├── table_name=tb_one\\n...\\n...\\n...\\n├── prod/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one/\\n├── table_name=tb_one\\n...\\n...\\n... The new directory structure has a single seeds directory at the same level as the prod and dev directories. A seed created on a dev run will appear under its database name suffixed with _dev_dbt , as before, but the Hive path naming convention is not upheld. Instead we use the naming option schema_table provided by dbt-athena-community which is simply <database_name>/<table_name> ├── mojap_derived_tables\\n├── dev/\\n├── prod/\\n└── seeds/\\n├── database_one_dev_dbt/\\n├── table_one/\\n...\\n├── database_one/\\n├── table_one/\\n...\\n... The changes to the S3 location should not have any impact on users, unless they have specifically referenced a seed by its S3 location. References in create-a-derived-table using the ref function will be unaffected as this uses   Athena to reference the Glue Catalogue Registration, in the form <database_name.table_name> . The catalogue registrations will be updated with the new S3 locations automatically. License Unless stated otherwise, the codebase is released under the MIT License . This covers both the codebase and any sample code in the documentation. The documentation is © Crown copyright and available under the terms of the Open Government 3.0 licence. This page was last reviewed on 13 June 2023.\\n\\nIt needs to be reviewed again on 13 June 2024\\nby the page owner #ask-data-modelling . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 543}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5d79242522503657d55580d8ac80fb48'}>,\n",
       " <Document: {'content': 'This page was set to be reviewed before 13 June 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\ndbt-athena Upgrade Guidance - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 544}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eb014f360b6812b4d38a4192e392c3c0'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 545}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools dbt-athena Upgrade Guidance We are in the process of migrating from our in-house maintained fork of the dbt-athena adapter to the community maintained fork dbt-athena-community recommended by dbt . The guidance below details how you can test your models using the dbt-athena-community adapter. If you have any issues please get in touch via the #ask-data-modelling channel. Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Test set up We have created a branch called DMT-236/dbt-athena-upgrade-main which contains all the latest models, sources, seeds, macros from the main branch (that is everything that exists in prod ) and all the required upgrades. The main upgrades which you need to be aware of are: dbt-athena-community 1.5.0. dbt-core 1.5.0. macro generate_s3_location.sql to support our S3 file path Hive style naming convention. script scripts/insert_external_location_config.py to insert the required external_location configuration at the top of every model .sql file. running sqlfluff with the --ignore=templating option. seeds S3 location has changed (this does not effect any references to seeds ) To set up for testing you will need to checkout this branch, uninstall the old adapater and rerun the requirements files to update your local venv with the correct versions. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 546}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '597609ab553d2b8d6506564cb965ae34'}>,\n",
       " <Document: {'content': 'In Terminal (with your venv active) in the root directory run the following to pull the latest from main , switch to DMT-236/dbt-athena-upgrade-main and update your venv : git switch main\\ngit fetch\\ngit pull\\ngit switch DMT-236/dbt-athena-upgrade-main At this point you can refresh the Git tab in the RStudio Environments panel (top right) to check you have switched to this branch. Next uninstall the old adapter: pip install --upgrade pip\\npip uninstall dbt-athena-adapter Here you will be asked to type “Y” to proceed with the uninstall; do so and continue to install requirements: pip install -r requirements.txt\\npip install -r requirements-lint.txt To check you have the correct set up list your local environment packages with pip list --local and check the list output for dbt-core 1.5.0 , dbt-athena-community 1.5.0 and not dbt-athena-adapter 1.0.1 (or any other version of it). If you still have the latter try to uninstall it again; if both old and new adapters are installed there will be conflicts. Full evironment set up guidance here. Test prod models To test your prod models you need to create your own branch off the DMT-236/dbt-athena-upgrade-main branch, deploy your models in dev , run your dbt tests and lint. You can also manually run equality tests in Athena to compare tables from prod created using the old dbt-athena adapter to the tables you have just created in dev using dbt-athena-community adapter. To explicitly create a new branch off DMT-236/dbt-athena-upgrade-main run the following: git checkout -b <new-branch-name> DMT-236/dbt-athena-upgrade-main All your prod models have the external_location parameter inserted into a config block at the top of each .sql file (see the Insert external_location section for more details). As a consequence all prod models are already deployed into dev by the deploy-dev workflow. However, for robustness, we would still like you to test your prod models by deploying them yourselves; cd into the mojap_derived_tables directory to run dbt commands as usual. If you have any issues due to redeploying please delete your dev models and try again, see Delete dev models instructions . Once you have deployed your models please run your tests and lint. ⚠️ See the section below on SQLFluff linting changes ⚠️ Please keep us up to date with your progress in the #ask-data-modelling channel. When all users are happy that prod models are deploying as expected using the upgrades we will merge the branch DMT-236/dbt-athena-upgrade-main into main . Test dev models Once you have completed testing of your prod models you may wish to continue testing with your dev models. To do this you will need to create another branch off DMT-236/dbt-athena-upgrade-main ( see instructions above ) and then merge into this from your feature branch. For example, I have some dev models on a branch called my-feature-branch so: git checkout -b new-test-branch DMT-236/dbt-athena-upgrade-main\\ngit fetch origin my-feature-branch\\ngit pull origin my-feature-branch\\ngit merge origin/my-feature-branch This creates a new branch new-test-branch off DMT-236/dbt-athena-upgrade-main branch and then collects the changes from my-feature-branch and merges these in to new-test-branch . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 547}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f969738045645acd7c69b1ef28d578a8'}>,\n",
       " <Document: {'content': \"After the first command check you are on the new-test-branch before proceeding. ⚠️ WARNING ⚠️ Your dev models will not have the external_location parameter set, which is required to store the output model in the correct location. See instructions in the Insert external_location section to insert the external_location and then cd into the mojap_derived_tables directory to run dbt commands as usual. Once you have deployed your models run tests and lint. See the section below on SQLFluff linting changes . Insert external_location The external_location parameter is set by the macro generate_s3_location which is invoked at run time. This combines information from the schema name with the names from the repo directory structure to create the desired S3 location in the form: s3://<bucket_name>/<env_name>/<table_type>/domain_name=<domain_name>/database_name=db_name/table_name=<tb_name> To make this as painless as possible we have prepared a script insert_external_location_config.py which you can run locally to automatically insert the required line or full config into your .sql files. The script must be run from the root directory and requires user input to determine the path to the files that you wish to run it on. In Terminal, in the root directory run python scripts/insert_external_location_config.py You will be prompted to enter the path to the directory containing the files you wish to apply the script to. Type your input starting with the domain directory name and continuing to whichever subdirectory you require (note there is no tab auto complete available) and hit Enter : Enter path to directory, starting with domain directory: <domain_name>/<database_name> If you are copy/pasting be careful not to introduce leading or trailing spaces. The next prompt shows the full file path you have selected and asks you to confirm by typing “Y” (any other input will exit the program): Path selected: 'mojap_derived_tables/models/<domain_name>/<database_name>' Continue with selected path? Enter Y/n: Y The final prompt shows the number of files selected and their paths and asks you to confirm by typing “Y”: Files selected:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Number of files selected:  3\\nContinue with selected files? Enter Y/n: Y Upon entering “Y” the script starts to scan the files and make the required changes. There are three possibilities: Scanning files... Config block exists but no external location set for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql\\nInserting external_location line into existing config block... External location not set and no config block for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql\\nInserting config block... External location set correctly in config block - nothing to do for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Once inserted the config block will look similar to this, but may include additional parameters that you have set previously: {{ config(\\nexternal_location=generate_s3_location()\\n) }} Note that if there was no existing config block it is inserted at the top of the file, displacing (but not overwriting) any existing comments or code. Files are automatically saved once the changes are applied. SQLFluff linting changes As you may be aware we have had issues with SQLFluff being unable to cope with complex Jinja templating mostly in macros. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 548}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c76495430a765e1b9f97637a87fbb983'}>,\n",
       " <Document: {'content': 'The new generate_s3_location.sql macro is no exception and is added to the sqlfluffignore file so that it is skipped during linting. However, since all models now reference this macro SQLfluff throws the Undefined jinja template variable error. We cannot add all models to sqlfluffignore , hence to circumvent the perceived error please use the --ignore=templating option when running SQLFluff lint or fix, thus: sqlfluff lint --ignore=templating path/to/files/to/lint sqlfluff fix --ignore=templating path/to/files/to/lint/and/fix Update your branch with the dbt-athena upgrade As mentioned above we have created a branch containing all the upgrades called DMT-236/dbt-athena-upgrade-main . While we are testing we may make changes to the DMT-236/dbt-athena-upgrade-main branch which you will then need to merge into your branches. Whilst on the branch you want to update with the latest from DMT-236/dbt-athena-upgrade-main run: git fetch origin DMT-236/dbt-athena-upgrade-main\\ngit pull origin DMT-236/dbt-athena-upgrade-main\\ngit merge origin/DMT-236/dbt-athena-upgrade-main At this point you may have merge conflicts that need to be resolved; please see GitHub resolve merge conflicts . If required, ask for help on the #ask-data-modelling slack channel. S3 location change for seeds Previously the seed S3 location was split between the dev and prod environments and followed the same Hive style path naming convention as for models. It was not straightforward to preserve this feature with the dbt-athena-community adapter and since seeds change little it was not a priority. The old directory structure for the mojap-derived-tables bucket is as below, with the seed directory appearing under both prod and dev directories. Note the database name is suffixed with _dev_dbt under the dev directory: ├── mojap_derived_tables\\n├── dev/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one_dev_dbt/\\n├── table_name=tb_one\\n...\\n...\\n...\\n├── prod/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one/\\n├── table_name=tb_one\\n...\\n...\\n... The new directory structure has a single seeds directory at the same level as the prod and dev directories. A seed created on a dev run will appear under its database name suffixed with _dev_dbt , as before, but the Hive path naming convention is not upheld. Instead we use the naming option schema_table provided by dbt-athena-community which is simply <database_name>/<table_name> ├── mojap_derived_tables\\n├── dev/\\n├── prod/\\n└── seeds/\\n├── database_one_dev_dbt/\\n├── table_one/\\n...\\n├── database_one/\\n├── table_one/\\n...\\n... The changes to the S3 location should not have any impact on users, unless they have specifically referenced a seed by its S3 location. References in create-a-derived-table using the ref function will be unaffected as this uses   Athena to reference the Glue Catalogue Registration, in the form <database_name.table_name> . The catalogue registrations will be updated with the new S3 locations automatically. License Unless stated otherwise, the codebase is released under the MIT License . This covers both the codebase and any sample code in the documentation. The documentation is © Crown copyright and available under the terms of the Open Government 3.0 licence. This page was last reviewed on 13 June 2023.\\n\\nIt needs to be reviewed again on 13 June 2024\\nby the page owner #ask-data-modelling . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 549}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5d79242522503657d55580d8ac80fb48'}>,\n",
       " <Document: {'content': 'This page was set to be reviewed before 13 June 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\ndbt-athena Upgrade Guidance - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 550}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eb014f360b6812b4d38a4192e392c3c0'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 551}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools dbt-athena Upgrade Guidance We are in the process of migrating from our in-house maintained fork of the dbt-athena adapter to the community maintained fork dbt-athena-community recommended by dbt . The guidance below details how you can test your models using the dbt-athena-community adapter. If you have any issues please get in touch via the #ask-data-modelling channel. Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Test set up We have created a branch called DMT-236/dbt-athena-upgrade-main which contains all the latest models, sources, seeds, macros from the main branch (that is everything that exists in prod ) and all the required upgrades. The main upgrades which you need to be aware of are: dbt-athena-community 1.5.0. dbt-core 1.5.0. macro generate_s3_location.sql to support our S3 file path Hive style naming convention. script scripts/insert_external_location_config.py to insert the required external_location configuration at the top of every model .sql file. running sqlfluff with the --ignore=templating option. seeds S3 location has changed (this does not effect any references to seeds ) To set up for testing you will need to checkout this branch, uninstall the old adapater and rerun the requirements files to update your local venv with the correct versions. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 552}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '597609ab553d2b8d6506564cb965ae34'}>,\n",
       " <Document: {'content': 'In Terminal (with your venv active) in the root directory run the following to pull the latest from main , switch to DMT-236/dbt-athena-upgrade-main and update your venv : git switch main\\ngit fetch\\ngit pull\\ngit switch DMT-236/dbt-athena-upgrade-main At this point you can refresh the Git tab in the RStudio Environments panel (top right) to check you have switched to this branch. Next uninstall the old adapter: pip install --upgrade pip\\npip uninstall dbt-athena-adapter Here you will be asked to type “Y” to proceed with the uninstall; do so and continue to install requirements: pip install -r requirements.txt\\npip install -r requirements-lint.txt To check you have the correct set up list your local environment packages with pip list --local and check the list output for dbt-core 1.5.0 , dbt-athena-community 1.5.0 and not dbt-athena-adapter 1.0.1 (or any other version of it). If you still have the latter try to uninstall it again; if both old and new adapters are installed there will be conflicts. Full evironment set up guidance here. Test prod models To test your prod models you need to create your own branch off the DMT-236/dbt-athena-upgrade-main branch, deploy your models in dev , run your dbt tests and lint. You can also manually run equality tests in Athena to compare tables from prod created using the old dbt-athena adapter to the tables you have just created in dev using dbt-athena-community adapter. To explicitly create a new branch off DMT-236/dbt-athena-upgrade-main run the following: git checkout -b <new-branch-name> DMT-236/dbt-athena-upgrade-main All your prod models have the external_location parameter inserted into a config block at the top of each .sql file (see the Insert external_location section for more details). As a consequence all prod models are already deployed into dev by the deploy-dev workflow. However, for robustness, we would still like you to test your prod models by deploying them yourselves; cd into the mojap_derived_tables directory to run dbt commands as usual. If you have any issues due to redeploying please delete your dev models and try again, see Delete dev models instructions . Once you have deployed your models please run your tests and lint. ⚠️ See the section below on SQLFluff linting changes ⚠️ Please keep us up to date with your progress in the #ask-data-modelling channel. When all users are happy that prod models are deploying as expected using the upgrades we will merge the branch DMT-236/dbt-athena-upgrade-main into main . Test dev models Once you have completed testing of your prod models you may wish to continue testing with your dev models. To do this you will need to create another branch off DMT-236/dbt-athena-upgrade-main ( see instructions above ) and then merge into this from your feature branch. For example, I have some dev models on a branch called my-feature-branch so: git checkout -b new-test-branch DMT-236/dbt-athena-upgrade-main\\ngit fetch origin my-feature-branch\\ngit pull origin my-feature-branch\\ngit merge origin/my-feature-branch This creates a new branch new-test-branch off DMT-236/dbt-athena-upgrade-main branch and then collects the changes from my-feature-branch and merges these in to new-test-branch . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 553}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f969738045645acd7c69b1ef28d578a8'}>,\n",
       " <Document: {'content': \"After the first command check you are on the new-test-branch before proceeding. ⚠️ WARNING ⚠️ Your dev models will not have the external_location parameter set, which is required to store the output model in the correct location. See instructions in the Insert external_location section to insert the external_location and then cd into the mojap_derived_tables directory to run dbt commands as usual. Once you have deployed your models run tests and lint. See the section below on SQLFluff linting changes . Insert external_location The external_location parameter is set by the macro generate_s3_location which is invoked at run time. This combines information from the schema name with the names from the repo directory structure to create the desired S3 location in the form: s3://<bucket_name>/<env_name>/<table_type>/domain_name=<domain_name>/database_name=db_name/table_name=<tb_name> To make this as painless as possible we have prepared a script insert_external_location_config.py which you can run locally to automatically insert the required line or full config into your .sql files. The script must be run from the root directory and requires user input to determine the path to the files that you wish to run it on. In Terminal, in the root directory run python scripts/insert_external_location_config.py You will be prompted to enter the path to the directory containing the files you wish to apply the script to. Type your input starting with the domain directory name and continuing to whichever subdirectory you require (note there is no tab auto complete available) and hit Enter : Enter path to directory, starting with domain directory: <domain_name>/<database_name> If you are copy/pasting be careful not to introduce leading or trailing spaces. The next prompt shows the full file path you have selected and asks you to confirm by typing “Y” (any other input will exit the program): Path selected: 'mojap_derived_tables/models/<domain_name>/<database_name>' Continue with selected path? Enter Y/n: Y The final prompt shows the number of files selected and their paths and asks you to confirm by typing “Y”: Files selected:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql mojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Number of files selected:  3\\nContinue with selected files? Enter Y/n: Y Upon entering “Y” the script starts to scan the files and make the required changes. There are three possibilities: Scanning files... Config block exists but no external location set for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_0>.sql\\nInserting external_location line into existing config block... External location not set and no config block for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_1>.sql\\nInserting config block... External location set correctly in config block - nothing to do for file:\\nmojap_derived_tables/models/<domain_name>/<database_name>/<database_name__table_name_3>.sql Once inserted the config block will look similar to this, but may include additional parameters that you have set previously: {{ config(\\nexternal_location=generate_s3_location()\\n) }} Note that if there was no existing config block it is inserted at the top of the file, displacing (but not overwriting) any existing comments or code. Files are automatically saved once the changes are applied. SQLFluff linting changes As you may be aware we have had issues with SQLFluff being unable to cope with complex Jinja templating mostly in macros. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 554}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c76495430a765e1b9f97637a87fbb983'}>,\n",
       " <Document: {'content': 'The new generate_s3_location.sql macro is no exception and is added to the sqlfluffignore file so that it is skipped during linting. However, since all models now reference this macro SQLfluff throws the Undefined jinja template variable error. We cannot add all models to sqlfluffignore , hence to circumvent the perceived error please use the --ignore=templating option when running SQLFluff lint or fix, thus: sqlfluff lint --ignore=templating path/to/files/to/lint sqlfluff fix --ignore=templating path/to/files/to/lint/and/fix Update your branch with the dbt-athena upgrade As mentioned above we have created a branch containing all the upgrades called DMT-236/dbt-athena-upgrade-main . While we are testing we may make changes to the DMT-236/dbt-athena-upgrade-main branch which you will then need to merge into your branches. Whilst on the branch you want to update with the latest from DMT-236/dbt-athena-upgrade-main run: git fetch origin DMT-236/dbt-athena-upgrade-main\\ngit pull origin DMT-236/dbt-athena-upgrade-main\\ngit merge origin/DMT-236/dbt-athena-upgrade-main At this point you may have merge conflicts that need to be resolved; please see GitHub resolve merge conflicts . If required, ask for help on the #ask-data-modelling slack channel. S3 location change for seeds Previously the seed S3 location was split between the dev and prod environments and followed the same Hive style path naming convention as for models. It was not straightforward to preserve this feature with the dbt-athena-community adapter and since seeds change little it was not a priority. The old directory structure for the mojap-derived-tables bucket is as below, with the seed directory appearing under both prod and dev directories. Note the database name is suffixed with _dev_dbt under the dev directory: ├── mojap_derived_tables\\n├── dev/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one_dev_dbt/\\n├── table_name=tb_one\\n...\\n...\\n...\\n├── prod/\\n├── models/\\n├── run_artefacts/\\n└── seeds/\\n├── domain_name=domain_one/\\n├── database_name=db_one/\\n├── table_name=tb_one\\n...\\n...\\n... The new directory structure has a single seeds directory at the same level as the prod and dev directories. A seed created on a dev run will appear under its database name suffixed with _dev_dbt , as before, but the Hive path naming convention is not upheld. Instead we use the naming option schema_table provided by dbt-athena-community which is simply <database_name>/<table_name> ├── mojap_derived_tables\\n├── dev/\\n├── prod/\\n└── seeds/\\n├── database_one_dev_dbt/\\n├── table_one/\\n...\\n├── database_one/\\n├── table_one/\\n...\\n... The changes to the S3 location should not have any impact on users, unless they have specifically referenced a seed by its S3 location. References in create-a-derived-table using the ref function will be unaffected as this uses   Athena to reference the Glue Catalogue Registration, in the form <database_name.table_name> . The catalogue registrations will be updated with the new S3 locations automatically. License Unless stated otherwise, the codebase is released under the MIT License . This covers both the codebase and any sample code in the documentation. The documentation is © Crown copyright and available under the terms of the Open Government 3.0 licence. This page was last reviewed on 13 June 2023.\\n\\nIt needs to be reviewed again on 13 June 2024\\nby the page owner #ask-data-modelling . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 555}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5d79242522503657d55580d8ac80fb48'}>,\n",
       " <Document: {'content': 'This page was set to be reviewed before 13 June 2024\\nby the page owner #ask-data-modelling .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nControl Panel - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 556}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ba96b6796adf1f8f5e7423737504aec8'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 557}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Control panel The Analytical Platform Control Panel is the main entry point to the Analytical Platform. You can access different resources by clicking on the different tabs. Analytical tools Tools on the Analytical Platform include RStudio and JupyterLab. To use these, you need to start your own copy of the software. This gives you individual reserved memory space (compared to the more common shared R Studio Server) and some control over the version of R that is running. Use the buttons shown against each tool to manage your copy of the tool: “Deploy” - the tool is not yet deployed - this is the initial state. You need to “Deploy” to be able to use the tool for the first time. It sets you up with the latest version and starts it. This may take a few minutes. “Open” - the tool is either “Idled” (configured but not running) or “Ready” (running). If your RStudio or JupyterLab is inactive on a Tuesday evening it will be idled. Press “Open” to navigate to the tool in your browser, and if it is not running it will start it (run or “unidle” it). Starting a tool usually takes about 30 seconds, but occasionally will take a few minutes. “Restart” - often problems with the tool can be solved by restarting the software on the server. “Upgrade” - another release of the tool is available. Occasionally new versions of tools are made available on the Analytical Platform. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 558}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'edcd236c7f4991313dea44a2a078d1fc'}>,\n",
       " <Document: {'content': 'In this case you’ll get the opportunity to upgrade on the control panel. New versions provide new features and bug fixes. In addition, some releases come with improvements to the way tools are containerized and integrated with the Analytical Platform. You should aim to upgrade when it is offered, although in case it may causes minor incompatibilities with your  code, you should not do it in the days just before you have a critical release of your work. When pressed, the status will change to ‘Deploying’ and then ‘Upgraded’. The Upgrade button will no longer be visible (until another version becomes available). At times very old versions of tools are deprecated/removed from platform but we always inform our users in advance over our slack channel.If you are using an old version of a tool which is due to be removed soon, please do upgrade to an appropriate version. The Control Panel also provides access to Airflow 2 as an AWS managed service.\\nYou can access the development and production instances of Airflow from the Tools page.\\nFor further details, see the Airflow documentation . This page was last reviewed on 6 July 2022.\\n\\nIt needs to be reviewed again on 6 July 2023\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 6 July 2023\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nControl Panel - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 559}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '749b14e4f13baf97b7951ad68e540231'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 560}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Control panel The Analytical Platform Control Panel is the main entry point to the Analytical Platform. You can access different resources by clicking on the different tabs. Analytical tools Tools on the Analytical Platform include RStudio and JupyterLab. To use these, you need to start your own copy of the software. This gives you individual reserved memory space (compared to the more common shared R Studio Server) and some control over the version of R that is running. Use the buttons shown against each tool to manage your copy of the tool: “Deploy” - the tool is not yet deployed - this is the initial state. You need to “Deploy” to be able to use the tool for the first time. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 561}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8c27f6628f625981decd458782d8d944'}>,\n",
       " <Document: {'content': 'It sets you up with the latest version and starts it. This may take a few minutes. “Open” - the tool is either “Idled” (configured but not running) or “Ready” (running). If your RStudio or JupyterLab is inactive on a Tuesday evening it will be idled. Press “Open” to navigate to the tool in your browser, and if it is not running it will start it (run or “unidle” it). Starting a tool usually takes about 30 seconds, but occasionally will take a few minutes. “Restart” - often problems with the tool can be solved by restarting the software on the server. “Upgrade” - another release of the tool is available. Occasionally new versions of tools are made available on the Analytical Platform. In this case you’ll get the opportunity to upgrade on the control panel. New versions provide new features and bug fixes. In addition, some releases come with improvements to the way tools are containerized and integrated with the Analytical Platform. You should aim to upgrade when it is offered, although in case it may causes minor incompatibilities with your  code, you should not do it in the days just before you have a critical release of your work. When pressed, the status will change to ‘Deploying’ and then ‘Upgraded’. The Upgrade button will no longer be visible (until another version becomes available). At times very old versions of tools are deprecated/removed from platform but we always inform our users in advance over our slack channel.If you are using an old version of a tool which is due to be removed soon, please do upgrade to an appropriate version. The Control Panel also provides access to Airflow 2 as an AWS managed service.\\nYou can access the development and production instances of Airflow from the Tools page.\\nFor further details, see the Airflow documentation . This page was last reviewed on 6 July 2022.\\n\\nIt needs to be reviewed again on 6 July 2023\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 6 July 2023\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nRStudio - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 562}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a097f7ed6d7da70b88759d0806cb29c0'}>,\n",
       " <Document: {'content': 'I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 563}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '165700989970d698eee8f1a9a15a7bc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools RStudio For general guidance in using RStudio, see the RStudio documentation . RStudio memory issues RStudio crashes when it runs out of memory. This is because memory is a finite resource, and it’s not easy to predict memory usage or exact availability. But if your data is of order of a couple of gigabytes or more, then simply putting it all into a dataframe, or doing processing on it, may mean you run out of memory. For more about memory capacity in the Analytical Platform, and how to work with larger datasets, see the memory limits section. To find out if you have hit the memory limit, you can check Grafana . For guidance in using it, see the memory limits section. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 564}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fc5afa974e74260d5acd48bf4f6bb1f9'}>,\n",
       " <Document: {'content': 'If RStudio crashes on startup, and you’ve identified from Grafana that it is because the memory is full, then you can fix it by clearing your RStudio session . Once RStudio is running again, you can get a better understanding of what takes up memory by using the pryr package. To free up a bit of memory, for example when a variable points to a big dataframe, you can instead assign something a null to the variable, and then run gc() . To free up all the user memory you can click on the ‘broom’ to clear the environment . An alternative is to use Airflow and run your R job as an Airflow task on a high-memory node. Clearing your RStudio session The RStudio session is the short term ‘state’ of RStudio, including: which project and files are open (displayed in the RStudio interface) console and terminal history the global environment – R variables that have been set and are held in memory The session is persisted between restarts of RStudio, to make it easier for you to pick up where you left off. However you can clear it to solve a number of problems, including if the memory is full. To clear the RStudio session: Close RStudio, if it is open in any window (because it continually saves its session to disk). Open the control panel, navigate to Analytical tool, click resetting your home directory . Select the Restart button for RStudio. In the control panel, select Open for RStudio. It may take between one and five minutes before RStudio is available. You may need to refresh your browser for the tool to load. This page was last reviewed on 1 May 2022.\\n\\nIt needs to be reviewed again on 1 July 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 1 July 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nR Package Management - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 565}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e306d360bfd270849fca9dff707e5a95'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 566}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools R package management There are multiple package managers available for RStudio depending on the version you are using: renv Conda packrat Why use a package manager? This enables analysts to maintain a reproducible workflow by including a snapshot of all packages used within a project saved within the project files themselves that can be loaded and installed with a single consistent and reproducible method. This means that if you create some code one day, you (or another analyst who comes after you) should be able to pick it up several years later and run it without any difficulty - even if the packages used have themselves changed in the meantime. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 567}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '775dbc223c61ad5f8824c790f2fc063c'}>,\n",
       " <Document: {'content': 'For Rstudio there is the added imperative to use a package manager (usually renv) because the analytical platform will remove installed packages when the docker image is restarted (which occurs automatically, roughly once a week). Renv Renv is the current standard for Rstudio on the Analytical Platform as it provides simpler package management than Conda or packrat which were previously recommended. The basic renv commands are: Command Description renv::init() first time a project is created renv::install() install new packages renv::snapshot() save a description of packages to renv.lock renv::restore() install packages to match renv.lock The following gives an overview of these basic renv commands.\\nFor more details check out the Coffee and Coding video and slides , or for a full guide to installing packages, workflow and installing custom packages please see the introduction to renv website . Getting started with renv If you are using version 4 or greater of R on the analytical platform then renv should work straightaway.\\nThe only other things to note if you’ve not used renv before are: The first time you use renv, you may be asked to consent to some changes it makes to the way packages are installed - please select yes to this. If you previously used a different package management system (like Conda or packrat) remove any configuration files for these systems from your R files first. Starting a new project with renv or adding renv to an existing project Basic commands to follow to install packages for renv are: # install renv (if not already installed) install.packages ( \"renv\" ) # If you are starting a fresh repository, run this: renv :: init ( bare = TRUE ) # or if you are starting a fresh repository but would like to move your existing packages over to renv: renv :: init () Then ensure you have committed and pushed the relevant files (.Rprofile, renv.lock, and renv/activate.R) to your github repository.\\nThese should be the only files which git suggests you commit - you should not commit the whole contents of the renv folder created when initialising a project. Now you are ready to work on your project! Working on a renv project You can work on your project as normal now, but when you install new packages and want to save the state of your package environment you must “snapshot” your packages. For instance, if you wanted to install dplyr and then update your package environment then the process would be: # install a package (the default is the latest available) renv :: install ( \"dplyr\" ) # or install a specific version of a package renv :: install ( \"dplyr@0.8.5\" ) # snapshot your project renv :: snapshot () # don’t forget to commit # renv.lock! You can use renv::install or install.packages - renv will intercept any calls to install.packages and runs renv::install under the hood anyway. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 568}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '641f6d8499188ac7dae5c147f95a0207'}>,\n",
       " <Document: {'content': 'Picking up a renv project If you pick up someone else’s project from github who has been using renv then simply run renv::restore() to update your local package environment so it matches the renv.lock file. # clone the project into # Rstudio # grab the packages renv :: restore () Any time you pull a commit where the renv.lock file has changed, you will need to renv::restore() in order to make sure your package enviroment matches to the new renv.lock file.\\nYou will also have to do this if you change branches in your repository to one with a different renv.lock file. Using renv with python If you are installing the recommended package for accessing data from s3, botor , you will need to do the following: renv :: use_python () # at the prompt, choose to use python3 renv :: install ( \\'reticulate\\' ) Restart the session (Ctrl+Alt+F10 on a windows machine). And then: reticulate :: py_install ( \\'boto3\\' ) renv :: install ( \\'botor\\' ) See the Renv Python documentation for further guidance. To activate Python integration within renv, type renv::use_python() Common pitfalls with renv Situation Why/What happens? Packages   disappearing You aren’t using renv! Forgetting to renv::snapshot() This won’t affect you running your   code, but anyone picking it up later will be out of sync. You can use renv::status() to check if packages and renv.lock match Switching branches If different package requirements in   branches then must remember to renv::restore() when switching between them – otherwise library reflects the previous branch Initialising renv   outside a project renv will ask you not to do this – do not   use force   = TRUE ! Stuck on old CRAN/MRAN Packages (or versions) you know exist   won’t appear using install functions. Run options(repos   = \"https://cloud.r-project.org/\") renv tips and tricks Situation Solution Got into a total mess? Start again! Run renv::deactivate() and then delete the renv.lock file and the renv/ folder Add a package from github Use renv::install(\"username/packagename\") or for a private package renv::install(\"git@github.com:username/packagename.git\") Upgrade all packages to latest Run renv::update() or renv::update(\"packagename\") for specific package. Always check that upgrading packages does not break your code before pushing to github for other users. Update renv itself renv::upgrade() . Useful if renv gains new functionality that you want to use. Error in file(filename, “r”, encoding = encoding) : cannot open the connection Oops, you’ve accidentally installed renv in your home directory 🏠 ! Delete all of the files created by renv from your home directory and retry. Conda NB Use of conda is now considered outdated for Rstudio on the Analytical Platform. When exploring this section, you may also find the slides from the Coffee and Coding session on conda useful. Conda is a unified package management system that supports managing both Python and R dependencies in a single environment . It can make sure all of these libraries are compatible with each other. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 569}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cbde220878e455d3a4ecd1598b4a5ffa'}>,\n",
       " <Document: {'content': 'Conda is available for both RStudio and JupyterLab on the Analytical Platform, though note that RStudio and JupyterLab have separate environments so dependencies won’t be shared between the applications. A key example within Analytical Services where conda is useful: both dbtools and s3tools rely on Python packages through the reticulate R-to-Python bridge. packrat only handles R dependencies; this means that packrat is not enough to reproducibly and reliably manage all of your application’s dependencies. Installing Packages The Anaconda organisation has its own repository of packages hosted on https://anaconda.org . If you need to find a package name you can use the anaconda search to find the package name. To install a package through conda, run the command conda install PACKAGENAME in the Terminal tab. This is recommended over using install.packages() as the package will be installed into the conda environment in a way that can be repeated when replicating the analysis - see Environment management section for more. Most (around 95%) R packages on CRAN are available through conda. They have the same name as the CRAN package name with an additional r- prefix. This is to avoid clashes with Python packages with the same name. Example In the terminal run: conda install numpy . You can now access in your R session: library ( reticulate ) np <- import ( \"numpy\" ) np $ arange ( 15 ) Comparison with install.packages() The following tables show conda commands and their base R analogues. Installing a package: install.packages (in R-Console) conda install (in Terminal) install.packages(\\'Rcpp\\') conda install r-Rcpp Installing a specific version of a package install.packages conda install require(devtools) install_version(\"ggplot2\", version = \"2.2.1\", repos = \"http://cran.us.r-project.org\") conda install r-ggplot2=2.2.1 You can also use conda to install Python packages, for use in R through the reticulate package. Python packages do not require a prefix and can simply be installed using theirname. Operating System Packages Even if you want to continue using packrat or renv to manage your R packages,  some packages have operating system-level dependencies, which can’t be handled by packrat / renv themselves. You can use conda to resolve these operating system dependencies, such as libxml2. Examples Installing a package that relies on OS dependency Suppose you want to install the R package bigIntegerAlgos , but it fails because it depends on a system level library called gmp . To resolve this, switch to the terminal and use conda to install it. Then switch back to the R console and try to use install.packages again. Environment Management You can use conda to make a snapshot of the environment you are using, so others can reproduce your results using the same versions of your code. Note: usually when using conda, it makes sense to have one environment per project,\\nbut because we are using the Open Source version of R Studio, there is only a\\nsingle conda environment available. This means having to be careful to make sure packages don’t pollute your environment from another project. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 570}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3b058fa0e6cb6175ac09648d371e53cc'}>,\n",
       " <Document: {'content': 'The following commands can be used to manage your environments. Reset your conda environment to default This will delete packages that you have installed in your rstudio conda environment, leaving only the base packages: conda env export -n base| grep -v \"^prefix: \" > /tmp/base.yml && conda env create --force -n rstudio -f /tmp/base.yml && rm /tmp/base.yml It is recommended to do this before starting a new project, to ensure that no unused dependencies are exported when you export an environment.yml for this project. Hard reset of your conda environment This will completely delete your rstudio conda environment, and recreate it with the base packages: Deleting all the files in the environment. For example, to clear the rstudio conda environment (which is the default one): rm -rf ~/.conda/envs/rstudio You might get errors about Directory not empty or Device or resource busy but usually these can be ignored - the bulk of these packages will be gone. In Control Panel, for R Studio, select the “Restart” button It can be useful to do this if you have tried to reset your conda environment to default and are still having problems. Exporting your Environment This is similar to making a packrat.lock file, it catalogues all of the\\ndependencies installed in your environment so that another user can restore a\\nworking environment for your application. Check this environment.yml file into\\nyour git repository. conda env export | grep -v \"^prefix: \" > environment.yml Making your R Studio Environment match an environment.yml When checking out a project that has an environment.yml , run the below command to install any packages required by the project that you don’t have in your working environment. conda env update -f environment.yml --prune Conda tips Conda version When you run conda (In R Studio at least) it says: ==> WARNING: A newer version of conda exists. < == current version: 4.7.5\\nlatest version: 4.8.3 Please update conda by running <span class=\"nv\">$ </span>conda update <span class=\"nt\">-n</span> base conda Please ignore this warning - this can only be done centrally by Analytical Platform team. If you try to upgrade conda yourself, it will fail: EnvironmentNotWritableError: The current user does not have write permissions to the target environment.\\nenvironment location: /opt/conda This is because conda is installed into the read-only part of the docker image. Users can only edit things in /home/$USER. Package installed with a different R version - when using conda Typical error output: > conda install ggplot2\\n...\\nError : package ‘tibble’ was installed by an R version with different internals ; it needs to be reinstalled for use with this R version It’s saying that this package, which is a dependency of the one you’re installing, was installed with an R version you used to have. To fix this, wipe your installed packages and reinstall them from your environment.yml. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 571}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1df8a8259d136c54232dbbb7de9f4614'}>,\n",
       " <Document: {'content': '# reset your conda environment conda env export -n base| grep -v \"^prefix: \" > /tmp/base.yml && conda env update --prune -n rstudio -f /tmp/base.yml && rm /tmp/base.yml # reinstall packages conda env update -f environment.yml --prune Packrat NB Use of packrat is deprecated on the Analytical Platform - the guidance below is for information only because legacy projects may still use packrat . Packrat is the most well-known package management tool for R. There’s more information about it here: https://rstudio.github.io/packrat/ It has some significant downsides. It can be quite temperamental, and difficult to debug when things go wrong - in the earlier days of the Analytical Platform, the majority of support issues related to getting Packrat working. Furthermore, the Analytical Platform version of RStudio runs on a Linux virtual machine, and CRAN mirrors do not provide Linux compiled binaries for packages. This means that packages need to be compiled on the Analytical Platform every time they’re installed, which can take a long time. This means a long wait when doing install.packages both in an RStudio session, and when running a Docker build for an RShiny application. Packrat usage To use packrat, ensure that it is enabled for your project in RStudio: select Tools > Project Options… > Packrat > Use packrat with this project . When packrat is enabled, run packrat::snapshot() to generate a list of packages used in the project, their sources and their current versions. You may also wish to run packrat::clean() to remove unused packages from the list. The list is stored in a file called packrat/packrat.lock . You must ensure that you have committed this file to GitHub before deploying your app. R’s install.packages() NB Only use this method for playing - use Conda for project work. You can install R packages from the R Console: install.packages ( \"ggplot2\" ) This will find the latest version of the package in CRAN and install it in: ~/R/library . However this method is pretty basic. Refer to the tips in the following sections. Package version incompatible with R version Often if you try to install the latest version of a package, it will require a more recent version of R than you have: > install.packages ( \"text2vec\" ) Installing package into ‘/home/davidread/R/library’ ( as ‘lib’ is unspecified ) Warning in install.packages :\\npackage ‘text2vec’ is not available ( for R version 3.5.1 ) There are a few options to avoid this: Solution 1: AP may have a newer version of RStudio tool which might have the version of R needed. To upgrade, see: Managing your analytical tools Solution 2: Use conda - it’s recommended for use with Analytical Platform in general. It works out which version is compatible with your R version (make sure you run this in the Terminal): conda install r-text2vec Solution 3: Specify a version that is compatible with your R version. e.g. at https://www.rdocumentation.org/packages/text2vec look at the “depends” field for the R version it requires. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 572}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '362741613a46bfd9650b6f9e5f78a056'}>,\n",
       " <Document: {'content': 'Change the version (drop-down at the top) to go back to see how it changes for older releases. You can see that text2vec 6.0 requires R (>= 3.6.0), but text2vec 5.1 requires only R (>= 3.2.0). devtools :: install_version ( \\'text2vec\\' , version = \\'0.5.1\\' ) Package installed with a different R version - when using install.packages() Typical error output > install.packages ( \"ggplot2\" ) ... Error : package ‘ tibble ’ was installed by an R version with different internals ; it needs to be reinstalled for use with this R version It’s saying that this package, which is a dependency of the one you’re installing, was installed with an R version you used to have. Solution 1 - You might fix this by installing the package it names: > install.packages ( \\'tibble\\' ) However you may have to do this for a lot of packages. Solution 2 - Wipe your packages and reinstall them. It begs the question of what you have installed. Although you can get a list it’s often unmanageably long, including all the little dependencies of what you actually installed in the first place. Best use conda next time! But you can get rid of all the installed packages (use the terminal): rm -rf ~/R/library/ * “Broken” packages (typically r-pillar ) When installing packages (e.g. during a concourse build of a webapp) you may see an error like this: $ conda env export -n base grep -v \"\" prefix: \" > /tmp/base.yml &\\nconda env update --prune -n rstudio -f /tmp/base.yml && rm /tmp/base.yml Collecting package metadata (repodata.json): done\\nSolving environment: failed\\nResolvePackageNotFound:\\n- r-pillar=1.4.2=h6115d3f_O This happens when a package on conda is marked as broken . r-pillar seems to suffer this frequently. To fix this there are a couple of things you can try: Remove r-pillar (or the offending package) from environment.yml. r-pillar is provided by the base conda environment and chances are that the user doesn’t need it in their app, so it can be safely removed. Update the version of r-pillar to the latest one on conda-forge. This page was last reviewed on 1 May 2022.\\n\\nIt needs to be reviewed again on 1 July 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 1 July 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nR Package Management - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 573}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8c36d0bd5431ab4da3c592cc97d7209c'}>,\n",
       " <Document: {'content': 'Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 574}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9ba4b74f4fa634ec10c04b4f7c578f23'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools R package management There are multiple package managers available for RStudio depending on the version you are using: renv Conda packrat Why use a package manager? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 575}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c44a54c47fec23e780b109aec221d067'}>,\n",
       " <Document: {'content': 'This enables analysts to maintain a reproducible workflow by including a snapshot of all packages used within a project saved within the project files themselves that can be loaded and installed with a single consistent and reproducible method. This means that if you create some code one day, you (or another analyst who comes after you) should be able to pick it up several years later and run it without any difficulty - even if the packages used have themselves changed in the meantime. For Rstudio there is the added imperative to use a package manager (usually renv) because the analytical platform will remove installed packages when the docker image is restarted (which occurs automatically, roughly once a week). Renv Renv is the current standard for Rstudio on the Analytical Platform as it provides simpler package management than Conda or packrat which were previously recommended. The basic renv commands are: Command Description renv::init() first time a project is created renv::install() install new packages renv::snapshot() save a description of packages to renv.lock renv::restore() install packages to match renv.lock The following gives an overview of these basic renv commands.\\nFor more details check out the Coffee and Coding video and slides , or for a full guide to installing packages, workflow and installing custom packages please see the introduction to renv website . Getting started with renv If you are using version 4 or greater of R on the analytical platform then renv should work straightaway.\\nThe only other things to note if you’ve not used renv before are: The first time you use renv, you may be asked to consent to some changes it makes to the way packages are installed - please select yes to this. If you previously used a different package management system (like Conda or packrat) remove any configuration files for these systems from your R files first. Starting a new project with renv or adding renv to an existing project Basic commands to follow to install packages for renv are: # install renv (if not already installed) install.packages ( \"renv\" ) # If you are starting a fresh repository, run this: renv :: init ( bare = TRUE ) # or if you are starting a fresh repository but would like to move your existing packages over to renv: renv :: init () Then ensure you have committed and pushed the relevant files (.Rprofile, renv.lock, and renv/activate.R) to your github repository.\\nThese should be the only files which git suggests you commit - you should not commit the whole contents of the renv folder created when initialising a project. Now you are ready to work on your project! Working on a renv project You can work on your project as normal now, but when you install new packages and want to save the state of your package environment you must “snapshot” your packages. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 576}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '86f5f252ed9f2c06f53b08083388e47'}>,\n",
       " <Document: {'content': 'For instance, if you wanted to install dplyr and then update your package environment then the process would be: # install a package (the default is the latest available) renv :: install ( \"dplyr\" ) # or install a specific version of a package renv :: install ( \"dplyr@0.8.5\" ) # snapshot your project renv :: snapshot () # don’t forget to commit # renv.lock! You can use renv::install or install.packages - renv will intercept any calls to install.packages and runs renv::install under the hood anyway. Picking up a renv project If you pick up someone else’s project from github who has been using renv then simply run renv::restore() to update your local package environment so it matches the renv.lock file. # clone the project into # Rstudio # grab the packages renv :: restore () Any time you pull a commit where the renv.lock file has changed, you will need to renv::restore() in order to make sure your package enviroment matches to the new renv.lock file.\\nYou will also have to do this if you change branches in your repository to one with a different renv.lock file. Using renv with python If you are installing the recommended package for accessing data from s3, botor , you will need to do the following: renv :: use_python () # at the prompt, choose to use python3 renv :: install ( \\'reticulate\\' ) Restart the session (Ctrl+Alt+F10 on a windows machine). And then: reticulate :: py_install ( \\'boto3\\' ) renv :: install ( \\'botor\\' ) See the Renv Python documentation for further guidance. To activate Python integration within renv, type renv::use_python() Common pitfalls with renv Situation Why/What happens? Packages   disappearing You aren’t using renv! Forgetting to renv::snapshot() This won’t affect you running your   code, but anyone picking it up later will be out of sync. You can use renv::status() to check if packages and renv.lock match Switching branches If different package requirements in   branches then must remember to renv::restore() when switching between them – otherwise library reflects the previous branch Initialising renv   outside a project renv will ask you not to do this – do not   use force   = TRUE ! Stuck on old CRAN/MRAN Packages (or versions) you know exist   won’t appear using install functions. Run options(repos   = \"https://cloud.r-project.org/\") renv tips and tricks Situation Solution Got into a total mess? Start again! Run renv::deactivate() and then delete the renv.lock file and the renv/ folder Add a package from github Use renv::install(\"username/packagename\") or for a private package renv::install(\"git@github.com:username/packagename.git\") Upgrade all packages to latest Run renv::update() or renv::update(\"packagename\") for specific package. Always check that upgrading packages does not break your code before pushing to github for other users. Update renv itself renv::upgrade() . Useful if renv gains new functionality that you want to use. Error in file(filename, “r”, encoding = encoding) : cannot open the connection Oops, you’ve accidentally installed renv in your home directory 🏠 ! Delete all of the files created by renv from your home directory and retry. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 577}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e447cedcf0626b5ee10171d41d6079a7'}>,\n",
       " <Document: {'content': 'Conda NB Use of conda is now considered outdated for Rstudio on the Analytical Platform. When exploring this section, you may also find the slides from the Coffee and Coding session on conda useful. Conda is a unified package management system that supports managing both Python and R dependencies in a single environment . It can make sure all of these libraries are compatible with each other. Conda is available for both RStudio and JupyterLab on the Analytical Platform, though note that RStudio and JupyterLab have separate environments so dependencies won’t be shared between the applications. A key example within Analytical Services where conda is useful: both dbtools and s3tools rely on Python packages through the reticulate R-to-Python bridge. packrat only handles R dependencies; this means that packrat is not enough to reproducibly and reliably manage all of your application’s dependencies. Installing Packages The Anaconda organisation has its own repository of packages hosted on https://anaconda.org . If you need to find a package name you can use the anaconda search to find the package name. To install a package through conda, run the command conda install PACKAGENAME in the Terminal tab. This is recommended over using install.packages() as the package will be installed into the conda environment in a way that can be repeated when replicating the analysis - see Environment management section for more. Most (around 95%) R packages on CRAN are available through conda. They have the same name as the CRAN package name with an additional r- prefix. This is to avoid clashes with Python packages with the same name. Example In the terminal run: conda install numpy . You can now access in your R session: library ( reticulate ) np <- import ( \"numpy\" ) np $ arange ( 15 ) Comparison with install.packages() The following tables show conda commands and their base R analogues. Installing a package: install.packages (in R-Console) conda install (in Terminal) install.packages(\\'Rcpp\\') conda install r-Rcpp Installing a specific version of a package install.packages conda install require(devtools) install_version(\"ggplot2\", version = \"2.2.1\", repos = \"http://cran.us.r-project.org\") conda install r-ggplot2=2.2.1 You can also use conda to install Python packages, for use in R through the reticulate package. Python packages do not require a prefix and can simply be installed using theirname. Operating System Packages Even if you want to continue using packrat or renv to manage your R packages,  some packages have operating system-level dependencies, which can’t be handled by packrat / renv themselves. You can use conda to resolve these operating system dependencies, such as libxml2. Examples Installing a package that relies on OS dependency Suppose you want to install the R package bigIntegerAlgos , but it fails because it depends on a system level library called gmp . To resolve this, switch to the terminal and use conda to install it. Then switch back to the R console and try to use install.packages again. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 578}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2c083f67ece32cc29f1ea076a1afd58'}>,\n",
       " <Document: {'content': 'Environment Management You can use conda to make a snapshot of the environment you are using, so others can reproduce your results using the same versions of your code. Note: usually when using conda, it makes sense to have one environment per project,\\nbut because we are using the Open Source version of R Studio, there is only a\\nsingle conda environment available. This means having to be careful to make sure packages don’t pollute your environment from another project. The following commands can be used to manage your environments. Reset your conda environment to default This will delete packages that you have installed in your rstudio conda environment, leaving only the base packages: conda env export -n base| grep -v \"^prefix: \" > /tmp/base.yml && conda env create --force -n rstudio -f /tmp/base.yml && rm /tmp/base.yml It is recommended to do this before starting a new project, to ensure that no unused dependencies are exported when you export an environment.yml for this project. Hard reset of your conda environment This will completely delete your rstudio conda environment, and recreate it with the base packages: Deleting all the files in the environment. For example, to clear the rstudio conda environment (which is the default one): rm -rf ~/.conda/envs/rstudio You might get errors about Directory not empty or Device or resource busy but usually these can be ignored - the bulk of these packages will be gone. In Control Panel, for R Studio, select the “Restart” button It can be useful to do this if you have tried to reset your conda environment to default and are still having problems. Exporting your Environment This is similar to making a packrat.lock file, it catalogues all of the\\ndependencies installed in your environment so that another user can restore a\\nworking environment for your application. Check this environment.yml file into\\nyour git repository. conda env export | grep -v \"^prefix: \" > environment.yml Making your R Studio Environment match an environment.yml When checking out a project that has an environment.yml , run the below command to install any packages required by the project that you don’t have in your working environment. conda env update -f environment.yml --prune Conda tips Conda version When you run conda (In R Studio at least) it says: ==> WARNING: A newer version of conda exists. < == current version: 4.7.5\\nlatest version: 4.8.3 Please update conda by running <span class=\"nv\">$ </span>conda update <span class=\"nt\">-n</span> base conda Please ignore this warning - this can only be done centrally by Analytical Platform team. If you try to upgrade conda yourself, it will fail: EnvironmentNotWritableError: The current user does not have write permissions to the target environment.\\nenvironment location: /opt/conda This is because conda is installed into the read-only part of the docker image. Users can only edit things in /home/$USER. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 579}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b8c2363a87d3406501894d8a6ff97ef9'}>,\n",
       " <Document: {'content': 'Package installed with a different R version - when using conda Typical error output: > conda install ggplot2\\n...\\nError : package ‘tibble’ was installed by an R version with different internals ; it needs to be reinstalled for use with this R version It’s saying that this package, which is a dependency of the one you’re installing, was installed with an R version you used to have. To fix this, wipe your installed packages and reinstall them from your environment.yml. # reset your conda environment conda env export -n base| grep -v \"^prefix: \" > /tmp/base.yml && conda env update --prune -n rstudio -f /tmp/base.yml && rm /tmp/base.yml # reinstall packages conda env update -f environment.yml --prune Packrat NB Use of packrat is deprecated on the Analytical Platform - the guidance below is for information only because legacy projects may still use packrat . Packrat is the most well-known package management tool for R. There’s more information about it here: https://rstudio.github.io/packrat/ It has some significant downsides. It can be quite temperamental, and difficult to debug when things go wrong - in the earlier days of the Analytical Platform, the majority of support issues related to getting Packrat working. Furthermore, the Analytical Platform version of RStudio runs on a Linux virtual machine, and CRAN mirrors do not provide Linux compiled binaries for packages. This means that packages need to be compiled on the Analytical Platform every time they’re installed, which can take a long time. This means a long wait when doing install.packages both in an RStudio session, and when running a Docker build for an RShiny application. Packrat usage To use packrat, ensure that it is enabled for your project in RStudio: select Tools > Project Options… > Packrat > Use packrat with this project . When packrat is enabled, run packrat::snapshot() to generate a list of packages used in the project, their sources and their current versions. You may also wish to run packrat::clean() to remove unused packages from the list. The list is stored in a file called packrat/packrat.lock . You must ensure that you have committed this file to GitHub before deploying your app. R’s install.packages() NB Only use this method for playing - use Conda for project work. You can install R packages from the R Console: install.packages ( \"ggplot2\" ) This will find the latest version of the package in CRAN and install it in: ~/R/library . However this method is pretty basic. Refer to the tips in the following sections. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 580}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7ade713f6a0ea1efb4e60bc9a0aae4eb'}>,\n",
       " <Document: {'content': 'Package version incompatible with R version Often if you try to install the latest version of a package, it will require a more recent version of R than you have: > install.packages ( \"text2vec\" ) Installing package into ‘/home/davidread/R/library’ ( as ‘lib’ is unspecified ) Warning in install.packages :\\npackage ‘text2vec’ is not available ( for R version 3.5.1 ) There are a few options to avoid this: Solution 1: AP may have a newer version of RStudio tool which might have the version of R needed. To upgrade, see: Managing your analytical tools Solution 2: Use conda - it’s recommended for use with Analytical Platform in general. It works out which version is compatible with your R version (make sure you run this in the Terminal): conda install r-text2vec Solution 3: Specify a version that is compatible with your R version. e.g. at https://www.rdocumentation.org/packages/text2vec look at the “depends” field for the R version it requires. Change the version (drop-down at the top) to go back to see how it changes for older releases. You can see that text2vec 6.0 requires R (>= 3.6.0), but text2vec 5.1 requires only R (>= 3.2.0). devtools :: install_version ( \\'text2vec\\' , version = \\'0.5.1\\' ) Package installed with a different R version - when using install.packages() Typical error output > install.packages ( \"ggplot2\" ) ... Error : package ‘ tibble ’ was installed by an R version with different internals ; it needs to be reinstalled for use with this R version It’s saying that this package, which is a dependency of the one you’re installing, was installed with an R version you used to have. Solution 1 - You might fix this by installing the package it names: > install.packages ( \\'tibble\\' ) However you may have to do this for a lot of packages. Solution 2 - Wipe your packages and reinstall them. It begs the question of what you have installed. Although you can get a list it’s often unmanageably long, including all the little dependencies of what you actually installed in the first place. Best use conda next time! But you can get rid of all the installed packages (use the terminal): rm -rf ~/R/library/ * “Broken” packages (typically r-pillar ) When installing packages (e.g. during a concourse build of a webapp) you may see an error like this: $ conda env export -n base grep -v \"\" prefix: \" > /tmp/base.yml &\\nconda env update --prune -n rstudio -f /tmp/base.yml && rm /tmp/base.yml Collecting package metadata (repodata.json): done\\nSolving environment: failed\\nResolvePackageNotFound:\\n- r-pillar=1.4.2=h6115d3f_O This happens when a package on conda is marked as broken . r-pillar seems to suffer this frequently. To fix this there are a couple of things you can try: Remove r-pillar (or the offending package) from environment.yml. r-pillar is provided by the base conda environment and chances are that the user doesn’t need it in their app, so it can be safely removed. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 581}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6fb23112e92bbd06427e430ba9556f49'}>,\n",
       " <Document: {'content': 'Update the version of r-pillar to the latest one on conda-forge. This page was last reviewed on 1 May 2022.\\n\\nIt needs to be reviewed again on 1 July 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 1 July 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nR Package Management - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 582}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ac4ffe67db5b18155c16e514c04c4c11'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 583}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools R package management There are multiple package managers available for RStudio depending on the version you are using: renv Conda packrat Why use a package manager? This enables analysts to maintain a reproducible workflow by including a snapshot of all packages used within a project saved within the project files themselves that can be loaded and installed with a single consistent and reproducible method. This means that if you create some code one day, you (or another analyst who comes after you) should be able to pick it up several years later and run it without any difficulty - even if the packages used have themselves changed in the meantime. For Rstudio there is the added imperative to use a package manager (usually renv) because the analytical platform will remove installed packages when the docker image is restarted (which occurs automatically, roughly once a week). Renv Renv is the current standard for Rstudio on the Analytical Platform as it provides simpler package management than Conda or packrat which were previously recommended. The basic renv commands are: Command Description renv::init() first time a project is created renv::install() install new packages renv::snapshot() save a description of packages to renv.lock renv::restore() install packages to match renv.lock The following gives an overview of these basic renv commands.\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 584}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c997c68a844599a86aa8d0287700ef7c'}>,\n",
       " <Document: {'content': 'For more details check out the Coffee and Coding video and slides , or for a full guide to installing packages, workflow and installing custom packages please see the introduction to renv website . Getting started with renv If you are using version 4 or greater of R on the analytical platform then renv should work straightaway.\\nThe only other things to note if you’ve not used renv before are: The first time you use renv, you may be asked to consent to some changes it makes to the way packages are installed - please select yes to this. If you previously used a different package management system (like Conda or packrat) remove any configuration files for these systems from your R files first. Starting a new project with renv or adding renv to an existing project Basic commands to follow to install packages for renv are: # install renv (if not already installed) install.packages ( \"renv\" ) # If you are starting a fresh repository, run this: renv :: init ( bare = TRUE ) # or if you are starting a fresh repository but would like to move your existing packages over to renv: renv :: init () Then ensure you have committed and pushed the relevant files (.Rprofile, renv.lock, and renv/activate.R) to your github repository.\\nThese should be the only files which git suggests you commit - you should not commit the whole contents of the renv folder created when initialising a project. Now you are ready to work on your project! Working on a renv project You can work on your project as normal now, but when you install new packages and want to save the state of your package environment you must “snapshot” your packages. For instance, if you wanted to install dplyr and then update your package environment then the process would be: # install a package (the default is the latest available) renv :: install ( \"dplyr\" ) # or install a specific version of a package renv :: install ( \"dplyr@0.8.5\" ) # snapshot your project renv :: snapshot () # don’t forget to commit # renv.lock! You can use renv::install or install.packages - renv will intercept any calls to install.packages and runs renv::install under the hood anyway. Picking up a renv project If you pick up someone else’s project from github who has been using renv then simply run renv::restore() to update your local package environment so it matches the renv.lock file. # clone the project into # Rstudio # grab the packages renv :: restore () Any time you pull a commit where the renv.lock file has changed, you will need to renv::restore() in order to make sure your package enviroment matches to the new renv.lock file.\\nYou will also have to do this if you change branches in your repository to one with a different renv.lock file. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 585}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '47deb0f931b7269fa4196f7fc9c57867'}>,\n",
       " <Document: {'content': 'Using renv with python If you are installing the recommended package for accessing data from s3, botor , you will need to do the following: renv :: use_python () # at the prompt, choose to use python3 renv :: install ( \\'reticulate\\' ) Restart the session (Ctrl+Alt+F10 on a windows machine). And then: reticulate :: py_install ( \\'boto3\\' ) renv :: install ( \\'botor\\' ) See the Renv Python documentation for further guidance. To activate Python integration within renv, type renv::use_python() Common pitfalls with renv Situation Why/What happens? Packages   disappearing You aren’t using renv! Forgetting to renv::snapshot() This won’t affect you running your   code, but anyone picking it up later will be out of sync. You can use renv::status() to check if packages and renv.lock match Switching branches If different package requirements in   branches then must remember to renv::restore() when switching between them – otherwise library reflects the previous branch Initialising renv   outside a project renv will ask you not to do this – do not   use force   = TRUE ! Stuck on old CRAN/MRAN Packages (or versions) you know exist   won’t appear using install functions. Run options(repos   = \"https://cloud.r-project.org/\") renv tips and tricks Situation Solution Got into a total mess? Start again! Run renv::deactivate() and then delete the renv.lock file and the renv/ folder Add a package from github Use renv::install(\"username/packagename\") or for a private package renv::install(\"git@github.com:username/packagename.git\") Upgrade all packages to latest Run renv::update() or renv::update(\"packagename\") for specific package. Always check that upgrading packages does not break your code before pushing to github for other users. Update renv itself renv::upgrade() . Useful if renv gains new functionality that you want to use. Error in file(filename, “r”, encoding = encoding) : cannot open the connection Oops, you’ve accidentally installed renv in your home directory 🏠 ! Delete all of the files created by renv from your home directory and retry. Conda NB Use of conda is now considered outdated for Rstudio on the Analytical Platform. When exploring this section, you may also find the slides from the Coffee and Coding session on conda useful. Conda is a unified package management system that supports managing both Python and R dependencies in a single environment . It can make sure all of these libraries are compatible with each other. Conda is available for both RStudio and JupyterLab on the Analytical Platform, though note that RStudio and JupyterLab have separate environments so dependencies won’t be shared between the applications. A key example within Analytical Services where conda is useful: both dbtools and s3tools rely on Python packages through the reticulate R-to-Python bridge. packrat only handles R dependencies; this means that packrat is not enough to reproducibly and reliably manage all of your application’s dependencies. Installing Packages The Anaconda organisation has its own repository of packages hosted on https://anaconda.org . If you need to find a package name you can use the anaconda search to find the package name. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 586}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f3818bd8baa493220244329b8806cba2'}>,\n",
       " <Document: {'content': 'To install a package through conda, run the command conda install PACKAGENAME in the Terminal tab. This is recommended over using install.packages() as the package will be installed into the conda environment in a way that can be repeated when replicating the analysis - see Environment management section for more. Most (around 95%) R packages on CRAN are available through conda. They have the same name as the CRAN package name with an additional r- prefix. This is to avoid clashes with Python packages with the same name. Example In the terminal run: conda install numpy . You can now access in your R session: library ( reticulate ) np <- import ( \"numpy\" ) np $ arange ( 15 ) Comparison with install.packages() The following tables show conda commands and their base R analogues. Installing a package: install.packages (in R-Console) conda install (in Terminal) install.packages(\\'Rcpp\\') conda install r-Rcpp Installing a specific version of a package install.packages conda install require(devtools) install_version(\"ggplot2\", version = \"2.2.1\", repos = \"http://cran.us.r-project.org\") conda install r-ggplot2=2.2.1 You can also use conda to install Python packages, for use in R through the reticulate package. Python packages do not require a prefix and can simply be installed using theirname. Operating System Packages Even if you want to continue using packrat or renv to manage your R packages,  some packages have operating system-level dependencies, which can’t be handled by packrat / renv themselves. You can use conda to resolve these operating system dependencies, such as libxml2. Examples Installing a package that relies on OS dependency Suppose you want to install the R package bigIntegerAlgos , but it fails because it depends on a system level library called gmp . To resolve this, switch to the terminal and use conda to install it. Then switch back to the R console and try to use install.packages again. Environment Management You can use conda to make a snapshot of the environment you are using, so others can reproduce your results using the same versions of your code. Note: usually when using conda, it makes sense to have one environment per project,\\nbut because we are using the Open Source version of R Studio, there is only a\\nsingle conda environment available. This means having to be careful to make sure packages don’t pollute your environment from another project. The following commands can be used to manage your environments. Reset your conda environment to default This will delete packages that you have installed in your rstudio conda environment, leaving only the base packages: conda env export -n base| grep -v \"^prefix: \" > /tmp/base.yml && conda env create --force -n rstudio -f /tmp/base.yml && rm /tmp/base.yml It is recommended to do this before starting a new project, to ensure that no unused dependencies are exported when you export an environment.yml for this project. Hard reset of your conda environment This will completely delete your rstudio conda environment, and recreate it with the base packages: Deleting all the files in the environment. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 587}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4fee1914f0561da65d11c015aff1ca3b'}>,\n",
       " <Document: {'content': 'For example, to clear the rstudio conda environment (which is the default one): rm -rf ~/.conda/envs/rstudio You might get errors about Directory not empty or Device or resource busy but usually these can be ignored - the bulk of these packages will be gone. In Control Panel, for R Studio, select the “Restart” button It can be useful to do this if you have tried to reset your conda environment to default and are still having problems. Exporting your Environment This is similar to making a packrat.lock file, it catalogues all of the\\ndependencies installed in your environment so that another user can restore a\\nworking environment for your application. Check this environment.yml file into\\nyour git repository. conda env export | grep -v \"^prefix: \" > environment.yml Making your R Studio Environment match an environment.yml When checking out a project that has an environment.yml , run the below command to install any packages required by the project that you don’t have in your working environment. conda env update -f environment.yml --prune Conda tips Conda version When you run conda (In R Studio at least) it says: ==> WARNING: A newer version of conda exists. < == current version: 4.7.5\\nlatest version: 4.8.3 Please update conda by running <span class=\"nv\">$ </span>conda update <span class=\"nt\">-n</span> base conda Please ignore this warning - this can only be done centrally by Analytical Platform team. If you try to upgrade conda yourself, it will fail: EnvironmentNotWritableError: The current user does not have write permissions to the target environment.\\nenvironment location: /opt/conda This is because conda is installed into the read-only part of the docker image. Users can only edit things in /home/$USER. Package installed with a different R version - when using conda Typical error output: > conda install ggplot2\\n...\\nError : package ‘tibble’ was installed by an R version with different internals ; it needs to be reinstalled for use with this R version It’s saying that this package, which is a dependency of the one you’re installing, was installed with an R version you used to have. To fix this, wipe your installed packages and reinstall them from your environment.yml. # reset your conda environment conda env export -n base| grep -v \"^prefix: \" > /tmp/base.yml && conda env update --prune -n rstudio -f /tmp/base.yml && rm /tmp/base.yml # reinstall packages conda env update -f environment.yml --prune Packrat NB Use of packrat is deprecated on the Analytical Platform - the guidance below is for information only because legacy projects may still use packrat . Packrat is the most well-known package management tool for R. There’s more information about it here: https://rstudio.github.io/packrat/ It has some significant downsides. It can be quite temperamental, and difficult to debug when things go wrong - in the earlier days of the Analytical Platform, the majority of support issues related to getting Packrat working. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 588}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ff32e2c43eca4a04048b80c27ee1b451'}>,\n",
       " <Document: {'content': 'Furthermore, the Analytical Platform version of RStudio runs on a Linux virtual machine, and CRAN mirrors do not provide Linux compiled binaries for packages. This means that packages need to be compiled on the Analytical Platform every time they’re installed, which can take a long time. This means a long wait when doing install.packages both in an RStudio session, and when running a Docker build for an RShiny application. Packrat usage To use packrat, ensure that it is enabled for your project in RStudio: select Tools > Project Options… > Packrat > Use packrat with this project . When packrat is enabled, run packrat::snapshot() to generate a list of packages used in the project, their sources and their current versions. You may also wish to run packrat::clean() to remove unused packages from the list. The list is stored in a file called packrat/packrat.lock . You must ensure that you have committed this file to GitHub before deploying your app. R’s install.packages() NB Only use this method for playing - use Conda for project work. You can install R packages from the R Console: install.packages ( \"ggplot2\" ) This will find the latest version of the package in CRAN and install it in: ~/R/library . However this method is pretty basic. Refer to the tips in the following sections. Package version incompatible with R version Often if you try to install the latest version of a package, it will require a more recent version of R than you have: > install.packages ( \"text2vec\" ) Installing package into ‘/home/davidread/R/library’ ( as ‘lib’ is unspecified ) Warning in install.packages :\\npackage ‘text2vec’ is not available ( for R version 3.5.1 ) There are a few options to avoid this: Solution 1: AP may have a newer version of RStudio tool which might have the version of R needed. To upgrade, see: Managing your analytical tools Solution 2: Use conda - it’s recommended for use with Analytical Platform in general. It works out which version is compatible with your R version (make sure you run this in the Terminal): conda install r-text2vec Solution 3: Specify a version that is compatible with your R version. e.g. at https://www.rdocumentation.org/packages/text2vec look at the “depends” field for the R version it requires. Change the version (drop-down at the top) to go back to see how it changes for older releases. You can see that text2vec 6.0 requires R (>= 3.6.0), but text2vec 5.1 requires only R (>= 3.2.0). devtools :: install_version ( \\'text2vec\\' , version = \\'0.5.1\\' ) Package installed with a different R version - when using install.packages() Typical error output > install.packages ( \"ggplot2\" ) ... Error : package ‘ tibble ’ was installed by an R version with different internals ; it needs to be reinstalled for use with this R version It’s saying that this package, which is a dependency of the one you’re installing, was installed with an R version you used to have. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 589}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '473ddd966fdd71e78af805e4b0cf64fc'}>,\n",
       " <Document: {'content': 'Solution 1 - You might fix this by installing the package it names: > install.packages ( \\'tibble\\' ) However you may have to do this for a lot of packages. Solution 2 - Wipe your packages and reinstall them. It begs the question of what you have installed. Although you can get a list it’s often unmanageably long, including all the little dependencies of what you actually installed in the first place. Best use conda next time! But you can get rid of all the installed packages (use the terminal): rm -rf ~/R/library/ * “Broken” packages (typically r-pillar ) When installing packages (e.g. during a concourse build of a webapp) you may see an error like this: $ conda env export -n base grep -v \"\" prefix: \" > /tmp/base.yml &\\nconda env update --prune -n rstudio -f /tmp/base.yml && rm /tmp/base.yml Collecting package metadata (repodata.json): done\\nSolving environment: failed\\nResolvePackageNotFound:\\n- r-pillar=1.4.2=h6115d3f_O This happens when a package on conda is marked as broken . r-pillar seems to suffer this frequently. To fix this there are a couple of things you can try: Remove r-pillar (or the offending package) from environment.yml. r-pillar is provided by the base conda environment and chances are that the user doesn’t need it in their app, so it can be safely removed. Update the version of r-pillar to the latest one on conda-forge. This page was last reviewed on 1 May 2022.\\n\\nIt needs to be reviewed again on 1 July 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 1 July 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nR Package Management - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 590}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3da162ab45d9dc0f203c68a80c0902b0'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 591}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools R package management There are multiple package managers available for RStudio depending on the version you are using: renv Conda packrat Why use a package manager? This enables analysts to maintain a reproducible workflow by including a snapshot of all packages used within a project saved within the project files themselves that can be loaded and installed with a single consistent and reproducible method. This means that if you create some code one day, you (or another analyst who comes after you) should be able to pick it up several years later and run it without any difficulty - even if the packages used have themselves changed in the meantime. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 592}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '775dbc223c61ad5f8824c790f2fc063c'}>,\n",
       " <Document: {'content': 'For Rstudio there is the added imperative to use a package manager (usually renv) because the analytical platform will remove installed packages when the docker image is restarted (which occurs automatically, roughly once a week). Renv Renv is the current standard for Rstudio on the Analytical Platform as it provides simpler package management than Conda or packrat which were previously recommended. The basic renv commands are: Command Description renv::init() first time a project is created renv::install() install new packages renv::snapshot() save a description of packages to renv.lock renv::restore() install packages to match renv.lock The following gives an overview of these basic renv commands.\\nFor more details check out the Coffee and Coding video and slides , or for a full guide to installing packages, workflow and installing custom packages please see the introduction to renv website . Getting started with renv If you are using version 4 or greater of R on the analytical platform then renv should work straightaway.\\nThe only other things to note if you’ve not used renv before are: The first time you use renv, you may be asked to consent to some changes it makes to the way packages are installed - please select yes to this. If you previously used a different package management system (like Conda or packrat) remove any configuration files for these systems from your R files first. Starting a new project with renv or adding renv to an existing project Basic commands to follow to install packages for renv are: # install renv (if not already installed) install.packages ( \"renv\" ) # If you are starting a fresh repository, run this: renv :: init ( bare = TRUE ) # or if you are starting a fresh repository but would like to move your existing packages over to renv: renv :: init () Then ensure you have committed and pushed the relevant files (.Rprofile, renv.lock, and renv/activate.R) to your github repository.\\nThese should be the only files which git suggests you commit - you should not commit the whole contents of the renv folder created when initialising a project. Now you are ready to work on your project! Working on a renv project You can work on your project as normal now, but when you install new packages and want to save the state of your package environment you must “snapshot” your packages. For instance, if you wanted to install dplyr and then update your package environment then the process would be: # install a package (the default is the latest available) renv :: install ( \"dplyr\" ) # or install a specific version of a package renv :: install ( \"dplyr@0.8.5\" ) # snapshot your project renv :: snapshot () # don’t forget to commit # renv.lock! You can use renv::install or install.packages - renv will intercept any calls to install.packages and runs renv::install under the hood anyway. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 593}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '641f6d8499188ac7dae5c147f95a0207'}>,\n",
       " <Document: {'content': 'Picking up a renv project If you pick up someone else’s project from github who has been using renv then simply run renv::restore() to update your local package environment so it matches the renv.lock file. # clone the project into # Rstudio # grab the packages renv :: restore () Any time you pull a commit where the renv.lock file has changed, you will need to renv::restore() in order to make sure your package enviroment matches to the new renv.lock file.\\nYou will also have to do this if you change branches in your repository to one with a different renv.lock file. Using renv with python If you are installing the recommended package for accessing data from s3, botor , you will need to do the following: renv :: use_python () # at the prompt, choose to use python3 renv :: install ( \\'reticulate\\' ) Restart the session (Ctrl+Alt+F10 on a windows machine). And then: reticulate :: py_install ( \\'boto3\\' ) renv :: install ( \\'botor\\' ) See the Renv Python documentation for further guidance. To activate Python integration within renv, type renv::use_python() Common pitfalls with renv Situation Why/What happens? Packages   disappearing You aren’t using renv! Forgetting to renv::snapshot() This won’t affect you running your   code, but anyone picking it up later will be out of sync. You can use renv::status() to check if packages and renv.lock match Switching branches If different package requirements in   branches then must remember to renv::restore() when switching between them – otherwise library reflects the previous branch Initialising renv   outside a project renv will ask you not to do this – do not   use force   = TRUE ! Stuck on old CRAN/MRAN Packages (or versions) you know exist   won’t appear using install functions. Run options(repos   = \"https://cloud.r-project.org/\") renv tips and tricks Situation Solution Got into a total mess? Start again! Run renv::deactivate() and then delete the renv.lock file and the renv/ folder Add a package from github Use renv::install(\"username/packagename\") or for a private package renv::install(\"git@github.com:username/packagename.git\") Upgrade all packages to latest Run renv::update() or renv::update(\"packagename\") for specific package. Always check that upgrading packages does not break your code before pushing to github for other users. Update renv itself renv::upgrade() . Useful if renv gains new functionality that you want to use. Error in file(filename, “r”, encoding = encoding) : cannot open the connection Oops, you’ve accidentally installed renv in your home directory 🏠 ! Delete all of the files created by renv from your home directory and retry. Conda NB Use of conda is now considered outdated for Rstudio on the Analytical Platform. When exploring this section, you may also find the slides from the Coffee and Coding session on conda useful. Conda is a unified package management system that supports managing both Python and R dependencies in a single environment . It can make sure all of these libraries are compatible with each other. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 594}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cbde220878e455d3a4ecd1598b4a5ffa'}>,\n",
       " <Document: {'content': 'Conda is available for both RStudio and JupyterLab on the Analytical Platform, though note that RStudio and JupyterLab have separate environments so dependencies won’t be shared between the applications. A key example within Analytical Services where conda is useful: both dbtools and s3tools rely on Python packages through the reticulate R-to-Python bridge. packrat only handles R dependencies; this means that packrat is not enough to reproducibly and reliably manage all of your application’s dependencies. Installing Packages The Anaconda organisation has its own repository of packages hosted on https://anaconda.org . If you need to find a package name you can use the anaconda search to find the package name. To install a package through conda, run the command conda install PACKAGENAME in the Terminal tab. This is recommended over using install.packages() as the package will be installed into the conda environment in a way that can be repeated when replicating the analysis - see Environment management section for more. Most (around 95%) R packages on CRAN are available through conda. They have the same name as the CRAN package name with an additional r- prefix. This is to avoid clashes with Python packages with the same name. Example In the terminal run: conda install numpy . You can now access in your R session: library ( reticulate ) np <- import ( \"numpy\" ) np $ arange ( 15 ) Comparison with install.packages() The following tables show conda commands and their base R analogues. Installing a package: install.packages (in R-Console) conda install (in Terminal) install.packages(\\'Rcpp\\') conda install r-Rcpp Installing a specific version of a package install.packages conda install require(devtools) install_version(\"ggplot2\", version = \"2.2.1\", repos = \"http://cran.us.r-project.org\") conda install r-ggplot2=2.2.1 You can also use conda to install Python packages, for use in R through the reticulate package. Python packages do not require a prefix and can simply be installed using theirname. Operating System Packages Even if you want to continue using packrat or renv to manage your R packages,  some packages have operating system-level dependencies, which can’t be handled by packrat / renv themselves. You can use conda to resolve these operating system dependencies, such as libxml2. Examples Installing a package that relies on OS dependency Suppose you want to install the R package bigIntegerAlgos , but it fails because it depends on a system level library called gmp . To resolve this, switch to the terminal and use conda to install it. Then switch back to the R console and try to use install.packages again. Environment Management You can use conda to make a snapshot of the environment you are using, so others can reproduce your results using the same versions of your code. Note: usually when using conda, it makes sense to have one environment per project,\\nbut because we are using the Open Source version of R Studio, there is only a\\nsingle conda environment available. This means having to be careful to make sure packages don’t pollute your environment from another project. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 595}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3b058fa0e6cb6175ac09648d371e53cc'}>,\n",
       " <Document: {'content': 'The following commands can be used to manage your environments. Reset your conda environment to default This will delete packages that you have installed in your rstudio conda environment, leaving only the base packages: conda env export -n base| grep -v \"^prefix: \" > /tmp/base.yml && conda env create --force -n rstudio -f /tmp/base.yml && rm /tmp/base.yml It is recommended to do this before starting a new project, to ensure that no unused dependencies are exported when you export an environment.yml for this project. Hard reset of your conda environment This will completely delete your rstudio conda environment, and recreate it with the base packages: Deleting all the files in the environment. For example, to clear the rstudio conda environment (which is the default one): rm -rf ~/.conda/envs/rstudio You might get errors about Directory not empty or Device or resource busy but usually these can be ignored - the bulk of these packages will be gone. In Control Panel, for R Studio, select the “Restart” button It can be useful to do this if you have tried to reset your conda environment to default and are still having problems. Exporting your Environment This is similar to making a packrat.lock file, it catalogues all of the\\ndependencies installed in your environment so that another user can restore a\\nworking environment for your application. Check this environment.yml file into\\nyour git repository. conda env export | grep -v \"^prefix: \" > environment.yml Making your R Studio Environment match an environment.yml When checking out a project that has an environment.yml , run the below command to install any packages required by the project that you don’t have in your working environment. conda env update -f environment.yml --prune Conda tips Conda version When you run conda (In R Studio at least) it says: ==> WARNING: A newer version of conda exists. < == current version: 4.7.5\\nlatest version: 4.8.3 Please update conda by running <span class=\"nv\">$ </span>conda update <span class=\"nt\">-n</span> base conda Please ignore this warning - this can only be done centrally by Analytical Platform team. If you try to upgrade conda yourself, it will fail: EnvironmentNotWritableError: The current user does not have write permissions to the target environment.\\nenvironment location: /opt/conda This is because conda is installed into the read-only part of the docker image. Users can only edit things in /home/$USER. Package installed with a different R version - when using conda Typical error output: > conda install ggplot2\\n...\\nError : package ‘tibble’ was installed by an R version with different internals ; it needs to be reinstalled for use with this R version It’s saying that this package, which is a dependency of the one you’re installing, was installed with an R version you used to have. To fix this, wipe your installed packages and reinstall them from your environment.yml. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 596}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1df8a8259d136c54232dbbb7de9f4614'}>,\n",
       " <Document: {'content': '# reset your conda environment conda env export -n base| grep -v \"^prefix: \" > /tmp/base.yml && conda env update --prune -n rstudio -f /tmp/base.yml && rm /tmp/base.yml # reinstall packages conda env update -f environment.yml --prune Packrat NB Use of packrat is deprecated on the Analytical Platform - the guidance below is for information only because legacy projects may still use packrat . Packrat is the most well-known package management tool for R. There’s more information about it here: https://rstudio.github.io/packrat/ It has some significant downsides. It can be quite temperamental, and difficult to debug when things go wrong - in the earlier days of the Analytical Platform, the majority of support issues related to getting Packrat working. Furthermore, the Analytical Platform version of RStudio runs on a Linux virtual machine, and CRAN mirrors do not provide Linux compiled binaries for packages. This means that packages need to be compiled on the Analytical Platform every time they’re installed, which can take a long time. This means a long wait when doing install.packages both in an RStudio session, and when running a Docker build for an RShiny application. Packrat usage To use packrat, ensure that it is enabled for your project in RStudio: select Tools > Project Options… > Packrat > Use packrat with this project . When packrat is enabled, run packrat::snapshot() to generate a list of packages used in the project, their sources and their current versions. You may also wish to run packrat::clean() to remove unused packages from the list. The list is stored in a file called packrat/packrat.lock . You must ensure that you have committed this file to GitHub before deploying your app. R’s install.packages() NB Only use this method for playing - use Conda for project work. You can install R packages from the R Console: install.packages ( \"ggplot2\" ) This will find the latest version of the package in CRAN and install it in: ~/R/library . However this method is pretty basic. Refer to the tips in the following sections. Package version incompatible with R version Often if you try to install the latest version of a package, it will require a more recent version of R than you have: > install.packages ( \"text2vec\" ) Installing package into ‘/home/davidread/R/library’ ( as ‘lib’ is unspecified ) Warning in install.packages :\\npackage ‘text2vec’ is not available ( for R version 3.5.1 ) There are a few options to avoid this: Solution 1: AP may have a newer version of RStudio tool which might have the version of R needed. To upgrade, see: Managing your analytical tools Solution 2: Use conda - it’s recommended for use with Analytical Platform in general. It works out which version is compatible with your R version (make sure you run this in the Terminal): conda install r-text2vec Solution 3: Specify a version that is compatible with your R version. e.g. at https://www.rdocumentation.org/packages/text2vec look at the “depends” field for the R version it requires. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 597}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '362741613a46bfd9650b6f9e5f78a056'}>,\n",
       " <Document: {'content': 'Change the version (drop-down at the top) to go back to see how it changes for older releases. You can see that text2vec 6.0 requires R (>= 3.6.0), but text2vec 5.1 requires only R (>= 3.2.0). devtools :: install_version ( \\'text2vec\\' , version = \\'0.5.1\\' ) Package installed with a different R version - when using install.packages() Typical error output > install.packages ( \"ggplot2\" ) ... Error : package ‘ tibble ’ was installed by an R version with different internals ; it needs to be reinstalled for use with this R version It’s saying that this package, which is a dependency of the one you’re installing, was installed with an R version you used to have. Solution 1 - You might fix this by installing the package it names: > install.packages ( \\'tibble\\' ) However you may have to do this for a lot of packages. Solution 2 - Wipe your packages and reinstall them. It begs the question of what you have installed. Although you can get a list it’s often unmanageably long, including all the little dependencies of what you actually installed in the first place. Best use conda next time! But you can get rid of all the installed packages (use the terminal): rm -rf ~/R/library/ * “Broken” packages (typically r-pillar ) When installing packages (e.g. during a concourse build of a webapp) you may see an error like this: $ conda env export -n base grep -v \"\" prefix: \" > /tmp/base.yml &\\nconda env update --prune -n rstudio -f /tmp/base.yml && rm /tmp/base.yml Collecting package metadata (repodata.json): done\\nSolving environment: failed\\nResolvePackageNotFound:\\n- r-pillar=1.4.2=h6115d3f_O This happens when a package on conda is marked as broken . r-pillar seems to suffer this frequently. To fix this there are a couple of things you can try: Remove r-pillar (or the offending package) from environment.yml. r-pillar is provided by the base conda environment and chances are that the user doesn’t need it in their app, so it can be safely removed. Update the version of r-pillar to the latest one on conda-forge. This page was last reviewed on 1 May 2022.\\n\\nIt needs to be reviewed again on 1 July 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 1 July 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nR Package Management - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 598}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8c36d0bd5431ab4da3c592cc97d7209c'}>,\n",
       " <Document: {'content': 'Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 599}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9ba4b74f4fa634ec10c04b4f7c578f23'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools R package management There are multiple package managers available for RStudio depending on the version you are using: renv Conda packrat Why use a package manager? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 600}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c44a54c47fec23e780b109aec221d067'}>,\n",
       " <Document: {'content': 'This enables analysts to maintain a reproducible workflow by including a snapshot of all packages used within a project saved within the project files themselves that can be loaded and installed with a single consistent and reproducible method. This means that if you create some code one day, you (or another analyst who comes after you) should be able to pick it up several years later and run it without any difficulty - even if the packages used have themselves changed in the meantime. For Rstudio there is the added imperative to use a package manager (usually renv) because the analytical platform will remove installed packages when the docker image is restarted (which occurs automatically, roughly once a week). Renv Renv is the current standard for Rstudio on the Analytical Platform as it provides simpler package management than Conda or packrat which were previously recommended. The basic renv commands are: Command Description renv::init() first time a project is created renv::install() install new packages renv::snapshot() save a description of packages to renv.lock renv::restore() install packages to match renv.lock The following gives an overview of these basic renv commands.\\nFor more details check out the Coffee and Coding video and slides , or for a full guide to installing packages, workflow and installing custom packages please see the introduction to renv website . Getting started with renv If you are using version 4 or greater of R on the analytical platform then renv should work straightaway.\\nThe only other things to note if you’ve not used renv before are: The first time you use renv, you may be asked to consent to some changes it makes to the way packages are installed - please select yes to this. If you previously used a different package management system (like Conda or packrat) remove any configuration files for these systems from your R files first. Starting a new project with renv or adding renv to an existing project Basic commands to follow to install packages for renv are: # install renv (if not already installed) install.packages ( \"renv\" ) # If you are starting a fresh repository, run this: renv :: init ( bare = TRUE ) # or if you are starting a fresh repository but would like to move your existing packages over to renv: renv :: init () Then ensure you have committed and pushed the relevant files (.Rprofile, renv.lock, and renv/activate.R) to your github repository.\\nThese should be the only files which git suggests you commit - you should not commit the whole contents of the renv folder created when initialising a project. Now you are ready to work on your project! Working on a renv project You can work on your project as normal now, but when you install new packages and want to save the state of your package environment you must “snapshot” your packages. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 601}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '86f5f252ed9f2c06f53b08083388e47'}>,\n",
       " <Document: {'content': 'For instance, if you wanted to install dplyr and then update your package environment then the process would be: # install a package (the default is the latest available) renv :: install ( \"dplyr\" ) # or install a specific version of a package renv :: install ( \"dplyr@0.8.5\" ) # snapshot your project renv :: snapshot () # don’t forget to commit # renv.lock! You can use renv::install or install.packages - renv will intercept any calls to install.packages and runs renv::install under the hood anyway. Picking up a renv project If you pick up someone else’s project from github who has been using renv then simply run renv::restore() to update your local package environment so it matches the renv.lock file. # clone the project into # Rstudio # grab the packages renv :: restore () Any time you pull a commit where the renv.lock file has changed, you will need to renv::restore() in order to make sure your package enviroment matches to the new renv.lock file.\\nYou will also have to do this if you change branches in your repository to one with a different renv.lock file. Using renv with python If you are installing the recommended package for accessing data from s3, botor , you will need to do the following: renv :: use_python () # at the prompt, choose to use python3 renv :: install ( \\'reticulate\\' ) Restart the session (Ctrl+Alt+F10 on a windows machine). And then: reticulate :: py_install ( \\'boto3\\' ) renv :: install ( \\'botor\\' ) See the Renv Python documentation for further guidance. To activate Python integration within renv, type renv::use_python() Common pitfalls with renv Situation Why/What happens? Packages   disappearing You aren’t using renv! Forgetting to renv::snapshot() This won’t affect you running your   code, but anyone picking it up later will be out of sync. You can use renv::status() to check if packages and renv.lock match Switching branches If different package requirements in   branches then must remember to renv::restore() when switching between them – otherwise library reflects the previous branch Initialising renv   outside a project renv will ask you not to do this – do not   use force   = TRUE ! Stuck on old CRAN/MRAN Packages (or versions) you know exist   won’t appear using install functions. Run options(repos   = \"https://cloud.r-project.org/\") renv tips and tricks Situation Solution Got into a total mess? Start again! Run renv::deactivate() and then delete the renv.lock file and the renv/ folder Add a package from github Use renv::install(\"username/packagename\") or for a private package renv::install(\"git@github.com:username/packagename.git\") Upgrade all packages to latest Run renv::update() or renv::update(\"packagename\") for specific package. Always check that upgrading packages does not break your code before pushing to github for other users. Update renv itself renv::upgrade() . Useful if renv gains new functionality that you want to use. Error in file(filename, “r”, encoding = encoding) : cannot open the connection Oops, you’ve accidentally installed renv in your home directory 🏠 ! Delete all of the files created by renv from your home directory and retry. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 602}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e447cedcf0626b5ee10171d41d6079a7'}>,\n",
       " <Document: {'content': 'Conda NB Use of conda is now considered outdated for Rstudio on the Analytical Platform. When exploring this section, you may also find the slides from the Coffee and Coding session on conda useful. Conda is a unified package management system that supports managing both Python and R dependencies in a single environment . It can make sure all of these libraries are compatible with each other. Conda is available for both RStudio and JupyterLab on the Analytical Platform, though note that RStudio and JupyterLab have separate environments so dependencies won’t be shared between the applications. A key example within Analytical Services where conda is useful: both dbtools and s3tools rely on Python packages through the reticulate R-to-Python bridge. packrat only handles R dependencies; this means that packrat is not enough to reproducibly and reliably manage all of your application’s dependencies. Installing Packages The Anaconda organisation has its own repository of packages hosted on https://anaconda.org . If you need to find a package name you can use the anaconda search to find the package name. To install a package through conda, run the command conda install PACKAGENAME in the Terminal tab. This is recommended over using install.packages() as the package will be installed into the conda environment in a way that can be repeated when replicating the analysis - see Environment management section for more. Most (around 95%) R packages on CRAN are available through conda. They have the same name as the CRAN package name with an additional r- prefix. This is to avoid clashes with Python packages with the same name. Example In the terminal run: conda install numpy . You can now access in your R session: library ( reticulate ) np <- import ( \"numpy\" ) np $ arange ( 15 ) Comparison with install.packages() The following tables show conda commands and their base R analogues. Installing a package: install.packages (in R-Console) conda install (in Terminal) install.packages(\\'Rcpp\\') conda install r-Rcpp Installing a specific version of a package install.packages conda install require(devtools) install_version(\"ggplot2\", version = \"2.2.1\", repos = \"http://cran.us.r-project.org\") conda install r-ggplot2=2.2.1 You can also use conda to install Python packages, for use in R through the reticulate package. Python packages do not require a prefix and can simply be installed using theirname. Operating System Packages Even if you want to continue using packrat or renv to manage your R packages,  some packages have operating system-level dependencies, which can’t be handled by packrat / renv themselves. You can use conda to resolve these operating system dependencies, such as libxml2. Examples Installing a package that relies on OS dependency Suppose you want to install the R package bigIntegerAlgos , but it fails because it depends on a system level library called gmp . To resolve this, switch to the terminal and use conda to install it. Then switch back to the R console and try to use install.packages again. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 603}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2c083f67ece32cc29f1ea076a1afd58'}>,\n",
       " <Document: {'content': 'Environment Management You can use conda to make a snapshot of the environment you are using, so others can reproduce your results using the same versions of your code. Note: usually when using conda, it makes sense to have one environment per project,\\nbut because we are using the Open Source version of R Studio, there is only a\\nsingle conda environment available. This means having to be careful to make sure packages don’t pollute your environment from another project. The following commands can be used to manage your environments. Reset your conda environment to default This will delete packages that you have installed in your rstudio conda environment, leaving only the base packages: conda env export -n base| grep -v \"^prefix: \" > /tmp/base.yml && conda env create --force -n rstudio -f /tmp/base.yml && rm /tmp/base.yml It is recommended to do this before starting a new project, to ensure that no unused dependencies are exported when you export an environment.yml for this project. Hard reset of your conda environment This will completely delete your rstudio conda environment, and recreate it with the base packages: Deleting all the files in the environment. For example, to clear the rstudio conda environment (which is the default one): rm -rf ~/.conda/envs/rstudio You might get errors about Directory not empty or Device or resource busy but usually these can be ignored - the bulk of these packages will be gone. In Control Panel, for R Studio, select the “Restart” button It can be useful to do this if you have tried to reset your conda environment to default and are still having problems. Exporting your Environment This is similar to making a packrat.lock file, it catalogues all of the\\ndependencies installed in your environment so that another user can restore a\\nworking environment for your application. Check this environment.yml file into\\nyour git repository. conda env export | grep -v \"^prefix: \" > environment.yml Making your R Studio Environment match an environment.yml When checking out a project that has an environment.yml , run the below command to install any packages required by the project that you don’t have in your working environment. conda env update -f environment.yml --prune Conda tips Conda version When you run conda (In R Studio at least) it says: ==> WARNING: A newer version of conda exists. < == current version: 4.7.5\\nlatest version: 4.8.3 Please update conda by running <span class=\"nv\">$ </span>conda update <span class=\"nt\">-n</span> base conda Please ignore this warning - this can only be done centrally by Analytical Platform team. If you try to upgrade conda yourself, it will fail: EnvironmentNotWritableError: The current user does not have write permissions to the target environment.\\nenvironment location: /opt/conda This is because conda is installed into the read-only part of the docker image. Users can only edit things in /home/$USER. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 604}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b8c2363a87d3406501894d8a6ff97ef9'}>,\n",
       " <Document: {'content': 'Package installed with a different R version - when using conda Typical error output: > conda install ggplot2\\n...\\nError : package ‘tibble’ was installed by an R version with different internals ; it needs to be reinstalled for use with this R version It’s saying that this package, which is a dependency of the one you’re installing, was installed with an R version you used to have. To fix this, wipe your installed packages and reinstall them from your environment.yml. # reset your conda environment conda env export -n base| grep -v \"^prefix: \" > /tmp/base.yml && conda env update --prune -n rstudio -f /tmp/base.yml && rm /tmp/base.yml # reinstall packages conda env update -f environment.yml --prune Packrat NB Use of packrat is deprecated on the Analytical Platform - the guidance below is for information only because legacy projects may still use packrat . Packrat is the most well-known package management tool for R. There’s more information about it here: https://rstudio.github.io/packrat/ It has some significant downsides. It can be quite temperamental, and difficult to debug when things go wrong - in the earlier days of the Analytical Platform, the majority of support issues related to getting Packrat working. Furthermore, the Analytical Platform version of RStudio runs on a Linux virtual machine, and CRAN mirrors do not provide Linux compiled binaries for packages. This means that packages need to be compiled on the Analytical Platform every time they’re installed, which can take a long time. This means a long wait when doing install.packages both in an RStudio session, and when running a Docker build for an RShiny application. Packrat usage To use packrat, ensure that it is enabled for your project in RStudio: select Tools > Project Options… > Packrat > Use packrat with this project . When packrat is enabled, run packrat::snapshot() to generate a list of packages used in the project, their sources and their current versions. You may also wish to run packrat::clean() to remove unused packages from the list. The list is stored in a file called packrat/packrat.lock . You must ensure that you have committed this file to GitHub before deploying your app. R’s install.packages() NB Only use this method for playing - use Conda for project work. You can install R packages from the R Console: install.packages ( \"ggplot2\" ) This will find the latest version of the package in CRAN and install it in: ~/R/library . However this method is pretty basic. Refer to the tips in the following sections. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 605}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7ade713f6a0ea1efb4e60bc9a0aae4eb'}>,\n",
       " <Document: {'content': 'Package version incompatible with R version Often if you try to install the latest version of a package, it will require a more recent version of R than you have: > install.packages ( \"text2vec\" ) Installing package into ‘/home/davidread/R/library’ ( as ‘lib’ is unspecified ) Warning in install.packages :\\npackage ‘text2vec’ is not available ( for R version 3.5.1 ) There are a few options to avoid this: Solution 1: AP may have a newer version of RStudio tool which might have the version of R needed. To upgrade, see: Managing your analytical tools Solution 2: Use conda - it’s recommended for use with Analytical Platform in general. It works out which version is compatible with your R version (make sure you run this in the Terminal): conda install r-text2vec Solution 3: Specify a version that is compatible with your R version. e.g. at https://www.rdocumentation.org/packages/text2vec look at the “depends” field for the R version it requires. Change the version (drop-down at the top) to go back to see how it changes for older releases. You can see that text2vec 6.0 requires R (>= 3.6.0), but text2vec 5.1 requires only R (>= 3.2.0). devtools :: install_version ( \\'text2vec\\' , version = \\'0.5.1\\' ) Package installed with a different R version - when using install.packages() Typical error output > install.packages ( \"ggplot2\" ) ... Error : package ‘ tibble ’ was installed by an R version with different internals ; it needs to be reinstalled for use with this R version It’s saying that this package, which is a dependency of the one you’re installing, was installed with an R version you used to have. Solution 1 - You might fix this by installing the package it names: > install.packages ( \\'tibble\\' ) However you may have to do this for a lot of packages. Solution 2 - Wipe your packages and reinstall them. It begs the question of what you have installed. Although you can get a list it’s often unmanageably long, including all the little dependencies of what you actually installed in the first place. Best use conda next time! But you can get rid of all the installed packages (use the terminal): rm -rf ~/R/library/ * “Broken” packages (typically r-pillar ) When installing packages (e.g. during a concourse build of a webapp) you may see an error like this: $ conda env export -n base grep -v \"\" prefix: \" > /tmp/base.yml &\\nconda env update --prune -n rstudio -f /tmp/base.yml && rm /tmp/base.yml Collecting package metadata (repodata.json): done\\nSolving environment: failed\\nResolvePackageNotFound:\\n- r-pillar=1.4.2=h6115d3f_O This happens when a package on conda is marked as broken . r-pillar seems to suffer this frequently. To fix this there are a couple of things you can try: Remove r-pillar (or the offending package) from environment.yml. r-pillar is provided by the base conda environment and chances are that the user doesn’t need it in their app, so it can be safely removed. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 606}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6fb23112e92bbd06427e430ba9556f49'}>,\n",
       " <Document: {'content': 'Update the version of r-pillar to the latest one on conda-forge. This page was last reviewed on 1 May 2022.\\n\\nIt needs to be reviewed again on 1 July 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 1 July 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nR Package Management - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 607}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ac4ffe67db5b18155c16e514c04c4c11'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 608}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools R package management There are multiple package managers available for RStudio depending on the version you are using: renv Conda packrat Why use a package manager? This enables analysts to maintain a reproducible workflow by including a snapshot of all packages used within a project saved within the project files themselves that can be loaded and installed with a single consistent and reproducible method. This means that if you create some code one day, you (or another analyst who comes after you) should be able to pick it up several years later and run it without any difficulty - even if the packages used have themselves changed in the meantime. For Rstudio there is the added imperative to use a package manager (usually renv) because the analytical platform will remove installed packages when the docker image is restarted (which occurs automatically, roughly once a week). Renv Renv is the current standard for Rstudio on the Analytical Platform as it provides simpler package management than Conda or packrat which were previously recommended. The basic renv commands are: Command Description renv::init() first time a project is created renv::install() install new packages renv::snapshot() save a description of packages to renv.lock renv::restore() install packages to match renv.lock The following gives an overview of these basic renv commands.\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 609}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c997c68a844599a86aa8d0287700ef7c'}>,\n",
       " <Document: {'content': 'For more details check out the Coffee and Coding video and slides , or for a full guide to installing packages, workflow and installing custom packages please see the introduction to renv website . Getting started with renv If you are using version 4 or greater of R on the analytical platform then renv should work straightaway.\\nThe only other things to note if you’ve not used renv before are: The first time you use renv, you may be asked to consent to some changes it makes to the way packages are installed - please select yes to this. If you previously used a different package management system (like Conda or packrat) remove any configuration files for these systems from your R files first. Starting a new project with renv or adding renv to an existing project Basic commands to follow to install packages for renv are: # install renv (if not already installed) install.packages ( \"renv\" ) # If you are starting a fresh repository, run this: renv :: init ( bare = TRUE ) # or if you are starting a fresh repository but would like to move your existing packages over to renv: renv :: init () Then ensure you have committed and pushed the relevant files (.Rprofile, renv.lock, and renv/activate.R) to your github repository.\\nThese should be the only files which git suggests you commit - you should not commit the whole contents of the renv folder created when initialising a project. Now you are ready to work on your project! Working on a renv project You can work on your project as normal now, but when you install new packages and want to save the state of your package environment you must “snapshot” your packages. For instance, if you wanted to install dplyr and then update your package environment then the process would be: # install a package (the default is the latest available) renv :: install ( \"dplyr\" ) # or install a specific version of a package renv :: install ( \"dplyr@0.8.5\" ) # snapshot your project renv :: snapshot () # don’t forget to commit # renv.lock! You can use renv::install or install.packages - renv will intercept any calls to install.packages and runs renv::install under the hood anyway. Picking up a renv project If you pick up someone else’s project from github who has been using renv then simply run renv::restore() to update your local package environment so it matches the renv.lock file. # clone the project into # Rstudio # grab the packages renv :: restore () Any time you pull a commit where the renv.lock file has changed, you will need to renv::restore() in order to make sure your package enviroment matches to the new renv.lock file.\\nYou will also have to do this if you change branches in your repository to one with a different renv.lock file. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 610}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '47deb0f931b7269fa4196f7fc9c57867'}>,\n",
       " <Document: {'content': 'Using renv with python If you are installing the recommended package for accessing data from s3, botor , you will need to do the following: renv :: use_python () # at the prompt, choose to use python3 renv :: install ( \\'reticulate\\' ) Restart the session (Ctrl+Alt+F10 on a windows machine). And then: reticulate :: py_install ( \\'boto3\\' ) renv :: install ( \\'botor\\' ) See the Renv Python documentation for further guidance. To activate Python integration within renv, type renv::use_python() Common pitfalls with renv Situation Why/What happens? Packages   disappearing You aren’t using renv! Forgetting to renv::snapshot() This won’t affect you running your   code, but anyone picking it up later will be out of sync. You can use renv::status() to check if packages and renv.lock match Switching branches If different package requirements in   branches then must remember to renv::restore() when switching between them – otherwise library reflects the previous branch Initialising renv   outside a project renv will ask you not to do this – do not   use force   = TRUE ! Stuck on old CRAN/MRAN Packages (or versions) you know exist   won’t appear using install functions. Run options(repos   = \"https://cloud.r-project.org/\") renv tips and tricks Situation Solution Got into a total mess? Start again! Run renv::deactivate() and then delete the renv.lock file and the renv/ folder Add a package from github Use renv::install(\"username/packagename\") or for a private package renv::install(\"git@github.com:username/packagename.git\") Upgrade all packages to latest Run renv::update() or renv::update(\"packagename\") for specific package. Always check that upgrading packages does not break your code before pushing to github for other users. Update renv itself renv::upgrade() . Useful if renv gains new functionality that you want to use. Error in file(filename, “r”, encoding = encoding) : cannot open the connection Oops, you’ve accidentally installed renv in your home directory 🏠 ! Delete all of the files created by renv from your home directory and retry. Conda NB Use of conda is now considered outdated for Rstudio on the Analytical Platform. When exploring this section, you may also find the slides from the Coffee and Coding session on conda useful. Conda is a unified package management system that supports managing both Python and R dependencies in a single environment . It can make sure all of these libraries are compatible with each other. Conda is available for both RStudio and JupyterLab on the Analytical Platform, though note that RStudio and JupyterLab have separate environments so dependencies won’t be shared between the applications. A key example within Analytical Services where conda is useful: both dbtools and s3tools rely on Python packages through the reticulate R-to-Python bridge. packrat only handles R dependencies; this means that packrat is not enough to reproducibly and reliably manage all of your application’s dependencies. Installing Packages The Anaconda organisation has its own repository of packages hosted on https://anaconda.org . If you need to find a package name you can use the anaconda search to find the package name. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 611}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f3818bd8baa493220244329b8806cba2'}>,\n",
       " <Document: {'content': 'To install a package through conda, run the command conda install PACKAGENAME in the Terminal tab. This is recommended over using install.packages() as the package will be installed into the conda environment in a way that can be repeated when replicating the analysis - see Environment management section for more. Most (around 95%) R packages on CRAN are available through conda. They have the same name as the CRAN package name with an additional r- prefix. This is to avoid clashes with Python packages with the same name. Example In the terminal run: conda install numpy . You can now access in your R session: library ( reticulate ) np <- import ( \"numpy\" ) np $ arange ( 15 ) Comparison with install.packages() The following tables show conda commands and their base R analogues. Installing a package: install.packages (in R-Console) conda install (in Terminal) install.packages(\\'Rcpp\\') conda install r-Rcpp Installing a specific version of a package install.packages conda install require(devtools) install_version(\"ggplot2\", version = \"2.2.1\", repos = \"http://cran.us.r-project.org\") conda install r-ggplot2=2.2.1 You can also use conda to install Python packages, for use in R through the reticulate package. Python packages do not require a prefix and can simply be installed using theirname. Operating System Packages Even if you want to continue using packrat or renv to manage your R packages,  some packages have operating system-level dependencies, which can’t be handled by packrat / renv themselves. You can use conda to resolve these operating system dependencies, such as libxml2. Examples Installing a package that relies on OS dependency Suppose you want to install the R package bigIntegerAlgos , but it fails because it depends on a system level library called gmp . To resolve this, switch to the terminal and use conda to install it. Then switch back to the R console and try to use install.packages again. Environment Management You can use conda to make a snapshot of the environment you are using, so others can reproduce your results using the same versions of your code. Note: usually when using conda, it makes sense to have one environment per project,\\nbut because we are using the Open Source version of R Studio, there is only a\\nsingle conda environment available. This means having to be careful to make sure packages don’t pollute your environment from another project. The following commands can be used to manage your environments. Reset your conda environment to default This will delete packages that you have installed in your rstudio conda environment, leaving only the base packages: conda env export -n base| grep -v \"^prefix: \" > /tmp/base.yml && conda env create --force -n rstudio -f /tmp/base.yml && rm /tmp/base.yml It is recommended to do this before starting a new project, to ensure that no unused dependencies are exported when you export an environment.yml for this project. Hard reset of your conda environment This will completely delete your rstudio conda environment, and recreate it with the base packages: Deleting all the files in the environment. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 612}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4fee1914f0561da65d11c015aff1ca3b'}>,\n",
       " <Document: {'content': 'For example, to clear the rstudio conda environment (which is the default one): rm -rf ~/.conda/envs/rstudio You might get errors about Directory not empty or Device or resource busy but usually these can be ignored - the bulk of these packages will be gone. In Control Panel, for R Studio, select the “Restart” button It can be useful to do this if you have tried to reset your conda environment to default and are still having problems. Exporting your Environment This is similar to making a packrat.lock file, it catalogues all of the\\ndependencies installed in your environment so that another user can restore a\\nworking environment for your application. Check this environment.yml file into\\nyour git repository. conda env export | grep -v \"^prefix: \" > environment.yml Making your R Studio Environment match an environment.yml When checking out a project that has an environment.yml , run the below command to install any packages required by the project that you don’t have in your working environment. conda env update -f environment.yml --prune Conda tips Conda version When you run conda (In R Studio at least) it says: ==> WARNING: A newer version of conda exists. < == current version: 4.7.5\\nlatest version: 4.8.3 Please update conda by running <span class=\"nv\">$ </span>conda update <span class=\"nt\">-n</span> base conda Please ignore this warning - this can only be done centrally by Analytical Platform team. If you try to upgrade conda yourself, it will fail: EnvironmentNotWritableError: The current user does not have write permissions to the target environment.\\nenvironment location: /opt/conda This is because conda is installed into the read-only part of the docker image. Users can only edit things in /home/$USER. Package installed with a different R version - when using conda Typical error output: > conda install ggplot2\\n...\\nError : package ‘tibble’ was installed by an R version with different internals ; it needs to be reinstalled for use with this R version It’s saying that this package, which is a dependency of the one you’re installing, was installed with an R version you used to have. To fix this, wipe your installed packages and reinstall them from your environment.yml. # reset your conda environment conda env export -n base| grep -v \"^prefix: \" > /tmp/base.yml && conda env update --prune -n rstudio -f /tmp/base.yml && rm /tmp/base.yml # reinstall packages conda env update -f environment.yml --prune Packrat NB Use of packrat is deprecated on the Analytical Platform - the guidance below is for information only because legacy projects may still use packrat . Packrat is the most well-known package management tool for R. There’s more information about it here: https://rstudio.github.io/packrat/ It has some significant downsides. It can be quite temperamental, and difficult to debug when things go wrong - in the earlier days of the Analytical Platform, the majority of support issues related to getting Packrat working. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 613}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ff32e2c43eca4a04048b80c27ee1b451'}>,\n",
       " <Document: {'content': 'Furthermore, the Analytical Platform version of RStudio runs on a Linux virtual machine, and CRAN mirrors do not provide Linux compiled binaries for packages. This means that packages need to be compiled on the Analytical Platform every time they’re installed, which can take a long time. This means a long wait when doing install.packages both in an RStudio session, and when running a Docker build for an RShiny application. Packrat usage To use packrat, ensure that it is enabled for your project in RStudio: select Tools > Project Options… > Packrat > Use packrat with this project . When packrat is enabled, run packrat::snapshot() to generate a list of packages used in the project, their sources and their current versions. You may also wish to run packrat::clean() to remove unused packages from the list. The list is stored in a file called packrat/packrat.lock . You must ensure that you have committed this file to GitHub before deploying your app. R’s install.packages() NB Only use this method for playing - use Conda for project work. You can install R packages from the R Console: install.packages ( \"ggplot2\" ) This will find the latest version of the package in CRAN and install it in: ~/R/library . However this method is pretty basic. Refer to the tips in the following sections. Package version incompatible with R version Often if you try to install the latest version of a package, it will require a more recent version of R than you have: > install.packages ( \"text2vec\" ) Installing package into ‘/home/davidread/R/library’ ( as ‘lib’ is unspecified ) Warning in install.packages :\\npackage ‘text2vec’ is not available ( for R version 3.5.1 ) There are a few options to avoid this: Solution 1: AP may have a newer version of RStudio tool which might have the version of R needed. To upgrade, see: Managing your analytical tools Solution 2: Use conda - it’s recommended for use with Analytical Platform in general. It works out which version is compatible with your R version (make sure you run this in the Terminal): conda install r-text2vec Solution 3: Specify a version that is compatible with your R version. e.g. at https://www.rdocumentation.org/packages/text2vec look at the “depends” field for the R version it requires. Change the version (drop-down at the top) to go back to see how it changes for older releases. You can see that text2vec 6.0 requires R (>= 3.6.0), but text2vec 5.1 requires only R (>= 3.2.0). devtools :: install_version ( \\'text2vec\\' , version = \\'0.5.1\\' ) Package installed with a different R version - when using install.packages() Typical error output > install.packages ( \"ggplot2\" ) ... Error : package ‘ tibble ’ was installed by an R version with different internals ; it needs to be reinstalled for use with this R version It’s saying that this package, which is a dependency of the one you’re installing, was installed with an R version you used to have. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 614}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '473ddd966fdd71e78af805e4b0cf64fc'}>,\n",
       " <Document: {'content': 'Solution 1 - You might fix this by installing the package it names: > install.packages ( \\'tibble\\' ) However you may have to do this for a lot of packages. Solution 2 - Wipe your packages and reinstall them. It begs the question of what you have installed. Although you can get a list it’s often unmanageably long, including all the little dependencies of what you actually installed in the first place. Best use conda next time! But you can get rid of all the installed packages (use the terminal): rm -rf ~/R/library/ * “Broken” packages (typically r-pillar ) When installing packages (e.g. during a concourse build of a webapp) you may see an error like this: $ conda env export -n base grep -v \"\" prefix: \" > /tmp/base.yml &\\nconda env update --prune -n rstudio -f /tmp/base.yml && rm /tmp/base.yml Collecting package metadata (repodata.json): done\\nSolving environment: failed\\nResolvePackageNotFound:\\n- r-pillar=1.4.2=h6115d3f_O This happens when a package on conda is marked as broken . r-pillar seems to suffer this frequently. To fix this there are a couple of things you can try: Remove r-pillar (or the offending package) from environment.yml. r-pillar is provided by the base conda environment and chances are that the user doesn’t need it in their app, so it can be safely removed. Update the version of r-pillar to the latest one on conda-forge. This page was last reviewed on 1 May 2022.\\n\\nIt needs to be reviewed again on 1 July 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 1 July 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nUpgrading RStudio - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 615}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e32429953f0e5464173ed1e0a27270ef'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 616}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Upgrading RStudio When moving between RStudio releases there are some differences that require either user intervention or caution. Why should I upgrade? Upgrading to the latest version of RStudio has a number of benefits, including a new version of R and improved package management options. 2.2.6 to 3.0.12 Key Points: Conda has been replaced with Renv for package management RStudio 1.4 R version 4.0.3 Conda has now been removed. You will need to install packages using Renv. See package management Preparation Please unidle your RStudio if it is currently idled Select RStudio 3.0.12 from the drop down and click yes on the prompt to install Notable Packages dbtools dbtools (i.e. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 617}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ade8751dd89825278b50f33d917f6017'}>,\n",
       " <Document: {'content': 'the R wrapper for pydbtools ) is the data engineering maintained package for accessing Athena databases from R. It requires a little setup: Call renv::use_python() This will create a\\xa0virtualenv\\xa0for your project, which should be stored in the folder\\xa0renv/python/virtualenvs/renv-python-3.8.5/, and associate\\xa0reticulate\\xa0with that virtualenv. Install pydbtools into that environment You can either do that with reticulate::py_install(\"pydbtools\") , or if you prefer the terminal, you can activate that virtualenv with the command\\xa0renv/python/virtualenvs/renv-python-3.8.5/bin/activate. Install dbtools using remotes::install_github(\"moj-analytical-services/dbtools\") This page was last reviewed on 1 May 2022.\\n\\nIt needs to be reviewed again on 1 July 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 1 July 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nUpgrading RStudio - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 618}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'daf60a48bab9298a1f616ed33a7a509a'}>,\n",
       " <Document: {'content': 'Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 619}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9efc545c7905be5a0aa914883b6669ab'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Upgrading RStudio When moving between RStudio releases there are some differences that require either user intervention or caution. Why should I upgrade? Upgrading to the latest version of RStudio has a number of benefits, including a new version of R and improved package management options. 2.2.6 to 3.0.12 Key Points: Conda has been replaced with Renv for package management RStudio 1.4 R version 4.0.3 Conda has now been removed. You will need to install packages using Renv. See package management Preparation Please unidle your RStudio if it is currently idled Select RStudio 3.0.12 from the drop down and click yes on the prompt to install Notable Packages dbtools dbtools (i.e. the R wrapper for pydbtools ) is the data engineering maintained package for accessing Athena databases from R. It requires a little setup: Call renv::use_python() This will create a\\xa0virtualenv\\xa0for your project, which should be stored in the folder\\xa0renv/python/virtualenvs/renv-python-3.8.5/, and associate\\xa0reticulate\\xa0with that virtualenv. Install pydbtools into that environment You can either do that with reticulate::py_install(\"pydbtools\") , or if you prefer the terminal, you can activate that virtualenv with the command\\xa0renv/python/virtualenvs/renv-python-3.8.5/bin/activate. Install dbtools using remotes::install_github(\"moj-analytical-services/dbtools\") This page was last reviewed on 1 May 2022.\\n\\nIt needs to be reviewed again on 1 July 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 1 July 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 620}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c850f7fe43561e116c21ba42b7ad6a28'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nUpgrading RStudio - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 621}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '10652fe42cc2bc9c897a27c2b5ffb110'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 622}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Upgrading RStudio When moving between RStudio releases there are some differences that require either user intervention or caution. Why should I upgrade? Upgrading to the latest version of RStudio has a number of benefits, including a new version of R and improved package management options. 2.2.6 to 3.0.12 Key Points: Conda has been replaced with Renv for package management RStudio 1.4 R version 4.0.3 Conda has now been removed. You will need to install packages using Renv. See package management Preparation Please unidle your RStudio if it is currently idled Select RStudio 3.0.12 from the drop down and click yes on the prompt to install Notable Packages dbtools dbtools (i.e. the R wrapper for pydbtools ) is the data engineering maintained package for accessing Athena databases from R. It requires a little setup: Call renv::use_python() This will create a\\xa0virtualenv\\xa0for your project, which should be stored in the folder\\xa0renv/python/virtualenvs/renv-python-3.8.5/, and associate\\xa0reticulate\\xa0with that virtualenv. Install pydbtools into that environment You can either do that with reticulate::py_install(\"pydbtools\") , or if you prefer the terminal, you can activate that virtualenv with the command\\xa0renv/python/virtualenvs/renv-python-3.8.5/bin/activate. Install dbtools using remotes::install_github(\"moj-analytical-services/dbtools\") This page was last reviewed on 1 May 2022.\\n\\nIt needs to be reviewed again on 1 July 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 1 July 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 623}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c850f7fe43561e116c21ba42b7ad6a28'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nJupyterLab - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 624}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3f43fcdf20466d92e6736b6d43d788bf'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 625}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools JupyterLab A development environment for writing Python code including Python notebooks. If you are new to python and juptyerlab, there is a self-paced introduction which can be found here . Run notebooks In Jupyter, before you can successfully run the notebook, you’ll need to select the Jupyter kernel for this project. If it doesn’t appear in the drop-down list, run this in a terminal: . myproject/venv/bin/activate\\npython3 -m ipykernel install --user --name = \"venv_PROJECTNAMEHERE\" --display-name = \"My project (Python3)\" Run scripts And if your project has analytical scripts that run in a terminal you could add: To run the python scripts, you’ll need to activate the virtual env first: cd myproject . venv/bin/activate\\npython3 myscript.py Using a virtual environment in Jupyter It is advisable to use a different virtual environment (venv) for each project you do in Python. There is a little bit of set up to get Jupyter working with a venv. Follow the instructions below to get started: If you haven’t yet created a virtual environment for your project, in terminal run: cd myproject\\npython3 -m venv venv In the terminal, inside your project directory, activate your venv: source venv/bin/activate Install the module ipykernel within this venv (for creating/managing kernels for ipython which is what Jupyter sits on top of): pip install ipykernel Create a Jupyter kernel which is configured to use your venv. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 626}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd48f5709c1056f72c0176f86034c1493'}>,\n",
       " <Document: {'content': '(Change the display name to match your project name): python3 -m ipykernel install --user --name = \"venv_PROJECTNAMEHERE\" --display-name = \"My project (Python3)\" In Jupyter, open your notebook and then select this new kernel by its pretty name in the top right hand corner. It might take a little time/refreshes for it to show up. To resume work on this after working on another project: Activate the environment: cd myproject source venv/bin/activate Now you’ve activated this terminal with your venv, things you run on the command-line will default to using your venv for python packages, rather than the system’s packages. That’s useful if you run ‘python3’, run python scripts or ‘pip install’ more packages. Open the notebook - it’s remembered which kernel you wanted to use for this notebook and you can carry on working with the packages available. Note: Once you have associated the kernel with the venv you dont need to recreate/update it . Any packages that are installed to the venv via pip after the kernel is established are immediately available to the kernel. Using pipenv in Jupyter pipenv is another environment manager for Python. In general, please refer to their basic guidance . Set-up for a project results in the creation of Pipfile and Pipfile.lock in the root directory of your project folder. The instructions for someone to install the packages specified in Pipefile/Pipefile.lock, are as follows (you don’t create a venv yourself, nor is it necessary to ‘activate’ the pipenv environment): cd myproject\\npipenv install To use the pipenv in Jupyter, compared to using a venv in Jupyter , the syntax of creating the kernel is simply adjusted to: pipenv install ipykernel\\npython3 -m ipykernel install --user --name = \"pipenv-name\" --display-name = \"My project (Python3)\" And then select the kernel in Jupyter as normal . This page was last reviewed on 1 May 2022.\\n\\nIt needs to be reviewed again on 1 July 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 1 July 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nPython Package Management - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 627}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '884b5937f6005d085dc6ba0da9380c5c'}>,\n",
       " <Document: {'content': 'I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 628}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '165700989970d698eee8f1a9a15a7bc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Python package management venv and pip pip is a terminal command used to install and upgrade Python packages. PyPI is the main Python package repository. It’s ‘official’, but that doesn’t mean a lot - like most of these open source package repositories, a poor quality or even malicious package can easily be uploaded there, so do your diligence when picking them. A Python virtual environment (or venv , for short) is a directory you can install a particular Python executable and Python packages into, away from your machine’s default ones. Typically each project/repo you work on should have a different venv, and then you never have to deal with conflicting requirements between projects. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 629}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e20c1760c462df13604fcfb4030a5d80'}>,\n",
       " <Document: {'content': 'When you ‘activate’ a particular venv, then run python or pip , those commands will work with that venv’s Python executable and Python packages. Basic usage NOTE: You may need to delete the .bash_aliases file ( rm .bash_aliases ) from your home directory for pip to work properly within a virtual environment. Create a venv for your project, called ‘venv’ (make sure you run this in the Terminal): cd myproject\\npython3 -m venv venv (Add ‘venv’ to your .gitignore file, because this shouldn’t be added to your git repo.) When you work with your project’s packages in a terminal, you’ll want to ‘activate’ your venv: . venv/bin/activate You’ll notice the prompt changes to show that the venv is activated: (venv) jovyan@jupyter-lab-davidread-ju-6966d9b9b4-7zvsk:~/myproject$ With the venv activated you can install some packages using pip: ( venv ) $ pip install --user pandas The packages will get installed to your venv, in venv/lib/python3.7/site-packages/ . You can see what packages are installed using ‘pip freeze’: ( venv ) $ pip freeze numpy == 1.18.4 pandas == 1.0.4\\npython-dateutil == 2.8.1 pytz == 2020.1 six == 1.15.0 With the venv activated, if you run a Python script from the terminal, the package will be available to it. For example: ( venv ) $ python3 -c \\'import pandas; print(pandas); print(\"It worked\")\\' <module \\'pandas\\' from \\'/home/jovyan/myproject/venv/lib/python3.7/site-packages/pandas/ init .py\\' > It worked In JupyterLab, to be able to use the venv’s packages (instead of the system packages), see Using a venv in Jupyter When you commit your code, to ensure reproducibility, you should also commit an up-to-date record of what packages you’ve installed. The simplest way is to do: ( venv ) $ pip freeze > requirements.txt ( venv ) $ git add requirements.txt You should also add to your README file the instructions for using requirements.txt - see the following section. Using a project that has a requirements.txt If a project has a ‘requirements.txt’ then you should install that into a venv. A project’s README file is the traditional place to communicate usage of a requirements.txt. Because of that, this section is provided in markdown format so it can be copied into your project’s README, and tailored as necessary: ## Setup Before you can run this project, you need to install some Python packages using the terminal: # create a virtual environment\\ncd myproject\\npython3 -m venv venv\\n\\n# install the python packages required\\n. venv/bin/activate\\npip install -r requirements.txt Library conflicts & warnings If you come across any conflicts or warnings when installing your libraries using pip we advise you use poetry to resolve them. This page was last reviewed on 1 May 2022.\\n\\nIt needs to be reviewed again on 1 July 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 1 July 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 630}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b200a9a279d858a905a3887b096b0553'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nPython Package Management - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 631}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8aafd2b1d29f998aac21e7d0b5a86b5f'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 632}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Python package management venv and pip pip is a terminal command used to install and upgrade Python packages. PyPI is the main Python package repository. It’s ‘official’, but that doesn’t mean a lot - like most of these open source package repositories, a poor quality or even malicious package can easily be uploaded there, so do your diligence when picking them. A Python virtual environment (or venv , for short) is a directory you can install a particular Python executable and Python packages into, away from your machine’s default ones. Typically each project/repo you work on should have a different venv, and then you never have to deal with conflicting requirements between projects. When you ‘activate’ a particular venv, then run python or pip , those commands will work with that venv’s Python executable and Python packages. Basic usage NOTE: You may need to delete the .bash_aliases file ( rm .bash_aliases ) from your home directory for pip to work properly within a virtual environment. Create a venv for your project, called ‘venv’ (make sure you run this in the Terminal): cd myproject\\npython3 -m venv venv (Add ‘venv’ to your .gitignore file, because this shouldn’t be added to your git repo.) When you work with your project’s packages in a terminal, you’ll want to ‘activate’ your venv: . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 633}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '57a43afe25d5cdc656c4ce6cb3208385'}>,\n",
       " <Document: {'content': 'venv/bin/activate You’ll notice the prompt changes to show that the venv is activated: (venv) jovyan@jupyter-lab-davidread-ju-6966d9b9b4-7zvsk:~/myproject$ With the venv activated you can install some packages using pip: ( venv ) $ pip install --user pandas The packages will get installed to your venv, in venv/lib/python3.7/site-packages/ . You can see what packages are installed using ‘pip freeze’: ( venv ) $ pip freeze numpy == 1.18.4 pandas == 1.0.4\\npython-dateutil == 2.8.1 pytz == 2020.1 six == 1.15.0 With the venv activated, if you run a Python script from the terminal, the package will be available to it. For example: ( venv ) $ python3 -c \\'import pandas; print(pandas); print(\"It worked\")\\' <module \\'pandas\\' from \\'/home/jovyan/myproject/venv/lib/python3.7/site-packages/pandas/ init .py\\' > It worked In JupyterLab, to be able to use the venv’s packages (instead of the system packages), see Using a venv in Jupyter When you commit your code, to ensure reproducibility, you should also commit an up-to-date record of what packages you’ve installed. The simplest way is to do: ( venv ) $ pip freeze > requirements.txt ( venv ) $ git add requirements.txt You should also add to your README file the instructions for using requirements.txt - see the following section. Using a project that has a requirements.txt If a project has a ‘requirements.txt’ then you should install that into a venv. A project’s README file is the traditional place to communicate usage of a requirements.txt. Because of that, this section is provided in markdown format so it can be copied into your project’s README, and tailored as necessary: ## Setup Before you can run this project, you need to install some Python packages using the terminal: # create a virtual environment\\ncd myproject\\npython3 -m venv venv\\n\\n# install the python packages required\\n. venv/bin/activate\\npip install -r requirements.txt Library conflicts & warnings If you come across any conflicts or warnings when installing your libraries using pip we advise you use poetry to resolve them. This page was last reviewed on 1 May 2022.\\n\\nIt needs to be reviewed again on 1 July 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 1 July 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nPython Package Management - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 634}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '587e461384f51572722052f54d25b46c'}>,\n",
       " <Document: {'content': 'How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 635}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7c4cd5826a95361bc83ec8203d47a614'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Python package management venv and pip pip is a terminal command used to install and upgrade Python packages. PyPI is the main Python package repository. It’s ‘official’, but that doesn’t mean a lot - like most of these open source package repositories, a poor quality or even malicious package can easily be uploaded there, so do your diligence when picking them. A Python virtual environment (or venv , for short) is a directory you can install a particular Python executable and Python packages into, away from your machine’s default ones. Typically each project/repo you work on should have a different venv, and then you never have to deal with conflicting requirements between projects. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 636}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e20c1760c462df13604fcfb4030a5d80'}>,\n",
       " <Document: {'content': 'When you ‘activate’ a particular venv, then run python or pip , those commands will work with that venv’s Python executable and Python packages. Basic usage NOTE: You may need to delete the .bash_aliases file ( rm .bash_aliases ) from your home directory for pip to work properly within a virtual environment. Create a venv for your project, called ‘venv’ (make sure you run this in the Terminal): cd myproject\\npython3 -m venv venv (Add ‘venv’ to your .gitignore file, because this shouldn’t be added to your git repo.) When you work with your project’s packages in a terminal, you’ll want to ‘activate’ your venv: . venv/bin/activate You’ll notice the prompt changes to show that the venv is activated: (venv) jovyan@jupyter-lab-davidread-ju-6966d9b9b4-7zvsk:~/myproject$ With the venv activated you can install some packages using pip: ( venv ) $ pip install --user pandas The packages will get installed to your venv, in venv/lib/python3.7/site-packages/ . You can see what packages are installed using ‘pip freeze’: ( venv ) $ pip freeze numpy == 1.18.4 pandas == 1.0.4\\npython-dateutil == 2.8.1 pytz == 2020.1 six == 1.15.0 With the venv activated, if you run a Python script from the terminal, the package will be available to it. For example: ( venv ) $ python3 -c \\'import pandas; print(pandas); print(\"It worked\")\\' <module \\'pandas\\' from \\'/home/jovyan/myproject/venv/lib/python3.7/site-packages/pandas/ init .py\\' > It worked In JupyterLab, to be able to use the venv’s packages (instead of the system packages), see Using a venv in Jupyter When you commit your code, to ensure reproducibility, you should also commit an up-to-date record of what packages you’ve installed. The simplest way is to do: ( venv ) $ pip freeze > requirements.txt ( venv ) $ git add requirements.txt You should also add to your README file the instructions for using requirements.txt - see the following section. Using a project that has a requirements.txt If a project has a ‘requirements.txt’ then you should install that into a venv. A project’s README file is the traditional place to communicate usage of a requirements.txt. Because of that, this section is provided in markdown format so it can be copied into your project’s README, and tailored as necessary: ## Setup Before you can run this project, you need to install some Python packages using the terminal: # create a virtual environment\\ncd myproject\\npython3 -m venv venv\\n\\n# install the python packages required\\n. venv/bin/activate\\npip install -r requirements.txt Library conflicts & warnings If you come across any conflicts or warnings when installing your libraries using pip we advise you use poetry to resolve them. This page was last reviewed on 1 May 2022.\\n\\nIt needs to be reviewed again on 1 July 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 1 July 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 637}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b200a9a279d858a905a3887b096b0553'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nPython Package Management - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 638}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8aafd2b1d29f998aac21e7d0b5a86b5f'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 639}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Python package management venv and pip pip is a terminal command used to install and upgrade Python packages. PyPI is the main Python package repository. It’s ‘official’, but that doesn’t mean a lot - like most of these open source package repositories, a poor quality or even malicious package can easily be uploaded there, so do your diligence when picking them. A Python virtual environment (or venv , for short) is a directory you can install a particular Python executable and Python packages into, away from your machine’s default ones. Typically each project/repo you work on should have a different venv, and then you never have to deal with conflicting requirements between projects. When you ‘activate’ a particular venv, then run python or pip , those commands will work with that venv’s Python executable and Python packages. Basic usage NOTE: You may need to delete the .bash_aliases file ( rm .bash_aliases ) from your home directory for pip to work properly within a virtual environment. Create a venv for your project, called ‘venv’ (make sure you run this in the Terminal): cd myproject\\npython3 -m venv venv (Add ‘venv’ to your .gitignore file, because this shouldn’t be added to your git repo.) When you work with your project’s packages in a terminal, you’ll want to ‘activate’ your venv: . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 640}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '57a43afe25d5cdc656c4ce6cb3208385'}>,\n",
       " <Document: {'content': 'venv/bin/activate You’ll notice the prompt changes to show that the venv is activated: (venv) jovyan@jupyter-lab-davidread-ju-6966d9b9b4-7zvsk:~/myproject$ With the venv activated you can install some packages using pip: ( venv ) $ pip install --user pandas The packages will get installed to your venv, in venv/lib/python3.7/site-packages/ . You can see what packages are installed using ‘pip freeze’: ( venv ) $ pip freeze numpy == 1.18.4 pandas == 1.0.4\\npython-dateutil == 2.8.1 pytz == 2020.1 six == 1.15.0 With the venv activated, if you run a Python script from the terminal, the package will be available to it. For example: ( venv ) $ python3 -c \\'import pandas; print(pandas); print(\"It worked\")\\' <module \\'pandas\\' from \\'/home/jovyan/myproject/venv/lib/python3.7/site-packages/pandas/ init .py\\' > It worked In JupyterLab, to be able to use the venv’s packages (instead of the system packages), see Using a venv in Jupyter When you commit your code, to ensure reproducibility, you should also commit an up-to-date record of what packages you’ve installed. The simplest way is to do: ( venv ) $ pip freeze > requirements.txt ( venv ) $ git add requirements.txt You should also add to your README file the instructions for using requirements.txt - see the following section. Using a project that has a requirements.txt If a project has a ‘requirements.txt’ then you should install that into a venv. A project’s README file is the traditional place to communicate usage of a requirements.txt. Because of that, this section is provided in markdown format so it can be copied into your project’s README, and tailored as necessary: ## Setup Before you can run this project, you need to install some Python packages using the terminal: # create a virtual environment\\ncd myproject\\npython3 -m venv venv\\n\\n# install the python packages required\\n. venv/bin/activate\\npip install -r requirements.txt Library conflicts & warnings If you come across any conflicts or warnings when installing your libraries using pip we advise you use poetry to resolve them. This page was last reviewed on 1 May 2022.\\n\\nIt needs to be reviewed again on 1 July 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 1 July 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nPython Package Management - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 641}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '587e461384f51572722052f54d25b46c'}>,\n",
       " <Document: {'content': 'How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 642}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7c4cd5826a95361bc83ec8203d47a614'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Python package management venv and pip pip is a terminal command used to install and upgrade Python packages. PyPI is the main Python package repository. It’s ‘official’, but that doesn’t mean a lot - like most of these open source package repositories, a poor quality or even malicious package can easily be uploaded there, so do your diligence when picking them. A Python virtual environment (or venv , for short) is a directory you can install a particular Python executable and Python packages into, away from your machine’s default ones. Typically each project/repo you work on should have a different venv, and then you never have to deal with conflicting requirements between projects. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 643}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e20c1760c462df13604fcfb4030a5d80'}>,\n",
       " <Document: {'content': 'When you ‘activate’ a particular venv, then run python or pip , those commands will work with that venv’s Python executable and Python packages. Basic usage NOTE: You may need to delete the .bash_aliases file ( rm .bash_aliases ) from your home directory for pip to work properly within a virtual environment. Create a venv for your project, called ‘venv’ (make sure you run this in the Terminal): cd myproject\\npython3 -m venv venv (Add ‘venv’ to your .gitignore file, because this shouldn’t be added to your git repo.) When you work with your project’s packages in a terminal, you’ll want to ‘activate’ your venv: . venv/bin/activate You’ll notice the prompt changes to show that the venv is activated: (venv) jovyan@jupyter-lab-davidread-ju-6966d9b9b4-7zvsk:~/myproject$ With the venv activated you can install some packages using pip: ( venv ) $ pip install --user pandas The packages will get installed to your venv, in venv/lib/python3.7/site-packages/ . You can see what packages are installed using ‘pip freeze’: ( venv ) $ pip freeze numpy == 1.18.4 pandas == 1.0.4\\npython-dateutil == 2.8.1 pytz == 2020.1 six == 1.15.0 With the venv activated, if you run a Python script from the terminal, the package will be available to it. For example: ( venv ) $ python3 -c \\'import pandas; print(pandas); print(\"It worked\")\\' <module \\'pandas\\' from \\'/home/jovyan/myproject/venv/lib/python3.7/site-packages/pandas/ init .py\\' > It worked In JupyterLab, to be able to use the venv’s packages (instead of the system packages), see Using a venv in Jupyter When you commit your code, to ensure reproducibility, you should also commit an up-to-date record of what packages you’ve installed. The simplest way is to do: ( venv ) $ pip freeze > requirements.txt ( venv ) $ git add requirements.txt You should also add to your README file the instructions for using requirements.txt - see the following section. Using a project that has a requirements.txt If a project has a ‘requirements.txt’ then you should install that into a venv. A project’s README file is the traditional place to communicate usage of a requirements.txt. Because of that, this section is provided in markdown format so it can be copied into your project’s README, and tailored as necessary: ## Setup Before you can run this project, you need to install some Python packages using the terminal: # create a virtual environment\\ncd myproject\\npython3 -m venv venv\\n\\n# install the python packages required\\n. venv/bin/activate\\npip install -r requirements.txt Library conflicts & warnings If you come across any conflicts or warnings when installing your libraries using pip we advise you use poetry to resolve them. This page was last reviewed on 1 May 2022.\\n\\nIt needs to be reviewed again on 1 July 2022\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 1 July 2022\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 644}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b200a9a279d858a905a3887b096b0553'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAirflow - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 645}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b50cc851e504f70ebf735d0156adc477'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 646}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Airflow Airflow is a platform to programmatically author, schedule and monitor workflows Important links AWS control panel : to login to AWS and access AP tools including the Airflow dev and prod UI Airflow dev UI : for running and monitoring development and training workflows on the Airflow UI (you will need to login to AWS first) Airflow prod UI : for running and monitoring production workflows on the Airflow UI (you will need to login to AWS first) Airflow repo : Github repo to store Airflow DAGs and roles Airflow template for Python : Github template repository for creating a Python image to run an Airflow pipeline Airflow template for R : Github template repository for creating an R image to run an Airflow pipeline Support: contact the Data Engineering team on #ask-data-engineering To find out more Airflow pipeline concepts : What is Airflow and why you should use it Airflow pipeline instructions : Step by step guide for creating an example Airflow pipeline and related resources Troubleshooting Airflow pipelines : Common pitfalls This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 May 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 May 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 647}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '28d41c3a51bae7a7878ce92e65f36cc6'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAirflow Concepts - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 648}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eebe4306444a15c51568c8ae4cb8682c'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 649}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Airflow Concepts Why use Airflow Run tasks on a regular schedule For example the bentham app airflow pipeline runs daily and processes prisoner information from NOMIS and outputs the data to feather files. This forms the backend for the bentham app which is used by prison-based intelligence analysts to search through seized media. Run memory-intensive workloads For example the prison network airflow pipeline runs daily and downloads and processes ~100GB of data. Each prisoner is assigned a node and edges to link the nodes are created from data such as financial transactions, prison incidents, visits, seized mobile phone data. The pipeline cannot run using standard nodes and uses a high-memory node. Run end-to-end processing workflows involving multiple steps and dependencies The Safety Diagnostic Tool (SDT) airflow pipeline is extremely complex and made up of 7 tasks, each with different dependencies. For example, the ‘spells-correction’ task cleans prisoner spells data in NOMIS, which is then used to fit the Violence in Prisons Estimator (VIPER) model in the ‘viper’ task. This task provides a summary of prisoners’s violence that is used in the SDT RShiny application. Monitor the performance of workflows, identify and resolve issues Airflow allows users to visually monitor failed tasks and receive email notifications. Once corrected, users can restart the pipeline from that task instead of restarting the entire pipeline What is Airflow Below are some key concepts in Airflow. What is discussed is covered in more detail in this talk . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 650}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8178309534ed4f5147612e822d491e3c'}>,\n",
       " <Document: {'content': 'Tasks These are the things you want to do; the building blocks of the pipeline. Let’s say I want to run a script that does some basic data processing when a specific file is created in a particular location. Once this processing is done, a second script writes a report based on the processed data. You might want to model this as a set of tasks i.e. file_created_sensor -> script_1 -> script_2. Operator An Operator is a Task template. There are loads of pre-made operators to carry out common tasks such as running python ( PythonOperator ) or bash ( BashOperator ) scripts. However it is best practice to only use operators that trigger computation out of Airflow in a separate processing solution. We use a Kubernetes cluster and the KubernetesPodOperator to launch a docker container (see below). DAG (Directed Acyclic Graph) DAGs define the tasks, the order of operations, and the frequency/schedule that the pipeline is run. DAGs have a schedule interval (a cron-expression detailing when and how often to run the pipeline), and are considered “acyclic” because the sequence flow must never loop back on itself. There can be branches and joins, but no cycles. For example task A -> B -> C -> A is not allowed. Schedule Interval This is an expression describing the start date and end date bound to the data ingested by the pipeline , and the frequency at which to run the pipeline. For example, a @daily task defined as 19:00:00, starting on 01-01-2022. This task would be triggered just after 18:59:59 the following day (02-01-2022) after the full day’s worth of data exists. As such, the scheduler runs the job at the end of each period. Airflow UI The Airflow UI makes it easy to monitor and troubleshoot your data pipelines. You can also use it to manually trigger your workflow. What is Kubernetes Kubernetes is a platform for automating the deployment, scaling, and management of containers. We use it in conjunction with Airflow to run Airflow pipelines. This means Airflow can concentrate on scheduling and monitoring the pipelines, whilst the Kubernetes cluster can concentrate on doing the heavy-lifting processing. You will not have access to the Kubernetes cluster but it’s helpful to understand the key concepts. Container The term to describe a portable software package of code for an application along with the dependencies it needs at run time. Containers isolate software from its environment and ensure that it works uniformly regardless of underlying infrastructure (e.g. running on Windows vs. Linux). Your containers will usually contain R/python code. Image act as a set of instructions to build a container, like a template. Public and private registries are used to store and share images such as Amazon Elastic Container Registry ECR . Docker Software framework for building and running containers. Cluster When you deploy Kubernetes, you get a cluster. A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 651}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '34f0e3685f6ba5dd75603eed716155d4'}>,\n",
       " <Document: {'content': 'Node Kubernetes runs your workload by placing containers into Pods to run on Nodes. A node may be a virtual or physical machine, depending on the cluster. Kubernetes can create nodes with different specifications (e.g. high-memory) and force pods to run on specific nodes. Pod a group of one or more\\xa0containers with shared resources, and a specification for how to run the containers. Airflow Environments There are two separate Airflow environments, each with it’s own Kubernetes cluster: Dev : for training and testing new/updates to pipelines Prod : for running production pipelines There is also a sandpit environement for data engineers to test upgrades to the Airflow platform but you will not have access to this environment. Airflow Pipeline Within the MoJ Analytical Platform, a typical Airflow pipeline consists of the following steps (Actions inside the grey box are automated): The DAG can be triggered by an analyst through the Airflow UI or through a schedule The DAG script is stored in an AWS S3 bucket and defines a Kubernetes pod operator and an image which contains the required python/R code The Kubernetes pod operator launches a single-container pod in a Kubernetes cluster The pod pulls the image from the ECR registry in the Data Engineering AWS account The pod will need permission to access various AWS resources (i.e. run an Athena query, read-write to a bucket, etc). This is achieved by assuming an Identity and Access Management (IAM) role with the relevant permissions The output is then usually saved to an S3 bucket The next two sections summarises the process for creating (and maintaining) the DAG, image and IAM roles. This uses two deployment pipelines which are automated using various Github actions that we have created to facilitate this process. Image Pipeline Note that you can skip this pipeline if you already have a working docker image saved to the Data Engineering ECR. This deployment pipeline creates a docker image which contains the analytical code (python or R) that will be run in the dockerised task. Actions highlighted in grey are automated. The image repo must contain the build-and-push-to-ecr Github action to push the docker image to the Data Engineering Elastic Container Registry (ECR). This can be done by: copying template-airflow-python for python images copying template-airflow-r for R images copying the Github action to an existing image repo Please see Image pipeline for more details. DAG Pipeline This deployment pipeline creates the Directed Acyclic Graph (DAG) which defines the tasks that will be run, as well as the IAM role which the Kubernetes pod will need in order to access relevant AWS resources and services. Actions highlighted in grey are automated. You must add the DAG and role policies to airflow following specific rules. See DAG pipeline for more details. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 652}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '78685025a2231db0831d5f7ee933faa9'}>,\n",
       " <Document: {'content': 'Once you raise the PR and it is approved by data engineering, various Github actions will automatically: validate the DAG and policies adhere to the rules notify DE through a slack notification create/update the IAM role save the DAG to an S3 bucket which can be accessed by Airflow Component Responsibilities Since an Airflow pipeline consists of so many moving components, it is helpful to summarise everyone’s remit. Analysts are responsible for creating/maintaining the following components: DAG which defines the analytical tasks that the pipeline will run IAM policies for the IAM Role that the Kubernetes pod will use Image repo and the code that will be run in the tasks Data Engineering is responsible for maintaining the following components: airflow environments kubernetes clusters github actions for automating the deployment pipelines template image repo to base the image repo from user guidance as well as approving the DAG and IAM Role PR. When not to use an Airflow to trigger an Analytical Platform app (the Analytical Platform team can create cron jobs for this) This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAirflow Concepts - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 653}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cb8803b9637da0734230b75fc653d12b'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 654}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Airflow Concepts Why use Airflow Run tasks on a regular schedule For example the bentham app airflow pipeline runs daily and processes prisoner information from NOMIS and outputs the data to feather files. This forms the backend for the bentham app which is used by prison-based intelligence analysts to search through seized media. Run memory-intensive workloads For example the prison network airflow pipeline runs daily and downloads and processes ~100GB of data. Each prisoner is assigned a node and edges to link the nodes are created from data such as financial transactions, prison incidents, visits, seized mobile phone data. The pipeline cannot run using standard nodes and uses a high-memory node. Run end-to-end processing workflows involving multiple steps and dependencies The Safety Diagnostic Tool (SDT) airflow pipeline is extremely complex and made up of 7 tasks, each with different dependencies. For example, the ‘spells-correction’ task cleans prisoner spells data in NOMIS, which is then used to fit the Violence in Prisons Estimator (VIPER) model in the ‘viper’ task. This task provides a summary of prisoners’s violence that is used in the SDT RShiny application. Monitor the performance of workflows, identify and resolve issues Airflow allows users to visually monitor failed tasks and receive email notifications. Once corrected, users can restart the pipeline from that task instead of restarting the entire pipeline What is Airflow Below are some key concepts in Airflow. What is discussed is covered in more detail in this talk . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 655}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8178309534ed4f5147612e822d491e3c'}>,\n",
       " <Document: {'content': 'Tasks These are the things you want to do; the building blocks of the pipeline. Let’s say I want to run a script that does some basic data processing when a specific file is created in a particular location. Once this processing is done, a second script writes a report based on the processed data. You might want to model this as a set of tasks i.e. file_created_sensor -> script_1 -> script_2. Operator An Operator is a Task template. There are loads of pre-made operators to carry out common tasks such as running python ( PythonOperator ) or bash ( BashOperator ) scripts. However it is best practice to only use operators that trigger computation out of Airflow in a separate processing solution. We use a Kubernetes cluster and the KubernetesPodOperator to launch a docker container (see below). DAG (Directed Acyclic Graph) DAGs define the tasks, the order of operations, and the frequency/schedule that the pipeline is run. DAGs have a schedule interval (a cron-expression detailing when and how often to run the pipeline), and are considered “acyclic” because the sequence flow must never loop back on itself. There can be branches and joins, but no cycles. For example task A -> B -> C -> A is not allowed. Schedule Interval This is an expression describing the start date and end date bound to the data ingested by the pipeline , and the frequency at which to run the pipeline. For example, a @daily task defined as 19:00:00, starting on 01-01-2022. This task would be triggered just after 18:59:59 the following day (02-01-2022) after the full day’s worth of data exists. As such, the scheduler runs the job at the end of each period. Airflow UI The Airflow UI makes it easy to monitor and troubleshoot your data pipelines. You can also use it to manually trigger your workflow. What is Kubernetes Kubernetes is a platform for automating the deployment, scaling, and management of containers. We use it in conjunction with Airflow to run Airflow pipelines. This means Airflow can concentrate on scheduling and monitoring the pipelines, whilst the Kubernetes cluster can concentrate on doing the heavy-lifting processing. You will not have access to the Kubernetes cluster but it’s helpful to understand the key concepts. Container The term to describe a portable software package of code for an application along with the dependencies it needs at run time. Containers isolate software from its environment and ensure that it works uniformly regardless of underlying infrastructure (e.g. running on Windows vs. Linux). Your containers will usually contain R/python code. Image act as a set of instructions to build a container, like a template. Public and private registries are used to store and share images such as Amazon Elastic Container Registry ECR . Docker Software framework for building and running containers. Cluster When you deploy Kubernetes, you get a cluster. A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 656}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '34f0e3685f6ba5dd75603eed716155d4'}>,\n",
       " <Document: {'content': 'Node Kubernetes runs your workload by placing containers into Pods to run on Nodes. A node may be a virtual or physical machine, depending on the cluster. Kubernetes can create nodes with different specifications (e.g. high-memory) and force pods to run on specific nodes. Pod a group of one or more\\xa0containers with shared resources, and a specification for how to run the containers. Airflow Environments There are two separate Airflow environments, each with it’s own Kubernetes cluster: Dev : for training and testing new/updates to pipelines Prod : for running production pipelines There is also a sandpit environement for data engineers to test upgrades to the Airflow platform but you will not have access to this environment. Airflow Pipeline Within the MoJ Analytical Platform, a typical Airflow pipeline consists of the following steps (Actions inside the grey box are automated): The DAG can be triggered by an analyst through the Airflow UI or through a schedule The DAG script is stored in an AWS S3 bucket and defines a Kubernetes pod operator and an image which contains the required python/R code The Kubernetes pod operator launches a single-container pod in a Kubernetes cluster The pod pulls the image from the ECR registry in the Data Engineering AWS account The pod will need permission to access various AWS resources (i.e. run an Athena query, read-write to a bucket, etc). This is achieved by assuming an Identity and Access Management (IAM) role with the relevant permissions The output is then usually saved to an S3 bucket The next two sections summarises the process for creating (and maintaining) the DAG, image and IAM roles. This uses two deployment pipelines which are automated using various Github actions that we have created to facilitate this process. Image Pipeline Note that you can skip this pipeline if you already have a working docker image saved to the Data Engineering ECR. This deployment pipeline creates a docker image which contains the analytical code (python or R) that will be run in the dockerised task. Actions highlighted in grey are automated. The image repo must contain the build-and-push-to-ecr Github action to push the docker image to the Data Engineering Elastic Container Registry (ECR). This can be done by: copying template-airflow-python for python images copying template-airflow-r for R images copying the Github action to an existing image repo Please see Image pipeline for more details. DAG Pipeline This deployment pipeline creates the Directed Acyclic Graph (DAG) which defines the tasks that will be run, as well as the IAM role which the Kubernetes pod will need in order to access relevant AWS resources and services. Actions highlighted in grey are automated. You must add the DAG and role policies to airflow following specific rules. See DAG pipeline for more details. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 657}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '78685025a2231db0831d5f7ee933faa9'}>,\n",
       " <Document: {'content': 'Once you raise the PR and it is approved by data engineering, various Github actions will automatically: validate the DAG and policies adhere to the rules notify DE through a slack notification create/update the IAM role save the DAG to an S3 bucket which can be accessed by Airflow Component Responsibilities Since an Airflow pipeline consists of so many moving components, it is helpful to summarise everyone’s remit. Analysts are responsible for creating/maintaining the following components: DAG which defines the analytical tasks that the pipeline will run IAM policies for the IAM Role that the Kubernetes pod will use Image repo and the code that will be run in the tasks Data Engineering is responsible for maintaining the following components: airflow environments kubernetes clusters github actions for automating the deployment pipelines template image repo to base the image repo from user guidance as well as approving the DAG and IAM Role PR. When not to use an Airflow to trigger an Analytical Platform app (the Analytical Platform team can create cron jobs for this) This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAirflow Concepts - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 658}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cb8803b9637da0734230b75fc653d12b'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 659}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Airflow Concepts Why use Airflow Run tasks on a regular schedule For example the bentham app airflow pipeline runs daily and processes prisoner information from NOMIS and outputs the data to feather files. This forms the backend for the bentham app which is used by prison-based intelligence analysts to search through seized media. Run memory-intensive workloads For example the prison network airflow pipeline runs daily and downloads and processes ~100GB of data. Each prisoner is assigned a node and edges to link the nodes are created from data such as financial transactions, prison incidents, visits, seized mobile phone data. The pipeline cannot run using standard nodes and uses a high-memory node. Run end-to-end processing workflows involving multiple steps and dependencies The Safety Diagnostic Tool (SDT) airflow pipeline is extremely complex and made up of 7 tasks, each with different dependencies. For example, the ‘spells-correction’ task cleans prisoner spells data in NOMIS, which is then used to fit the Violence in Prisons Estimator (VIPER) model in the ‘viper’ task. This task provides a summary of prisoners’s violence that is used in the SDT RShiny application. Monitor the performance of workflows, identify and resolve issues Airflow allows users to visually monitor failed tasks and receive email notifications. Once corrected, users can restart the pipeline from that task instead of restarting the entire pipeline What is Airflow Below are some key concepts in Airflow. What is discussed is covered in more detail in this talk . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 660}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8178309534ed4f5147612e822d491e3c'}>,\n",
       " <Document: {'content': 'Tasks These are the things you want to do; the building blocks of the pipeline. Let’s say I want to run a script that does some basic data processing when a specific file is created in a particular location. Once this processing is done, a second script writes a report based on the processed data. You might want to model this as a set of tasks i.e. file_created_sensor -> script_1 -> script_2. Operator An Operator is a Task template. There are loads of pre-made operators to carry out common tasks such as running python ( PythonOperator ) or bash ( BashOperator ) scripts. However it is best practice to only use operators that trigger computation out of Airflow in a separate processing solution. We use a Kubernetes cluster and the KubernetesPodOperator to launch a docker container (see below). DAG (Directed Acyclic Graph) DAGs define the tasks, the order of operations, and the frequency/schedule that the pipeline is run. DAGs have a schedule interval (a cron-expression detailing when and how often to run the pipeline), and are considered “acyclic” because the sequence flow must never loop back on itself. There can be branches and joins, but no cycles. For example task A -> B -> C -> A is not allowed. Schedule Interval This is an expression describing the start date and end date bound to the data ingested by the pipeline , and the frequency at which to run the pipeline. For example, a @daily task defined as 19:00:00, starting on 01-01-2022. This task would be triggered just after 18:59:59 the following day (02-01-2022) after the full day’s worth of data exists. As such, the scheduler runs the job at the end of each period. Airflow UI The Airflow UI makes it easy to monitor and troubleshoot your data pipelines. You can also use it to manually trigger your workflow. What is Kubernetes Kubernetes is a platform for automating the deployment, scaling, and management of containers. We use it in conjunction with Airflow to run Airflow pipelines. This means Airflow can concentrate on scheduling and monitoring the pipelines, whilst the Kubernetes cluster can concentrate on doing the heavy-lifting processing. You will not have access to the Kubernetes cluster but it’s helpful to understand the key concepts. Container The term to describe a portable software package of code for an application along with the dependencies it needs at run time. Containers isolate software from its environment and ensure that it works uniformly regardless of underlying infrastructure (e.g. running on Windows vs. Linux). Your containers will usually contain R/python code. Image act as a set of instructions to build a container, like a template. Public and private registries are used to store and share images such as Amazon Elastic Container Registry ECR . Docker Software framework for building and running containers. Cluster When you deploy Kubernetes, you get a cluster. A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 661}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '34f0e3685f6ba5dd75603eed716155d4'}>,\n",
       " <Document: {'content': 'Node Kubernetes runs your workload by placing containers into Pods to run on Nodes. A node may be a virtual or physical machine, depending on the cluster. Kubernetes can create nodes with different specifications (e.g. high-memory) and force pods to run on specific nodes. Pod a group of one or more\\xa0containers with shared resources, and a specification for how to run the containers. Airflow Environments There are two separate Airflow environments, each with it’s own Kubernetes cluster: Dev : for training and testing new/updates to pipelines Prod : for running production pipelines There is also a sandpit environement for data engineers to test upgrades to the Airflow platform but you will not have access to this environment. Airflow Pipeline Within the MoJ Analytical Platform, a typical Airflow pipeline consists of the following steps (Actions inside the grey box are automated): The DAG can be triggered by an analyst through the Airflow UI or through a schedule The DAG script is stored in an AWS S3 bucket and defines a Kubernetes pod operator and an image which contains the required python/R code The Kubernetes pod operator launches a single-container pod in a Kubernetes cluster The pod pulls the image from the ECR registry in the Data Engineering AWS account The pod will need permission to access various AWS resources (i.e. run an Athena query, read-write to a bucket, etc). This is achieved by assuming an Identity and Access Management (IAM) role with the relevant permissions The output is then usually saved to an S3 bucket The next two sections summarises the process for creating (and maintaining) the DAG, image and IAM roles. This uses two deployment pipelines which are automated using various Github actions that we have created to facilitate this process. Image Pipeline Note that you can skip this pipeline if you already have a working docker image saved to the Data Engineering ECR. This deployment pipeline creates a docker image which contains the analytical code (python or R) that will be run in the dockerised task. Actions highlighted in grey are automated. The image repo must contain the build-and-push-to-ecr Github action to push the docker image to the Data Engineering Elastic Container Registry (ECR). This can be done by: copying template-airflow-python for python images copying template-airflow-r for R images copying the Github action to an existing image repo Please see Image pipeline for more details. DAG Pipeline This deployment pipeline creates the Directed Acyclic Graph (DAG) which defines the tasks that will be run, as well as the IAM role which the Kubernetes pod will need in order to access relevant AWS resources and services. Actions highlighted in grey are automated. You must add the DAG and role policies to airflow following specific rules. See DAG pipeline for more details. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 662}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '78685025a2231db0831d5f7ee933faa9'}>,\n",
       " <Document: {'content': 'Once you raise the PR and it is approved by data engineering, various Github actions will automatically: validate the DAG and policies adhere to the rules notify DE through a slack notification create/update the IAM role save the DAG to an S3 bucket which can be accessed by Airflow Component Responsibilities Since an Airflow pipeline consists of so many moving components, it is helpful to summarise everyone’s remit. Analysts are responsible for creating/maintaining the following components: DAG which defines the analytical tasks that the pipeline will run IAM policies for the IAM Role that the Kubernetes pod will use Image repo and the code that will be run in the tasks Data Engineering is responsible for maintaining the following components: airflow environments kubernetes clusters github actions for automating the deployment pipelines template image repo to base the image repo from user guidance as well as approving the DAG and IAM Role PR. When not to use an Airflow to trigger an Analytical Platform app (the Analytical Platform team can create cron jobs for this) This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAirflow Concepts - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 663}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cb8803b9637da0734230b75fc653d12b'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 664}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Airflow Concepts Why use Airflow Run tasks on a regular schedule For example the bentham app airflow pipeline runs daily and processes prisoner information from NOMIS and outputs the data to feather files. This forms the backend for the bentham app which is used by prison-based intelligence analysts to search through seized media. Run memory-intensive workloads For example the prison network airflow pipeline runs daily and downloads and processes ~100GB of data. Each prisoner is assigned a node and edges to link the nodes are created from data such as financial transactions, prison incidents, visits, seized mobile phone data. The pipeline cannot run using standard nodes and uses a high-memory node. Run end-to-end processing workflows involving multiple steps and dependencies The Safety Diagnostic Tool (SDT) airflow pipeline is extremely complex and made up of 7 tasks, each with different dependencies. For example, the ‘spells-correction’ task cleans prisoner spells data in NOMIS, which is then used to fit the Violence in Prisons Estimator (VIPER) model in the ‘viper’ task. This task provides a summary of prisoners’s violence that is used in the SDT RShiny application. Monitor the performance of workflows, identify and resolve issues Airflow allows users to visually monitor failed tasks and receive email notifications. Once corrected, users can restart the pipeline from that task instead of restarting the entire pipeline What is Airflow Below are some key concepts in Airflow. What is discussed is covered in more detail in this talk . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 665}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8178309534ed4f5147612e822d491e3c'}>,\n",
       " <Document: {'content': 'Tasks These are the things you want to do; the building blocks of the pipeline. Let’s say I want to run a script that does some basic data processing when a specific file is created in a particular location. Once this processing is done, a second script writes a report based on the processed data. You might want to model this as a set of tasks i.e. file_created_sensor -> script_1 -> script_2. Operator An Operator is a Task template. There are loads of pre-made operators to carry out common tasks such as running python ( PythonOperator ) or bash ( BashOperator ) scripts. However it is best practice to only use operators that trigger computation out of Airflow in a separate processing solution. We use a Kubernetes cluster and the KubernetesPodOperator to launch a docker container (see below). DAG (Directed Acyclic Graph) DAGs define the tasks, the order of operations, and the frequency/schedule that the pipeline is run. DAGs have a schedule interval (a cron-expression detailing when and how often to run the pipeline), and are considered “acyclic” because the sequence flow must never loop back on itself. There can be branches and joins, but no cycles. For example task A -> B -> C -> A is not allowed. Schedule Interval This is an expression describing the start date and end date bound to the data ingested by the pipeline , and the frequency at which to run the pipeline. For example, a @daily task defined as 19:00:00, starting on 01-01-2022. This task would be triggered just after 18:59:59 the following day (02-01-2022) after the full day’s worth of data exists. As such, the scheduler runs the job at the end of each period. Airflow UI The Airflow UI makes it easy to monitor and troubleshoot your data pipelines. You can also use it to manually trigger your workflow. What is Kubernetes Kubernetes is a platform for automating the deployment, scaling, and management of containers. We use it in conjunction with Airflow to run Airflow pipelines. This means Airflow can concentrate on scheduling and monitoring the pipelines, whilst the Kubernetes cluster can concentrate on doing the heavy-lifting processing. You will not have access to the Kubernetes cluster but it’s helpful to understand the key concepts. Container The term to describe a portable software package of code for an application along with the dependencies it needs at run time. Containers isolate software from its environment and ensure that it works uniformly regardless of underlying infrastructure (e.g. running on Windows vs. Linux). Your containers will usually contain R/python code. Image act as a set of instructions to build a container, like a template. Public and private registries are used to store and share images such as Amazon Elastic Container Registry ECR . Docker Software framework for building and running containers. Cluster When you deploy Kubernetes, you get a cluster. A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 666}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '34f0e3685f6ba5dd75603eed716155d4'}>,\n",
       " <Document: {'content': 'Node Kubernetes runs your workload by placing containers into Pods to run on Nodes. A node may be a virtual or physical machine, depending on the cluster. Kubernetes can create nodes with different specifications (e.g. high-memory) and force pods to run on specific nodes. Pod a group of one or more\\xa0containers with shared resources, and a specification for how to run the containers. Airflow Environments There are two separate Airflow environments, each with it’s own Kubernetes cluster: Dev : for training and testing new/updates to pipelines Prod : for running production pipelines There is also a sandpit environement for data engineers to test upgrades to the Airflow platform but you will not have access to this environment. Airflow Pipeline Within the MoJ Analytical Platform, a typical Airflow pipeline consists of the following steps (Actions inside the grey box are automated): The DAG can be triggered by an analyst through the Airflow UI or through a schedule The DAG script is stored in an AWS S3 bucket and defines a Kubernetes pod operator and an image which contains the required python/R code The Kubernetes pod operator launches a single-container pod in a Kubernetes cluster The pod pulls the image from the ECR registry in the Data Engineering AWS account The pod will need permission to access various AWS resources (i.e. run an Athena query, read-write to a bucket, etc). This is achieved by assuming an Identity and Access Management (IAM) role with the relevant permissions The output is then usually saved to an S3 bucket The next two sections summarises the process for creating (and maintaining) the DAG, image and IAM roles. This uses two deployment pipelines which are automated using various Github actions that we have created to facilitate this process. Image Pipeline Note that you can skip this pipeline if you already have a working docker image saved to the Data Engineering ECR. This deployment pipeline creates a docker image which contains the analytical code (python or R) that will be run in the dockerised task. Actions highlighted in grey are automated. The image repo must contain the build-and-push-to-ecr Github action to push the docker image to the Data Engineering Elastic Container Registry (ECR). This can be done by: copying template-airflow-python for python images copying template-airflow-r for R images copying the Github action to an existing image repo Please see Image pipeline for more details. DAG Pipeline This deployment pipeline creates the Directed Acyclic Graph (DAG) which defines the tasks that will be run, as well as the IAM role which the Kubernetes pod will need in order to access relevant AWS resources and services. Actions highlighted in grey are automated. You must add the DAG and role policies to airflow following specific rules. See DAG pipeline for more details. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 667}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '78685025a2231db0831d5f7ee933faa9'}>,\n",
       " <Document: {'content': 'Once you raise the PR and it is approved by data engineering, various Github actions will automatically: validate the DAG and policies adhere to the rules notify DE through a slack notification create/update the IAM role save the DAG to an S3 bucket which can be accessed by Airflow Component Responsibilities Since an Airflow pipeline consists of so many moving components, it is helpful to summarise everyone’s remit. Analysts are responsible for creating/maintaining the following components: DAG which defines the analytical tasks that the pipeline will run IAM policies for the IAM Role that the Kubernetes pod will use Image repo and the code that will be run in the tasks Data Engineering is responsible for maintaining the following components: airflow environments kubernetes clusters github actions for automating the deployment pipelines template image repo to base the image repo from user guidance as well as approving the DAG and IAM Role PR. When not to use an Airflow to trigger an Analytical Platform app (the Analytical Platform team can create cron jobs for this) This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAirflow Concepts - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 668}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cb8803b9637da0734230b75fc653d12b'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 669}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Airflow Concepts Why use Airflow Run tasks on a regular schedule For example the bentham app airflow pipeline runs daily and processes prisoner information from NOMIS and outputs the data to feather files. This forms the backend for the bentham app which is used by prison-based intelligence analysts to search through seized media. Run memory-intensive workloads For example the prison network airflow pipeline runs daily and downloads and processes ~100GB of data. Each prisoner is assigned a node and edges to link the nodes are created from data such as financial transactions, prison incidents, visits, seized mobile phone data. The pipeline cannot run using standard nodes and uses a high-memory node. Run end-to-end processing workflows involving multiple steps and dependencies The Safety Diagnostic Tool (SDT) airflow pipeline is extremely complex and made up of 7 tasks, each with different dependencies. For example, the ‘spells-correction’ task cleans prisoner spells data in NOMIS, which is then used to fit the Violence in Prisons Estimator (VIPER) model in the ‘viper’ task. This task provides a summary of prisoners’s violence that is used in the SDT RShiny application. Monitor the performance of workflows, identify and resolve issues Airflow allows users to visually monitor failed tasks and receive email notifications. Once corrected, users can restart the pipeline from that task instead of restarting the entire pipeline What is Airflow Below are some key concepts in Airflow. What is discussed is covered in more detail in this talk . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 670}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8178309534ed4f5147612e822d491e3c'}>,\n",
       " <Document: {'content': 'Tasks These are the things you want to do; the building blocks of the pipeline. Let’s say I want to run a script that does some basic data processing when a specific file is created in a particular location. Once this processing is done, a second script writes a report based on the processed data. You might want to model this as a set of tasks i.e. file_created_sensor -> script_1 -> script_2. Operator An Operator is a Task template. There are loads of pre-made operators to carry out common tasks such as running python ( PythonOperator ) or bash ( BashOperator ) scripts. However it is best practice to only use operators that trigger computation out of Airflow in a separate processing solution. We use a Kubernetes cluster and the KubernetesPodOperator to launch a docker container (see below). DAG (Directed Acyclic Graph) DAGs define the tasks, the order of operations, and the frequency/schedule that the pipeline is run. DAGs have a schedule interval (a cron-expression detailing when and how often to run the pipeline), and are considered “acyclic” because the sequence flow must never loop back on itself. There can be branches and joins, but no cycles. For example task A -> B -> C -> A is not allowed. Schedule Interval This is an expression describing the start date and end date bound to the data ingested by the pipeline , and the frequency at which to run the pipeline. For example, a @daily task defined as 19:00:00, starting on 01-01-2022. This task would be triggered just after 18:59:59 the following day (02-01-2022) after the full day’s worth of data exists. As such, the scheduler runs the job at the end of each period. Airflow UI The Airflow UI makes it easy to monitor and troubleshoot your data pipelines. You can also use it to manually trigger your workflow. What is Kubernetes Kubernetes is a platform for automating the deployment, scaling, and management of containers. We use it in conjunction with Airflow to run Airflow pipelines. This means Airflow can concentrate on scheduling and monitoring the pipelines, whilst the Kubernetes cluster can concentrate on doing the heavy-lifting processing. You will not have access to the Kubernetes cluster but it’s helpful to understand the key concepts. Container The term to describe a portable software package of code for an application along with the dependencies it needs at run time. Containers isolate software from its environment and ensure that it works uniformly regardless of underlying infrastructure (e.g. running on Windows vs. Linux). Your containers will usually contain R/python code. Image act as a set of instructions to build a container, like a template. Public and private registries are used to store and share images such as Amazon Elastic Container Registry ECR . Docker Software framework for building and running containers. Cluster When you deploy Kubernetes, you get a cluster. A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 671}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '34f0e3685f6ba5dd75603eed716155d4'}>,\n",
       " <Document: {'content': 'Node Kubernetes runs your workload by placing containers into Pods to run on Nodes. A node may be a virtual or physical machine, depending on the cluster. Kubernetes can create nodes with different specifications (e.g. high-memory) and force pods to run on specific nodes. Pod a group of one or more\\xa0containers with shared resources, and a specification for how to run the containers. Airflow Environments There are two separate Airflow environments, each with it’s own Kubernetes cluster: Dev : for training and testing new/updates to pipelines Prod : for running production pipelines There is also a sandpit environement for data engineers to test upgrades to the Airflow platform but you will not have access to this environment. Airflow Pipeline Within the MoJ Analytical Platform, a typical Airflow pipeline consists of the following steps (Actions inside the grey box are automated): The DAG can be triggered by an analyst through the Airflow UI or through a schedule The DAG script is stored in an AWS S3 bucket and defines a Kubernetes pod operator and an image which contains the required python/R code The Kubernetes pod operator launches a single-container pod in a Kubernetes cluster The pod pulls the image from the ECR registry in the Data Engineering AWS account The pod will need permission to access various AWS resources (i.e. run an Athena query, read-write to a bucket, etc). This is achieved by assuming an Identity and Access Management (IAM) role with the relevant permissions The output is then usually saved to an S3 bucket The next two sections summarises the process for creating (and maintaining) the DAG, image and IAM roles. This uses two deployment pipelines which are automated using various Github actions that we have created to facilitate this process. Image Pipeline Note that you can skip this pipeline if you already have a working docker image saved to the Data Engineering ECR. This deployment pipeline creates a docker image which contains the analytical code (python or R) that will be run in the dockerised task. Actions highlighted in grey are automated. The image repo must contain the build-and-push-to-ecr Github action to push the docker image to the Data Engineering Elastic Container Registry (ECR). This can be done by: copying template-airflow-python for python images copying template-airflow-r for R images copying the Github action to an existing image repo Please see Image pipeline for more details. DAG Pipeline This deployment pipeline creates the Directed Acyclic Graph (DAG) which defines the tasks that will be run, as well as the IAM role which the Kubernetes pod will need in order to access relevant AWS resources and services. Actions highlighted in grey are automated. You must add the DAG and role policies to airflow following specific rules. See DAG pipeline for more details. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 672}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '78685025a2231db0831d5f7ee933faa9'}>,\n",
       " <Document: {'content': 'Once you raise the PR and it is approved by data engineering, various Github actions will automatically: validate the DAG and policies adhere to the rules notify DE through a slack notification create/update the IAM role save the DAG to an S3 bucket which can be accessed by Airflow Component Responsibilities Since an Airflow pipeline consists of so many moving components, it is helpful to summarise everyone’s remit. Analysts are responsible for creating/maintaining the following components: DAG which defines the analytical tasks that the pipeline will run IAM policies for the IAM Role that the Kubernetes pod will use Image repo and the code that will be run in the tasks Data Engineering is responsible for maintaining the following components: airflow environments kubernetes clusters github actions for automating the deployment pipelines template image repo to base the image repo from user guidance as well as approving the DAG and IAM Role PR. When not to use an Airflow to trigger an Analytical Platform app (the Analytical Platform team can create cron jobs for this) This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAirflow Concepts - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 673}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cb8803b9637da0734230b75fc653d12b'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 674}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Airflow Concepts Why use Airflow Run tasks on a regular schedule For example the bentham app airflow pipeline runs daily and processes prisoner information from NOMIS and outputs the data to feather files. This forms the backend for the bentham app which is used by prison-based intelligence analysts to search through seized media. Run memory-intensive workloads For example the prison network airflow pipeline runs daily and downloads and processes ~100GB of data. Each prisoner is assigned a node and edges to link the nodes are created from data such as financial transactions, prison incidents, visits, seized mobile phone data. The pipeline cannot run using standard nodes and uses a high-memory node. Run end-to-end processing workflows involving multiple steps and dependencies The Safety Diagnostic Tool (SDT) airflow pipeline is extremely complex and made up of 7 tasks, each with different dependencies. For example, the ‘spells-correction’ task cleans prisoner spells data in NOMIS, which is then used to fit the Violence in Prisons Estimator (VIPER) model in the ‘viper’ task. This task provides a summary of prisoners’s violence that is used in the SDT RShiny application. Monitor the performance of workflows, identify and resolve issues Airflow allows users to visually monitor failed tasks and receive email notifications. Once corrected, users can restart the pipeline from that task instead of restarting the entire pipeline What is Airflow Below are some key concepts in Airflow. What is discussed is covered in more detail in this talk . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 675}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8178309534ed4f5147612e822d491e3c'}>,\n",
       " <Document: {'content': 'Tasks These are the things you want to do; the building blocks of the pipeline. Let’s say I want to run a script that does some basic data processing when a specific file is created in a particular location. Once this processing is done, a second script writes a report based on the processed data. You might want to model this as a set of tasks i.e. file_created_sensor -> script_1 -> script_2. Operator An Operator is a Task template. There are loads of pre-made operators to carry out common tasks such as running python ( PythonOperator ) or bash ( BashOperator ) scripts. However it is best practice to only use operators that trigger computation out of Airflow in a separate processing solution. We use a Kubernetes cluster and the KubernetesPodOperator to launch a docker container (see below). DAG (Directed Acyclic Graph) DAGs define the tasks, the order of operations, and the frequency/schedule that the pipeline is run. DAGs have a schedule interval (a cron-expression detailing when and how often to run the pipeline), and are considered “acyclic” because the sequence flow must never loop back on itself. There can be branches and joins, but no cycles. For example task A -> B -> C -> A is not allowed. Schedule Interval This is an expression describing the start date and end date bound to the data ingested by the pipeline , and the frequency at which to run the pipeline. For example, a @daily task defined as 19:00:00, starting on 01-01-2022. This task would be triggered just after 18:59:59 the following day (02-01-2022) after the full day’s worth of data exists. As such, the scheduler runs the job at the end of each period. Airflow UI The Airflow UI makes it easy to monitor and troubleshoot your data pipelines. You can also use it to manually trigger your workflow. What is Kubernetes Kubernetes is a platform for automating the deployment, scaling, and management of containers. We use it in conjunction with Airflow to run Airflow pipelines. This means Airflow can concentrate on scheduling and monitoring the pipelines, whilst the Kubernetes cluster can concentrate on doing the heavy-lifting processing. You will not have access to the Kubernetes cluster but it’s helpful to understand the key concepts. Container The term to describe a portable software package of code for an application along with the dependencies it needs at run time. Containers isolate software from its environment and ensure that it works uniformly regardless of underlying infrastructure (e.g. running on Windows vs. Linux). Your containers will usually contain R/python code. Image act as a set of instructions to build a container, like a template. Public and private registries are used to store and share images such as Amazon Elastic Container Registry ECR . Docker Software framework for building and running containers. Cluster When you deploy Kubernetes, you get a cluster. A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 676}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '34f0e3685f6ba5dd75603eed716155d4'}>,\n",
       " <Document: {'content': 'Node Kubernetes runs your workload by placing containers into Pods to run on Nodes. A node may be a virtual or physical machine, depending on the cluster. Kubernetes can create nodes with different specifications (e.g. high-memory) and force pods to run on specific nodes. Pod a group of one or more\\xa0containers with shared resources, and a specification for how to run the containers. Airflow Environments There are two separate Airflow environments, each with it’s own Kubernetes cluster: Dev : for training and testing new/updates to pipelines Prod : for running production pipelines There is also a sandpit environement for data engineers to test upgrades to the Airflow platform but you will not have access to this environment. Airflow Pipeline Within the MoJ Analytical Platform, a typical Airflow pipeline consists of the following steps (Actions inside the grey box are automated): The DAG can be triggered by an analyst through the Airflow UI or through a schedule The DAG script is stored in an AWS S3 bucket and defines a Kubernetes pod operator and an image which contains the required python/R code The Kubernetes pod operator launches a single-container pod in a Kubernetes cluster The pod pulls the image from the ECR registry in the Data Engineering AWS account The pod will need permission to access various AWS resources (i.e. run an Athena query, read-write to a bucket, etc). This is achieved by assuming an Identity and Access Management (IAM) role with the relevant permissions The output is then usually saved to an S3 bucket The next two sections summarises the process for creating (and maintaining) the DAG, image and IAM roles. This uses two deployment pipelines which are automated using various Github actions that we have created to facilitate this process. Image Pipeline Note that you can skip this pipeline if you already have a working docker image saved to the Data Engineering ECR. This deployment pipeline creates a docker image which contains the analytical code (python or R) that will be run in the dockerised task. Actions highlighted in grey are automated. The image repo must contain the build-and-push-to-ecr Github action to push the docker image to the Data Engineering Elastic Container Registry (ECR). This can be done by: copying template-airflow-python for python images copying template-airflow-r for R images copying the Github action to an existing image repo Please see Image pipeline for more details. DAG Pipeline This deployment pipeline creates the Directed Acyclic Graph (DAG) which defines the tasks that will be run, as well as the IAM role which the Kubernetes pod will need in order to access relevant AWS resources and services. Actions highlighted in grey are automated. You must add the DAG and role policies to airflow following specific rules. See DAG pipeline for more details. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 677}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '78685025a2231db0831d5f7ee933faa9'}>,\n",
       " <Document: {'content': 'Once you raise the PR and it is approved by data engineering, various Github actions will automatically: validate the DAG and policies adhere to the rules notify DE through a slack notification create/update the IAM role save the DAG to an S3 bucket which can be accessed by Airflow Component Responsibilities Since an Airflow pipeline consists of so many moving components, it is helpful to summarise everyone’s remit. Analysts are responsible for creating/maintaining the following components: DAG which defines the analytical tasks that the pipeline will run IAM policies for the IAM Role that the Kubernetes pod will use Image repo and the code that will be run in the tasks Data Engineering is responsible for maintaining the following components: airflow environments kubernetes clusters github actions for automating the deployment pipelines template image repo to base the image repo from user guidance as well as approving the DAG and IAM Role PR. When not to use an Airflow to trigger an Analytical Platform app (the Analytical Platform team can create cron jobs for this) This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAirflow Concepts - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 678}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cb8803b9637da0734230b75fc653d12b'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 679}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Airflow Concepts Why use Airflow Run tasks on a regular schedule For example the bentham app airflow pipeline runs daily and processes prisoner information from NOMIS and outputs the data to feather files. This forms the backend for the bentham app which is used by prison-based intelligence analysts to search through seized media. Run memory-intensive workloads For example the prison network airflow pipeline runs daily and downloads and processes ~100GB of data. Each prisoner is assigned a node and edges to link the nodes are created from data such as financial transactions, prison incidents, visits, seized mobile phone data. The pipeline cannot run using standard nodes and uses a high-memory node. Run end-to-end processing workflows involving multiple steps and dependencies The Safety Diagnostic Tool (SDT) airflow pipeline is extremely complex and made up of 7 tasks, each with different dependencies. For example, the ‘spells-correction’ task cleans prisoner spells data in NOMIS, which is then used to fit the Violence in Prisons Estimator (VIPER) model in the ‘viper’ task. This task provides a summary of prisoners’s violence that is used in the SDT RShiny application. Monitor the performance of workflows, identify and resolve issues Airflow allows users to visually monitor failed tasks and receive email notifications. Once corrected, users can restart the pipeline from that task instead of restarting the entire pipeline What is Airflow Below are some key concepts in Airflow. What is discussed is covered in more detail in this talk . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 680}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8178309534ed4f5147612e822d491e3c'}>,\n",
       " <Document: {'content': 'Tasks These are the things you want to do; the building blocks of the pipeline. Let’s say I want to run a script that does some basic data processing when a specific file is created in a particular location. Once this processing is done, a second script writes a report based on the processed data. You might want to model this as a set of tasks i.e. file_created_sensor -> script_1 -> script_2. Operator An Operator is a Task template. There are loads of pre-made operators to carry out common tasks such as running python ( PythonOperator ) or bash ( BashOperator ) scripts. However it is best practice to only use operators that trigger computation out of Airflow in a separate processing solution. We use a Kubernetes cluster and the KubernetesPodOperator to launch a docker container (see below). DAG (Directed Acyclic Graph) DAGs define the tasks, the order of operations, and the frequency/schedule that the pipeline is run. DAGs have a schedule interval (a cron-expression detailing when and how often to run the pipeline), and are considered “acyclic” because the sequence flow must never loop back on itself. There can be branches and joins, but no cycles. For example task A -> B -> C -> A is not allowed. Schedule Interval This is an expression describing the start date and end date bound to the data ingested by the pipeline , and the frequency at which to run the pipeline. For example, a @daily task defined as 19:00:00, starting on 01-01-2022. This task would be triggered just after 18:59:59 the following day (02-01-2022) after the full day’s worth of data exists. As such, the scheduler runs the job at the end of each period. Airflow UI The Airflow UI makes it easy to monitor and troubleshoot your data pipelines. You can also use it to manually trigger your workflow. What is Kubernetes Kubernetes is a platform for automating the deployment, scaling, and management of containers. We use it in conjunction with Airflow to run Airflow pipelines. This means Airflow can concentrate on scheduling and monitoring the pipelines, whilst the Kubernetes cluster can concentrate on doing the heavy-lifting processing. You will not have access to the Kubernetes cluster but it’s helpful to understand the key concepts. Container The term to describe a portable software package of code for an application along with the dependencies it needs at run time. Containers isolate software from its environment and ensure that it works uniformly regardless of underlying infrastructure (e.g. running on Windows vs. Linux). Your containers will usually contain R/python code. Image act as a set of instructions to build a container, like a template. Public and private registries are used to store and share images such as Amazon Elastic Container Registry ECR . Docker Software framework for building and running containers. Cluster When you deploy Kubernetes, you get a cluster. A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 681}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '34f0e3685f6ba5dd75603eed716155d4'}>,\n",
       " <Document: {'content': 'Node Kubernetes runs your workload by placing containers into Pods to run on Nodes. A node may be a virtual or physical machine, depending on the cluster. Kubernetes can create nodes with different specifications (e.g. high-memory) and force pods to run on specific nodes. Pod a group of one or more\\xa0containers with shared resources, and a specification for how to run the containers. Airflow Environments There are two separate Airflow environments, each with it’s own Kubernetes cluster: Dev : for training and testing new/updates to pipelines Prod : for running production pipelines There is also a sandpit environement for data engineers to test upgrades to the Airflow platform but you will not have access to this environment. Airflow Pipeline Within the MoJ Analytical Platform, a typical Airflow pipeline consists of the following steps (Actions inside the grey box are automated): The DAG can be triggered by an analyst through the Airflow UI or through a schedule The DAG script is stored in an AWS S3 bucket and defines a Kubernetes pod operator and an image which contains the required python/R code The Kubernetes pod operator launches a single-container pod in a Kubernetes cluster The pod pulls the image from the ECR registry in the Data Engineering AWS account The pod will need permission to access various AWS resources (i.e. run an Athena query, read-write to a bucket, etc). This is achieved by assuming an Identity and Access Management (IAM) role with the relevant permissions The output is then usually saved to an S3 bucket The next two sections summarises the process for creating (and maintaining) the DAG, image and IAM roles. This uses two deployment pipelines which are automated using various Github actions that we have created to facilitate this process. Image Pipeline Note that you can skip this pipeline if you already have a working docker image saved to the Data Engineering ECR. This deployment pipeline creates a docker image which contains the analytical code (python or R) that will be run in the dockerised task. Actions highlighted in grey are automated. The image repo must contain the build-and-push-to-ecr Github action to push the docker image to the Data Engineering Elastic Container Registry (ECR). This can be done by: copying template-airflow-python for python images copying template-airflow-r for R images copying the Github action to an existing image repo Please see Image pipeline for more details. DAG Pipeline This deployment pipeline creates the Directed Acyclic Graph (DAG) which defines the tasks that will be run, as well as the IAM role which the Kubernetes pod will need in order to access relevant AWS resources and services. Actions highlighted in grey are automated. You must add the DAG and role policies to airflow following specific rules. See DAG pipeline for more details. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 682}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '78685025a2231db0831d5f7ee933faa9'}>,\n",
       " <Document: {'content': 'Once you raise the PR and it is approved by data engineering, various Github actions will automatically: validate the DAG and policies adhere to the rules notify DE through a slack notification create/update the IAM role save the DAG to an S3 bucket which can be accessed by Airflow Component Responsibilities Since an Airflow pipeline consists of so many moving components, it is helpful to summarise everyone’s remit. Analysts are responsible for creating/maintaining the following components: DAG which defines the analytical tasks that the pipeline will run IAM policies for the IAM Role that the Kubernetes pod will use Image repo and the code that will be run in the tasks Data Engineering is responsible for maintaining the following components: airflow environments kubernetes clusters github actions for automating the deployment pipelines template image repo to base the image repo from user guidance as well as approving the DAG and IAM Role PR. When not to use an Airflow to trigger an Analytical Platform app (the Analytical Platform team can create cron jobs for this) This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAirflow Concepts - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 683}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cb8803b9637da0734230b75fc653d12b'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 684}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Airflow Concepts Why use Airflow Run tasks on a regular schedule For example the bentham app airflow pipeline runs daily and processes prisoner information from NOMIS and outputs the data to feather files. This forms the backend for the bentham app which is used by prison-based intelligence analysts to search through seized media. Run memory-intensive workloads For example the prison network airflow pipeline runs daily and downloads and processes ~100GB of data. Each prisoner is assigned a node and edges to link the nodes are created from data such as financial transactions, prison incidents, visits, seized mobile phone data. The pipeline cannot run using standard nodes and uses a high-memory node. Run end-to-end processing workflows involving multiple steps and dependencies The Safety Diagnostic Tool (SDT) airflow pipeline is extremely complex and made up of 7 tasks, each with different dependencies. For example, the ‘spells-correction’ task cleans prisoner spells data in NOMIS, which is then used to fit the Violence in Prisons Estimator (VIPER) model in the ‘viper’ task. This task provides a summary of prisoners’s violence that is used in the SDT RShiny application. Monitor the performance of workflows, identify and resolve issues Airflow allows users to visually monitor failed tasks and receive email notifications. Once corrected, users can restart the pipeline from that task instead of restarting the entire pipeline What is Airflow Below are some key concepts in Airflow. What is discussed is covered in more detail in this talk . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 685}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8178309534ed4f5147612e822d491e3c'}>,\n",
       " <Document: {'content': 'Tasks These are the things you want to do; the building blocks of the pipeline. Let’s say I want to run a script that does some basic data processing when a specific file is created in a particular location. Once this processing is done, a second script writes a report based on the processed data. You might want to model this as a set of tasks i.e. file_created_sensor -> script_1 -> script_2. Operator An Operator is a Task template. There are loads of pre-made operators to carry out common tasks such as running python ( PythonOperator ) or bash ( BashOperator ) scripts. However it is best practice to only use operators that trigger computation out of Airflow in a separate processing solution. We use a Kubernetes cluster and the KubernetesPodOperator to launch a docker container (see below). DAG (Directed Acyclic Graph) DAGs define the tasks, the order of operations, and the frequency/schedule that the pipeline is run. DAGs have a schedule interval (a cron-expression detailing when and how often to run the pipeline), and are considered “acyclic” because the sequence flow must never loop back on itself. There can be branches and joins, but no cycles. For example task A -> B -> C -> A is not allowed. Schedule Interval This is an expression describing the start date and end date bound to the data ingested by the pipeline , and the frequency at which to run the pipeline. For example, a @daily task defined as 19:00:00, starting on 01-01-2022. This task would be triggered just after 18:59:59 the following day (02-01-2022) after the full day’s worth of data exists. As such, the scheduler runs the job at the end of each period. Airflow UI The Airflow UI makes it easy to monitor and troubleshoot your data pipelines. You can also use it to manually trigger your workflow. What is Kubernetes Kubernetes is a platform for automating the deployment, scaling, and management of containers. We use it in conjunction with Airflow to run Airflow pipelines. This means Airflow can concentrate on scheduling and monitoring the pipelines, whilst the Kubernetes cluster can concentrate on doing the heavy-lifting processing. You will not have access to the Kubernetes cluster but it’s helpful to understand the key concepts. Container The term to describe a portable software package of code for an application along with the dependencies it needs at run time. Containers isolate software from its environment and ensure that it works uniformly regardless of underlying infrastructure (e.g. running on Windows vs. Linux). Your containers will usually contain R/python code. Image act as a set of instructions to build a container, like a template. Public and private registries are used to store and share images such as Amazon Elastic Container Registry ECR . Docker Software framework for building and running containers. Cluster When you deploy Kubernetes, you get a cluster. A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 686}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '34f0e3685f6ba5dd75603eed716155d4'}>,\n",
       " <Document: {'content': 'Node Kubernetes runs your workload by placing containers into Pods to run on Nodes. A node may be a virtual or physical machine, depending on the cluster. Kubernetes can create nodes with different specifications (e.g. high-memory) and force pods to run on specific nodes. Pod a group of one or more\\xa0containers with shared resources, and a specification for how to run the containers. Airflow Environments There are two separate Airflow environments, each with it’s own Kubernetes cluster: Dev : for training and testing new/updates to pipelines Prod : for running production pipelines There is also a sandpit environement for data engineers to test upgrades to the Airflow platform but you will not have access to this environment. Airflow Pipeline Within the MoJ Analytical Platform, a typical Airflow pipeline consists of the following steps (Actions inside the grey box are automated): The DAG can be triggered by an analyst through the Airflow UI or through a schedule The DAG script is stored in an AWS S3 bucket and defines a Kubernetes pod operator and an image which contains the required python/R code The Kubernetes pod operator launches a single-container pod in a Kubernetes cluster The pod pulls the image from the ECR registry in the Data Engineering AWS account The pod will need permission to access various AWS resources (i.e. run an Athena query, read-write to a bucket, etc). This is achieved by assuming an Identity and Access Management (IAM) role with the relevant permissions The output is then usually saved to an S3 bucket The next two sections summarises the process for creating (and maintaining) the DAG, image and IAM roles. This uses two deployment pipelines which are automated using various Github actions that we have created to facilitate this process. Image Pipeline Note that you can skip this pipeline if you already have a working docker image saved to the Data Engineering ECR. This deployment pipeline creates a docker image which contains the analytical code (python or R) that will be run in the dockerised task. Actions highlighted in grey are automated. The image repo must contain the build-and-push-to-ecr Github action to push the docker image to the Data Engineering Elastic Container Registry (ECR). This can be done by: copying template-airflow-python for python images copying template-airflow-r for R images copying the Github action to an existing image repo Please see Image pipeline for more details. DAG Pipeline This deployment pipeline creates the Directed Acyclic Graph (DAG) which defines the tasks that will be run, as well as the IAM role which the Kubernetes pod will need in order to access relevant AWS resources and services. Actions highlighted in grey are automated. You must add the DAG and role policies to airflow following specific rules. See DAG pipeline for more details. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 687}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '78685025a2231db0831d5f7ee933faa9'}>,\n",
       " <Document: {'content': 'Once you raise the PR and it is approved by data engineering, various Github actions will automatically: validate the DAG and policies adhere to the rules notify DE through a slack notification create/update the IAM role save the DAG to an S3 bucket which can be accessed by Airflow Component Responsibilities Since an Airflow pipeline consists of so many moving components, it is helpful to summarise everyone’s remit. Analysts are responsible for creating/maintaining the following components: DAG which defines the analytical tasks that the pipeline will run IAM policies for the IAM Role that the Kubernetes pod will use Image repo and the code that will be run in the tasks Data Engineering is responsible for maintaining the following components: airflow environments kubernetes clusters github actions for automating the deployment pipelines template image repo to base the image repo from user guidance as well as approving the DAG and IAM Role PR. When not to use an Airflow to trigger an Analytical Platform app (the Analytical Platform team can create cron jobs for this) This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAirflow Concepts - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 688}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cb8803b9637da0734230b75fc653d12b'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 689}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Airflow Concepts Why use Airflow Run tasks on a regular schedule For example the bentham app airflow pipeline runs daily and processes prisoner information from NOMIS and outputs the data to feather files. This forms the backend for the bentham app which is used by prison-based intelligence analysts to search through seized media. Run memory-intensive workloads For example the prison network airflow pipeline runs daily and downloads and processes ~100GB of data. Each prisoner is assigned a node and edges to link the nodes are created from data such as financial transactions, prison incidents, visits, seized mobile phone data. The pipeline cannot run using standard nodes and uses a high-memory node. Run end-to-end processing workflows involving multiple steps and dependencies The Safety Diagnostic Tool (SDT) airflow pipeline is extremely complex and made up of 7 tasks, each with different dependencies. For example, the ‘spells-correction’ task cleans prisoner spells data in NOMIS, which is then used to fit the Violence in Prisons Estimator (VIPER) model in the ‘viper’ task. This task provides a summary of prisoners’s violence that is used in the SDT RShiny application. Monitor the performance of workflows, identify and resolve issues Airflow allows users to visually monitor failed tasks and receive email notifications. Once corrected, users can restart the pipeline from that task instead of restarting the entire pipeline What is Airflow Below are some key concepts in Airflow. What is discussed is covered in more detail in this talk . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 690}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8178309534ed4f5147612e822d491e3c'}>,\n",
       " <Document: {'content': 'Tasks These are the things you want to do; the building blocks of the pipeline. Let’s say I want to run a script that does some basic data processing when a specific file is created in a particular location. Once this processing is done, a second script writes a report based on the processed data. You might want to model this as a set of tasks i.e. file_created_sensor -> script_1 -> script_2. Operator An Operator is a Task template. There are loads of pre-made operators to carry out common tasks such as running python ( PythonOperator ) or bash ( BashOperator ) scripts. However it is best practice to only use operators that trigger computation out of Airflow in a separate processing solution. We use a Kubernetes cluster and the KubernetesPodOperator to launch a docker container (see below). DAG (Directed Acyclic Graph) DAGs define the tasks, the order of operations, and the frequency/schedule that the pipeline is run. DAGs have a schedule interval (a cron-expression detailing when and how often to run the pipeline), and are considered “acyclic” because the sequence flow must never loop back on itself. There can be branches and joins, but no cycles. For example task A -> B -> C -> A is not allowed. Schedule Interval This is an expression describing the start date and end date bound to the data ingested by the pipeline , and the frequency at which to run the pipeline. For example, a @daily task defined as 19:00:00, starting on 01-01-2022. This task would be triggered just after 18:59:59 the following day (02-01-2022) after the full day’s worth of data exists. As such, the scheduler runs the job at the end of each period. Airflow UI The Airflow UI makes it easy to monitor and troubleshoot your data pipelines. You can also use it to manually trigger your workflow. What is Kubernetes Kubernetes is a platform for automating the deployment, scaling, and management of containers. We use it in conjunction with Airflow to run Airflow pipelines. This means Airflow can concentrate on scheduling and monitoring the pipelines, whilst the Kubernetes cluster can concentrate on doing the heavy-lifting processing. You will not have access to the Kubernetes cluster but it’s helpful to understand the key concepts. Container The term to describe a portable software package of code for an application along with the dependencies it needs at run time. Containers isolate software from its environment and ensure that it works uniformly regardless of underlying infrastructure (e.g. running on Windows vs. Linux). Your containers will usually contain R/python code. Image act as a set of instructions to build a container, like a template. Public and private registries are used to store and share images such as Amazon Elastic Container Registry ECR . Docker Software framework for building and running containers. Cluster When you deploy Kubernetes, you get a cluster. A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 691}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '34f0e3685f6ba5dd75603eed716155d4'}>,\n",
       " <Document: {'content': 'Node Kubernetes runs your workload by placing containers into Pods to run on Nodes. A node may be a virtual or physical machine, depending on the cluster. Kubernetes can create nodes with different specifications (e.g. high-memory) and force pods to run on specific nodes. Pod a group of one or more\\xa0containers with shared resources, and a specification for how to run the containers. Airflow Environments There are two separate Airflow environments, each with it’s own Kubernetes cluster: Dev : for training and testing new/updates to pipelines Prod : for running production pipelines There is also a sandpit environement for data engineers to test upgrades to the Airflow platform but you will not have access to this environment. Airflow Pipeline Within the MoJ Analytical Platform, a typical Airflow pipeline consists of the following steps (Actions inside the grey box are automated): The DAG can be triggered by an analyst through the Airflow UI or through a schedule The DAG script is stored in an AWS S3 bucket and defines a Kubernetes pod operator and an image which contains the required python/R code The Kubernetes pod operator launches a single-container pod in a Kubernetes cluster The pod pulls the image from the ECR registry in the Data Engineering AWS account The pod will need permission to access various AWS resources (i.e. run an Athena query, read-write to a bucket, etc). This is achieved by assuming an Identity and Access Management (IAM) role with the relevant permissions The output is then usually saved to an S3 bucket The next two sections summarises the process for creating (and maintaining) the DAG, image and IAM roles. This uses two deployment pipelines which are automated using various Github actions that we have created to facilitate this process. Image Pipeline Note that you can skip this pipeline if you already have a working docker image saved to the Data Engineering ECR. This deployment pipeline creates a docker image which contains the analytical code (python or R) that will be run in the dockerised task. Actions highlighted in grey are automated. The image repo must contain the build-and-push-to-ecr Github action to push the docker image to the Data Engineering Elastic Container Registry (ECR). This can be done by: copying template-airflow-python for python images copying template-airflow-r for R images copying the Github action to an existing image repo Please see Image pipeline for more details. DAG Pipeline This deployment pipeline creates the Directed Acyclic Graph (DAG) which defines the tasks that will be run, as well as the IAM role which the Kubernetes pod will need in order to access relevant AWS resources and services. Actions highlighted in grey are automated. You must add the DAG and role policies to airflow following specific rules. See DAG pipeline for more details. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 692}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '78685025a2231db0831d5f7ee933faa9'}>,\n",
       " <Document: {'content': 'Once you raise the PR and it is approved by data engineering, various Github actions will automatically: validate the DAG and policies adhere to the rules notify DE through a slack notification create/update the IAM role save the DAG to an S3 bucket which can be accessed by Airflow Component Responsibilities Since an Airflow pipeline consists of so many moving components, it is helpful to summarise everyone’s remit. Analysts are responsible for creating/maintaining the following components: DAG which defines the analytical tasks that the pipeline will run IAM policies for the IAM Role that the Kubernetes pod will use Image repo and the code that will be run in the tasks Data Engineering is responsible for maintaining the following components: airflow environments kubernetes clusters github actions for automating the deployment pipelines template image repo to base the image repo from user guidance as well as approving the DAG and IAM Role PR. When not to use an Airflow to trigger an Analytical Platform app (the Analytical Platform team can create cron jobs for this) This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAirflow Concepts - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 693}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cb8803b9637da0734230b75fc653d12b'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 694}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Airflow Concepts Why use Airflow Run tasks on a regular schedule For example the bentham app airflow pipeline runs daily and processes prisoner information from NOMIS and outputs the data to feather files. This forms the backend for the bentham app which is used by prison-based intelligence analysts to search through seized media. Run memory-intensive workloads For example the prison network airflow pipeline runs daily and downloads and processes ~100GB of data. Each prisoner is assigned a node and edges to link the nodes are created from data such as financial transactions, prison incidents, visits, seized mobile phone data. The pipeline cannot run using standard nodes and uses a high-memory node. Run end-to-end processing workflows involving multiple steps and dependencies The Safety Diagnostic Tool (SDT) airflow pipeline is extremely complex and made up of 7 tasks, each with different dependencies. For example, the ‘spells-correction’ task cleans prisoner spells data in NOMIS, which is then used to fit the Violence in Prisons Estimator (VIPER) model in the ‘viper’ task. This task provides a summary of prisoners’s violence that is used in the SDT RShiny application. Monitor the performance of workflows, identify and resolve issues Airflow allows users to visually monitor failed tasks and receive email notifications. Once corrected, users can restart the pipeline from that task instead of restarting the entire pipeline What is Airflow Below are some key concepts in Airflow. What is discussed is covered in more detail in this talk . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 695}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8178309534ed4f5147612e822d491e3c'}>,\n",
       " <Document: {'content': 'Tasks These are the things you want to do; the building blocks of the pipeline. Let’s say I want to run a script that does some basic data processing when a specific file is created in a particular location. Once this processing is done, a second script writes a report based on the processed data. You might want to model this as a set of tasks i.e. file_created_sensor -> script_1 -> script_2. Operator An Operator is a Task template. There are loads of pre-made operators to carry out common tasks such as running python ( PythonOperator ) or bash ( BashOperator ) scripts. However it is best practice to only use operators that trigger computation out of Airflow in a separate processing solution. We use a Kubernetes cluster and the KubernetesPodOperator to launch a docker container (see below). DAG (Directed Acyclic Graph) DAGs define the tasks, the order of operations, and the frequency/schedule that the pipeline is run. DAGs have a schedule interval (a cron-expression detailing when and how often to run the pipeline), and are considered “acyclic” because the sequence flow must never loop back on itself. There can be branches and joins, but no cycles. For example task A -> B -> C -> A is not allowed. Schedule Interval This is an expression describing the start date and end date bound to the data ingested by the pipeline , and the frequency at which to run the pipeline. For example, a @daily task defined as 19:00:00, starting on 01-01-2022. This task would be triggered just after 18:59:59 the following day (02-01-2022) after the full day’s worth of data exists. As such, the scheduler runs the job at the end of each period. Airflow UI The Airflow UI makes it easy to monitor and troubleshoot your data pipelines. You can also use it to manually trigger your workflow. What is Kubernetes Kubernetes is a platform for automating the deployment, scaling, and management of containers. We use it in conjunction with Airflow to run Airflow pipelines. This means Airflow can concentrate on scheduling and monitoring the pipelines, whilst the Kubernetes cluster can concentrate on doing the heavy-lifting processing. You will not have access to the Kubernetes cluster but it’s helpful to understand the key concepts. Container The term to describe a portable software package of code for an application along with the dependencies it needs at run time. Containers isolate software from its environment and ensure that it works uniformly regardless of underlying infrastructure (e.g. running on Windows vs. Linux). Your containers will usually contain R/python code. Image act as a set of instructions to build a container, like a template. Public and private registries are used to store and share images such as Amazon Elastic Container Registry ECR . Docker Software framework for building and running containers. Cluster When you deploy Kubernetes, you get a cluster. A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 696}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '34f0e3685f6ba5dd75603eed716155d4'}>,\n",
       " <Document: {'content': 'Node Kubernetes runs your workload by placing containers into Pods to run on Nodes. A node may be a virtual or physical machine, depending on the cluster. Kubernetes can create nodes with different specifications (e.g. high-memory) and force pods to run on specific nodes. Pod a group of one or more\\xa0containers with shared resources, and a specification for how to run the containers. Airflow Environments There are two separate Airflow environments, each with it’s own Kubernetes cluster: Dev : for training and testing new/updates to pipelines Prod : for running production pipelines There is also a sandpit environement for data engineers to test upgrades to the Airflow platform but you will not have access to this environment. Airflow Pipeline Within the MoJ Analytical Platform, a typical Airflow pipeline consists of the following steps (Actions inside the grey box are automated): The DAG can be triggered by an analyst through the Airflow UI or through a schedule The DAG script is stored in an AWS S3 bucket and defines a Kubernetes pod operator and an image which contains the required python/R code The Kubernetes pod operator launches a single-container pod in a Kubernetes cluster The pod pulls the image from the ECR registry in the Data Engineering AWS account The pod will need permission to access various AWS resources (i.e. run an Athena query, read-write to a bucket, etc). This is achieved by assuming an Identity and Access Management (IAM) role with the relevant permissions The output is then usually saved to an S3 bucket The next two sections summarises the process for creating (and maintaining) the DAG, image and IAM roles. This uses two deployment pipelines which are automated using various Github actions that we have created to facilitate this process. Image Pipeline Note that you can skip this pipeline if you already have a working docker image saved to the Data Engineering ECR. This deployment pipeline creates a docker image which contains the analytical code (python or R) that will be run in the dockerised task. Actions highlighted in grey are automated. The image repo must contain the build-and-push-to-ecr Github action to push the docker image to the Data Engineering Elastic Container Registry (ECR). This can be done by: copying template-airflow-python for python images copying template-airflow-r for R images copying the Github action to an existing image repo Please see Image pipeline for more details. DAG Pipeline This deployment pipeline creates the Directed Acyclic Graph (DAG) which defines the tasks that will be run, as well as the IAM role which the Kubernetes pod will need in order to access relevant AWS resources and services. Actions highlighted in grey are automated. You must add the DAG and role policies to airflow following specific rules. See DAG pipeline for more details. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 697}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '78685025a2231db0831d5f7ee933faa9'}>,\n",
       " <Document: {'content': 'Once you raise the PR and it is approved by data engineering, various Github actions will automatically: validate the DAG and policies adhere to the rules notify DE through a slack notification create/update the IAM role save the DAG to an S3 bucket which can be accessed by Airflow Component Responsibilities Since an Airflow pipeline consists of so many moving components, it is helpful to summarise everyone’s remit. Analysts are responsible for creating/maintaining the following components: DAG which defines the analytical tasks that the pipeline will run IAM policies for the IAM Role that the Kubernetes pod will use Image repo and the code that will be run in the tasks Data Engineering is responsible for maintaining the following components: airflow environments kubernetes clusters github actions for automating the deployment pipelines template image repo to base the image repo from user guidance as well as approving the DAG and IAM Role PR. When not to use an Airflow to trigger an Analytical Platform app (the Analytical Platform team can create cron jobs for this) This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAirflow Instructions - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 698}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7178f666c7d383e4f6e465e13fb3850d'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 699}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Airflow Instructions These instructions will show you how to build and run a standard Airflow pipeline based on a Kubernetes operator in the dev environment. You will first complete the two deployment pipelines to build and save a docker image to ECR, as well as create a DAG and IAM role. You can use the provided scripts, DAG and IAM policy to create an “example pipeline” which will write the word “Hello” to a file and upload to an S3 bucket. Alternatively you can define your own scripts, DAG and IAM policy. You will then run the Airflow pipeline. Prerequisites Github Account Analytical Platform Account Slack Account Channel: #ask-data-engineering Create image and save to ECR See Image pipeline Create DAG and IAM Role See DAG and Role pipeline Run the Airflow Pipeline Log in to the dev Airflow UI Find your DAG. In the case of the example pipeline it will be called {username}.write_to_s3 Toggle the DAG unpause the DAG Trigger the DAG Click on the DAG to open the tree view Click on the GRAPH tab to see the graph view and wait until it goes green. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 700}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6aae315a551ad790b4a603cb235b258d'}>,\n",
       " <Document: {'content': 'This should take up to a minute for the example pipeline Click on the task and log to see the log output If you have permission and running the example pipeline, go the S3 directory s3://alpha-everyone/airflow-example and check that a file called test.txt has been created in the {username} folder This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nImage Pipeline - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 701}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '43a87ea07eb77f5c69e249cddc8df826'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 702}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Image Pipeline These instructions show you how to use the use the template github repos to build a Python or R image and save to the Data Engineering ECR. Create a new GitHub repo using: template-airflow-python if creating a python codebase template-airflow-R if creating an R codebase\\nWhile creating your own repo using the template of your choice, please ensure moj-analytical-services is designated as the owner of the repo. If your own GitHub account is left as owner, then the GitHub Action used to publish your image will fail when trying to run. The image will have the same name as the repo name so make sure it is appropriate and reflects the pipeline you intend to run. If you are creating an example pipeline call it airflow-{username}-example Review the scripts/run.py or scripts/run.R file. This has some code to write and copy to S3. Leave as-is if you are creating an example pipeline. Otherwise replace with your own logic (see Tips on writing the code ) Review the Dockerfile and the parent image and update as necessary (see Dockerfile ). Leave as-is if creating the example pipeline Review the requirements.txt file and update as necessary. Leave as-is if creating the example pipeline. See venv for more details (For R images only) Review the renv.lock file and update as necessary. Leave as-is if creating the example pipeline. See Renv for more details Create a tag and release, ensuring Target is set on the main branch. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 703}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cdcdc88e4292fb0075e6ee3ff9e2130d'}>,\n",
       " <Document: {'content': 'Set the tag and release to v0.0.1 if you are creating an example pipeline Go to the Actions tab and you should see the “Build, tag, push, and make available image to pods” action running. Make sure the action passes otherwise the image will not be built If you have permission, log in to ECR and search for your image and tag Tips on writing the code You can create scripts in any programming language, including R and Python. You may want to test your scripts in RStudio or JupyterLab on the Analytical Platform before running them as part of a pipeline. All Python scripts in your Airflow repository should be formatted according to flake8 rules. flake8 is a code linter that analyses your Python code and flags bugs, programming errors and stylistic errors. You can automatically format your code using tools like black , autopep8 and yapf . These tools are often able to resolve most formatting issues. Environment Variables You can use environment variables to pass in variables to the docker container. We tend to write them in caps to point out the fact they will be passed in as environmental variables. You can use the same Docker image for multiple tasks by passing an environment variable. In the use_kubernetes_pod_operator.py example, we pass in the environment variable “write” and “copy” to first write to S3, then copy the file across, using the same image. Dockerfile A Dockerfile is a text file that contains the commands used to build a Docker image. It starts with a FROM directive, which specifies the parent image that your image is based on. Each subsequent declaration in the Dockerfile modifies this parent image. We have a range of parent images to chose from, get in touch if the available images do not meet your requirements. You can use venv, conda, packrat, renv or other package management tools to capture the dependencies required by your pipeline. If using one of these tools, you will need to update the Dockerfile to install required packages correctly. (For R images only) If you’re not using Python at all, for example if you’re using Rdbtools and Rs3tools rather than dbtools and botor, then replace Dockerfile with Dockerfile.nopython and delete the requirements.txt file. You can do this by running: mv Dockerfile Dockerfile.backup\\ncp Dockerfile.nopython Dockerfile\\nrm requirements.txt Test Docker image (optional) If you have a MacBook, you can use Docker locally to build and test your Docker image. You can download Docker Desktop for Mac here . To build and test your Docker image locally, follow the steps below: Clone your Airflow repository to a new folder on your MacBook – this guarantees that the Docker image will be built using the same code as on the Analytical Platform. You may need to create a new connection to GitHub with SSH . Open a terminal session and navigate to the directory containing the Dockerfile using the cd command. Build the Docker image by running: docker build . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 704}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b46d965ae19af938caa89c57e69241d0'}>,\n",
       " <Document: {'content': '- t IMAGE: TAG where IMAGE is a name for the image, for example, my-docker-image , and TAG is the version number, for example, v0.1 . Run a Docker container created from the Docker image by running: docker run IMAGE: TAG This will run the command specified in the CMD line of the Dockerfile . This will fail if your command requires access to resources on the Analytical Platform, such as data stored in Amazon S3 unless the correct environment variables are passed to the docker container. You would need the following environment variables to ensure correct access to all the AP resources: docker run \\\\\\n--env AWS_REGION=$AWS_REGION \\\\\\n--env AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION \\\\\\n--env AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\\\\\n--env AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\\\\\n--env AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN \\\\\\n--env AWS_SECURITY_TOKEN=$AWS_SECURITY_TOKEN \\\\\\nIMAGE:TAG Oher environment variables such as PYTHON_SCRIPT_NAME or R_SCRIPT_NAME can be passed in the same way. You can start a bash session in a running Docker container for debugging and troubleshooting purposes by running: docker run - it IMAGE: TAG bash This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nImage Pipeline - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 705}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f47248dd8f46060381c27d28e391873'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 706}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Image Pipeline These instructions show you how to use the use the template github repos to build a Python or R image and save to the Data Engineering ECR. Create a new GitHub repo using: template-airflow-python if creating a python codebase template-airflow-R if creating an R codebase\\nWhile creating your own repo using the template of your choice, please ensure moj-analytical-services is designated as the owner of the repo. If your own GitHub account is left as owner, then the GitHub Action used to publish your image will fail when trying to run. The image will have the same name as the repo name so make sure it is appropriate and reflects the pipeline you intend to run. If you are creating an example pipeline call it airflow-{username}-example Review the scripts/run.py or scripts/run.R file. This has some code to write and copy to S3. Leave as-is if you are creating an example pipeline. Otherwise replace with your own logic (see Tips on writing the code ) Review the Dockerfile and the parent image and update as necessary (see Dockerfile ). Leave as-is if creating the example pipeline Review the requirements.txt file and update as necessary. Leave as-is if creating the example pipeline. See venv for more details (For R images only) Review the renv.lock file and update as necessary. Leave as-is if creating the example pipeline. See Renv for more details Create a tag and release, ensuring Target is set on the main branch. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 707}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cdcdc88e4292fb0075e6ee3ff9e2130d'}>,\n",
       " <Document: {'content': 'Set the tag and release to v0.0.1 if you are creating an example pipeline Go to the Actions tab and you should see the “Build, tag, push, and make available image to pods” action running. Make sure the action passes otherwise the image will not be built If you have permission, log in to ECR and search for your image and tag Tips on writing the code You can create scripts in any programming language, including R and Python. You may want to test your scripts in RStudio or JupyterLab on the Analytical Platform before running them as part of a pipeline. All Python scripts in your Airflow repository should be formatted according to flake8 rules. flake8 is a code linter that analyses your Python code and flags bugs, programming errors and stylistic errors. You can automatically format your code using tools like black , autopep8 and yapf . These tools are often able to resolve most formatting issues. Environment Variables You can use environment variables to pass in variables to the docker container. We tend to write them in caps to point out the fact they will be passed in as environmental variables. You can use the same Docker image for multiple tasks by passing an environment variable. In the use_kubernetes_pod_operator.py example, we pass in the environment variable “write” and “copy” to first write to S3, then copy the file across, using the same image. Dockerfile A Dockerfile is a text file that contains the commands used to build a Docker image. It starts with a FROM directive, which specifies the parent image that your image is based on. Each subsequent declaration in the Dockerfile modifies this parent image. We have a range of parent images to chose from, get in touch if the available images do not meet your requirements. You can use venv, conda, packrat, renv or other package management tools to capture the dependencies required by your pipeline. If using one of these tools, you will need to update the Dockerfile to install required packages correctly. (For R images only) If you’re not using Python at all, for example if you’re using Rdbtools and Rs3tools rather than dbtools and botor, then replace Dockerfile with Dockerfile.nopython and delete the requirements.txt file. You can do this by running: mv Dockerfile Dockerfile.backup\\ncp Dockerfile.nopython Dockerfile\\nrm requirements.txt Test Docker image (optional) If you have a MacBook, you can use Docker locally to build and test your Docker image. You can download Docker Desktop for Mac here . To build and test your Docker image locally, follow the steps below: Clone your Airflow repository to a new folder on your MacBook – this guarantees that the Docker image will be built using the same code as on the Analytical Platform. You may need to create a new connection to GitHub with SSH . Open a terminal session and navigate to the directory containing the Dockerfile using the cd command. Build the Docker image by running: docker build . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 708}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b46d965ae19af938caa89c57e69241d0'}>,\n",
       " <Document: {'content': '- t IMAGE: TAG where IMAGE is a name for the image, for example, my-docker-image , and TAG is the version number, for example, v0.1 . Run a Docker container created from the Docker image by running: docker run IMAGE: TAG This will run the command specified in the CMD line of the Dockerfile . This will fail if your command requires access to resources on the Analytical Platform, such as data stored in Amazon S3 unless the correct environment variables are passed to the docker container. You would need the following environment variables to ensure correct access to all the AP resources: docker run \\\\\\n--env AWS_REGION=$AWS_REGION \\\\\\n--env AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION \\\\\\n--env AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\\\\\n--env AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\\\\\n--env AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN \\\\\\n--env AWS_SECURITY_TOKEN=$AWS_SECURITY_TOKEN \\\\\\nIMAGE:TAG Oher environment variables such as PYTHON_SCRIPT_NAME or R_SCRIPT_NAME can be passed in the same way. You can start a bash session in a running Docker container for debugging and troubleshooting purposes by running: docker run - it IMAGE: TAG bash This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nImage Pipeline - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 709}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f47248dd8f46060381c27d28e391873'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 710}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Image Pipeline These instructions show you how to use the use the template github repos to build a Python or R image and save to the Data Engineering ECR. Create a new GitHub repo using: template-airflow-python if creating a python codebase template-airflow-R if creating an R codebase\\nWhile creating your own repo using the template of your choice, please ensure moj-analytical-services is designated as the owner of the repo. If your own GitHub account is left as owner, then the GitHub Action used to publish your image will fail when trying to run. The image will have the same name as the repo name so make sure it is appropriate and reflects the pipeline you intend to run. If you are creating an example pipeline call it airflow-{username}-example Review the scripts/run.py or scripts/run.R file. This has some code to write and copy to S3. Leave as-is if you are creating an example pipeline. Otherwise replace with your own logic (see Tips on writing the code ) Review the Dockerfile and the parent image and update as necessary (see Dockerfile ). Leave as-is if creating the example pipeline Review the requirements.txt file and update as necessary. Leave as-is if creating the example pipeline. See venv for more details (For R images only) Review the renv.lock file and update as necessary. Leave as-is if creating the example pipeline. See Renv for more details Create a tag and release, ensuring Target is set on the main branch. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 711}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cdcdc88e4292fb0075e6ee3ff9e2130d'}>,\n",
       " <Document: {'content': 'Set the tag and release to v0.0.1 if you are creating an example pipeline Go to the Actions tab and you should see the “Build, tag, push, and make available image to pods” action running. Make sure the action passes otherwise the image will not be built If you have permission, log in to ECR and search for your image and tag Tips on writing the code You can create scripts in any programming language, including R and Python. You may want to test your scripts in RStudio or JupyterLab on the Analytical Platform before running them as part of a pipeline. All Python scripts in your Airflow repository should be formatted according to flake8 rules. flake8 is a code linter that analyses your Python code and flags bugs, programming errors and stylistic errors. You can automatically format your code using tools like black , autopep8 and yapf . These tools are often able to resolve most formatting issues. Environment Variables You can use environment variables to pass in variables to the docker container. We tend to write them in caps to point out the fact they will be passed in as environmental variables. You can use the same Docker image for multiple tasks by passing an environment variable. In the use_kubernetes_pod_operator.py example, we pass in the environment variable “write” and “copy” to first write to S3, then copy the file across, using the same image. Dockerfile A Dockerfile is a text file that contains the commands used to build a Docker image. It starts with a FROM directive, which specifies the parent image that your image is based on. Each subsequent declaration in the Dockerfile modifies this parent image. We have a range of parent images to chose from, get in touch if the available images do not meet your requirements. You can use venv, conda, packrat, renv or other package management tools to capture the dependencies required by your pipeline. If using one of these tools, you will need to update the Dockerfile to install required packages correctly. (For R images only) If you’re not using Python at all, for example if you’re using Rdbtools and Rs3tools rather than dbtools and botor, then replace Dockerfile with Dockerfile.nopython and delete the requirements.txt file. You can do this by running: mv Dockerfile Dockerfile.backup\\ncp Dockerfile.nopython Dockerfile\\nrm requirements.txt Test Docker image (optional) If you have a MacBook, you can use Docker locally to build and test your Docker image. You can download Docker Desktop for Mac here . To build and test your Docker image locally, follow the steps below: Clone your Airflow repository to a new folder on your MacBook – this guarantees that the Docker image will be built using the same code as on the Analytical Platform. You may need to create a new connection to GitHub with SSH . Open a terminal session and navigate to the directory containing the Dockerfile using the cd command. Build the Docker image by running: docker build . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 712}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b46d965ae19af938caa89c57e69241d0'}>,\n",
       " <Document: {'content': '- t IMAGE: TAG where IMAGE is a name for the image, for example, my-docker-image , and TAG is the version number, for example, v0.1 . Run a Docker container created from the Docker image by running: docker run IMAGE: TAG This will run the command specified in the CMD line of the Dockerfile . This will fail if your command requires access to resources on the Analytical Platform, such as data stored in Amazon S3 unless the correct environment variables are passed to the docker container. You would need the following environment variables to ensure correct access to all the AP resources: docker run \\\\\\n--env AWS_REGION=$AWS_REGION \\\\\\n--env AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION \\\\\\n--env AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\\\\\n--env AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\\\\\n--env AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN \\\\\\n--env AWS_SECURITY_TOKEN=$AWS_SECURITY_TOKEN \\\\\\nIMAGE:TAG Oher environment variables such as PYTHON_SCRIPT_NAME or R_SCRIPT_NAME can be passed in the same way. You can start a bash session in a running Docker container for debugging and troubleshooting purposes by running: docker run - it IMAGE: TAG bash This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nImage Pipeline - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 713}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f47248dd8f46060381c27d28e391873'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 714}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Image Pipeline These instructions show you how to use the use the template github repos to build a Python or R image and save to the Data Engineering ECR. Create a new GitHub repo using: template-airflow-python if creating a python codebase template-airflow-R if creating an R codebase\\nWhile creating your own repo using the template of your choice, please ensure moj-analytical-services is designated as the owner of the repo. If your own GitHub account is left as owner, then the GitHub Action used to publish your image will fail when trying to run. The image will have the same name as the repo name so make sure it is appropriate and reflects the pipeline you intend to run. If you are creating an example pipeline call it airflow-{username}-example Review the scripts/run.py or scripts/run.R file. This has some code to write and copy to S3. Leave as-is if you are creating an example pipeline. Otherwise replace with your own logic (see Tips on writing the code ) Review the Dockerfile and the parent image and update as necessary (see Dockerfile ). Leave as-is if creating the example pipeline Review the requirements.txt file and update as necessary. Leave as-is if creating the example pipeline. See venv for more details (For R images only) Review the renv.lock file and update as necessary. Leave as-is if creating the example pipeline. See Renv for more details Create a tag and release, ensuring Target is set on the main branch. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 715}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cdcdc88e4292fb0075e6ee3ff9e2130d'}>,\n",
       " <Document: {'content': 'Set the tag and release to v0.0.1 if you are creating an example pipeline Go to the Actions tab and you should see the “Build, tag, push, and make available image to pods” action running. Make sure the action passes otherwise the image will not be built If you have permission, log in to ECR and search for your image and tag Tips on writing the code You can create scripts in any programming language, including R and Python. You may want to test your scripts in RStudio or JupyterLab on the Analytical Platform before running them as part of a pipeline. All Python scripts in your Airflow repository should be formatted according to flake8 rules. flake8 is a code linter that analyses your Python code and flags bugs, programming errors and stylistic errors. You can automatically format your code using tools like black , autopep8 and yapf . These tools are often able to resolve most formatting issues. Environment Variables You can use environment variables to pass in variables to the docker container. We tend to write them in caps to point out the fact they will be passed in as environmental variables. You can use the same Docker image for multiple tasks by passing an environment variable. In the use_kubernetes_pod_operator.py example, we pass in the environment variable “write” and “copy” to first write to S3, then copy the file across, using the same image. Dockerfile A Dockerfile is a text file that contains the commands used to build a Docker image. It starts with a FROM directive, which specifies the parent image that your image is based on. Each subsequent declaration in the Dockerfile modifies this parent image. We have a range of parent images to chose from, get in touch if the available images do not meet your requirements. You can use venv, conda, packrat, renv or other package management tools to capture the dependencies required by your pipeline. If using one of these tools, you will need to update the Dockerfile to install required packages correctly. (For R images only) If you’re not using Python at all, for example if you’re using Rdbtools and Rs3tools rather than dbtools and botor, then replace Dockerfile with Dockerfile.nopython and delete the requirements.txt file. You can do this by running: mv Dockerfile Dockerfile.backup\\ncp Dockerfile.nopython Dockerfile\\nrm requirements.txt Test Docker image (optional) If you have a MacBook, you can use Docker locally to build and test your Docker image. You can download Docker Desktop for Mac here . To build and test your Docker image locally, follow the steps below: Clone your Airflow repository to a new folder on your MacBook – this guarantees that the Docker image will be built using the same code as on the Analytical Platform. You may need to create a new connection to GitHub with SSH . Open a terminal session and navigate to the directory containing the Dockerfile using the cd command. Build the Docker image by running: docker build . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 716}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b46d965ae19af938caa89c57e69241d0'}>,\n",
       " <Document: {'content': '- t IMAGE: TAG where IMAGE is a name for the image, for example, my-docker-image , and TAG is the version number, for example, v0.1 . Run a Docker container created from the Docker image by running: docker run IMAGE: TAG This will run the command specified in the CMD line of the Dockerfile . This will fail if your command requires access to resources on the Analytical Platform, such as data stored in Amazon S3 unless the correct environment variables are passed to the docker container. You would need the following environment variables to ensure correct access to all the AP resources: docker run \\\\\\n--env AWS_REGION=$AWS_REGION \\\\\\n--env AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION \\\\\\n--env AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\\\\\n--env AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\\\\\n--env AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN \\\\\\n--env AWS_SECURITY_TOKEN=$AWS_SECURITY_TOKEN \\\\\\nIMAGE:TAG Oher environment variables such as PYTHON_SCRIPT_NAME or R_SCRIPT_NAME can be passed in the same way. You can start a bash session in a running Docker container for debugging and troubleshooting purposes by running: docker run - it IMAGE: TAG bash This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nImage Pipeline - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 717}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f47248dd8f46060381c27d28e391873'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 718}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Image Pipeline These instructions show you how to use the use the template github repos to build a Python or R image and save to the Data Engineering ECR. Create a new GitHub repo using: template-airflow-python if creating a python codebase template-airflow-R if creating an R codebase\\nWhile creating your own repo using the template of your choice, please ensure moj-analytical-services is designated as the owner of the repo. If your own GitHub account is left as owner, then the GitHub Action used to publish your image will fail when trying to run. The image will have the same name as the repo name so make sure it is appropriate and reflects the pipeline you intend to run. If you are creating an example pipeline call it airflow-{username}-example Review the scripts/run.py or scripts/run.R file. This has some code to write and copy to S3. Leave as-is if you are creating an example pipeline. Otherwise replace with your own logic (see Tips on writing the code ) Review the Dockerfile and the parent image and update as necessary (see Dockerfile ). Leave as-is if creating the example pipeline Review the requirements.txt file and update as necessary. Leave as-is if creating the example pipeline. See venv for more details (For R images only) Review the renv.lock file and update as necessary. Leave as-is if creating the example pipeline. See Renv for more details Create a tag and release, ensuring Target is set on the main branch. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 719}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cdcdc88e4292fb0075e6ee3ff9e2130d'}>,\n",
       " <Document: {'content': 'Set the tag and release to v0.0.1 if you are creating an example pipeline Go to the Actions tab and you should see the “Build, tag, push, and make available image to pods” action running. Make sure the action passes otherwise the image will not be built If you have permission, log in to ECR and search for your image and tag Tips on writing the code You can create scripts in any programming language, including R and Python. You may want to test your scripts in RStudio or JupyterLab on the Analytical Platform before running them as part of a pipeline. All Python scripts in your Airflow repository should be formatted according to flake8 rules. flake8 is a code linter that analyses your Python code and flags bugs, programming errors and stylistic errors. You can automatically format your code using tools like black , autopep8 and yapf . These tools are often able to resolve most formatting issues. Environment Variables You can use environment variables to pass in variables to the docker container. We tend to write them in caps to point out the fact they will be passed in as environmental variables. You can use the same Docker image for multiple tasks by passing an environment variable. In the use_kubernetes_pod_operator.py example, we pass in the environment variable “write” and “copy” to first write to S3, then copy the file across, using the same image. Dockerfile A Dockerfile is a text file that contains the commands used to build a Docker image. It starts with a FROM directive, which specifies the parent image that your image is based on. Each subsequent declaration in the Dockerfile modifies this parent image. We have a range of parent images to chose from, get in touch if the available images do not meet your requirements. You can use venv, conda, packrat, renv or other package management tools to capture the dependencies required by your pipeline. If using one of these tools, you will need to update the Dockerfile to install required packages correctly. (For R images only) If you’re not using Python at all, for example if you’re using Rdbtools and Rs3tools rather than dbtools and botor, then replace Dockerfile with Dockerfile.nopython and delete the requirements.txt file. You can do this by running: mv Dockerfile Dockerfile.backup\\ncp Dockerfile.nopython Dockerfile\\nrm requirements.txt Test Docker image (optional) If you have a MacBook, you can use Docker locally to build and test your Docker image. You can download Docker Desktop for Mac here . To build and test your Docker image locally, follow the steps below: Clone your Airflow repository to a new folder on your MacBook – this guarantees that the Docker image will be built using the same code as on the Analytical Platform. You may need to create a new connection to GitHub with SSH . Open a terminal session and navigate to the directory containing the Dockerfile using the cd command. Build the Docker image by running: docker build . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 720}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b46d965ae19af938caa89c57e69241d0'}>,\n",
       " <Document: {'content': '- t IMAGE: TAG where IMAGE is a name for the image, for example, my-docker-image , and TAG is the version number, for example, v0.1 . Run a Docker container created from the Docker image by running: docker run IMAGE: TAG This will run the command specified in the CMD line of the Dockerfile . This will fail if your command requires access to resources on the Analytical Platform, such as data stored in Amazon S3 unless the correct environment variables are passed to the docker container. You would need the following environment variables to ensure correct access to all the AP resources: docker run \\\\\\n--env AWS_REGION=$AWS_REGION \\\\\\n--env AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION \\\\\\n--env AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\\\\\n--env AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\\\\\n--env AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN \\\\\\n--env AWS_SECURITY_TOKEN=$AWS_SECURITY_TOKEN \\\\\\nIMAGE:TAG Oher environment variables such as PYTHON_SCRIPT_NAME or R_SCRIPT_NAME can be passed in the same way. You can start a bash session in a running Docker container for debugging and troubleshooting purposes by running: docker run - it IMAGE: TAG bash This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDAG Pipeline - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 721}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b5ca857e85acde2fcac9a8be50de898a'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 722}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools DAG and Role pipeline The airflow repo contains the DAG definitions and roles used in our Airflow pipelines and is structured in the following way: airflow ├── environments │ ├── dev │ │ ├── dags │ │ │ ├── project_1 │ │ │ │ ├── dag . py │ │ │ │ ├── ... │ │ │ │ └── README . md │ │ │ ├── project_2 │ │ │ │ ├── dag_1 . py │ │ │ │ ├── dag_2 . py │ │ │ │ ├── ... │ │ │ │ └── README . md │ │ │ ├── project_3 │ │ │ │ ├── dag . py │ │ │ │ └── README . md │ │ │ └── ... │ │ └── roles │ │ ├── airflow_dev_role_1 . yaml │ │ ├── airflow_dev_role_2 . yaml │ │ └── ... │ ├── prod │ │ └── ... │ └── ... ├── imports │ ├── __init__ . py │ ├── project_3_constants . py │ └── ... └── ... Anything in the folder environment/dev/ is deployed to the Airflow Dev Instance and anything in enivronment/prod/ is deployed to the Airflow production instance. Both prod and dev have the exact same folder structures. We’ll just refer to dev in the user guide (knowing the same is true for prod ). The idea is to test a pipeline in dev, before promoting it to the prod environment. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 723}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd88c0c2b76cd5983a863926aea167dfb'}>,\n",
       " <Document: {'content': 'You should ensure that DAGs or Roles across environments do not share write access to the same AWS resources or reference each other (e.g. a DAG defined in prod can only use roles defined in prod). However they can both reference the same docker image. Define the DAG A DAG is defined in a Python script. An example of a DAG script is provided below, based on the example pipeline. A few more example DAGs are available in example_DAGS . DAG example from datetime import datetime from airflow.models import DAG from mojap_airflow_tools.operators import BasicKubernetesPodOperator # Replace <<username>> with your username username = << username >> # As defined in the image repo IMAGE_TAG = \"v0.0.1\" REPO_NAME = f \"airflow- { username } -example\" # The user_id can be any number apart from 1000. In the case of R images, it must match the userid in the dockerfile USER_ID = 1337 \"\"\"\\nThe role used by Airflow to run a task.\\nThis role must be specified in the corresponding `roles/` folder in the same environment\\n(i.e. the role is defined in environments/dev/roles/airflow_dev_{username}_example.yaml).\\nThe role must only contain alphanumeric or underscore characters [a-zA-Z0-9_].\\nThis can be enforced using:\\nROLE = re.sub(r\"[\\\\W]+\", \"\", f\"airflow_dev_{username}_example\")\\n\"\"\" # ROLE = re.sub(r\"[\\\\W]+\", \"\", f\"airflow_dev_{username}_example\") ROLE = f \"airflow_dev_ { username } _example\" # For tips/advice on the default args see the use_dummy_operator.py example default_args = { # Normally you want to set this to False. \"depends_on_past\" : False , \"email_on_failure\" : False , # Name of DAG owner as it will appear on Airflow UI \"owner\" : f \" { username } \" , } dag = DAG ( # Name of the dag (how it will appear on the Airflow UI) # We use the naming convention: <folder_name>.<filename> dag_id = f \" { username } .copy_file_s3\" , default_args = default_args , description = \"A basic Kubernetes DAG\" , # Requires a start_date as a datetime object. This will be when the # DAG will start running from. DO NOT use datetime.now(). start_date = datetime ( 2022 , 1 , 1 ), # How often should I run the dag. If set to None your dag # will only run when manually triggered. schedule_interval = None , ) # Environmental variables for passing to the docker container env_vars = { \"RUN\" : \"write\" , \"TEXT\" : \"Hello\" , \"OUTPATH\" : f \"s3://alpha-everyone/airflow-example/ { username } /test.txt\" , } \"\"\"\\nIt is good practice to set task_id as a variable above your task\\nbecause it is used for both the name and task_id parameters.\\nYou should also only use alpha numerical characters and dashes (-)\\nfor the task_id value.\\n\"\"\"', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 724}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ecca433c992dc825d5cbe0cd34ecd2ce'}>,\n",
       " <Document: {'content': 'task_id = \"task-1\" task = BasicKubernetesPodOperator ( dag = dag , run_as_user = USER_ID , repo_name = REPO_NAME , release = IMAGE_TAG , role = ROLE , task_id = task_id , env_vars = env_vars ) task The DAG uses the BasicKubernetesPodPperator , a function created and managed by the Data Engineering Team here to make it easier to run tasks. If you require something with more functionality you can use KubernetesPodOperator (on which BasicKubernetesPodPperator is based). In the example the schedule_interval is set to None. The schedule_interval can be defined using: a cron expression as a str (such as 0 0 * * * ) fields are “minute hour day-of-month month day-of-week” a cron preset (such as @once, @daily, @weekly, etc) a datetime.timedelta object. You can find more detailed guidance on DAG scripts, including on how to set up dependencies between tasks, in the Airflow documentation . You can group multiple DAGs together in a folder making it easier for others to understand how they relate to one another. This also allows you to add READMEs to these folders to again help others understand what the DAGs are for. Actions Create a new directory in airflow/dev/dags and give it an appropriate name. Name the directory {username} if you are creating an example pipeline Create a new python file and give it an appropriate name. Name the file copy_file_s3.py if you are creating an example pipeline If you are creating an example pipeline, paste the DAG example as-is but replace < > making sure to include quotation marks. You might need to update the IMAGE_TAG version number depending on the image that builds successfully in the GitHub repository. Delete the comments If you are creating your own pipeline modify the IMAGE_TAG, REPO_NAME, ROLE, owner, dag_id, scheduling and env_vars as appropriate. You can also add additional tasks if required but stick to one DAG per python file. Using a High-Memory Node (Optional) You have the option to use a high-memory node for workloads that process large data sets in memory (large datasets here typically meaning datasets larger than 2 GB). Please note that a high-memory node is expensive to run so only use after testing and failing with a standard node. Also note that high-memory nodes need to be provisioned which means pods will take longer to start. Finally we have restricted the number of high-memory nodes that can be provisioned so please schedule appropriately. Please see use_high_memory_node.py for an example usage. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 725}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9a8cb0035d06e20dea4a4e1479c39a08'}>,\n",
       " <Document: {'content': 'You’ll need to import the toleration and affinity: from imports.high_memory_constants import tolerations , affinity and add the following arguments to the KubernetesPodOperator: # Increase startup time to give time for node creation startup_timeout_seconds = 600 , tolerations = tolerations , affinity = affinity , We have a ticket to PDE-1830 - Modify BasicKubernetesPodOperator to specify high-memory node To Do Using imports (Optional) If your DAG relies on custom modules, be this for storing shared constant, or reused functions, you can implement this as seen in the folder structure for project_3 : Place any modules you might need in the imports folder Make sure the module starts with the name of the DAG folder e.g. project_3_constants.py These can then be imported by your dags. For example, dag.py in project_3 could import project_3_constants.py using from imports.project_3_constants import some_function Define the IAM Policy We use IAM Builder to generate an IAM policy based on a yaml configuration. If you are unfamiliar with the syntax the README of the previous link will help you. There are also some useful test IAM configs which are good examples of what you might want for your own role definition in the in IAM builder tests (note: ignore the ones prefixed with bad_ as they are invalid configs - used to check the package is working). The github action then creates the IAM role and attaches the IAM policy. Steps Create a new yaml file in airflow/dev/roles to store your IAM role policy. The name of the file must start with airflow_dev and must match the ROLE variable in the DAG. Name the file airflow_dev_{username}_example.yaml if you are creating an example pipeline Define the IAM policy. You can optionally include the field iam_role_name in your IAM config yaml but this must match the file name and the ROLE variable in the DAG. You only need to include the iam_role_name in the IAM config if you also have glue_job: true or secrets: true , otherwise it is optional. If you are creating an example pipeline, paste the following code, replacing “{username}” with your username: iam_role_name: airflow_dev_{username}_example\\n\\ns3:\\nread_write:\\n- alpha-everyone/airflow-example/* Validate from the command line (optional) The airflow repo validates the folder structure, DAGs ( validation.dags ) and Roles ( validation.roles ). The validation will run automatically when you raise a PR, but you can also validate using the command line to spot errors sooner. You’ll have to create and activate the python environment as specified in requirements-validation.txt. Make sure you are in the root of airflow repo and that you specify the full path to the file(s) you wish to validate The preferred method for running this validation is: python - W ignore - m validation filepath1 filepath2 etc flake8 . yamllint . When running these tests for the example pipeline: filepath1 should be environments/dev/dags/{username}/copy_file_s3.py and filepath2 should be environments/dev/roles/airflow_dev_{username}_example.yaml Deploy the changes Raise a PR into the main branch with your changes (ideally within a single commit). This will start a set of github action workflows. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 726}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f3ea858aef16e8edac167e3bf629fbee'}>,\n",
       " <Document: {'content': 'Check that all workflows pass after your PR is submitted. If any of the workflows fail the PR cannot be merged in main (and therefore not deployed) DE will be notified through a slack integration to approve the PR. Code changes made to files in the environments/dev/dags folder do not need approval: airflow ├── environments │ ├── dev │ │ ├── dags │ │ │ ├── APPROVAL NOT REQUIRED │ │ └── roles │ │ ├── APPROVAL REQUIRED │ ├── prod │ │ └── APPROVAL REQUIRED Once checks pass and PR approved by DE, please merge the PR into main The code in main is then automatically deployed to the Airflow prod and dev instances This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDAG Pipeline - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 727}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27398c5f540330103cb15b2e97201895'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 728}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools DAG and Role pipeline The airflow repo contains the DAG definitions and roles used in our Airflow pipelines and is structured in the following way: airflow ├── environments │ ├── dev │ │ ├── dags │ │ │ ├── project_1 │ │ │ │ ├── dag . py │ │ │ │ ├── ... │ │ │ │ └── README . md │ │ │ ├── project_2 │ │ │ │ ├── dag_1 . py │ │ │ │ ├── dag_2 . py │ │ │ │ ├── ... │ │ │ │ └── README . md │ │ │ ├── project_3 │ │ │ │ ├── dag . py │ │ │ │ └── README . md │ │ │ └── ... │ │ └── roles │ │ ├── airflow_dev_role_1 . yaml │ │ ├── airflow_dev_role_2 . yaml │ │ └── ... │ ├── prod │ │ └── ... │ └── ... ├── imports │ ├── __init__ . py │ ├── project_3_constants . py │ └── ... └── ... Anything in the folder environment/dev/ is deployed to the Airflow Dev Instance and anything in enivronment/prod/ is deployed to the Airflow production instance. Both prod and dev have the exact same folder structures. We’ll just refer to dev in the user guide (knowing the same is true for prod ). The idea is to test a pipeline in dev, before promoting it to the prod environment. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 729}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd88c0c2b76cd5983a863926aea167dfb'}>,\n",
       " <Document: {'content': 'You should ensure that DAGs or Roles across environments do not share write access to the same AWS resources or reference each other (e.g. a DAG defined in prod can only use roles defined in prod). However they can both reference the same docker image. Define the DAG A DAG is defined in a Python script. An example of a DAG script is provided below, based on the example pipeline. A few more example DAGs are available in example_DAGS . DAG example from datetime import datetime from airflow.models import DAG from mojap_airflow_tools.operators import BasicKubernetesPodOperator # Replace <<username>> with your username username = << username >> # As defined in the image repo IMAGE_TAG = \"v0.0.1\" REPO_NAME = f \"airflow- { username } -example\" # The user_id can be any number apart from 1000. In the case of R images, it must match the userid in the dockerfile USER_ID = 1337 \"\"\"\\nThe role used by Airflow to run a task.\\nThis role must be specified in the corresponding `roles/` folder in the same environment\\n(i.e. the role is defined in environments/dev/roles/airflow_dev_{username}_example.yaml).\\nThe role must only contain alphanumeric or underscore characters [a-zA-Z0-9_].\\nThis can be enforced using:\\nROLE = re.sub(r\"[\\\\W]+\", \"\", f\"airflow_dev_{username}_example\")\\n\"\"\" # ROLE = re.sub(r\"[\\\\W]+\", \"\", f\"airflow_dev_{username}_example\") ROLE = f \"airflow_dev_ { username } _example\" # For tips/advice on the default args see the use_dummy_operator.py example default_args = { # Normally you want to set this to False. \"depends_on_past\" : False , \"email_on_failure\" : False , # Name of DAG owner as it will appear on Airflow UI \"owner\" : f \" { username } \" , } dag = DAG ( # Name of the dag (how it will appear on the Airflow UI) # We use the naming convention: <folder_name>.<filename> dag_id = f \" { username } .copy_file_s3\" , default_args = default_args , description = \"A basic Kubernetes DAG\" , # Requires a start_date as a datetime object. This will be when the # DAG will start running from. DO NOT use datetime.now(). start_date = datetime ( 2022 , 1 , 1 ), # How often should I run the dag. If set to None your dag # will only run when manually triggered. schedule_interval = None , ) # Environmental variables for passing to the docker container env_vars = { \"RUN\" : \"write\" , \"TEXT\" : \"Hello\" , \"OUTPATH\" : f \"s3://alpha-everyone/airflow-example/ { username } /test.txt\" , } \"\"\"\\nIt is good practice to set task_id as a variable above your task\\nbecause it is used for both the name and task_id parameters.\\nYou should also only use alpha numerical characters and dashes (-)\\nfor the task_id value.\\n\"\"\"', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 730}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ecca433c992dc825d5cbe0cd34ecd2ce'}>,\n",
       " <Document: {'content': 'task_id = \"task-1\" task = BasicKubernetesPodOperator ( dag = dag , run_as_user = USER_ID , repo_name = REPO_NAME , release = IMAGE_TAG , role = ROLE , task_id = task_id , env_vars = env_vars ) task The DAG uses the BasicKubernetesPodPperator , a function created and managed by the Data Engineering Team here to make it easier to run tasks. If you require something with more functionality you can use KubernetesPodOperator (on which BasicKubernetesPodPperator is based). In the example the schedule_interval is set to None. The schedule_interval can be defined using: a cron expression as a str (such as 0 0 * * * ) fields are “minute hour day-of-month month day-of-week” a cron preset (such as @once, @daily, @weekly, etc) a datetime.timedelta object. You can find more detailed guidance on DAG scripts, including on how to set up dependencies between tasks, in the Airflow documentation . You can group multiple DAGs together in a folder making it easier for others to understand how they relate to one another. This also allows you to add READMEs to these folders to again help others understand what the DAGs are for. Actions Create a new directory in airflow/dev/dags and give it an appropriate name. Name the directory {username} if you are creating an example pipeline Create a new python file and give it an appropriate name. Name the file copy_file_s3.py if you are creating an example pipeline If you are creating an example pipeline, paste the DAG example as-is but replace < > making sure to include quotation marks. You might need to update the IMAGE_TAG version number depending on the image that builds successfully in the GitHub repository. Delete the comments If you are creating your own pipeline modify the IMAGE_TAG, REPO_NAME, ROLE, owner, dag_id, scheduling and env_vars as appropriate. You can also add additional tasks if required but stick to one DAG per python file. Using a High-Memory Node (Optional) You have the option to use a high-memory node for workloads that process large data sets in memory (large datasets here typically meaning datasets larger than 2 GB). Please note that a high-memory node is expensive to run so only use after testing and failing with a standard node. Also note that high-memory nodes need to be provisioned which means pods will take longer to start. Finally we have restricted the number of high-memory nodes that can be provisioned so please schedule appropriately. Please see use_high_memory_node.py for an example usage. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 731}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9a8cb0035d06e20dea4a4e1479c39a08'}>,\n",
       " <Document: {'content': 'You’ll need to import the toleration and affinity: from imports.high_memory_constants import tolerations , affinity and add the following arguments to the KubernetesPodOperator: # Increase startup time to give time for node creation startup_timeout_seconds = 600 , tolerations = tolerations , affinity = affinity , We have a ticket to PDE-1830 - Modify BasicKubernetesPodOperator to specify high-memory node To Do Using imports (Optional) If your DAG relies on custom modules, be this for storing shared constant, or reused functions, you can implement this as seen in the folder structure for project_3 : Place any modules you might need in the imports folder Make sure the module starts with the name of the DAG folder e.g. project_3_constants.py These can then be imported by your dags. For example, dag.py in project_3 could import project_3_constants.py using from imports.project_3_constants import some_function Define the IAM Policy We use IAM Builder to generate an IAM policy based on a yaml configuration. If you are unfamiliar with the syntax the README of the previous link will help you. There are also some useful test IAM configs which are good examples of what you might want for your own role definition in the in IAM builder tests (note: ignore the ones prefixed with bad_ as they are invalid configs - used to check the package is working). The github action then creates the IAM role and attaches the IAM policy. Steps Create a new yaml file in airflow/dev/roles to store your IAM role policy. The name of the file must start with airflow_dev and must match the ROLE variable in the DAG. Name the file airflow_dev_{username}_example.yaml if you are creating an example pipeline Define the IAM policy. You can optionally include the field iam_role_name in your IAM config yaml but this must match the file name and the ROLE variable in the DAG. You only need to include the iam_role_name in the IAM config if you also have glue_job: true or secrets: true , otherwise it is optional. If you are creating an example pipeline, paste the following code, replacing “{username}” with your username: iam_role_name: airflow_dev_{username}_example\\n\\ns3:\\nread_write:\\n- alpha-everyone/airflow-example/* Validate from the command line (optional) The airflow repo validates the folder structure, DAGs ( validation.dags ) and Roles ( validation.roles ). The validation will run automatically when you raise a PR, but you can also validate using the command line to spot errors sooner. You’ll have to create and activate the python environment as specified in requirements-validation.txt. Make sure you are in the root of airflow repo and that you specify the full path to the file(s) you wish to validate The preferred method for running this validation is: python - W ignore - m validation filepath1 filepath2 etc flake8 . yamllint . When running these tests for the example pipeline: filepath1 should be environments/dev/dags/{username}/copy_file_s3.py and filepath2 should be environments/dev/roles/airflow_dev_{username}_example.yaml Deploy the changes Raise a PR into the main branch with your changes (ideally within a single commit). This will start a set of github action workflows. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 732}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f3ea858aef16e8edac167e3bf629fbee'}>,\n",
       " <Document: {'content': 'Check that all workflows pass after your PR is submitted. If any of the workflows fail the PR cannot be merged in main (and therefore not deployed) DE will be notified through a slack integration to approve the PR. Code changes made to files in the environments/dev/dags folder do not need approval: airflow ├── environments │ ├── dev │ │ ├── dags │ │ │ ├── APPROVAL NOT REQUIRED │ │ └── roles │ │ ├── APPROVAL REQUIRED │ ├── prod │ │ └── APPROVAL REQUIRED Once checks pass and PR approved by DE, please merge the PR into main The code in main is then automatically deployed to the Airflow prod and dev instances This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDAG Pipeline - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 733}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27398c5f540330103cb15b2e97201895'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 734}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools DAG and Role pipeline The airflow repo contains the DAG definitions and roles used in our Airflow pipelines and is structured in the following way: airflow ├── environments │ ├── dev │ │ ├── dags │ │ │ ├── project_1 │ │ │ │ ├── dag . py │ │ │ │ ├── ... │ │ │ │ └── README . md │ │ │ ├── project_2 │ │ │ │ ├── dag_1 . py │ │ │ │ ├── dag_2 . py │ │ │ │ ├── ... │ │ │ │ └── README . md │ │ │ ├── project_3 │ │ │ │ ├── dag . py │ │ │ │ └── README . md │ │ │ └── ... │ │ └── roles │ │ ├── airflow_dev_role_1 . yaml │ │ ├── airflow_dev_role_2 . yaml │ │ └── ... │ ├── prod │ │ └── ... │ └── ... ├── imports │ ├── __init__ . py │ ├── project_3_constants . py │ └── ... └── ... Anything in the folder environment/dev/ is deployed to the Airflow Dev Instance and anything in enivronment/prod/ is deployed to the Airflow production instance. Both prod and dev have the exact same folder structures. We’ll just refer to dev in the user guide (knowing the same is true for prod ). The idea is to test a pipeline in dev, before promoting it to the prod environment. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 735}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd88c0c2b76cd5983a863926aea167dfb'}>,\n",
       " <Document: {'content': 'You should ensure that DAGs or Roles across environments do not share write access to the same AWS resources or reference each other (e.g. a DAG defined in prod can only use roles defined in prod). However they can both reference the same docker image. Define the DAG A DAG is defined in a Python script. An example of a DAG script is provided below, based on the example pipeline. A few more example DAGs are available in example_DAGS . DAG example from datetime import datetime from airflow.models import DAG from mojap_airflow_tools.operators import BasicKubernetesPodOperator # Replace <<username>> with your username username = << username >> # As defined in the image repo IMAGE_TAG = \"v0.0.1\" REPO_NAME = f \"airflow- { username } -example\" # The user_id can be any number apart from 1000. In the case of R images, it must match the userid in the dockerfile USER_ID = 1337 \"\"\"\\nThe role used by Airflow to run a task.\\nThis role must be specified in the corresponding `roles/` folder in the same environment\\n(i.e. the role is defined in environments/dev/roles/airflow_dev_{username}_example.yaml).\\nThe role must only contain alphanumeric or underscore characters [a-zA-Z0-9_].\\nThis can be enforced using:\\nROLE = re.sub(r\"[\\\\W]+\", \"\", f\"airflow_dev_{username}_example\")\\n\"\"\" # ROLE = re.sub(r\"[\\\\W]+\", \"\", f\"airflow_dev_{username}_example\") ROLE = f \"airflow_dev_ { username } _example\" # For tips/advice on the default args see the use_dummy_operator.py example default_args = { # Normally you want to set this to False. \"depends_on_past\" : False , \"email_on_failure\" : False , # Name of DAG owner as it will appear on Airflow UI \"owner\" : f \" { username } \" , } dag = DAG ( # Name of the dag (how it will appear on the Airflow UI) # We use the naming convention: <folder_name>.<filename> dag_id = f \" { username } .copy_file_s3\" , default_args = default_args , description = \"A basic Kubernetes DAG\" , # Requires a start_date as a datetime object. This will be when the # DAG will start running from. DO NOT use datetime.now(). start_date = datetime ( 2022 , 1 , 1 ), # How often should I run the dag. If set to None your dag # will only run when manually triggered. schedule_interval = None , ) # Environmental variables for passing to the docker container env_vars = { \"RUN\" : \"write\" , \"TEXT\" : \"Hello\" , \"OUTPATH\" : f \"s3://alpha-everyone/airflow-example/ { username } /test.txt\" , } \"\"\"\\nIt is good practice to set task_id as a variable above your task\\nbecause it is used for both the name and task_id parameters.\\nYou should also only use alpha numerical characters and dashes (-)\\nfor the task_id value.\\n\"\"\"', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 736}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ecca433c992dc825d5cbe0cd34ecd2ce'}>,\n",
       " <Document: {'content': 'task_id = \"task-1\" task = BasicKubernetesPodOperator ( dag = dag , run_as_user = USER_ID , repo_name = REPO_NAME , release = IMAGE_TAG , role = ROLE , task_id = task_id , env_vars = env_vars ) task The DAG uses the BasicKubernetesPodPperator , a function created and managed by the Data Engineering Team here to make it easier to run tasks. If you require something with more functionality you can use KubernetesPodOperator (on which BasicKubernetesPodPperator is based). In the example the schedule_interval is set to None. The schedule_interval can be defined using: a cron expression as a str (such as 0 0 * * * ) fields are “minute hour day-of-month month day-of-week” a cron preset (such as @once, @daily, @weekly, etc) a datetime.timedelta object. You can find more detailed guidance on DAG scripts, including on how to set up dependencies between tasks, in the Airflow documentation . You can group multiple DAGs together in a folder making it easier for others to understand how they relate to one another. This also allows you to add READMEs to these folders to again help others understand what the DAGs are for. Actions Create a new directory in airflow/dev/dags and give it an appropriate name. Name the directory {username} if you are creating an example pipeline Create a new python file and give it an appropriate name. Name the file copy_file_s3.py if you are creating an example pipeline If you are creating an example pipeline, paste the DAG example as-is but replace < > making sure to include quotation marks. You might need to update the IMAGE_TAG version number depending on the image that builds successfully in the GitHub repository. Delete the comments If you are creating your own pipeline modify the IMAGE_TAG, REPO_NAME, ROLE, owner, dag_id, scheduling and env_vars as appropriate. You can also add additional tasks if required but stick to one DAG per python file. Using a High-Memory Node (Optional) You have the option to use a high-memory node for workloads that process large data sets in memory (large datasets here typically meaning datasets larger than 2 GB). Please note that a high-memory node is expensive to run so only use after testing and failing with a standard node. Also note that high-memory nodes need to be provisioned which means pods will take longer to start. Finally we have restricted the number of high-memory nodes that can be provisioned so please schedule appropriately. Please see use_high_memory_node.py for an example usage. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 737}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9a8cb0035d06e20dea4a4e1479c39a08'}>,\n",
       " <Document: {'content': 'You’ll need to import the toleration and affinity: from imports.high_memory_constants import tolerations , affinity and add the following arguments to the KubernetesPodOperator: # Increase startup time to give time for node creation startup_timeout_seconds = 600 , tolerations = tolerations , affinity = affinity , We have a ticket to PDE-1830 - Modify BasicKubernetesPodOperator to specify high-memory node To Do Using imports (Optional) If your DAG relies on custom modules, be this for storing shared constant, or reused functions, you can implement this as seen in the folder structure for project_3 : Place any modules you might need in the imports folder Make sure the module starts with the name of the DAG folder e.g. project_3_constants.py These can then be imported by your dags. For example, dag.py in project_3 could import project_3_constants.py using from imports.project_3_constants import some_function Define the IAM Policy We use IAM Builder to generate an IAM policy based on a yaml configuration. If you are unfamiliar with the syntax the README of the previous link will help you. There are also some useful test IAM configs which are good examples of what you might want for your own role definition in the in IAM builder tests (note: ignore the ones prefixed with bad_ as they are invalid configs - used to check the package is working). The github action then creates the IAM role and attaches the IAM policy. Steps Create a new yaml file in airflow/dev/roles to store your IAM role policy. The name of the file must start with airflow_dev and must match the ROLE variable in the DAG. Name the file airflow_dev_{username}_example.yaml if you are creating an example pipeline Define the IAM policy. You can optionally include the field iam_role_name in your IAM config yaml but this must match the file name and the ROLE variable in the DAG. You only need to include the iam_role_name in the IAM config if you also have glue_job: true or secrets: true , otherwise it is optional. If you are creating an example pipeline, paste the following code, replacing “{username}” with your username: iam_role_name: airflow_dev_{username}_example\\n\\ns3:\\nread_write:\\n- alpha-everyone/airflow-example/* Validate from the command line (optional) The airflow repo validates the folder structure, DAGs ( validation.dags ) and Roles ( validation.roles ). The validation will run automatically when you raise a PR, but you can also validate using the command line to spot errors sooner. You’ll have to create and activate the python environment as specified in requirements-validation.txt. Make sure you are in the root of airflow repo and that you specify the full path to the file(s) you wish to validate The preferred method for running this validation is: python - W ignore - m validation filepath1 filepath2 etc flake8 . yamllint . When running these tests for the example pipeline: filepath1 should be environments/dev/dags/{username}/copy_file_s3.py and filepath2 should be environments/dev/roles/airflow_dev_{username}_example.yaml Deploy the changes Raise a PR into the main branch with your changes (ideally within a single commit). This will start a set of github action workflows. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 738}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f3ea858aef16e8edac167e3bf629fbee'}>,\n",
       " <Document: {'content': 'Check that all workflows pass after your PR is submitted. If any of the workflows fail the PR cannot be merged in main (and therefore not deployed) DE will be notified through a slack integration to approve the PR. Code changes made to files in the environments/dev/dags folder do not need approval: airflow ├── environments │ ├── dev │ │ ├── dags │ │ │ ├── APPROVAL NOT REQUIRED │ │ └── roles │ │ ├── APPROVAL REQUIRED │ ├── prod │ │ └── APPROVAL REQUIRED Once checks pass and PR approved by DE, please merge the PR into main The code in main is then automatically deployed to the Airflow prod and dev instances This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDAG Pipeline - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 739}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27398c5f540330103cb15b2e97201895'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 740}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools DAG and Role pipeline The airflow repo contains the DAG definitions and roles used in our Airflow pipelines and is structured in the following way: airflow ├── environments │ ├── dev │ │ ├── dags │ │ │ ├── project_1 │ │ │ │ ├── dag . py │ │ │ │ ├── ... │ │ │ │ └── README . md │ │ │ ├── project_2 │ │ │ │ ├── dag_1 . py │ │ │ │ ├── dag_2 . py │ │ │ │ ├── ... │ │ │ │ └── README . md │ │ │ ├── project_3 │ │ │ │ ├── dag . py │ │ │ │ └── README . md │ │ │ └── ... │ │ └── roles │ │ ├── airflow_dev_role_1 . yaml │ │ ├── airflow_dev_role_2 . yaml │ │ └── ... │ ├── prod │ │ └── ... │ └── ... ├── imports │ ├── __init__ . py │ ├── project_3_constants . py │ └── ... └── ... Anything in the folder environment/dev/ is deployed to the Airflow Dev Instance and anything in enivronment/prod/ is deployed to the Airflow production instance. Both prod and dev have the exact same folder structures. We’ll just refer to dev in the user guide (knowing the same is true for prod ). The idea is to test a pipeline in dev, before promoting it to the prod environment. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 741}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd88c0c2b76cd5983a863926aea167dfb'}>,\n",
       " <Document: {'content': 'You should ensure that DAGs or Roles across environments do not share write access to the same AWS resources or reference each other (e.g. a DAG defined in prod can only use roles defined in prod). However they can both reference the same docker image. Define the DAG A DAG is defined in a Python script. An example of a DAG script is provided below, based on the example pipeline. A few more example DAGs are available in example_DAGS . DAG example from datetime import datetime from airflow.models import DAG from mojap_airflow_tools.operators import BasicKubernetesPodOperator # Replace <<username>> with your username username = << username >> # As defined in the image repo IMAGE_TAG = \"v0.0.1\" REPO_NAME = f \"airflow- { username } -example\" # The user_id can be any number apart from 1000. In the case of R images, it must match the userid in the dockerfile USER_ID = 1337 \"\"\"\\nThe role used by Airflow to run a task.\\nThis role must be specified in the corresponding `roles/` folder in the same environment\\n(i.e. the role is defined in environments/dev/roles/airflow_dev_{username}_example.yaml).\\nThe role must only contain alphanumeric or underscore characters [a-zA-Z0-9_].\\nThis can be enforced using:\\nROLE = re.sub(r\"[\\\\W]+\", \"\", f\"airflow_dev_{username}_example\")\\n\"\"\" # ROLE = re.sub(r\"[\\\\W]+\", \"\", f\"airflow_dev_{username}_example\") ROLE = f \"airflow_dev_ { username } _example\" # For tips/advice on the default args see the use_dummy_operator.py example default_args = { # Normally you want to set this to False. \"depends_on_past\" : False , \"email_on_failure\" : False , # Name of DAG owner as it will appear on Airflow UI \"owner\" : f \" { username } \" , } dag = DAG ( # Name of the dag (how it will appear on the Airflow UI) # We use the naming convention: <folder_name>.<filename> dag_id = f \" { username } .copy_file_s3\" , default_args = default_args , description = \"A basic Kubernetes DAG\" , # Requires a start_date as a datetime object. This will be when the # DAG will start running from. DO NOT use datetime.now(). start_date = datetime ( 2022 , 1 , 1 ), # How often should I run the dag. If set to None your dag # will only run when manually triggered. schedule_interval = None , ) # Environmental variables for passing to the docker container env_vars = { \"RUN\" : \"write\" , \"TEXT\" : \"Hello\" , \"OUTPATH\" : f \"s3://alpha-everyone/airflow-example/ { username } /test.txt\" , } \"\"\"\\nIt is good practice to set task_id as a variable above your task\\nbecause it is used for both the name and task_id parameters.\\nYou should also only use alpha numerical characters and dashes (-)\\nfor the task_id value.\\n\"\"\"', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 742}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ecca433c992dc825d5cbe0cd34ecd2ce'}>,\n",
       " <Document: {'content': 'task_id = \"task-1\" task = BasicKubernetesPodOperator ( dag = dag , run_as_user = USER_ID , repo_name = REPO_NAME , release = IMAGE_TAG , role = ROLE , task_id = task_id , env_vars = env_vars ) task The DAG uses the BasicKubernetesPodPperator , a function created and managed by the Data Engineering Team here to make it easier to run tasks. If you require something with more functionality you can use KubernetesPodOperator (on which BasicKubernetesPodPperator is based). In the example the schedule_interval is set to None. The schedule_interval can be defined using: a cron expression as a str (such as 0 0 * * * ) fields are “minute hour day-of-month month day-of-week” a cron preset (such as @once, @daily, @weekly, etc) a datetime.timedelta object. You can find more detailed guidance on DAG scripts, including on how to set up dependencies between tasks, in the Airflow documentation . You can group multiple DAGs together in a folder making it easier for others to understand how they relate to one another. This also allows you to add READMEs to these folders to again help others understand what the DAGs are for. Actions Create a new directory in airflow/dev/dags and give it an appropriate name. Name the directory {username} if you are creating an example pipeline Create a new python file and give it an appropriate name. Name the file copy_file_s3.py if you are creating an example pipeline If you are creating an example pipeline, paste the DAG example as-is but replace < > making sure to include quotation marks. You might need to update the IMAGE_TAG version number depending on the image that builds successfully in the GitHub repository. Delete the comments If you are creating your own pipeline modify the IMAGE_TAG, REPO_NAME, ROLE, owner, dag_id, scheduling and env_vars as appropriate. You can also add additional tasks if required but stick to one DAG per python file. Using a High-Memory Node (Optional) You have the option to use a high-memory node for workloads that process large data sets in memory (large datasets here typically meaning datasets larger than 2 GB). Please note that a high-memory node is expensive to run so only use after testing and failing with a standard node. Also note that high-memory nodes need to be provisioned which means pods will take longer to start. Finally we have restricted the number of high-memory nodes that can be provisioned so please schedule appropriately. Please see use_high_memory_node.py for an example usage. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 743}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9a8cb0035d06e20dea4a4e1479c39a08'}>,\n",
       " <Document: {'content': 'You’ll need to import the toleration and affinity: from imports.high_memory_constants import tolerations , affinity and add the following arguments to the KubernetesPodOperator: # Increase startup time to give time for node creation startup_timeout_seconds = 600 , tolerations = tolerations , affinity = affinity , We have a ticket to PDE-1830 - Modify BasicKubernetesPodOperator to specify high-memory node To Do Using imports (Optional) If your DAG relies on custom modules, be this for storing shared constant, or reused functions, you can implement this as seen in the folder structure for project_3 : Place any modules you might need in the imports folder Make sure the module starts with the name of the DAG folder e.g. project_3_constants.py These can then be imported by your dags. For example, dag.py in project_3 could import project_3_constants.py using from imports.project_3_constants import some_function Define the IAM Policy We use IAM Builder to generate an IAM policy based on a yaml configuration. If you are unfamiliar with the syntax the README of the previous link will help you. There are also some useful test IAM configs which are good examples of what you might want for your own role definition in the in IAM builder tests (note: ignore the ones prefixed with bad_ as they are invalid configs - used to check the package is working). The github action then creates the IAM role and attaches the IAM policy. Steps Create a new yaml file in airflow/dev/roles to store your IAM role policy. The name of the file must start with airflow_dev and must match the ROLE variable in the DAG. Name the file airflow_dev_{username}_example.yaml if you are creating an example pipeline Define the IAM policy. You can optionally include the field iam_role_name in your IAM config yaml but this must match the file name and the ROLE variable in the DAG. You only need to include the iam_role_name in the IAM config if you also have glue_job: true or secrets: true , otherwise it is optional. If you are creating an example pipeline, paste the following code, replacing “{username}” with your username: iam_role_name: airflow_dev_{username}_example\\n\\ns3:\\nread_write:\\n- alpha-everyone/airflow-example/* Validate from the command line (optional) The airflow repo validates the folder structure, DAGs ( validation.dags ) and Roles ( validation.roles ). The validation will run automatically when you raise a PR, but you can also validate using the command line to spot errors sooner. You’ll have to create and activate the python environment as specified in requirements-validation.txt. Make sure you are in the root of airflow repo and that you specify the full path to the file(s) you wish to validate The preferred method for running this validation is: python - W ignore - m validation filepath1 filepath2 etc flake8 . yamllint . When running these tests for the example pipeline: filepath1 should be environments/dev/dags/{username}/copy_file_s3.py and filepath2 should be environments/dev/roles/airflow_dev_{username}_example.yaml Deploy the changes Raise a PR into the main branch with your changes (ideally within a single commit). This will start a set of github action workflows. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 744}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f3ea858aef16e8edac167e3bf629fbee'}>,\n",
       " <Document: {'content': 'Check that all workflows pass after your PR is submitted. If any of the workflows fail the PR cannot be merged in main (and therefore not deployed) DE will be notified through a slack integration to approve the PR. Code changes made to files in the environments/dev/dags folder do not need approval: airflow ├── environments │ ├── dev │ │ ├── dags │ │ │ ├── APPROVAL NOT REQUIRED │ │ └── roles │ │ ├── APPROVAL REQUIRED │ ├── prod │ │ └── APPROVAL REQUIRED Once checks pass and PR approved by DE, please merge the PR into main The code in main is then automatically deployed to the Airflow prod and dev instances This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDAG Pipeline - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 745}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27398c5f540330103cb15b2e97201895'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 746}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools DAG and Role pipeline The airflow repo contains the DAG definitions and roles used in our Airflow pipelines and is structured in the following way: airflow ├── environments │ ├── dev │ │ ├── dags │ │ │ ├── project_1 │ │ │ │ ├── dag . py │ │ │ │ ├── ... │ │ │ │ └── README . md │ │ │ ├── project_2 │ │ │ │ ├── dag_1 . py │ │ │ │ ├── dag_2 . py │ │ │ │ ├── ... │ │ │ │ └── README . md │ │ │ ├── project_3 │ │ │ │ ├── dag . py │ │ │ │ └── README . md │ │ │ └── ... │ │ └── roles │ │ ├── airflow_dev_role_1 . yaml │ │ ├── airflow_dev_role_2 . yaml │ │ └── ... │ ├── prod │ │ └── ... │ └── ... ├── imports │ ├── __init__ . py │ ├── project_3_constants . py │ └── ... └── ... Anything in the folder environment/dev/ is deployed to the Airflow Dev Instance and anything in enivronment/prod/ is deployed to the Airflow production instance. Both prod and dev have the exact same folder structures. We’ll just refer to dev in the user guide (knowing the same is true for prod ). The idea is to test a pipeline in dev, before promoting it to the prod environment. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 747}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd88c0c2b76cd5983a863926aea167dfb'}>,\n",
       " <Document: {'content': 'You should ensure that DAGs or Roles across environments do not share write access to the same AWS resources or reference each other (e.g. a DAG defined in prod can only use roles defined in prod). However they can both reference the same docker image. Define the DAG A DAG is defined in a Python script. An example of a DAG script is provided below, based on the example pipeline. A few more example DAGs are available in example_DAGS . DAG example from datetime import datetime from airflow.models import DAG from mojap_airflow_tools.operators import BasicKubernetesPodOperator # Replace <<username>> with your username username = << username >> # As defined in the image repo IMAGE_TAG = \"v0.0.1\" REPO_NAME = f \"airflow- { username } -example\" # The user_id can be any number apart from 1000. In the case of R images, it must match the userid in the dockerfile USER_ID = 1337 \"\"\"\\nThe role used by Airflow to run a task.\\nThis role must be specified in the corresponding `roles/` folder in the same environment\\n(i.e. the role is defined in environments/dev/roles/airflow_dev_{username}_example.yaml).\\nThe role must only contain alphanumeric or underscore characters [a-zA-Z0-9_].\\nThis can be enforced using:\\nROLE = re.sub(r\"[\\\\W]+\", \"\", f\"airflow_dev_{username}_example\")\\n\"\"\" # ROLE = re.sub(r\"[\\\\W]+\", \"\", f\"airflow_dev_{username}_example\") ROLE = f \"airflow_dev_ { username } _example\" # For tips/advice on the default args see the use_dummy_operator.py example default_args = { # Normally you want to set this to False. \"depends_on_past\" : False , \"email_on_failure\" : False , # Name of DAG owner as it will appear on Airflow UI \"owner\" : f \" { username } \" , } dag = DAG ( # Name of the dag (how it will appear on the Airflow UI) # We use the naming convention: <folder_name>.<filename> dag_id = f \" { username } .copy_file_s3\" , default_args = default_args , description = \"A basic Kubernetes DAG\" , # Requires a start_date as a datetime object. This will be when the # DAG will start running from. DO NOT use datetime.now(). start_date = datetime ( 2022 , 1 , 1 ), # How often should I run the dag. If set to None your dag # will only run when manually triggered. schedule_interval = None , ) # Environmental variables for passing to the docker container env_vars = { \"RUN\" : \"write\" , \"TEXT\" : \"Hello\" , \"OUTPATH\" : f \"s3://alpha-everyone/airflow-example/ { username } /test.txt\" , } \"\"\"\\nIt is good practice to set task_id as a variable above your task\\nbecause it is used for both the name and task_id parameters.\\nYou should also only use alpha numerical characters and dashes (-)\\nfor the task_id value.\\n\"\"\"', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 748}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ecca433c992dc825d5cbe0cd34ecd2ce'}>,\n",
       " <Document: {'content': 'task_id = \"task-1\" task = BasicKubernetesPodOperator ( dag = dag , run_as_user = USER_ID , repo_name = REPO_NAME , release = IMAGE_TAG , role = ROLE , task_id = task_id , env_vars = env_vars ) task The DAG uses the BasicKubernetesPodPperator , a function created and managed by the Data Engineering Team here to make it easier to run tasks. If you require something with more functionality you can use KubernetesPodOperator (on which BasicKubernetesPodPperator is based). In the example the schedule_interval is set to None. The schedule_interval can be defined using: a cron expression as a str (such as 0 0 * * * ) fields are “minute hour day-of-month month day-of-week” a cron preset (such as @once, @daily, @weekly, etc) a datetime.timedelta object. You can find more detailed guidance on DAG scripts, including on how to set up dependencies between tasks, in the Airflow documentation . You can group multiple DAGs together in a folder making it easier for others to understand how they relate to one another. This also allows you to add READMEs to these folders to again help others understand what the DAGs are for. Actions Create a new directory in airflow/dev/dags and give it an appropriate name. Name the directory {username} if you are creating an example pipeline Create a new python file and give it an appropriate name. Name the file copy_file_s3.py if you are creating an example pipeline If you are creating an example pipeline, paste the DAG example as-is but replace < > making sure to include quotation marks. You might need to update the IMAGE_TAG version number depending on the image that builds successfully in the GitHub repository. Delete the comments If you are creating your own pipeline modify the IMAGE_TAG, REPO_NAME, ROLE, owner, dag_id, scheduling and env_vars as appropriate. You can also add additional tasks if required but stick to one DAG per python file. Using a High-Memory Node (Optional) You have the option to use a high-memory node for workloads that process large data sets in memory (large datasets here typically meaning datasets larger than 2 GB). Please note that a high-memory node is expensive to run so only use after testing and failing with a standard node. Also note that high-memory nodes need to be provisioned which means pods will take longer to start. Finally we have restricted the number of high-memory nodes that can be provisioned so please schedule appropriately. Please see use_high_memory_node.py for an example usage. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 749}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9a8cb0035d06e20dea4a4e1479c39a08'}>,\n",
       " <Document: {'content': 'You’ll need to import the toleration and affinity: from imports.high_memory_constants import tolerations , affinity and add the following arguments to the KubernetesPodOperator: # Increase startup time to give time for node creation startup_timeout_seconds = 600 , tolerations = tolerations , affinity = affinity , We have a ticket to PDE-1830 - Modify BasicKubernetesPodOperator to specify high-memory node To Do Using imports (Optional) If your DAG relies on custom modules, be this for storing shared constant, or reused functions, you can implement this as seen in the folder structure for project_3 : Place any modules you might need in the imports folder Make sure the module starts with the name of the DAG folder e.g. project_3_constants.py These can then be imported by your dags. For example, dag.py in project_3 could import project_3_constants.py using from imports.project_3_constants import some_function Define the IAM Policy We use IAM Builder to generate an IAM policy based on a yaml configuration. If you are unfamiliar with the syntax the README of the previous link will help you. There are also some useful test IAM configs which are good examples of what you might want for your own role definition in the in IAM builder tests (note: ignore the ones prefixed with bad_ as they are invalid configs - used to check the package is working). The github action then creates the IAM role and attaches the IAM policy. Steps Create a new yaml file in airflow/dev/roles to store your IAM role policy. The name of the file must start with airflow_dev and must match the ROLE variable in the DAG. Name the file airflow_dev_{username}_example.yaml if you are creating an example pipeline Define the IAM policy. You can optionally include the field iam_role_name in your IAM config yaml but this must match the file name and the ROLE variable in the DAG. You only need to include the iam_role_name in the IAM config if you also have glue_job: true or secrets: true , otherwise it is optional. If you are creating an example pipeline, paste the following code, replacing “{username}” with your username: iam_role_name: airflow_dev_{username}_example\\n\\ns3:\\nread_write:\\n- alpha-everyone/airflow-example/* Validate from the command line (optional) The airflow repo validates the folder structure, DAGs ( validation.dags ) and Roles ( validation.roles ). The validation will run automatically when you raise a PR, but you can also validate using the command line to spot errors sooner. You’ll have to create and activate the python environment as specified in requirements-validation.txt. Make sure you are in the root of airflow repo and that you specify the full path to the file(s) you wish to validate The preferred method for running this validation is: python - W ignore - m validation filepath1 filepath2 etc flake8 . yamllint . When running these tests for the example pipeline: filepath1 should be environments/dev/dags/{username}/copy_file_s3.py and filepath2 should be environments/dev/roles/airflow_dev_{username}_example.yaml Deploy the changes Raise a PR into the main branch with your changes (ideally within a single commit). This will start a set of github action workflows. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 750}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f3ea858aef16e8edac167e3bf629fbee'}>,\n",
       " <Document: {'content': 'Check that all workflows pass after your PR is submitted. If any of the workflows fail the PR cannot be merged in main (and therefore not deployed) DE will be notified through a slack integration to approve the PR. Code changes made to files in the environments/dev/dags folder do not need approval: airflow ├── environments │ ├── dev │ │ ├── dags │ │ │ ├── APPROVAL NOT REQUIRED │ │ └── roles │ │ ├── APPROVAL REQUIRED │ ├── prod │ │ └── APPROVAL REQUIRED Once checks pass and PR approved by DE, please merge the PR into main The code in main is then automatically deployed to the Airflow prod and dev instances This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nTroubleshooting Airflow Pipelines - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 751}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2e431dbcdf9d5976cc24a898d5d88608'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 752}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Troubleshooting Airflow Pipelines Note that airflow time is UTC (not BST) Manual vs Scheduled run on the UI (DAG runs are not sorted in order of when they ran but when they are scheduled. THESE ARE DIFFERENT THINGS) Airflow will run your DAG at the end of each interval. For example, if you create a DAG with start_date=datetime(2019, 9, 30) and schedule_interval=@daily , the first run marked 2019-09-30 will be triggered at 2019-09-30T23:59 and subsequent runs will be triggered every 24 hours thereafter This can be confusing when looking at the “Next Run” column on the UI, which gives the start time of the next interval. To find out the time when the job is next expected to run, click on the “i” which will give you the “Run After” time Jobs using the high-memory nodes will take longer to start If you are using a package manager such a renv , venv , or packrat , you will need to ensure this environment is created inside a folder that you have run chmod on (we would recommend this be your WORKDIR ). There are a limited number of nodes, so your job might be queued until a node becomes available If your job fails, you can click on individual tasks to get more info - Clicking on the ‘logs’ button will give you access to the logs for all 4 attempts at running your code. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 753}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '568bdc628d3e150b18440e36e0c8fbbe'}>,\n",
       " <Document: {'content': 'This may help you diagnose what exactly your DAG is doing that is causing a failure. This will also give you the specific error message your DAG is producing, which will help us with diagnosis if you need to raise the issue with a data engineer. If you get the following error message in your logs: Pod took too long to start it’s likely that the pod is trying to pull an image that doesn’t exist so eventually times out. Have a look at the github action in your image repo ( not airflow repo) to make sure it was successful and the image was built and pushed to ECR This page was last reviewed on 7 May 2022.\\n\\nIt needs to be reviewed again on 7 July 2022\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 7 July 2022\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData Uploader - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 754}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fd3b0f19845f5e079d15fe2e0aaa9f8'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 755}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data Uploader Quickstart link to the production deployment Uploader . We have had problems with firefox; please use an alternate browser. Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Troubleshooting Limitations and awareness Why use the Uploader? The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. Uploader flowchart Data is stored in the S3 bucket in an SQL like structure with one or more databases at the top level, each of which may contain one or more data tables. If the database or data table already exists it will be available to choose in a drop down list, if not it must be created. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 756}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '46cf5c29c442e692a23ea12879e71d47'}>,\n",
       " <Document: {'content': 'When subsequent new data files are added to a data table they will be stored as separate partitions of that data table, and any schema changes will be represented in the final table as a union of every partition. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Automated validation is applied to the data pre upload to check the basics - there are checks that column names exist and are in the character set [A-Za-z0-9_] . Data Uploader pre-requisites In order to utilise the data uploader, you are required to do a pull request. This can be done via this github page and then adding your email to the users list.  If your github user is unable to create a pull request you may need to be added to https://github.com/ministryofjustice which can be done on #ask-operations-engineering on the Justice Digital slack. If you cannot get access to the MoJ organisation on GitHub one of the dev team can raise the PR for you. Next you will need to get access to the DPIA (Data Protection Impact Assessment) or an MOJ analytical platform data movement Form . This supplies legal cover for new datasets and gives legal cover to send official sensitive data to the prod site. If not, you will only be able to send non-sensitive test data. Login page Go to the Uploader login page . Ensure you have set up an account to access the Uploader as in the Data Uploader pre-requisites section above. Click Login , enter the email address used for your Datauploader account and follow instructions. Front page Ensure your data meets the Before you start criteria and then click on Start now . Step 1 of 4: Data governance requirements It is your responsibility to complete any relevant data governance requirements before proceeding to upload data to the Analytical Platform. Step 2 of 4: Choose database If your data is not part of an existing database select the option to create a new database, and specify the new database name. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . Note that new database names will be automatically prefixed with data_eng_uploader_<env> , where <env> is either dev , preprod or prod . Otherwise, choose the existing database from the drop down menu. Step 3 of 4: Choose table If you have created a new database at step 2 the only option now is to create a new data table. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 757}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4ad09ec15b92765623f063436e0a2d15'}>,\n",
       " <Document: {'content': 'If you are adding to an existing database there will be options to create a new data table or add to an existing data table. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Step 4 of 4: Check your inputs before uploading your data Double check your inputs to the previous three steps. In particular if you are adding data to an existing table be absolutely certain you have the correct database and table name; the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Ensure you are happy with any new database or table names as you will not be able to change these easily. If all is OK proceed with Choose file and Upload file . Otherwise click on the corresponding Change navigation button to return to a previous page and amend your input. A progress bar is included for your convenience. Once the upload begins a Cancel button becomes available, so you can abort if required. Note that with a small file the upload is usually so quick that you will not have time to abort. Upload complete The details of the upload will be sent in a confirmation email. This includes essential information such as database name, table name, Athena reference path, S3 bucket path and extraction_timestamp . This information is required to locate your data on the MoJ Analytical Platform. All Data that has been uploaded is subsequently moved to the AP at 1am every day. It can take up to 24 hours for a newly created database to show, data table and newly uploaded data to appear on the Analytical Platform. If you have created a new database the next step is to request access to it from the MoJ Analytical Platform; instructions to do so may be found here . Getting access to uploaded data Dev example If you are using the dev version of the Uploader, then your data is accessible to all users listed in the Standard Database Access project .\\nIf your alpha_username is not already listed in standard_database_access.yaml , clone the repo , create a branch, add your alpha_username and raise a PR. Once approved, you will have access. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 758}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27bdb08e1502ed336d4c0afa1867b130'}>,\n",
       " <Document: {'content': 'Prod example Data uploaded via the uploader to prod are inacessible by default. To gain access, a database file and project access file are needed. When creating a database file, include a README.md file to describe the provenance of the data and list any governance around it. Here is an example database file and project access file for the cjs dashboard: Troubleshooting Problem Solution data file is a bit larger than 5 GB chop the data into multiple files and upload one at a time re max file upload see docs/blogs here , here (option 3) and here data file is very much larger than 5 GB Uploader tool is currently impractical; upload via AWS console. Future Uploader may allow multi-part upload to 5 TB. data is not in one of the 3 supported formats (.csv, .json, .jsonl) convert it; for example if is .xlsx you can easily convert to .csv uploading data violates data governance requirements STOP! you may not upload these data do not have access to Analytical Platform work through the steps to get an account here do not have access to the required database request access here I want to replace existing data on the Analytical Platform this is not possible with the Uploader. Consider this action may conflict with reproducibility principles. database is not listed in the drop down menu only databases created via the Uploader will be accessible and a newly created database may take up to 24 hours to appear I want to change the name of an existing database you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles data table is not listed in the drop down menu only data tables created via the Uploader will be accessible and a newly created data table may take up to 24 hours to appear there is no option to select an existing data table reconsider your input to step 2, click “Back” at the top of the screen I want to change the name of an existing data table you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles error “No column names detected…” Ensure that csv files contain columns names in the first row. Column names may only use the character set [A-Za-z0-9_] error “Column names contain special characters…” Column names may only use the character set [A-Za-z0-9_] Limitations and awareness Users will have to wait 24 hours before data is on the analytical platform. This means that users will have to wait a while before they can visualise their data. If you would like to see the data earlier you will need to upload directly via AWS instead. Cannot upload xls files, only csv, JSON and JSONL. Cannot replace existing data even if the data uploaded is inaccurate due to data governance requirements. File size can only be 5GB. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 759}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '20aba2e8491c8e11d3a405286e1cf55'}>,\n",
       " <Document: {'content': 'S3 Bucket Database Name has a limit of 63 characters, this requires delimiting with underscores and ensuring that the file name is kept in lowercase, if you would like more information, look at the MoJ guidance for naming conventions. S3 Table Names have a limit of 1024 characters. Similarly with database names this requires adhering to the MoJ guidance for naming conventions. We do not have access to the permissions infrastructure of the AP. This means that anyone can upload to any table created by the uploader through the uploader. This is not a security risk, as the uploader is purely a loading mechanism and not a viewing mechanism, but be aware users might upload data of a different schema to a table accidentally. You cannot upload files from a sharepoint location directly, you’ll need to download the file locally first This page was last reviewed on 31 August 2022.\\n\\nIt needs to be reviewed again on 31 August 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 31 August 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData Uploader - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 760}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5571f5b9b94895ee1aeb7500c9001aae'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 761}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data Uploader Quickstart link to the production deployment Uploader . We have had problems with firefox; please use an alternate browser. Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Troubleshooting Limitations and awareness Why use the Uploader? The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. Uploader flowchart Data is stored in the S3 bucket in an SQL like structure with one or more databases at the top level, each of which may contain one or more data tables. If the database or data table already exists it will be available to choose in a drop down list, if not it must be created. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 762}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '46cf5c29c442e692a23ea12879e71d47'}>,\n",
       " <Document: {'content': 'When subsequent new data files are added to a data table they will be stored as separate partitions of that data table, and any schema changes will be represented in the final table as a union of every partition. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Automated validation is applied to the data pre upload to check the basics - there are checks that column names exist and are in the character set [A-Za-z0-9_] . Data Uploader pre-requisites In order to utilise the data uploader, you are required to do a pull request. This can be done via this github page and then adding your email to the users list.  If your github user is unable to create a pull request you may need to be added to https://github.com/ministryofjustice which can be done on #ask-operations-engineering on the Justice Digital slack. If you cannot get access to the MoJ organisation on GitHub one of the dev team can raise the PR for you. Next you will need to get access to the DPIA (Data Protection Impact Assessment) or an MOJ analytical platform data movement Form . This supplies legal cover for new datasets and gives legal cover to send official sensitive data to the prod site. If not, you will only be able to send non-sensitive test data. Login page Go to the Uploader login page . Ensure you have set up an account to access the Uploader as in the Data Uploader pre-requisites section above. Click Login , enter the email address used for your Datauploader account and follow instructions. Front page Ensure your data meets the Before you start criteria and then click on Start now . Step 1 of 4: Data governance requirements It is your responsibility to complete any relevant data governance requirements before proceeding to upload data to the Analytical Platform. Step 2 of 4: Choose database If your data is not part of an existing database select the option to create a new database, and specify the new database name. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . Note that new database names will be automatically prefixed with data_eng_uploader_<env> , where <env> is either dev , preprod or prod . Otherwise, choose the existing database from the drop down menu. Step 3 of 4: Choose table If you have created a new database at step 2 the only option now is to create a new data table. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 763}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4ad09ec15b92765623f063436e0a2d15'}>,\n",
       " <Document: {'content': 'If you are adding to an existing database there will be options to create a new data table or add to an existing data table. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Step 4 of 4: Check your inputs before uploading your data Double check your inputs to the previous three steps. In particular if you are adding data to an existing table be absolutely certain you have the correct database and table name; the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Ensure you are happy with any new database or table names as you will not be able to change these easily. If all is OK proceed with Choose file and Upload file . Otherwise click on the corresponding Change navigation button to return to a previous page and amend your input. A progress bar is included for your convenience. Once the upload begins a Cancel button becomes available, so you can abort if required. Note that with a small file the upload is usually so quick that you will not have time to abort. Upload complete The details of the upload will be sent in a confirmation email. This includes essential information such as database name, table name, Athena reference path, S3 bucket path and extraction_timestamp . This information is required to locate your data on the MoJ Analytical Platform. All Data that has been uploaded is subsequently moved to the AP at 1am every day. It can take up to 24 hours for a newly created database to show, data table and newly uploaded data to appear on the Analytical Platform. If you have created a new database the next step is to request access to it from the MoJ Analytical Platform; instructions to do so may be found here . Getting access to uploaded data Dev example If you are using the dev version of the Uploader, then your data is accessible to all users listed in the Standard Database Access project .\\nIf your alpha_username is not already listed in standard_database_access.yaml , clone the repo , create a branch, add your alpha_username and raise a PR. Once approved, you will have access. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 764}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27bdb08e1502ed336d4c0afa1867b130'}>,\n",
       " <Document: {'content': 'Prod example Data uploaded via the uploader to prod are inacessible by default. To gain access, a database file and project access file are needed. When creating a database file, include a README.md file to describe the provenance of the data and list any governance around it. Here is an example database file and project access file for the cjs dashboard: Troubleshooting Problem Solution data file is a bit larger than 5 GB chop the data into multiple files and upload one at a time re max file upload see docs/blogs here , here (option 3) and here data file is very much larger than 5 GB Uploader tool is currently impractical; upload via AWS console. Future Uploader may allow multi-part upload to 5 TB. data is not in one of the 3 supported formats (.csv, .json, .jsonl) convert it; for example if is .xlsx you can easily convert to .csv uploading data violates data governance requirements STOP! you may not upload these data do not have access to Analytical Platform work through the steps to get an account here do not have access to the required database request access here I want to replace existing data on the Analytical Platform this is not possible with the Uploader. Consider this action may conflict with reproducibility principles. database is not listed in the drop down menu only databases created via the Uploader will be accessible and a newly created database may take up to 24 hours to appear I want to change the name of an existing database you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles data table is not listed in the drop down menu only data tables created via the Uploader will be accessible and a newly created data table may take up to 24 hours to appear there is no option to select an existing data table reconsider your input to step 2, click “Back” at the top of the screen I want to change the name of an existing data table you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles error “No column names detected…” Ensure that csv files contain columns names in the first row. Column names may only use the character set [A-Za-z0-9_] error “Column names contain special characters…” Column names may only use the character set [A-Za-z0-9_] Limitations and awareness Users will have to wait 24 hours before data is on the analytical platform. This means that users will have to wait a while before they can visualise their data. If you would like to see the data earlier you will need to upload directly via AWS instead. Cannot upload xls files, only csv, JSON and JSONL. Cannot replace existing data even if the data uploaded is inaccurate due to data governance requirements. File size can only be 5GB. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 765}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '20aba2e8491c8e11d3a405286e1cf55'}>,\n",
       " <Document: {'content': 'S3 Bucket Database Name has a limit of 63 characters, this requires delimiting with underscores and ensuring that the file name is kept in lowercase, if you would like more information, look at the MoJ guidance for naming conventions. S3 Table Names have a limit of 1024 characters. Similarly with database names this requires adhering to the MoJ guidance for naming conventions. We do not have access to the permissions infrastructure of the AP. This means that anyone can upload to any table created by the uploader through the uploader. This is not a security risk, as the uploader is purely a loading mechanism and not a viewing mechanism, but be aware users might upload data of a different schema to a table accidentally. You cannot upload files from a sharepoint location directly, you’ll need to download the file locally first This page was last reviewed on 31 August 2022.\\n\\nIt needs to be reviewed again on 31 August 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 31 August 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData Uploader - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 766}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5571f5b9b94895ee1aeb7500c9001aae'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 767}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data Uploader Quickstart link to the production deployment Uploader . We have had problems with firefox; please use an alternate browser. Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Troubleshooting Limitations and awareness Why use the Uploader? The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. Uploader flowchart Data is stored in the S3 bucket in an SQL like structure with one or more databases at the top level, each of which may contain one or more data tables. If the database or data table already exists it will be available to choose in a drop down list, if not it must be created. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 768}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '46cf5c29c442e692a23ea12879e71d47'}>,\n",
       " <Document: {'content': 'When subsequent new data files are added to a data table they will be stored as separate partitions of that data table, and any schema changes will be represented in the final table as a union of every partition. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Automated validation is applied to the data pre upload to check the basics - there are checks that column names exist and are in the character set [A-Za-z0-9_] . Data Uploader pre-requisites In order to utilise the data uploader, you are required to do a pull request. This can be done via this github page and then adding your email to the users list.  If your github user is unable to create a pull request you may need to be added to https://github.com/ministryofjustice which can be done on #ask-operations-engineering on the Justice Digital slack. If you cannot get access to the MoJ organisation on GitHub one of the dev team can raise the PR for you. Next you will need to get access to the DPIA (Data Protection Impact Assessment) or an MOJ analytical platform data movement Form . This supplies legal cover for new datasets and gives legal cover to send official sensitive data to the prod site. If not, you will only be able to send non-sensitive test data. Login page Go to the Uploader login page . Ensure you have set up an account to access the Uploader as in the Data Uploader pre-requisites section above. Click Login , enter the email address used for your Datauploader account and follow instructions. Front page Ensure your data meets the Before you start criteria and then click on Start now . Step 1 of 4: Data governance requirements It is your responsibility to complete any relevant data governance requirements before proceeding to upload data to the Analytical Platform. Step 2 of 4: Choose database If your data is not part of an existing database select the option to create a new database, and specify the new database name. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . Note that new database names will be automatically prefixed with data_eng_uploader_<env> , where <env> is either dev , preprod or prod . Otherwise, choose the existing database from the drop down menu. Step 3 of 4: Choose table If you have created a new database at step 2 the only option now is to create a new data table. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 769}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4ad09ec15b92765623f063436e0a2d15'}>,\n",
       " <Document: {'content': 'If you are adding to an existing database there will be options to create a new data table or add to an existing data table. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Step 4 of 4: Check your inputs before uploading your data Double check your inputs to the previous three steps. In particular if you are adding data to an existing table be absolutely certain you have the correct database and table name; the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Ensure you are happy with any new database or table names as you will not be able to change these easily. If all is OK proceed with Choose file and Upload file . Otherwise click on the corresponding Change navigation button to return to a previous page and amend your input. A progress bar is included for your convenience. Once the upload begins a Cancel button becomes available, so you can abort if required. Note that with a small file the upload is usually so quick that you will not have time to abort. Upload complete The details of the upload will be sent in a confirmation email. This includes essential information such as database name, table name, Athena reference path, S3 bucket path and extraction_timestamp . This information is required to locate your data on the MoJ Analytical Platform. All Data that has been uploaded is subsequently moved to the AP at 1am every day. It can take up to 24 hours for a newly created database to show, data table and newly uploaded data to appear on the Analytical Platform. If you have created a new database the next step is to request access to it from the MoJ Analytical Platform; instructions to do so may be found here . Getting access to uploaded data Dev example If you are using the dev version of the Uploader, then your data is accessible to all users listed in the Standard Database Access project .\\nIf your alpha_username is not already listed in standard_database_access.yaml , clone the repo , create a branch, add your alpha_username and raise a PR. Once approved, you will have access. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 770}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27bdb08e1502ed336d4c0afa1867b130'}>,\n",
       " <Document: {'content': 'Prod example Data uploaded via the uploader to prod are inacessible by default. To gain access, a database file and project access file are needed. When creating a database file, include a README.md file to describe the provenance of the data and list any governance around it. Here is an example database file and project access file for the cjs dashboard: Troubleshooting Problem Solution data file is a bit larger than 5 GB chop the data into multiple files and upload one at a time re max file upload see docs/blogs here , here (option 3) and here data file is very much larger than 5 GB Uploader tool is currently impractical; upload via AWS console. Future Uploader may allow multi-part upload to 5 TB. data is not in one of the 3 supported formats (.csv, .json, .jsonl) convert it; for example if is .xlsx you can easily convert to .csv uploading data violates data governance requirements STOP! you may not upload these data do not have access to Analytical Platform work through the steps to get an account here do not have access to the required database request access here I want to replace existing data on the Analytical Platform this is not possible with the Uploader. Consider this action may conflict with reproducibility principles. database is not listed in the drop down menu only databases created via the Uploader will be accessible and a newly created database may take up to 24 hours to appear I want to change the name of an existing database you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles data table is not listed in the drop down menu only data tables created via the Uploader will be accessible and a newly created data table may take up to 24 hours to appear there is no option to select an existing data table reconsider your input to step 2, click “Back” at the top of the screen I want to change the name of an existing data table you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles error “No column names detected…” Ensure that csv files contain columns names in the first row. Column names may only use the character set [A-Za-z0-9_] error “Column names contain special characters…” Column names may only use the character set [A-Za-z0-9_] Limitations and awareness Users will have to wait 24 hours before data is on the analytical platform. This means that users will have to wait a while before they can visualise their data. If you would like to see the data earlier you will need to upload directly via AWS instead. Cannot upload xls files, only csv, JSON and JSONL. Cannot replace existing data even if the data uploaded is inaccurate due to data governance requirements. File size can only be 5GB. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 771}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '20aba2e8491c8e11d3a405286e1cf55'}>,\n",
       " <Document: {'content': 'S3 Bucket Database Name has a limit of 63 characters, this requires delimiting with underscores and ensuring that the file name is kept in lowercase, if you would like more information, look at the MoJ guidance for naming conventions. S3 Table Names have a limit of 1024 characters. Similarly with database names this requires adhering to the MoJ guidance for naming conventions. We do not have access to the permissions infrastructure of the AP. This means that anyone can upload to any table created by the uploader through the uploader. This is not a security risk, as the uploader is purely a loading mechanism and not a viewing mechanism, but be aware users might upload data of a different schema to a table accidentally. You cannot upload files from a sharepoint location directly, you’ll need to download the file locally first This page was last reviewed on 31 August 2022.\\n\\nIt needs to be reviewed again on 31 August 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 31 August 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData Uploader - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 772}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5571f5b9b94895ee1aeb7500c9001aae'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 773}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data Uploader Quickstart link to the production deployment Uploader . We have had problems with firefox; please use an alternate browser. Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Troubleshooting Limitations and awareness Why use the Uploader? The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. Uploader flowchart Data is stored in the S3 bucket in an SQL like structure with one or more databases at the top level, each of which may contain one or more data tables. If the database or data table already exists it will be available to choose in a drop down list, if not it must be created. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 774}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '46cf5c29c442e692a23ea12879e71d47'}>,\n",
       " <Document: {'content': 'When subsequent new data files are added to a data table they will be stored as separate partitions of that data table, and any schema changes will be represented in the final table as a union of every partition. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Automated validation is applied to the data pre upload to check the basics - there are checks that column names exist and are in the character set [A-Za-z0-9_] . Data Uploader pre-requisites In order to utilise the data uploader, you are required to do a pull request. This can be done via this github page and then adding your email to the users list.  If your github user is unable to create a pull request you may need to be added to https://github.com/ministryofjustice which can be done on #ask-operations-engineering on the Justice Digital slack. If you cannot get access to the MoJ organisation on GitHub one of the dev team can raise the PR for you. Next you will need to get access to the DPIA (Data Protection Impact Assessment) or an MOJ analytical platform data movement Form . This supplies legal cover for new datasets and gives legal cover to send official sensitive data to the prod site. If not, you will only be able to send non-sensitive test data. Login page Go to the Uploader login page . Ensure you have set up an account to access the Uploader as in the Data Uploader pre-requisites section above. Click Login , enter the email address used for your Datauploader account and follow instructions. Front page Ensure your data meets the Before you start criteria and then click on Start now . Step 1 of 4: Data governance requirements It is your responsibility to complete any relevant data governance requirements before proceeding to upload data to the Analytical Platform. Step 2 of 4: Choose database If your data is not part of an existing database select the option to create a new database, and specify the new database name. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . Note that new database names will be automatically prefixed with data_eng_uploader_<env> , where <env> is either dev , preprod or prod . Otherwise, choose the existing database from the drop down menu. Step 3 of 4: Choose table If you have created a new database at step 2 the only option now is to create a new data table. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 775}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4ad09ec15b92765623f063436e0a2d15'}>,\n",
       " <Document: {'content': 'If you are adding to an existing database there will be options to create a new data table or add to an existing data table. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Step 4 of 4: Check your inputs before uploading your data Double check your inputs to the previous three steps. In particular if you are adding data to an existing table be absolutely certain you have the correct database and table name; the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Ensure you are happy with any new database or table names as you will not be able to change these easily. If all is OK proceed with Choose file and Upload file . Otherwise click on the corresponding Change navigation button to return to a previous page and amend your input. A progress bar is included for your convenience. Once the upload begins a Cancel button becomes available, so you can abort if required. Note that with a small file the upload is usually so quick that you will not have time to abort. Upload complete The details of the upload will be sent in a confirmation email. This includes essential information such as database name, table name, Athena reference path, S3 bucket path and extraction_timestamp . This information is required to locate your data on the MoJ Analytical Platform. All Data that has been uploaded is subsequently moved to the AP at 1am every day. It can take up to 24 hours for a newly created database to show, data table and newly uploaded data to appear on the Analytical Platform. If you have created a new database the next step is to request access to it from the MoJ Analytical Platform; instructions to do so may be found here . Getting access to uploaded data Dev example If you are using the dev version of the Uploader, then your data is accessible to all users listed in the Standard Database Access project .\\nIf your alpha_username is not already listed in standard_database_access.yaml , clone the repo , create a branch, add your alpha_username and raise a PR. Once approved, you will have access. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 776}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27bdb08e1502ed336d4c0afa1867b130'}>,\n",
       " <Document: {'content': 'Prod example Data uploaded via the uploader to prod are inacessible by default. To gain access, a database file and project access file are needed. When creating a database file, include a README.md file to describe the provenance of the data and list any governance around it. Here is an example database file and project access file for the cjs dashboard: Troubleshooting Problem Solution data file is a bit larger than 5 GB chop the data into multiple files and upload one at a time re max file upload see docs/blogs here , here (option 3) and here data file is very much larger than 5 GB Uploader tool is currently impractical; upload via AWS console. Future Uploader may allow multi-part upload to 5 TB. data is not in one of the 3 supported formats (.csv, .json, .jsonl) convert it; for example if is .xlsx you can easily convert to .csv uploading data violates data governance requirements STOP! you may not upload these data do not have access to Analytical Platform work through the steps to get an account here do not have access to the required database request access here I want to replace existing data on the Analytical Platform this is not possible with the Uploader. Consider this action may conflict with reproducibility principles. database is not listed in the drop down menu only databases created via the Uploader will be accessible and a newly created database may take up to 24 hours to appear I want to change the name of an existing database you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles data table is not listed in the drop down menu only data tables created via the Uploader will be accessible and a newly created data table may take up to 24 hours to appear there is no option to select an existing data table reconsider your input to step 2, click “Back” at the top of the screen I want to change the name of an existing data table you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles error “No column names detected…” Ensure that csv files contain columns names in the first row. Column names may only use the character set [A-Za-z0-9_] error “Column names contain special characters…” Column names may only use the character set [A-Za-z0-9_] Limitations and awareness Users will have to wait 24 hours before data is on the analytical platform. This means that users will have to wait a while before they can visualise their data. If you would like to see the data earlier you will need to upload directly via AWS instead. Cannot upload xls files, only csv, JSON and JSONL. Cannot replace existing data even if the data uploaded is inaccurate due to data governance requirements. File size can only be 5GB. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 777}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '20aba2e8491c8e11d3a405286e1cf55'}>,\n",
       " <Document: {'content': 'S3 Bucket Database Name has a limit of 63 characters, this requires delimiting with underscores and ensuring that the file name is kept in lowercase, if you would like more information, look at the MoJ guidance for naming conventions. S3 Table Names have a limit of 1024 characters. Similarly with database names this requires adhering to the MoJ guidance for naming conventions. We do not have access to the permissions infrastructure of the AP. This means that anyone can upload to any table created by the uploader through the uploader. This is not a security risk, as the uploader is purely a loading mechanism and not a viewing mechanism, but be aware users might upload data of a different schema to a table accidentally. You cannot upload files from a sharepoint location directly, you’ll need to download the file locally first This page was last reviewed on 31 August 2022.\\n\\nIt needs to be reviewed again on 31 August 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 31 August 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData Uploader - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 778}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5571f5b9b94895ee1aeb7500c9001aae'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 779}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data Uploader Quickstart link to the production deployment Uploader . We have had problems with firefox; please use an alternate browser. Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Troubleshooting Limitations and awareness Why use the Uploader? The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. Uploader flowchart Data is stored in the S3 bucket in an SQL like structure with one or more databases at the top level, each of which may contain one or more data tables. If the database or data table already exists it will be available to choose in a drop down list, if not it must be created. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 780}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '46cf5c29c442e692a23ea12879e71d47'}>,\n",
       " <Document: {'content': 'When subsequent new data files are added to a data table they will be stored as separate partitions of that data table, and any schema changes will be represented in the final table as a union of every partition. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Automated validation is applied to the data pre upload to check the basics - there are checks that column names exist and are in the character set [A-Za-z0-9_] . Data Uploader pre-requisites In order to utilise the data uploader, you are required to do a pull request. This can be done via this github page and then adding your email to the users list.  If your github user is unable to create a pull request you may need to be added to https://github.com/ministryofjustice which can be done on #ask-operations-engineering on the Justice Digital slack. If you cannot get access to the MoJ organisation on GitHub one of the dev team can raise the PR for you. Next you will need to get access to the DPIA (Data Protection Impact Assessment) or an MOJ analytical platform data movement Form . This supplies legal cover for new datasets and gives legal cover to send official sensitive data to the prod site. If not, you will only be able to send non-sensitive test data. Login page Go to the Uploader login page . Ensure you have set up an account to access the Uploader as in the Data Uploader pre-requisites section above. Click Login , enter the email address used for your Datauploader account and follow instructions. Front page Ensure your data meets the Before you start criteria and then click on Start now . Step 1 of 4: Data governance requirements It is your responsibility to complete any relevant data governance requirements before proceeding to upload data to the Analytical Platform. Step 2 of 4: Choose database If your data is not part of an existing database select the option to create a new database, and specify the new database name. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . Note that new database names will be automatically prefixed with data_eng_uploader_<env> , where <env> is either dev , preprod or prod . Otherwise, choose the existing database from the drop down menu. Step 3 of 4: Choose table If you have created a new database at step 2 the only option now is to create a new data table. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 781}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4ad09ec15b92765623f063436e0a2d15'}>,\n",
       " <Document: {'content': 'If you are adding to an existing database there will be options to create a new data table or add to an existing data table. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Step 4 of 4: Check your inputs before uploading your data Double check your inputs to the previous three steps. In particular if you are adding data to an existing table be absolutely certain you have the correct database and table name; the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Ensure you are happy with any new database or table names as you will not be able to change these easily. If all is OK proceed with Choose file and Upload file . Otherwise click on the corresponding Change navigation button to return to a previous page and amend your input. A progress bar is included for your convenience. Once the upload begins a Cancel button becomes available, so you can abort if required. Note that with a small file the upload is usually so quick that you will not have time to abort. Upload complete The details of the upload will be sent in a confirmation email. This includes essential information such as database name, table name, Athena reference path, S3 bucket path and extraction_timestamp . This information is required to locate your data on the MoJ Analytical Platform. All Data that has been uploaded is subsequently moved to the AP at 1am every day. It can take up to 24 hours for a newly created database to show, data table and newly uploaded data to appear on the Analytical Platform. If you have created a new database the next step is to request access to it from the MoJ Analytical Platform; instructions to do so may be found here . Getting access to uploaded data Dev example If you are using the dev version of the Uploader, then your data is accessible to all users listed in the Standard Database Access project .\\nIf your alpha_username is not already listed in standard_database_access.yaml , clone the repo , create a branch, add your alpha_username and raise a PR. Once approved, you will have access. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 782}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27bdb08e1502ed336d4c0afa1867b130'}>,\n",
       " <Document: {'content': 'Prod example Data uploaded via the uploader to prod are inacessible by default. To gain access, a database file and project access file are needed. When creating a database file, include a README.md file to describe the provenance of the data and list any governance around it. Here is an example database file and project access file for the cjs dashboard: Troubleshooting Problem Solution data file is a bit larger than 5 GB chop the data into multiple files and upload one at a time re max file upload see docs/blogs here , here (option 3) and here data file is very much larger than 5 GB Uploader tool is currently impractical; upload via AWS console. Future Uploader may allow multi-part upload to 5 TB. data is not in one of the 3 supported formats (.csv, .json, .jsonl) convert it; for example if is .xlsx you can easily convert to .csv uploading data violates data governance requirements STOP! you may not upload these data do not have access to Analytical Platform work through the steps to get an account here do not have access to the required database request access here I want to replace existing data on the Analytical Platform this is not possible with the Uploader. Consider this action may conflict with reproducibility principles. database is not listed in the drop down menu only databases created via the Uploader will be accessible and a newly created database may take up to 24 hours to appear I want to change the name of an existing database you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles data table is not listed in the drop down menu only data tables created via the Uploader will be accessible and a newly created data table may take up to 24 hours to appear there is no option to select an existing data table reconsider your input to step 2, click “Back” at the top of the screen I want to change the name of an existing data table you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles error “No column names detected…” Ensure that csv files contain columns names in the first row. Column names may only use the character set [A-Za-z0-9_] error “Column names contain special characters…” Column names may only use the character set [A-Za-z0-9_] Limitations and awareness Users will have to wait 24 hours before data is on the analytical platform. This means that users will have to wait a while before they can visualise their data. If you would like to see the data earlier you will need to upload directly via AWS instead. Cannot upload xls files, only csv, JSON and JSONL. Cannot replace existing data even if the data uploaded is inaccurate due to data governance requirements. File size can only be 5GB. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 783}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '20aba2e8491c8e11d3a405286e1cf55'}>,\n",
       " <Document: {'content': 'S3 Bucket Database Name has a limit of 63 characters, this requires delimiting with underscores and ensuring that the file name is kept in lowercase, if you would like more information, look at the MoJ guidance for naming conventions. S3 Table Names have a limit of 1024 characters. Similarly with database names this requires adhering to the MoJ guidance for naming conventions. We do not have access to the permissions infrastructure of the AP. This means that anyone can upload to any table created by the uploader through the uploader. This is not a security risk, as the uploader is purely a loading mechanism and not a viewing mechanism, but be aware users might upload data of a different schema to a table accidentally. You cannot upload files from a sharepoint location directly, you’ll need to download the file locally first This page was last reviewed on 31 August 2022.\\n\\nIt needs to be reviewed again on 31 August 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 31 August 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData Uploader - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 784}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5571f5b9b94895ee1aeb7500c9001aae'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 785}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data Uploader Quickstart link to the production deployment Uploader . We have had problems with firefox; please use an alternate browser. Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Troubleshooting Limitations and awareness Why use the Uploader? The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. Uploader flowchart Data is stored in the S3 bucket in an SQL like structure with one or more databases at the top level, each of which may contain one or more data tables. If the database or data table already exists it will be available to choose in a drop down list, if not it must be created. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 786}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '46cf5c29c442e692a23ea12879e71d47'}>,\n",
       " <Document: {'content': 'When subsequent new data files are added to a data table they will be stored as separate partitions of that data table, and any schema changes will be represented in the final table as a union of every partition. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Automated validation is applied to the data pre upload to check the basics - there are checks that column names exist and are in the character set [A-Za-z0-9_] . Data Uploader pre-requisites In order to utilise the data uploader, you are required to do a pull request. This can be done via this github page and then adding your email to the users list.  If your github user is unable to create a pull request you may need to be added to https://github.com/ministryofjustice which can be done on #ask-operations-engineering on the Justice Digital slack. If you cannot get access to the MoJ organisation on GitHub one of the dev team can raise the PR for you. Next you will need to get access to the DPIA (Data Protection Impact Assessment) or an MOJ analytical platform data movement Form . This supplies legal cover for new datasets and gives legal cover to send official sensitive data to the prod site. If not, you will only be able to send non-sensitive test data. Login page Go to the Uploader login page . Ensure you have set up an account to access the Uploader as in the Data Uploader pre-requisites section above. Click Login , enter the email address used for your Datauploader account and follow instructions. Front page Ensure your data meets the Before you start criteria and then click on Start now . Step 1 of 4: Data governance requirements It is your responsibility to complete any relevant data governance requirements before proceeding to upload data to the Analytical Platform. Step 2 of 4: Choose database If your data is not part of an existing database select the option to create a new database, and specify the new database name. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . Note that new database names will be automatically prefixed with data_eng_uploader_<env> , where <env> is either dev , preprod or prod . Otherwise, choose the existing database from the drop down menu. Step 3 of 4: Choose table If you have created a new database at step 2 the only option now is to create a new data table. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 787}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4ad09ec15b92765623f063436e0a2d15'}>,\n",
       " <Document: {'content': 'If you are adding to an existing database there will be options to create a new data table or add to an existing data table. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Step 4 of 4: Check your inputs before uploading your data Double check your inputs to the previous three steps. In particular if you are adding data to an existing table be absolutely certain you have the correct database and table name; the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Ensure you are happy with any new database or table names as you will not be able to change these easily. If all is OK proceed with Choose file and Upload file . Otherwise click on the corresponding Change navigation button to return to a previous page and amend your input. A progress bar is included for your convenience. Once the upload begins a Cancel button becomes available, so you can abort if required. Note that with a small file the upload is usually so quick that you will not have time to abort. Upload complete The details of the upload will be sent in a confirmation email. This includes essential information such as database name, table name, Athena reference path, S3 bucket path and extraction_timestamp . This information is required to locate your data on the MoJ Analytical Platform. All Data that has been uploaded is subsequently moved to the AP at 1am every day. It can take up to 24 hours for a newly created database to show, data table and newly uploaded data to appear on the Analytical Platform. If you have created a new database the next step is to request access to it from the MoJ Analytical Platform; instructions to do so may be found here . Getting access to uploaded data Dev example If you are using the dev version of the Uploader, then your data is accessible to all users listed in the Standard Database Access project .\\nIf your alpha_username is not already listed in standard_database_access.yaml , clone the repo , create a branch, add your alpha_username and raise a PR. Once approved, you will have access. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 788}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27bdb08e1502ed336d4c0afa1867b130'}>,\n",
       " <Document: {'content': 'Prod example Data uploaded via the uploader to prod are inacessible by default. To gain access, a database file and project access file are needed. When creating a database file, include a README.md file to describe the provenance of the data and list any governance around it. Here is an example database file and project access file for the cjs dashboard: Troubleshooting Problem Solution data file is a bit larger than 5 GB chop the data into multiple files and upload one at a time re max file upload see docs/blogs here , here (option 3) and here data file is very much larger than 5 GB Uploader tool is currently impractical; upload via AWS console. Future Uploader may allow multi-part upload to 5 TB. data is not in one of the 3 supported formats (.csv, .json, .jsonl) convert it; for example if is .xlsx you can easily convert to .csv uploading data violates data governance requirements STOP! you may not upload these data do not have access to Analytical Platform work through the steps to get an account here do not have access to the required database request access here I want to replace existing data on the Analytical Platform this is not possible with the Uploader. Consider this action may conflict with reproducibility principles. database is not listed in the drop down menu only databases created via the Uploader will be accessible and a newly created database may take up to 24 hours to appear I want to change the name of an existing database you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles data table is not listed in the drop down menu only data tables created via the Uploader will be accessible and a newly created data table may take up to 24 hours to appear there is no option to select an existing data table reconsider your input to step 2, click “Back” at the top of the screen I want to change the name of an existing data table you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles error “No column names detected…” Ensure that csv files contain columns names in the first row. Column names may only use the character set [A-Za-z0-9_] error “Column names contain special characters…” Column names may only use the character set [A-Za-z0-9_] Limitations and awareness Users will have to wait 24 hours before data is on the analytical platform. This means that users will have to wait a while before they can visualise their data. If you would like to see the data earlier you will need to upload directly via AWS instead. Cannot upload xls files, only csv, JSON and JSONL. Cannot replace existing data even if the data uploaded is inaccurate due to data governance requirements. File size can only be 5GB. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 789}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '20aba2e8491c8e11d3a405286e1cf55'}>,\n",
       " <Document: {'content': 'S3 Bucket Database Name has a limit of 63 characters, this requires delimiting with underscores and ensuring that the file name is kept in lowercase, if you would like more information, look at the MoJ guidance for naming conventions. S3 Table Names have a limit of 1024 characters. Similarly with database names this requires adhering to the MoJ guidance for naming conventions. We do not have access to the permissions infrastructure of the AP. This means that anyone can upload to any table created by the uploader through the uploader. This is not a security risk, as the uploader is purely a loading mechanism and not a viewing mechanism, but be aware users might upload data of a different schema to a table accidentally. You cannot upload files from a sharepoint location directly, you’ll need to download the file locally first This page was last reviewed on 31 August 2022.\\n\\nIt needs to be reviewed again on 31 August 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 31 August 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData Uploader - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 790}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5571f5b9b94895ee1aeb7500c9001aae'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 791}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data Uploader Quickstart link to the production deployment Uploader . We have had problems with firefox; please use an alternate browser. Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Troubleshooting Limitations and awareness Why use the Uploader? The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. Uploader flowchart Data is stored in the S3 bucket in an SQL like structure with one or more databases at the top level, each of which may contain one or more data tables. If the database or data table already exists it will be available to choose in a drop down list, if not it must be created. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 792}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '46cf5c29c442e692a23ea12879e71d47'}>,\n",
       " <Document: {'content': 'When subsequent new data files are added to a data table they will be stored as separate partitions of that data table, and any schema changes will be represented in the final table as a union of every partition. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Automated validation is applied to the data pre upload to check the basics - there are checks that column names exist and are in the character set [A-Za-z0-9_] . Data Uploader pre-requisites In order to utilise the data uploader, you are required to do a pull request. This can be done via this github page and then adding your email to the users list.  If your github user is unable to create a pull request you may need to be added to https://github.com/ministryofjustice which can be done on #ask-operations-engineering on the Justice Digital slack. If you cannot get access to the MoJ organisation on GitHub one of the dev team can raise the PR for you. Next you will need to get access to the DPIA (Data Protection Impact Assessment) or an MOJ analytical platform data movement Form . This supplies legal cover for new datasets and gives legal cover to send official sensitive data to the prod site. If not, you will only be able to send non-sensitive test data. Login page Go to the Uploader login page . Ensure you have set up an account to access the Uploader as in the Data Uploader pre-requisites section above. Click Login , enter the email address used for your Datauploader account and follow instructions. Front page Ensure your data meets the Before you start criteria and then click on Start now . Step 1 of 4: Data governance requirements It is your responsibility to complete any relevant data governance requirements before proceeding to upload data to the Analytical Platform. Step 2 of 4: Choose database If your data is not part of an existing database select the option to create a new database, and specify the new database name. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . Note that new database names will be automatically prefixed with data_eng_uploader_<env> , where <env> is either dev , preprod or prod . Otherwise, choose the existing database from the drop down menu. Step 3 of 4: Choose table If you have created a new database at step 2 the only option now is to create a new data table. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 793}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4ad09ec15b92765623f063436e0a2d15'}>,\n",
       " <Document: {'content': 'If you are adding to an existing database there will be options to create a new data table or add to an existing data table. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Step 4 of 4: Check your inputs before uploading your data Double check your inputs to the previous three steps. In particular if you are adding data to an existing table be absolutely certain you have the correct database and table name; the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Ensure you are happy with any new database or table names as you will not be able to change these easily. If all is OK proceed with Choose file and Upload file . Otherwise click on the corresponding Change navigation button to return to a previous page and amend your input. A progress bar is included for your convenience. Once the upload begins a Cancel button becomes available, so you can abort if required. Note that with a small file the upload is usually so quick that you will not have time to abort. Upload complete The details of the upload will be sent in a confirmation email. This includes essential information such as database name, table name, Athena reference path, S3 bucket path and extraction_timestamp . This information is required to locate your data on the MoJ Analytical Platform. All Data that has been uploaded is subsequently moved to the AP at 1am every day. It can take up to 24 hours for a newly created database to show, data table and newly uploaded data to appear on the Analytical Platform. If you have created a new database the next step is to request access to it from the MoJ Analytical Platform; instructions to do so may be found here . Getting access to uploaded data Dev example If you are using the dev version of the Uploader, then your data is accessible to all users listed in the Standard Database Access project .\\nIf your alpha_username is not already listed in standard_database_access.yaml , clone the repo , create a branch, add your alpha_username and raise a PR. Once approved, you will have access. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 794}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27bdb08e1502ed336d4c0afa1867b130'}>,\n",
       " <Document: {'content': 'Prod example Data uploaded via the uploader to prod are inacessible by default. To gain access, a database file and project access file are needed. When creating a database file, include a README.md file to describe the provenance of the data and list any governance around it. Here is an example database file and project access file for the cjs dashboard: Troubleshooting Problem Solution data file is a bit larger than 5 GB chop the data into multiple files and upload one at a time re max file upload see docs/blogs here , here (option 3) and here data file is very much larger than 5 GB Uploader tool is currently impractical; upload via AWS console. Future Uploader may allow multi-part upload to 5 TB. data is not in one of the 3 supported formats (.csv, .json, .jsonl) convert it; for example if is .xlsx you can easily convert to .csv uploading data violates data governance requirements STOP! you may not upload these data do not have access to Analytical Platform work through the steps to get an account here do not have access to the required database request access here I want to replace existing data on the Analytical Platform this is not possible with the Uploader. Consider this action may conflict with reproducibility principles. database is not listed in the drop down menu only databases created via the Uploader will be accessible and a newly created database may take up to 24 hours to appear I want to change the name of an existing database you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles data table is not listed in the drop down menu only data tables created via the Uploader will be accessible and a newly created data table may take up to 24 hours to appear there is no option to select an existing data table reconsider your input to step 2, click “Back” at the top of the screen I want to change the name of an existing data table you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles error “No column names detected…” Ensure that csv files contain columns names in the first row. Column names may only use the character set [A-Za-z0-9_] error “Column names contain special characters…” Column names may only use the character set [A-Za-z0-9_] Limitations and awareness Users will have to wait 24 hours before data is on the analytical platform. This means that users will have to wait a while before they can visualise their data. If you would like to see the data earlier you will need to upload directly via AWS instead. Cannot upload xls files, only csv, JSON and JSONL. Cannot replace existing data even if the data uploaded is inaccurate due to data governance requirements. File size can only be 5GB. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 795}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '20aba2e8491c8e11d3a405286e1cf55'}>,\n",
       " <Document: {'content': 'S3 Bucket Database Name has a limit of 63 characters, this requires delimiting with underscores and ensuring that the file name is kept in lowercase, if you would like more information, look at the MoJ guidance for naming conventions. S3 Table Names have a limit of 1024 characters. Similarly with database names this requires adhering to the MoJ guidance for naming conventions. We do not have access to the permissions infrastructure of the AP. This means that anyone can upload to any table created by the uploader through the uploader. This is not a security risk, as the uploader is purely a loading mechanism and not a viewing mechanism, but be aware users might upload data of a different schema to a table accidentally. You cannot upload files from a sharepoint location directly, you’ll need to download the file locally first This page was last reviewed on 31 August 2022.\\n\\nIt needs to be reviewed again on 31 August 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 31 August 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData Uploader - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 796}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5571f5b9b94895ee1aeb7500c9001aae'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 797}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data Uploader Quickstart link to the production deployment Uploader . We have had problems with firefox; please use an alternate browser. Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Troubleshooting Limitations and awareness Why use the Uploader? The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. Uploader flowchart Data is stored in the S3 bucket in an SQL like structure with one or more databases at the top level, each of which may contain one or more data tables. If the database or data table already exists it will be available to choose in a drop down list, if not it must be created. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 798}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '46cf5c29c442e692a23ea12879e71d47'}>,\n",
       " <Document: {'content': 'When subsequent new data files are added to a data table they will be stored as separate partitions of that data table, and any schema changes will be represented in the final table as a union of every partition. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Automated validation is applied to the data pre upload to check the basics - there are checks that column names exist and are in the character set [A-Za-z0-9_] . Data Uploader pre-requisites In order to utilise the data uploader, you are required to do a pull request. This can be done via this github page and then adding your email to the users list.  If your github user is unable to create a pull request you may need to be added to https://github.com/ministryofjustice which can be done on #ask-operations-engineering on the Justice Digital slack. If you cannot get access to the MoJ organisation on GitHub one of the dev team can raise the PR for you. Next you will need to get access to the DPIA (Data Protection Impact Assessment) or an MOJ analytical platform data movement Form . This supplies legal cover for new datasets and gives legal cover to send official sensitive data to the prod site. If not, you will only be able to send non-sensitive test data. Login page Go to the Uploader login page . Ensure you have set up an account to access the Uploader as in the Data Uploader pre-requisites section above. Click Login , enter the email address used for your Datauploader account and follow instructions. Front page Ensure your data meets the Before you start criteria and then click on Start now . Step 1 of 4: Data governance requirements It is your responsibility to complete any relevant data governance requirements before proceeding to upload data to the Analytical Platform. Step 2 of 4: Choose database If your data is not part of an existing database select the option to create a new database, and specify the new database name. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . Note that new database names will be automatically prefixed with data_eng_uploader_<env> , where <env> is either dev , preprod or prod . Otherwise, choose the existing database from the drop down menu. Step 3 of 4: Choose table If you have created a new database at step 2 the only option now is to create a new data table. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 799}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4ad09ec15b92765623f063436e0a2d15'}>,\n",
       " <Document: {'content': 'If you are adding to an existing database there will be options to create a new data table or add to an existing data table. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Step 4 of 4: Check your inputs before uploading your data Double check your inputs to the previous three steps. In particular if you are adding data to an existing table be absolutely certain you have the correct database and table name; the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Ensure you are happy with any new database or table names as you will not be able to change these easily. If all is OK proceed with Choose file and Upload file . Otherwise click on the corresponding Change navigation button to return to a previous page and amend your input. A progress bar is included for your convenience. Once the upload begins a Cancel button becomes available, so you can abort if required. Note that with a small file the upload is usually so quick that you will not have time to abort. Upload complete The details of the upload will be sent in a confirmation email. This includes essential information such as database name, table name, Athena reference path, S3 bucket path and extraction_timestamp . This information is required to locate your data on the MoJ Analytical Platform. All Data that has been uploaded is subsequently moved to the AP at 1am every day. It can take up to 24 hours for a newly created database to show, data table and newly uploaded data to appear on the Analytical Platform. If you have created a new database the next step is to request access to it from the MoJ Analytical Platform; instructions to do so may be found here . Getting access to uploaded data Dev example If you are using the dev version of the Uploader, then your data is accessible to all users listed in the Standard Database Access project .\\nIf your alpha_username is not already listed in standard_database_access.yaml , clone the repo , create a branch, add your alpha_username and raise a PR. Once approved, you will have access. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 800}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27bdb08e1502ed336d4c0afa1867b130'}>,\n",
       " <Document: {'content': 'Prod example Data uploaded via the uploader to prod are inacessible by default. To gain access, a database file and project access file are needed. When creating a database file, include a README.md file to describe the provenance of the data and list any governance around it. Here is an example database file and project access file for the cjs dashboard: Troubleshooting Problem Solution data file is a bit larger than 5 GB chop the data into multiple files and upload one at a time re max file upload see docs/blogs here , here (option 3) and here data file is very much larger than 5 GB Uploader tool is currently impractical; upload via AWS console. Future Uploader may allow multi-part upload to 5 TB. data is not in one of the 3 supported formats (.csv, .json, .jsonl) convert it; for example if is .xlsx you can easily convert to .csv uploading data violates data governance requirements STOP! you may not upload these data do not have access to Analytical Platform work through the steps to get an account here do not have access to the required database request access here I want to replace existing data on the Analytical Platform this is not possible with the Uploader. Consider this action may conflict with reproducibility principles. database is not listed in the drop down menu only databases created via the Uploader will be accessible and a newly created database may take up to 24 hours to appear I want to change the name of an existing database you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles data table is not listed in the drop down menu only data tables created via the Uploader will be accessible and a newly created data table may take up to 24 hours to appear there is no option to select an existing data table reconsider your input to step 2, click “Back” at the top of the screen I want to change the name of an existing data table you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles error “No column names detected…” Ensure that csv files contain columns names in the first row. Column names may only use the character set [A-Za-z0-9_] error “Column names contain special characters…” Column names may only use the character set [A-Za-z0-9_] Limitations and awareness Users will have to wait 24 hours before data is on the analytical platform. This means that users will have to wait a while before they can visualise their data. If you would like to see the data earlier you will need to upload directly via AWS instead. Cannot upload xls files, only csv, JSON and JSONL. Cannot replace existing data even if the data uploaded is inaccurate due to data governance requirements. File size can only be 5GB. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 801}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '20aba2e8491c8e11d3a405286e1cf55'}>,\n",
       " <Document: {'content': 'S3 Bucket Database Name has a limit of 63 characters, this requires delimiting with underscores and ensuring that the file name is kept in lowercase, if you would like more information, look at the MoJ guidance for naming conventions. S3 Table Names have a limit of 1024 characters. Similarly with database names this requires adhering to the MoJ guidance for naming conventions. We do not have access to the permissions infrastructure of the AP. This means that anyone can upload to any table created by the uploader through the uploader. This is not a security risk, as the uploader is purely a loading mechanism and not a viewing mechanism, but be aware users might upload data of a different schema to a table accidentally. You cannot upload files from a sharepoint location directly, you’ll need to download the file locally first This page was last reviewed on 31 August 2022.\\n\\nIt needs to be reviewed again on 31 August 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 31 August 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData Uploader - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 802}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5571f5b9b94895ee1aeb7500c9001aae'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 803}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data Uploader Quickstart link to the production deployment Uploader . We have had problems with firefox; please use an alternate browser. Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Troubleshooting Limitations and awareness Why use the Uploader? The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. Uploader flowchart Data is stored in the S3 bucket in an SQL like structure with one or more databases at the top level, each of which may contain one or more data tables. If the database or data table already exists it will be available to choose in a drop down list, if not it must be created. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 804}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '46cf5c29c442e692a23ea12879e71d47'}>,\n",
       " <Document: {'content': 'When subsequent new data files are added to a data table they will be stored as separate partitions of that data table, and any schema changes will be represented in the final table as a union of every partition. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Automated validation is applied to the data pre upload to check the basics - there are checks that column names exist and are in the character set [A-Za-z0-9_] . Data Uploader pre-requisites In order to utilise the data uploader, you are required to do a pull request. This can be done via this github page and then adding your email to the users list.  If your github user is unable to create a pull request you may need to be added to https://github.com/ministryofjustice which can be done on #ask-operations-engineering on the Justice Digital slack. If you cannot get access to the MoJ organisation on GitHub one of the dev team can raise the PR for you. Next you will need to get access to the DPIA (Data Protection Impact Assessment) or an MOJ analytical platform data movement Form . This supplies legal cover for new datasets and gives legal cover to send official sensitive data to the prod site. If not, you will only be able to send non-sensitive test data. Login page Go to the Uploader login page . Ensure you have set up an account to access the Uploader as in the Data Uploader pre-requisites section above. Click Login , enter the email address used for your Datauploader account and follow instructions. Front page Ensure your data meets the Before you start criteria and then click on Start now . Step 1 of 4: Data governance requirements It is your responsibility to complete any relevant data governance requirements before proceeding to upload data to the Analytical Platform. Step 2 of 4: Choose database If your data is not part of an existing database select the option to create a new database, and specify the new database name. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . Note that new database names will be automatically prefixed with data_eng_uploader_<env> , where <env> is either dev , preprod or prod . Otherwise, choose the existing database from the drop down menu. Step 3 of 4: Choose table If you have created a new database at step 2 the only option now is to create a new data table. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 805}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4ad09ec15b92765623f063436e0a2d15'}>,\n",
       " <Document: {'content': 'If you are adding to an existing database there will be options to create a new data table or add to an existing data table. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Step 4 of 4: Check your inputs before uploading your data Double check your inputs to the previous three steps. In particular if you are adding data to an existing table be absolutely certain you have the correct database and table name; the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Ensure you are happy with any new database or table names as you will not be able to change these easily. If all is OK proceed with Choose file and Upload file . Otherwise click on the corresponding Change navigation button to return to a previous page and amend your input. A progress bar is included for your convenience. Once the upload begins a Cancel button becomes available, so you can abort if required. Note that with a small file the upload is usually so quick that you will not have time to abort. Upload complete The details of the upload will be sent in a confirmation email. This includes essential information such as database name, table name, Athena reference path, S3 bucket path and extraction_timestamp . This information is required to locate your data on the MoJ Analytical Platform. All Data that has been uploaded is subsequently moved to the AP at 1am every day. It can take up to 24 hours for a newly created database to show, data table and newly uploaded data to appear on the Analytical Platform. If you have created a new database the next step is to request access to it from the MoJ Analytical Platform; instructions to do so may be found here . Getting access to uploaded data Dev example If you are using the dev version of the Uploader, then your data is accessible to all users listed in the Standard Database Access project .\\nIf your alpha_username is not already listed in standard_database_access.yaml , clone the repo , create a branch, add your alpha_username and raise a PR. Once approved, you will have access. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 806}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27bdb08e1502ed336d4c0afa1867b130'}>,\n",
       " <Document: {'content': 'Prod example Data uploaded via the uploader to prod are inacessible by default. To gain access, a database file and project access file are needed. When creating a database file, include a README.md file to describe the provenance of the data and list any governance around it. Here is an example database file and project access file for the cjs dashboard: Troubleshooting Problem Solution data file is a bit larger than 5 GB chop the data into multiple files and upload one at a time re max file upload see docs/blogs here , here (option 3) and here data file is very much larger than 5 GB Uploader tool is currently impractical; upload via AWS console. Future Uploader may allow multi-part upload to 5 TB. data is not in one of the 3 supported formats (.csv, .json, .jsonl) convert it; for example if is .xlsx you can easily convert to .csv uploading data violates data governance requirements STOP! you may not upload these data do not have access to Analytical Platform work through the steps to get an account here do not have access to the required database request access here I want to replace existing data on the Analytical Platform this is not possible with the Uploader. Consider this action may conflict with reproducibility principles. database is not listed in the drop down menu only databases created via the Uploader will be accessible and a newly created database may take up to 24 hours to appear I want to change the name of an existing database you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles data table is not listed in the drop down menu only data tables created via the Uploader will be accessible and a newly created data table may take up to 24 hours to appear there is no option to select an existing data table reconsider your input to step 2, click “Back” at the top of the screen I want to change the name of an existing data table you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles error “No column names detected…” Ensure that csv files contain columns names in the first row. Column names may only use the character set [A-Za-z0-9_] error “Column names contain special characters…” Column names may only use the character set [A-Za-z0-9_] Limitations and awareness Users will have to wait 24 hours before data is on the analytical platform. This means that users will have to wait a while before they can visualise their data. If you would like to see the data earlier you will need to upload directly via AWS instead. Cannot upload xls files, only csv, JSON and JSONL. Cannot replace existing data even if the data uploaded is inaccurate due to data governance requirements. File size can only be 5GB. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 807}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '20aba2e8491c8e11d3a405286e1cf55'}>,\n",
       " <Document: {'content': 'S3 Bucket Database Name has a limit of 63 characters, this requires delimiting with underscores and ensuring that the file name is kept in lowercase, if you would like more information, look at the MoJ guidance for naming conventions. S3 Table Names have a limit of 1024 characters. Similarly with database names this requires adhering to the MoJ guidance for naming conventions. We do not have access to the permissions infrastructure of the AP. This means that anyone can upload to any table created by the uploader through the uploader. This is not a security risk, as the uploader is purely a loading mechanism and not a viewing mechanism, but be aware users might upload data of a different schema to a table accidentally. You cannot upload files from a sharepoint location directly, you’ll need to download the file locally first This page was last reviewed on 31 August 2022.\\n\\nIt needs to be reviewed again on 31 August 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 31 August 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData Uploader - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 808}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5571f5b9b94895ee1aeb7500c9001aae'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 809}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data Uploader Quickstart link to the production deployment Uploader . We have had problems with firefox; please use an alternate browser. Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Troubleshooting Limitations and awareness Why use the Uploader? The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. Uploader flowchart Data is stored in the S3 bucket in an SQL like structure with one or more databases at the top level, each of which may contain one or more data tables. If the database or data table already exists it will be available to choose in a drop down list, if not it must be created. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 810}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '46cf5c29c442e692a23ea12879e71d47'}>,\n",
       " <Document: {'content': 'When subsequent new data files are added to a data table they will be stored as separate partitions of that data table, and any schema changes will be represented in the final table as a union of every partition. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Automated validation is applied to the data pre upload to check the basics - there are checks that column names exist and are in the character set [A-Za-z0-9_] . Data Uploader pre-requisites In order to utilise the data uploader, you are required to do a pull request. This can be done via this github page and then adding your email to the users list.  If your github user is unable to create a pull request you may need to be added to https://github.com/ministryofjustice which can be done on #ask-operations-engineering on the Justice Digital slack. If you cannot get access to the MoJ organisation on GitHub one of the dev team can raise the PR for you. Next you will need to get access to the DPIA (Data Protection Impact Assessment) or an MOJ analytical platform data movement Form . This supplies legal cover for new datasets and gives legal cover to send official sensitive data to the prod site. If not, you will only be able to send non-sensitive test data. Login page Go to the Uploader login page . Ensure you have set up an account to access the Uploader as in the Data Uploader pre-requisites section above. Click Login , enter the email address used for your Datauploader account and follow instructions. Front page Ensure your data meets the Before you start criteria and then click on Start now . Step 1 of 4: Data governance requirements It is your responsibility to complete any relevant data governance requirements before proceeding to upload data to the Analytical Platform. Step 2 of 4: Choose database If your data is not part of an existing database select the option to create a new database, and specify the new database name. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . Note that new database names will be automatically prefixed with data_eng_uploader_<env> , where <env> is either dev , preprod or prod . Otherwise, choose the existing database from the drop down menu. Step 3 of 4: Choose table If you have created a new database at step 2 the only option now is to create a new data table. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 811}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4ad09ec15b92765623f063436e0a2d15'}>,\n",
       " <Document: {'content': 'If you are adding to an existing database there will be options to create a new data table or add to an existing data table. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Step 4 of 4: Check your inputs before uploading your data Double check your inputs to the previous three steps. In particular if you are adding data to an existing table be absolutely certain you have the correct database and table name; the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Ensure you are happy with any new database or table names as you will not be able to change these easily. If all is OK proceed with Choose file and Upload file . Otherwise click on the corresponding Change navigation button to return to a previous page and amend your input. A progress bar is included for your convenience. Once the upload begins a Cancel button becomes available, so you can abort if required. Note that with a small file the upload is usually so quick that you will not have time to abort. Upload complete The details of the upload will be sent in a confirmation email. This includes essential information such as database name, table name, Athena reference path, S3 bucket path and extraction_timestamp . This information is required to locate your data on the MoJ Analytical Platform. All Data that has been uploaded is subsequently moved to the AP at 1am every day. It can take up to 24 hours for a newly created database to show, data table and newly uploaded data to appear on the Analytical Platform. If you have created a new database the next step is to request access to it from the MoJ Analytical Platform; instructions to do so may be found here . Getting access to uploaded data Dev example If you are using the dev version of the Uploader, then your data is accessible to all users listed in the Standard Database Access project .\\nIf your alpha_username is not already listed in standard_database_access.yaml , clone the repo , create a branch, add your alpha_username and raise a PR. Once approved, you will have access. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 812}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27bdb08e1502ed336d4c0afa1867b130'}>,\n",
       " <Document: {'content': 'Prod example Data uploaded via the uploader to prod are inacessible by default. To gain access, a database file and project access file are needed. When creating a database file, include a README.md file to describe the provenance of the data and list any governance around it. Here is an example database file and project access file for the cjs dashboard: Troubleshooting Problem Solution data file is a bit larger than 5 GB chop the data into multiple files and upload one at a time re max file upload see docs/blogs here , here (option 3) and here data file is very much larger than 5 GB Uploader tool is currently impractical; upload via AWS console. Future Uploader may allow multi-part upload to 5 TB. data is not in one of the 3 supported formats (.csv, .json, .jsonl) convert it; for example if is .xlsx you can easily convert to .csv uploading data violates data governance requirements STOP! you may not upload these data do not have access to Analytical Platform work through the steps to get an account here do not have access to the required database request access here I want to replace existing data on the Analytical Platform this is not possible with the Uploader. Consider this action may conflict with reproducibility principles. database is not listed in the drop down menu only databases created via the Uploader will be accessible and a newly created database may take up to 24 hours to appear I want to change the name of an existing database you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles data table is not listed in the drop down menu only data tables created via the Uploader will be accessible and a newly created data table may take up to 24 hours to appear there is no option to select an existing data table reconsider your input to step 2, click “Back” at the top of the screen I want to change the name of an existing data table you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles error “No column names detected…” Ensure that csv files contain columns names in the first row. Column names may only use the character set [A-Za-z0-9_] error “Column names contain special characters…” Column names may only use the character set [A-Za-z0-9_] Limitations and awareness Users will have to wait 24 hours before data is on the analytical platform. This means that users will have to wait a while before they can visualise their data. If you would like to see the data earlier you will need to upload directly via AWS instead. Cannot upload xls files, only csv, JSON and JSONL. Cannot replace existing data even if the data uploaded is inaccurate due to data governance requirements. File size can only be 5GB. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 813}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '20aba2e8491c8e11d3a405286e1cf55'}>,\n",
       " <Document: {'content': 'S3 Bucket Database Name has a limit of 63 characters, this requires delimiting with underscores and ensuring that the file name is kept in lowercase, if you would like more information, look at the MoJ guidance for naming conventions. S3 Table Names have a limit of 1024 characters. Similarly with database names this requires adhering to the MoJ guidance for naming conventions. We do not have access to the permissions infrastructure of the AP. This means that anyone can upload to any table created by the uploader through the uploader. This is not a security risk, as the uploader is purely a loading mechanism and not a viewing mechanism, but be aware users might upload data of a different schema to a table accidentally. You cannot upload files from a sharepoint location directly, you’ll need to download the file locally first This page was last reviewed on 31 August 2022.\\n\\nIt needs to be reviewed again on 31 August 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 31 August 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData Uploader - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 814}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5571f5b9b94895ee1aeb7500c9001aae'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 815}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data Uploader Quickstart link to the production deployment Uploader . We have had problems with firefox; please use an alternate browser. Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Troubleshooting Limitations and awareness Why use the Uploader? The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. Uploader flowchart Data is stored in the S3 bucket in an SQL like structure with one or more databases at the top level, each of which may contain one or more data tables. If the database or data table already exists it will be available to choose in a drop down list, if not it must be created. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 816}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '46cf5c29c442e692a23ea12879e71d47'}>,\n",
       " <Document: {'content': 'When subsequent new data files are added to a data table they will be stored as separate partitions of that data table, and any schema changes will be represented in the final table as a union of every partition. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Automated validation is applied to the data pre upload to check the basics - there are checks that column names exist and are in the character set [A-Za-z0-9_] . Data Uploader pre-requisites In order to utilise the data uploader, you are required to do a pull request. This can be done via this github page and then adding your email to the users list.  If your github user is unable to create a pull request you may need to be added to https://github.com/ministryofjustice which can be done on #ask-operations-engineering on the Justice Digital slack. If you cannot get access to the MoJ organisation on GitHub one of the dev team can raise the PR for you. Next you will need to get access to the DPIA (Data Protection Impact Assessment) or an MOJ analytical platform data movement Form . This supplies legal cover for new datasets and gives legal cover to send official sensitive data to the prod site. If not, you will only be able to send non-sensitive test data. Login page Go to the Uploader login page . Ensure you have set up an account to access the Uploader as in the Data Uploader pre-requisites section above. Click Login , enter the email address used for your Datauploader account and follow instructions. Front page Ensure your data meets the Before you start criteria and then click on Start now . Step 1 of 4: Data governance requirements It is your responsibility to complete any relevant data governance requirements before proceeding to upload data to the Analytical Platform. Step 2 of 4: Choose database If your data is not part of an existing database select the option to create a new database, and specify the new database name. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . Note that new database names will be automatically prefixed with data_eng_uploader_<env> , where <env> is either dev , preprod or prod . Otherwise, choose the existing database from the drop down menu. Step 3 of 4: Choose table If you have created a new database at step 2 the only option now is to create a new data table. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 817}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4ad09ec15b92765623f063436e0a2d15'}>,\n",
       " <Document: {'content': 'If you are adding to an existing database there will be options to create a new data table or add to an existing data table. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Step 4 of 4: Check your inputs before uploading your data Double check your inputs to the previous three steps. In particular if you are adding data to an existing table be absolutely certain you have the correct database and table name; the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Ensure you are happy with any new database or table names as you will not be able to change these easily. If all is OK proceed with Choose file and Upload file . Otherwise click on the corresponding Change navigation button to return to a previous page and amend your input. A progress bar is included for your convenience. Once the upload begins a Cancel button becomes available, so you can abort if required. Note that with a small file the upload is usually so quick that you will not have time to abort. Upload complete The details of the upload will be sent in a confirmation email. This includes essential information such as database name, table name, Athena reference path, S3 bucket path and extraction_timestamp . This information is required to locate your data on the MoJ Analytical Platform. All Data that has been uploaded is subsequently moved to the AP at 1am every day. It can take up to 24 hours for a newly created database to show, data table and newly uploaded data to appear on the Analytical Platform. If you have created a new database the next step is to request access to it from the MoJ Analytical Platform; instructions to do so may be found here . Getting access to uploaded data Dev example If you are using the dev version of the Uploader, then your data is accessible to all users listed in the Standard Database Access project .\\nIf your alpha_username is not already listed in standard_database_access.yaml , clone the repo , create a branch, add your alpha_username and raise a PR. Once approved, you will have access. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 818}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27bdb08e1502ed336d4c0afa1867b130'}>,\n",
       " <Document: {'content': 'Prod example Data uploaded via the uploader to prod are inacessible by default. To gain access, a database file and project access file are needed. When creating a database file, include a README.md file to describe the provenance of the data and list any governance around it. Here is an example database file and project access file for the cjs dashboard: Troubleshooting Problem Solution data file is a bit larger than 5 GB chop the data into multiple files and upload one at a time re max file upload see docs/blogs here , here (option 3) and here data file is very much larger than 5 GB Uploader tool is currently impractical; upload via AWS console. Future Uploader may allow multi-part upload to 5 TB. data is not in one of the 3 supported formats (.csv, .json, .jsonl) convert it; for example if is .xlsx you can easily convert to .csv uploading data violates data governance requirements STOP! you may not upload these data do not have access to Analytical Platform work through the steps to get an account here do not have access to the required database request access here I want to replace existing data on the Analytical Platform this is not possible with the Uploader. Consider this action may conflict with reproducibility principles. database is not listed in the drop down menu only databases created via the Uploader will be accessible and a newly created database may take up to 24 hours to appear I want to change the name of an existing database you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles data table is not listed in the drop down menu only data tables created via the Uploader will be accessible and a newly created data table may take up to 24 hours to appear there is no option to select an existing data table reconsider your input to step 2, click “Back” at the top of the screen I want to change the name of an existing data table you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles error “No column names detected…” Ensure that csv files contain columns names in the first row. Column names may only use the character set [A-Za-z0-9_] error “Column names contain special characters…” Column names may only use the character set [A-Za-z0-9_] Limitations and awareness Users will have to wait 24 hours before data is on the analytical platform. This means that users will have to wait a while before they can visualise their data. If you would like to see the data earlier you will need to upload directly via AWS instead. Cannot upload xls files, only csv, JSON and JSONL. Cannot replace existing data even if the data uploaded is inaccurate due to data governance requirements. File size can only be 5GB. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 819}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '20aba2e8491c8e11d3a405286e1cf55'}>,\n",
       " <Document: {'content': 'S3 Bucket Database Name has a limit of 63 characters, this requires delimiting with underscores and ensuring that the file name is kept in lowercase, if you would like more information, look at the MoJ guidance for naming conventions. S3 Table Names have a limit of 1024 characters. Similarly with database names this requires adhering to the MoJ guidance for naming conventions. We do not have access to the permissions infrastructure of the AP. This means that anyone can upload to any table created by the uploader through the uploader. This is not a security risk, as the uploader is purely a loading mechanism and not a viewing mechanism, but be aware users might upload data of a different schema to a table accidentally. You cannot upload files from a sharepoint location directly, you’ll need to download the file locally first This page was last reviewed on 31 August 2022.\\n\\nIt needs to be reviewed again on 31 August 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 31 August 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData Uploader - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 820}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5571f5b9b94895ee1aeb7500c9001aae'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 821}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data Uploader Quickstart link to the production deployment Uploader . We have had problems with firefox; please use an alternate browser. Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Troubleshooting Limitations and awareness Why use the Uploader? The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. Uploader flowchart Data is stored in the S3 bucket in an SQL like structure with one or more databases at the top level, each of which may contain one or more data tables. If the database or data table already exists it will be available to choose in a drop down list, if not it must be created. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 822}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '46cf5c29c442e692a23ea12879e71d47'}>,\n",
       " <Document: {'content': 'When subsequent new data files are added to a data table they will be stored as separate partitions of that data table, and any schema changes will be represented in the final table as a union of every partition. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Automated validation is applied to the data pre upload to check the basics - there are checks that column names exist and are in the character set [A-Za-z0-9_] . Data Uploader pre-requisites In order to utilise the data uploader, you are required to do a pull request. This can be done via this github page and then adding your email to the users list.  If your github user is unable to create a pull request you may need to be added to https://github.com/ministryofjustice which can be done on #ask-operations-engineering on the Justice Digital slack. If you cannot get access to the MoJ organisation on GitHub one of the dev team can raise the PR for you. Next you will need to get access to the DPIA (Data Protection Impact Assessment) or an MOJ analytical platform data movement Form . This supplies legal cover for new datasets and gives legal cover to send official sensitive data to the prod site. If not, you will only be able to send non-sensitive test data. Login page Go to the Uploader login page . Ensure you have set up an account to access the Uploader as in the Data Uploader pre-requisites section above. Click Login , enter the email address used for your Datauploader account and follow instructions. Front page Ensure your data meets the Before you start criteria and then click on Start now . Step 1 of 4: Data governance requirements It is your responsibility to complete any relevant data governance requirements before proceeding to upload data to the Analytical Platform. Step 2 of 4: Choose database If your data is not part of an existing database select the option to create a new database, and specify the new database name. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . Note that new database names will be automatically prefixed with data_eng_uploader_<env> , where <env> is either dev , preprod or prod . Otherwise, choose the existing database from the drop down menu. Step 3 of 4: Choose table If you have created a new database at step 2 the only option now is to create a new data table. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 823}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4ad09ec15b92765623f063436e0a2d15'}>,\n",
       " <Document: {'content': 'If you are adding to an existing database there will be options to create a new data table or add to an existing data table. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Step 4 of 4: Check your inputs before uploading your data Double check your inputs to the previous three steps. In particular if you are adding data to an existing table be absolutely certain you have the correct database and table name; the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Ensure you are happy with any new database or table names as you will not be able to change these easily. If all is OK proceed with Choose file and Upload file . Otherwise click on the corresponding Change navigation button to return to a previous page and amend your input. A progress bar is included for your convenience. Once the upload begins a Cancel button becomes available, so you can abort if required. Note that with a small file the upload is usually so quick that you will not have time to abort. Upload complete The details of the upload will be sent in a confirmation email. This includes essential information such as database name, table name, Athena reference path, S3 bucket path and extraction_timestamp . This information is required to locate your data on the MoJ Analytical Platform. All Data that has been uploaded is subsequently moved to the AP at 1am every day. It can take up to 24 hours for a newly created database to show, data table and newly uploaded data to appear on the Analytical Platform. If you have created a new database the next step is to request access to it from the MoJ Analytical Platform; instructions to do so may be found here . Getting access to uploaded data Dev example If you are using the dev version of the Uploader, then your data is accessible to all users listed in the Standard Database Access project .\\nIf your alpha_username is not already listed in standard_database_access.yaml , clone the repo , create a branch, add your alpha_username and raise a PR. Once approved, you will have access. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 824}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27bdb08e1502ed336d4c0afa1867b130'}>,\n",
       " <Document: {'content': 'Prod example Data uploaded via the uploader to prod are inacessible by default. To gain access, a database file and project access file are needed. When creating a database file, include a README.md file to describe the provenance of the data and list any governance around it. Here is an example database file and project access file for the cjs dashboard: Troubleshooting Problem Solution data file is a bit larger than 5 GB chop the data into multiple files and upload one at a time re max file upload see docs/blogs here , here (option 3) and here data file is very much larger than 5 GB Uploader tool is currently impractical; upload via AWS console. Future Uploader may allow multi-part upload to 5 TB. data is not in one of the 3 supported formats (.csv, .json, .jsonl) convert it; for example if is .xlsx you can easily convert to .csv uploading data violates data governance requirements STOP! you may not upload these data do not have access to Analytical Platform work through the steps to get an account here do not have access to the required database request access here I want to replace existing data on the Analytical Platform this is not possible with the Uploader. Consider this action may conflict with reproducibility principles. database is not listed in the drop down menu only databases created via the Uploader will be accessible and a newly created database may take up to 24 hours to appear I want to change the name of an existing database you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles data table is not listed in the drop down menu only data tables created via the Uploader will be accessible and a newly created data table may take up to 24 hours to appear there is no option to select an existing data table reconsider your input to step 2, click “Back” at the top of the screen I want to change the name of an existing data table you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles error “No column names detected…” Ensure that csv files contain columns names in the first row. Column names may only use the character set [A-Za-z0-9_] error “Column names contain special characters…” Column names may only use the character set [A-Za-z0-9_] Limitations and awareness Users will have to wait 24 hours before data is on the analytical platform. This means that users will have to wait a while before they can visualise their data. If you would like to see the data earlier you will need to upload directly via AWS instead. Cannot upload xls files, only csv, JSON and JSONL. Cannot replace existing data even if the data uploaded is inaccurate due to data governance requirements. File size can only be 5GB. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 825}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '20aba2e8491c8e11d3a405286e1cf55'}>,\n",
       " <Document: {'content': 'S3 Bucket Database Name has a limit of 63 characters, this requires delimiting with underscores and ensuring that the file name is kept in lowercase, if you would like more information, look at the MoJ guidance for naming conventions. S3 Table Names have a limit of 1024 characters. Similarly with database names this requires adhering to the MoJ guidance for naming conventions. We do not have access to the permissions infrastructure of the AP. This means that anyone can upload to any table created by the uploader through the uploader. This is not a security risk, as the uploader is purely a loading mechanism and not a viewing mechanism, but be aware users might upload data of a different schema to a table accidentally. You cannot upload files from a sharepoint location directly, you’ll need to download the file locally first This page was last reviewed on 31 August 2022.\\n\\nIt needs to be reviewed again on 31 August 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 31 August 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nData Uploader - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 826}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5571f5b9b94895ee1aeb7500c9001aae'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 827}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Data Uploader Quickstart link to the production deployment Uploader . We have had problems with firefox; please use an alternate browser. Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Troubleshooting Limitations and awareness Why use the Uploader? The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. The Uploader is aimed at colleagues who have data not already on the MoJ Analytical Platform and wish to upload them there to be able to apply Analytical Platform tools, such as Athena. Only tables created from this tool can be added to from this tool. Uploader flowchart Data is stored in the S3 bucket in an SQL like structure with one or more databases at the top level, each of which may contain one or more data tables. If the database or data table already exists it will be available to choose in a drop down list, if not it must be created. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 828}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '46cf5c29c442e692a23ea12879e71d47'}>,\n",
       " <Document: {'content': 'When subsequent new data files are added to a data table they will be stored as separate partitions of that data table, and any schema changes will be represented in the final table as a union of every partition. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Automated validation is applied to the data pre upload to check the basics - there are checks that column names exist and are in the character set [A-Za-z0-9_] . Data Uploader pre-requisites In order to utilise the data uploader, you are required to do a pull request. This can be done via this github page and then adding your email to the users list.  If your github user is unable to create a pull request you may need to be added to https://github.com/ministryofjustice which can be done on #ask-operations-engineering on the Justice Digital slack. If you cannot get access to the MoJ organisation on GitHub one of the dev team can raise the PR for you. Next you will need to get access to the DPIA (Data Protection Impact Assessment) or an MOJ analytical platform data movement Form . This supplies legal cover for new datasets and gives legal cover to send official sensitive data to the prod site. If not, you will only be able to send non-sensitive test data. Login page Go to the Uploader login page . Ensure you have set up an account to access the Uploader as in the Data Uploader pre-requisites section above. Click Login , enter the email address used for your Datauploader account and follow instructions. Front page Ensure your data meets the Before you start criteria and then click on Start now . Step 1 of 4: Data governance requirements It is your responsibility to complete any relevant data governance requirements before proceeding to upload data to the Analytical Platform. Step 2 of 4: Choose database If your data is not part of an existing database select the option to create a new database, and specify the new database name. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . Note that new database names will be automatically prefixed with data_eng_uploader_<env> , where <env> is either dev , preprod or prod . Otherwise, choose the existing database from the drop down menu. Step 3 of 4: Choose table If you have created a new database at step 2 the only option now is to create a new data table. Permitted characters include lower case alphanumeric characters and underscore [a-z0-9_] . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 829}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4ad09ec15b92765623f063436e0a2d15'}>,\n",
       " <Document: {'content': 'If you are adding to an existing database there will be options to create a new data table or add to an existing data table. Note: the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Step 4 of 4: Check your inputs before uploading your data Double check your inputs to the previous three steps. In particular if you are adding data to an existing table be absolutely certain you have the correct database and table name; the existing permissions infrastructure allows anyone with access to the Uploader to append data to any existing table, however, you can only view data you have permssion for. For example, if you accidentally upload data to the wrong table, anyone with permission to view that table will be able to access your data. Please check that the table selected for the data upload is correct to avoid granting view access to unauthorised persons. Ensure you are happy with any new database or table names as you will not be able to change these easily. If all is OK proceed with Choose file and Upload file . Otherwise click on the corresponding Change navigation button to return to a previous page and amend your input. A progress bar is included for your convenience. Once the upload begins a Cancel button becomes available, so you can abort if required. Note that with a small file the upload is usually so quick that you will not have time to abort. Upload complete The details of the upload will be sent in a confirmation email. This includes essential information such as database name, table name, Athena reference path, S3 bucket path and extraction_timestamp . This information is required to locate your data on the MoJ Analytical Platform. All Data that has been uploaded is subsequently moved to the AP at 1am every day. It can take up to 24 hours for a newly created database to show, data table and newly uploaded data to appear on the Analytical Platform. If you have created a new database the next step is to request access to it from the MoJ Analytical Platform; instructions to do so may be found here . Getting access to uploaded data Dev example If you are using the dev version of the Uploader, then your data is accessible to all users listed in the Standard Database Access project .\\nIf your alpha_username is not already listed in standard_database_access.yaml , clone the repo , create a branch, add your alpha_username and raise a PR. Once approved, you will have access. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 830}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27bdb08e1502ed336d4c0afa1867b130'}>,\n",
       " <Document: {'content': 'Prod example Data uploaded via the uploader to prod are inacessible by default. To gain access, a database file and project access file are needed. When creating a database file, include a README.md file to describe the provenance of the data and list any governance around it. Here is an example database file and project access file for the cjs dashboard: Troubleshooting Problem Solution data file is a bit larger than 5 GB chop the data into multiple files and upload one at a time re max file upload see docs/blogs here , here (option 3) and here data file is very much larger than 5 GB Uploader tool is currently impractical; upload via AWS console. Future Uploader may allow multi-part upload to 5 TB. data is not in one of the 3 supported formats (.csv, .json, .jsonl) convert it; for example if is .xlsx you can easily convert to .csv uploading data violates data governance requirements STOP! you may not upload these data do not have access to Analytical Platform work through the steps to get an account here do not have access to the required database request access here I want to replace existing data on the Analytical Platform this is not possible with the Uploader. Consider this action may conflict with reproducibility principles. database is not listed in the drop down menu only databases created via the Uploader will be accessible and a newly created database may take up to 24 hours to appear I want to change the name of an existing database you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles data table is not listed in the drop down menu only data tables created via the Uploader will be accessible and a newly created data table may take up to 24 hours to appear there is no option to select an existing data table reconsider your input to step 2, click “Back” at the top of the screen I want to change the name of an existing data table you cannot do this via Uploader; names may only be changed by someone with admin rights and doing so may violate reproducibility principles error “No column names detected…” Ensure that csv files contain columns names in the first row. Column names may only use the character set [A-Za-z0-9_] error “Column names contain special characters…” Column names may only use the character set [A-Za-z0-9_] Limitations and awareness Users will have to wait 24 hours before data is on the analytical platform. This means that users will have to wait a while before they can visualise their data. If you would like to see the data earlier you will need to upload directly via AWS instead. Cannot upload xls files, only csv, JSON and JSONL. Cannot replace existing data even if the data uploaded is inaccurate due to data governance requirements. File size can only be 5GB. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 831}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '20aba2e8491c8e11d3a405286e1cf55'}>,\n",
       " <Document: {'content': 'S3 Bucket Database Name has a limit of 63 characters, this requires delimiting with underscores and ensuring that the file name is kept in lowercase, if you would like more information, look at the MoJ guidance for naming conventions. S3 Table Names have a limit of 1024 characters. Similarly with database names this requires adhering to the MoJ guidance for naming conventions. We do not have access to the permissions infrastructure of the AP. This means that anyone can upload to any table created by the uploader through the uploader. This is not a security risk, as the uploader is purely a loading mechanism and not a viewing mechanism, but be aware users might upload data of a different schema to a table accidentally. You cannot upload files from a sharepoint location directly, you’ll need to download the file locally first This page was last reviewed on 31 August 2022.\\n\\nIt needs to be reviewed again on 31 August 2023\\nby the page owner #ask-data-engineering . This page was set to be reviewed before 31 August 2023\\nby the page owner #ask-data-engineering .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nApps - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 832}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '413b3679e40431816cf0a5cad4bba1a1'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 833}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Apps Once you’ve built your app, you can make it available to users through the Analytical Platform.\\nWe have guidance for Shiny apps and static apps: Deploying your app Accessing your deployed app Managing your app users Managing and Monitoring Deployments as well as common issues faced during these stages of publishing your app. ⚠️ Note on deployment ⚠️ It is not currently possible to deploy new apps on the Analytical Platform, though we are working to make this functionality available again as soon as possible. In the meantime, you can still access and manage existing apps. If you have an existing app that requires urgent redeployment, please submit a request via GitHub. We normally redeploy apps each Wednesday, where we have recevied a request by the Friday before. This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 834}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '133e65bffa50409c76cb4c82f8642f76'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAlerting on an app - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 835}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8d2e5fd138e1a3ec1dc8f8636979c21'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 836}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Enabling alerting To enable alerting on your application:\\n- Navigate to your application repository\\n- Clone the repo down locally\\n- Alerts are configured in the following paths\\n- <repo_name>.github/workflows/build-push-deploy-dev.yml - <repo_name>.github/workflows/build-push-deploy-prod.yml - Add this syntax in the helm upgrade section of the required workflow(s) --set Alerts.enabled=true - Merge your pr, alerts will then be enabled You can find the current configured alerts below once enabled all these alerts will become active provided you have the required resources to alert on. Alert config config default description Alerts.enabled false enable alerting Alerts.severity data-platform-application-migration-alerts severity label, used by CloudPlatform for sending Alerts to specific pager duty services Alerts.cpuUsage \"60\" Trigger alert if Pod CPU % exceeds this level Alerts.memoryUsage \"80\" Trigger alert if Pod memory exceeds the specified value of the resource limit (%) Alerts.pvcUsage \"90\" Trigger alert if PVC storage exceeds the specified value (%) Current configured alerts 5xxingress - The ingress for the namespace is returning a high average of 500 error codes over a 1 minute period. OOMKiller - Pod killed due to OOM issues TooManyContainerRestarts - pod container(s) are restarting too many times, exceeding 20 restarts (indicative of app crashing over and over again) CrashLoopBackOff - Container has entered into a crashloop backoff state. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 837}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '154c5a497397f6f8090ed7637c12ba58'}>,\n",
       " <Document: {'content': 'PodNotReadyKube - Pod stuck in Pending|Unknown state WebappRestartAlert - The webapp has restarted AuthProxyRestartAlert - The Auth Proxy has restarted CpuUsageHigh - The Pod CPU usage has exceeded the specified percentage MemUsageHigh - The Pod Memory usage has exceeded the specified percentage HighPersistantVolumeUsage - PVC storage has exceeded limit If you require any alerts not listed please send a feature request to our support channel This page was last reviewed on 19 October 2023.\\n\\nIt needs to be reviewed again on 19 October 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 19 October 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAlerting on an app - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 838}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '494f4a7cea5c9b8bf338c8db0668fac1'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 839}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Enabling alerting To enable alerting on your application:\\n- Navigate to your application repository\\n- Clone the repo down locally\\n- Alerts are configured in the following paths\\n- <repo_name>.github/workflows/build-push-deploy-dev.yml - <repo_name>.github/workflows/build-push-deploy-prod.yml - Add this syntax in the helm upgrade section of the required workflow(s) --set Alerts.enabled=true - Merge your pr, alerts will then be enabled You can find the current configured alerts below once enabled all these alerts will become active provided you have the required resources to alert on. Alert config config default description Alerts.enabled false enable alerting Alerts.severity data-platform-application-migration-alerts severity label, used by CloudPlatform for sending Alerts to specific pager duty services Alerts.cpuUsage \"60\" Trigger alert if Pod CPU % exceeds this level Alerts.memoryUsage \"80\" Trigger alert if Pod memory exceeds the specified value of the resource limit (%) Alerts.pvcUsage \"90\" Trigger alert if PVC storage exceeds the specified value (%) Current configured alerts 5xxingress - The ingress for the namespace is returning a high average of 500 error codes over a 1 minute period. OOMKiller - Pod killed due to OOM issues TooManyContainerRestarts - pod container(s) are restarting too many times, exceeding 20 restarts (indicative of app crashing over and over again) CrashLoopBackOff - Container has entered into a crashloop backoff state. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 840}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '154c5a497397f6f8090ed7637c12ba58'}>,\n",
       " <Document: {'content': 'PodNotReadyKube - Pod stuck in Pending|Unknown state WebappRestartAlert - The webapp has restarted AuthProxyRestartAlert - The Auth Proxy has restarted CpuUsageHigh - The Pod CPU usage has exceeded the specified percentage MemUsageHigh - The Pod Memory usage has exceeded the specified percentage HighPersistantVolumeUsage - PVC storage has exceeded limit If you require any alerts not listed please send a feature request to our support channel This page was last reviewed on 19 October 2023.\\n\\nIt needs to be reviewed again on 19 October 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 19 October 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nAlerting on an app - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 841}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '494f4a7cea5c9b8bf338c8db0668fac1'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 842}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Enabling alerting To enable alerting on your application:\\n- Navigate to your application repository\\n- Clone the repo down locally\\n- Alerts are configured in the following paths\\n- <repo_name>.github/workflows/build-push-deploy-dev.yml - <repo_name>.github/workflows/build-push-deploy-prod.yml - Add this syntax in the helm upgrade section of the required workflow(s) --set Alerts.enabled=true - Merge your pr, alerts will then be enabled You can find the current configured alerts below once enabled all these alerts will become active provided you have the required resources to alert on. Alert config config default description Alerts.enabled false enable alerting Alerts.severity data-platform-application-migration-alerts severity label, used by CloudPlatform for sending Alerts to specific pager duty services Alerts.cpuUsage \"60\" Trigger alert if Pod CPU % exceeds this level Alerts.memoryUsage \"80\" Trigger alert if Pod memory exceeds the specified value of the resource limit (%) Alerts.pvcUsage \"90\" Trigger alert if PVC storage exceeds the specified value (%) Current configured alerts 5xxingress - The ingress for the namespace is returning a high average of 500 error codes over a 1 minute period. OOMKiller - Pod killed due to OOM issues TooManyContainerRestarts - pod container(s) are restarting too many times, exceeding 20 restarts (indicative of app crashing over and over again) CrashLoopBackOff - Container has entered into a crashloop backoff state. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 843}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '154c5a497397f6f8090ed7637c12ba58'}>,\n",
       " <Document: {'content': 'PodNotReadyKube - Pod stuck in Pending|Unknown state WebappRestartAlert - The webapp has restarted AuthProxyRestartAlert - The Auth Proxy has restarted CpuUsageHigh - The Pod CPU usage has exceeded the specified percentage MemUsageHigh - The Pod Memory usage has exceeded the specified percentage HighPersistantVolumeUsage - PVC storage has exceeded limit If you require any alerts not listed please send a feature request to our support channel This page was last reviewed on 19 October 2023.\\n\\nIt needs to be reviewed again on 19 October 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 19 October 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDeploy an RShiny app - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 844}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3eaac4ddf1b9ab64d88cfb18a953dc7b'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 845}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools R Shiny app publishing Once you’ve built your Shiny app, you can make it available to users through the Analytical Platform.\\nWe have guidance for: Deploying your app Accessing your deployed app Managing your app users Managing and Monitoring Deployments as well as common issues faced during these stages of publishing your app. ⚠️ Note on deployment ⚠️ It is not currently possible to deploy new apps on the Analytical Platform, though we are working to make this functionality available again as soon as possible. In the meantime, you can still access and manage existing apps. If you have an existing app that requires urgent redeployment, please submit a request via GitHub.\\nWe normally redeploy apps each Wednesday, where we have recevied a request by the Friday before. App deployment New apps It is not currently possible to deploy new apps on the Analytical Platform, though we are working to make this functionality available again as soon as possible. Existing apps Overview Your app environment is made up of two key elements: A GitHub workflow A Cloud Platform namespace In brief, the workflow builds and deploys your code as a docker container, and then deploys it to Cloud Platform’s kubernetes cluster, in your application’s namespace. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 846}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cdd12045b1bed963ae1180ef82503aca'}>,\n",
       " <Document: {'content': 'The flow looks something like this: The rationale behind this change is to: Facilitate testing - including a working development app version and build artifacts like docker containers - before deploying to production Give teams more control over their workflows Restore the ability to deploy without the Analytical Platform team’s intervention Apps hosted on Cloud Platform After the move to Cloud Platform hosting for Analytical Platform apps, you’ll have two active deployments of your apps at all times.\\nThese are your ‘dev’ (development) and ‘prod’ (production) deployments. Be aware that both production and dev currently point to the same data. Your code repository within the ministryofjustice organisation was built from the data-platform-app-template repo , and has inherited the continuous integration and continuous delivery (CI/CD) pipelines (GitHub Action workflows) from that repo.\\nThese workflows will automatically build your docker images, push them to a remote image store, and then deploy the application based on how you’ve made your code changes: Deploying your application to your development environment is done through the -dev workflow, which will build-push-deploy when any pull request is made in the repo (e.g. feature-branch into main , or feature-branch into develop-branch ).\\nThe workflow can also be manually triggered, though this will only build and push the app, it will not deploy it: repo homepage > Actions (tab) > Run workflow (right hand side of the page). Opening any pull request will trigger a dev deployment.\\nAny subsequent pushes to that branch (or to any other open branches) will trigger dev deployments too.\\nIf you’re working on multiple PRs at once in the repo that you will need to coordinate pushes to your PRs so you can track your deployments.\\nYou can always cancel workflow runs and rerun deployments from the Actions page in the repo. Deploying your application to your production environment is done through the -prod workflow, which will build-push-deploy either: after a pull request is merged into main upon publishing a release You can view the status of your deployments either by checking the workflow runs in the Actions tab (repo-url/actions), or by checking out the deployments page (repo-url/deployments). The above describes how CI/CD will be set up by default in the ministryofjustice repo.\\nOnce you have ownership of the repo, you’ll have the ownership of the .github/workflow/ files too so you will be able to amend the processes and triggers so that they meet your needs. Your app deployment pipeline GitHub Actions has workflow definitions located in your repository under .github/workflows/ . By default, you will be provided with two workflows.\\nThe dev environment workflow is triggered when you open or contribute to a Pull Request from any branch (for example, into your main branch).\\nIts steps are: Check out the repository Authenticate to AWS Build a docker container from your development branch If the build is successful, push this container to a container registry Run helm against the development environment namespace. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 847}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9be9b6f8bc1f72a896bea83c822fdb5c'}>,\n",
       " <Document: {'content': 'The production workflow behaves in the exact same way, containing the same steps, but will be triggered when a Pull Request is merged into main , and will deploy to the production namespace instead Managing published apps Manage app users If authentication is enabled and you choose to use email as the login option, user access management to app can be done through Control panel. To manage the users of your app: Login Control panel as app admin Go to the Analytical Platform control panel . Select the Webapps tab. Select the name of the app you want to manage or select Manage customers . Select the environment where app is deployed ( dev/prod ) To add app users: Enter the email addresses of the users in the box titled ‘Add app customers by entering their email addresses’ – multiple email addresses can be separated by spaces, tabs, commas and semicolons, and can contain capital letters. Select Add customer . To remove an app user, select Remove customer next to the email address of the customer you want to remove. Accessing the deployed app Apps hosted on Cloud Platform Your deployed app can be accessed at two URLs: prod is at: repository-name.apps.live.cloud-platform.service.justice.gov.uk dev is at: repository-name-dev.apps.live.cloud-platform.service.justice.gov.uk (where repository-name is the name of the relevant GitHub repository) By default, the user list on dev empty and you will need to add any users requiring access via control panel. Authenticating to your app For the dashboard apps using passwordless flow (email login), when accessing an app, you can choose whether to sign in using a one-time passcode (default) or an email magic link.\\nTo sign in with an email magic link, add /login?method=link to the end of the app’s URL, for example, https://apps.live.cloud-platform.service.justice.gov.uk/login?method=code . Troubleshooting app sign-in “That email address is not authorized for this app (or possibly another error occurred)” error, after entering email address Check that the user is authorised to access the app: Log in to the control panel . Navigate to the app detail page. Choose the right deployment environment (dev/prod) Check if the user’s email address is listed under ‘App customers’. If it is not, refer them to the app owner to obtain access. Check that the user is using the correct email address – there is sometimes confusion between @justice and @digital.justice email addresses. “Access denied” error, having entered email address and copied the one-time passcode into the login page Please follow the same steps above to check whether the user is in the customer list of the app. “IP x.x.x.x is not whitelisted” Check that the user is trying to access the app from one of the trusted networks listed on app’s app-detail from Control panel The app admin can modify the IP_Ranges on the app’s app-detail detail page. Other troubleshooting tips Check that they are trying to access the app using a URL beginning with https:// not http:// . Look for similar issues log in the data-platform-support repository . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 848}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7651fbd3238627ec36d41114dafd2350'}>,\n",
       " <Document: {'content': 'Try asking the user to clear their cookies by visiting https://alpha-analytics-moj.eu.auth0.com/logout and try again. In addition the AP team can: Check the Auth0 logs for the app in Kibana Check the Auth0 logs in the Auth0 console Advanced Editing the Dockerfile A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. In most cases, you will not need to change the Dockerfile when deploying your app. If your app uses packages that have additional system dependencies, you will need to add these in the Dockerfile . If you are unsure how to do this, contact the Analytical Platform team. A Dockerfile reference can be found in the Docker documentation . Getting details of current users of the app An RShiny app can find out who is using it.\\nThis can be useful to log an audit trail of significant events.\\nSpecifically, it can determine the email address that the user logged into the app with.\\nThis is sensitive data, so you must ensure that you are following all relevant information governance processes. The shiny-headers-demo repository contains an example of how to do this. These features are only available in the Analytical Platform version of shiny-server and the open source shiny server image provided by AP ( see details of both options below ). Finding current users’ email addresses You can obtain the logged in user’s email address by using the following code in the server function of your app: get ( \"HTTP_USER_EMAIL\" , envir = session $ request ) This line in shiny-headers-demo shows the code in context. NOTE : Ensure the latest AuthProxy image is being used or the user email header will not be accessible. This is defined in the helm upgrade command in the GitHub actions files that handle deployments - build-push-deploy-dev.yml and build-push-deploy-prod.yml . Check these files for AuthProxy.Image.Tag and set it to latest as the example below: --set AuthProxy.Image.Tag=\"latest\" \\\\ A full example in a working deployed app can be found here . Finding current users’ user profiles You can access the full user profile by making a request directly from the RShiny app to the auth-proxy’s /userinfo endpoint using the following code inside your server function. # library(httr) # library(jsonlite) profile <- fromJSON ( content ( GET ( \"http://localhost:3001/userinfo\" , add_headers ( cookie = get ( \"HTTP_COOKIE\" , envir = session $ request ))), \"text\" )) This line shows the code in context. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 849}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2437a7af0dcd53eb10c2da05b594a7e2'}>,\n",
       " <Document: {'content': 'Example response { \"email\" : \"name@example.gov.uk\" , \"email_verified\" : true , \"user_id\" : \"email|12345121312\" , \"picture\" : \"https://s.gravatar.com/avatar/94deebe3b87fc5e9b3b4469112573cc0?s=480&r=pg&d=https%3A%2F%2Fcdn.auth0.com%2Favatars%2Fna.png\" , \"nickname\" : \"name\" , \"identities\" : [ { \"user_id\" : \"12345121312\" , \"provider\" : \"email\" , \"connection\" : \"email\" , \"isSocial\" : false } ], \"updated_at\" : \"2019-07-15T09:54:34.353Z\" , \"created_at\" : \"2018-07-12T14:26:45.663Z\" , \"name\" : \"name@example.gov.uk\" , \"last_ip\" : \"1.1.1.1\" , \"last_login\" : \"2019-07-15T09:54:34.353Z\" , \"logins_count\" : 20 , \"blocked_for\" : [], \"guardian_authenticators\" : [] } user-profile caching Due to the auth0 rate limit for /userinfo , the user-profile will be\\ncached for 10 minutes on auth-proxy. If somehow your app receives an exception, for example, token-expired, from the above call, you can add /userinfo?force=true to refresh the user-profle by force. Troubleshooting and monitoring Deploying locally If you have a MacBook, you can use Docker locally to test and troubleshoot your RShiny app.\\nYou can download Docker Desktop for Mac from the Docker website . To build and run your R Shiny app locally, follow the steps below: Clone your app’s repository to a new folder on your MacBook – this guarantees that the app will be built using the same code as on the Analytical Platform. You may need to create a new connection to GitHub with SSH . Open a terminal session and navigate to the directory containing the Dockerfile using the cd command. Build the Docker image by running: {bash}\\ndocker build . -t IMAGE:TAG where IMAGE is a name for the image, for example, my-docker-image , and TAG is the version number, for example, 0.1 . Run a Docker container created from the Docker image by running: {bash}\\ndocker run -p 80:9999 IMAGE:TAG Go to 127.0.0.1:80 to view the app. If the app does not work, follow the steps below to troubleshoot it: Start a bash session in a Docker container created from the Docker image by running: {bash}\\ndocker run -it -p 80:80 IMAGE:TAG bash Install the nano text editor by running: {bash}\\napt-get update\\napt-get install nano Open shiny-server.conf in the nano text editor by running: {bash}\\nnano /etc/shiny-server/shiny-server.conf Add the following lines at the beginning of shiny-server.conf : {bash}\\naccess_log /var/log/shiny-server/access.log tiny;\\npreserve_logs true; Write the changes by pressing Ctrl+O . Exit the nano text editor by pressing Ctrl+X . Increase the verbosity of logging and start the Shiny server by running: {bash}\\nexport SHINY_LOG_LEVEL=TRACE\\n/bin/shiny-server.sh Open a new terminal session. Start a new bash session in the Docker container by running: {bash}\\ndocker exec -it CONTAINER_ID bash You can find the CONTAINER_ID by running docker ps . View the logs by running: {bash}\\ncat /var/log/shiny-server/access.log For further details, see the Shiny server documentation . Managing and Monitoring Deployments Monitoring Deployments By default all applications deployed into Cloud platform have a basic level of monitoring which can be accessed on the following link Cloud Platform grafana namespace monitoring . Grafana dashboards are accessed using single-sign-on (SSO) via your GitHub account. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 850}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '72d5141ffd5dde9ba8d8dc7c01d02cdd'}>,\n",
       " <Document: {'content': 'This page shows all namespaces and can be refined by typing the name of your namespace in namespace drop down and selecting as needed. Note: Kubernetes defines Limits as the maximum amount of a resource to be used by a container. This means that the container can never consume more than the memory amount or CPU amount indicated. Requests, on the other hand, are the minimum guaranteed amount of a resource that is reserved for a container. A brief overview of namespaces Within Kubernetes a namespace provides a mechanism for isolating groups of resources within a single cluster, it can be thought of as a virtual cluster within the cluster. Your Application is deployed into its own namespace, this restricts access to your team and enables the setting of resource limits. Within the namespace are the various Kubernetes components: Pods the smallest deployable units of computing that you can create and manage in Kubernetes, usually one pod per function of your application ie web server, db server. Service is a method for exposing a network application that is running as one or more Pods in your cluster, basically simplifying the connections within your namespace. Deployment provides declarative updates for Pods and ReplicaSets. ReplicaSet’s their purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods. What information is displayed on the Grafana namespace chart: Namespace(s) usage on total cluster CPU in % - This is the total usage by all namespaces on the cluster Namespace(s) usage on total cluster RAM in % - This is the total usage by all namespaces on the cluster Kubernetes Resource Count - A simple count of resources running within your namespace. CPU usage by Pod - CPU usage per pod within your namespace. Memory usage by Pod - Memory usage per pod within your namespace. Nb of pods by state - Count of pods at various states  within your namespace. Nb of containers by pod - Number of containers within each pod. Replicas available by deployment - the number of replicas available to your deployment. Replicas unavailable by deployment - the number of replicas unavailable to your deployment. Persistent Volumes Capacity - If your application uses persistent volumes the total capacity is displayed here. Persistent Volumes Inodes - If your application uses persistent volumes the detail on the inodes is displayed here. Further technical details can be found in the Cloud Platform’s Monitoring section of the user guidance and Configuring a dashboard using Grafana UI documentation Accessing logs You can access your applications logs in Cloud platform by following the the CP guidance Accessing Application Log Data Kibana on Cloud Platform Below are some notes to aid you in working with the Kibana service, on Cloud Platform. All logs from deployed apps can be viewed in Kibana . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 851}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7c6f89dbabad16de18f16ab02e9c9009'}>,\n",
       " <Document: {'content': 'To view the logs for a specific app: Select live_kubernetes_cluster from the CHANGE INDEX PATTERN dropdown list. Select Add a filter . Select kubernetes.namespace_name . Select is as the operator. Insert the app’s name space by following the pattern data-platform-app-<app_name>-<dev/prod> . In order to filter out all the logs related health-check, you can put NOT log:  \"/healthz\" in the KQL field. Select Save . (Optional) - you can select to view only the log output by adding it from the Available Fields list in the left hand pane using the (+) button revealed on mouse hover Log messages are displayed in the message column. By default, Kibana only shows logs for the last 15 minutes.\\nIf no logs are available for that time range, you will receive the warning ‘No results match your search criteria’. To change the time range, select the clock icon in the menu bar.\\nThere are several presets or you can define a custom time range. Kibana also has experimental autocomplete and simple syntax tools that you can use to build custom searches.\\nTo enable these features, select Options_ from within the search bar, then toggle Turn on query features . Managing Deployments Cloud platform  allows teams to connect to the Kubernetes cluster and manage their applications, using kubectl , the command line tool for Kubernetes. To do this you will need to setup access through JupyterLab as follows: Go to Analytical Platform Control Panel ‘> Analytical Tools ’> open JupyterLab Data Science ‘> Terminal Installing Kubectl In the terminal session install kubectl curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" curl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\" echo \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check If valid, the output is: kubectl: OK chmod +x kubectl mkdir -p ~/.local/bin mv ./kubectl ~/.local/bin/kubectl which kubectl you should see: /home/jovyan/.local/bin/kubectl You should only need to carry out the above steps once. Setup Cloud Platform Access Follow the steps in CP’s Generating a Kubeconfig file documentation . As we are accessing via Jupyter Labs  step 7 “Move the config file to the location expected by kubectl” has to be carried out slightly differently. First upload the kubecfg.yaml from your local  PC to the Jupyter terminal session. click on the upload arrow (see below) and select the kubecfg.yaml Then continue with the remaining steps in CP’s documentation Accessing the AWS console If required you can access the Cloud platform via AWS, please follow the guidance in Accessing the AWS console (read-only) Adding cron job to your application A cronjob for restarting your application can be setup easier by adding the following line in to your app’s development or production GitHub workflow: --set Cron.schedule=\"0 6 * * *\" crontab.guru can be used to setup the schedule. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 852}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '874bfd5c88c37e73d90395929569b54c'}>,\n",
       " <Document: {'content': 'If you need to a cron job for the other jobs, more guides are available on the Cloud Platform kubernetes cronjobs Changing the resources available to the application When you deploy an update to the application the Kubernetes resources are built from a standard Helm chart with the following default memory/cpu resources: Limits -  CPU 25m.  Memory 1Gi Requests - CPU 3m.  Memory 1Gi To override this you will have to amend the GitHub  workflow in your application repository The file(s) to amend are .github/workflows/build-push-deploy-dev.yml or build-push-deploy-prod.yml Below is the section of code that calls the helm chart helm upgrade --install --wait --timeout 10m0s --namespace $KUBE_NAMESPACE $KUBE_NAMESPACE mojanalytics/webapp-cp \\\\\\n--set AuthProxy.Env.Auth0Domain=$AUTH0_DOMAIN \\\\\\n--set AuthProxy.Env.Auth0Passwordless=$AUTH0_PASSWORDLESS \\\\\\n--set AuthProxy.Env.Auth0TokenAlg=$AUTH0_TOKEN_ALG \\\\\\n--set AuthProxy.Env.AuthenticationRequired=$AUTHENTICATION_REQUIRED \\\\\\n--set AuthProxy.Env.IPRanges=$process_ip_range \\\\\\n--set AuthProxy.Image.Repository=$ECR_REPO_AUTH0 \\\\\\n--set AuthProxy.Image.Tag=\"latest\" \\\\\\n--set Namespace=$KUBE_NAMESPACE \\\\\\n--set Secrets.Auth0.ClientId=$AUTH0_CLIENT_ID \\\\\\n--set Secrets.Auth0.ClientSecret=$AUTH0_CLIENT_SECRET \\\\\\n--set Secrets.Auth0.CookieSecret=$COOKIE_SECRET \\\\\\n--set ServiceAccount.RoleARN=$APP_ROLE_ARN \\\\\\n--set WebApp.Image.Repository=$ECR_REPO_WEBAPP \\\\\\n--set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n$custom_variables ``` To change the resources  insert at  the end of the file just before the $custom_variables the following as appropriate, remembering to use the line continuation “\\\\” on the existing last line. `--set WebApp.resources.limits.cpu=100m \\\\`\\n`--set WebApp.resources.limits.memory=2Gi \\\\`\\n`--set WebApp.resources.requests.cpu=50m \\\\`\\n`--set WebApp.resources.requests.memory=2Gi \\\\`\\n`$custom_variables` Once the changes are pushed/merged to the repository the values will be applied to your Kubernetes namespace. Changing the number of instances (pods) on name space The number of app instances running on dev / prod is 1 by default, if users experience long response times from the application (apart from trying to improving the performance through the code itself) you can increase the number of instances to reduce the wating time on dev / prod GitHub workflows, for example :- --set ReplicaCount=3 Remember: “With great power comes great responsibility”  Your application’s namespace will be one of a number hosted on the same cluster, setting the values too high could crash the cluster! Storage guidance Guidance on storage within Cloud Platform can be found here Cloud platform storage Continuous Deployment Within Cloud platform your application is already automatically deployed by Github workflows but further guidance on continuous deployment within  CP can be found here Cloud Platform continuous deployment. Deploying Updates to the Application As the application is now hosted in Cloud Platform and GitHub workflow has been implemented, you will now be able to apply update to your application, full guidance can be found here Application deployment. Other guidance Guidance on  managing Auth and Secrets through the Control Panel can be found Manage deployment settings of an app on Control panel. Shiny server There are a few choices for running a rshiny app on Cloud Platform. The team responsible for developing and maintaining the dashboard app has full control to choose the best way and practices to run the app by building their own Dockerfile. Since the shiny-framework uses websocket as the primary communication protocol between frontend and backend, no matter which choice you decide, the minimum capabilities required are: Keeping the connection live as long as possible e.g. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 853}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'be1e86fd543bd23987b563fb2751e268'}>,\n",
       " <Document: {'content': 'implementing heart-beat mechanism. Being able to handle the lifecyle of session Providing a method for reconnecting when the websocket drops Working with auth-proxy, the AP component responsible for controlling user’s access,  unless the app is public facing or application handles authentication itself. The AP team offers two shiny-server solutions. AP version of shiny-server We developed a mini version of shiny-sever in nodejs, it provides a minimal implementation to support the required capabilities: Uses sockjs which supports heart-beat Create a new session for each new websocket connection Will try to reconnect to the shiny app automatically when the websocket connection drops If reconnection repeatedly fails and reaches the maximum number of attempts, a window will be appear asking the user to trigger a manual reconnect The majority of the shiny apps hosted on Cloud Platform use this version. The current tag for this docker image is: 593291632749.dkr.ecr.eu-west-1.amazonaws.com/rshiny:r4.1.3-shiny0.0.6 Open source shiny-server We also provide a solution for using the open source Shiny Server with a few minor tweaks to support USER_EMAIL and COOKIE headers.  The base docker image is defined here . The version of open source shiny server is defined by SHINY_SERVER_VERSION , currently set to 1.5.20.1002 . It offers more features than the AP shiny server and supports: Better reconnection behaviour: Reconnect and load existing session rather than creating a new session automatically If reconnection continues to fail, the manual reconnection pop-up will trigger an app reload rather than re-triggering the same reconnection behaviour Better session management: The session will be closed when the session is idle for a certain period of time This behaviour can result in:\\n- Session data (reactive values) is retained even after a reconnection happens\\n- Release resources e.g. memory linked to the session which avoids potential memory leaking It also provides more configuration options as outlined here . Note: options marked as “pro” are not available Instructions for using the open source shiny server image Example Dockerfile The following example can be used as the starting point when making your own Dockerfile # The base docker image\\nFROM ghcr.io/ministryofjustice/data-platform-rshiny-open-source-base:1.0.3\\n\\n# ** Optional step: only if some of R pakcages requires the system libraries which are not covered by base image\\n#   the one in the example below has been provided in base image.\\n# RUN apt-get update \\\\\\n#   && apt-get install -y --no-install-recommends \\\\\\n#     libglpk-dev\\n\\n# use renv for packages\\nADD renv.lock renv.lock\\n\\n# Install R packages\\nRUN R -e \"install.packages(\\'renv\\'); renv::restore()\"\\n\\n# ** Optional step: only if the app requires python packages\\n# Make sure reticulate uses the system Python\\n# ENV RETICULATE_PYTHON=\"/usr/bin/python3\"\\n# ensure requirements.txt exists (created automatically when making a venv in renv)\\n# COPY requirements.txt requirements.txt\\n# RUN python3 -m pip install -r requirements.txt\\n\\n# Add shiny app code\\nADD . .\\n\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 854}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '699af06bd4e59f01784f2b1c038c61a1'}>,\n",
       " <Document: {'content': 'USER 998 Instructions for switching from the AP shiny server to the open-source server If you already use the AP shiny server, and would like to switch to the open source server, the key changes you need to make are: Change the base docker image in your Dockerfile: FROM ghcr.io/ministryofjustice/data-platform-rshiny-open-source-base:1.0.3 If present, ensure the following redundant parts of your Dockerfile are removed: ENV PATH=\"/opt/shiny-server/bin:/opt/shiny-server/ext/node/bin:${PATH}\"\\nENV SHINY_APP=/srv/shiny-server\\nENV NODE_ENV=production RUN chown shiny:shiny /srv/shiny-server\\nCMD analytics-platform-shiny-server\\nEXPOSE 9999 Update GitHub actions work flows Assuming the app uses the GitHub actions workflows from AP, the following parameters for helm installation are required in both the build-push-deploy-dev.yml and build-push-deploy-prod.yml github action workflow files: WebApp.AlternativeHealthCheck.enabled=\"true\"\\nWebApp.AlternativeHealthCheck.port=9999 A complete example for installing the helm chart in the workflow below. These changes will need to be made in both: helm upgrade --install --wait --timeout 10m0s --namespace $KUBE_NAMESPACE $RELEASE_NAME mojanalytics/webapp-cp \\\\\\n--set AuthProxy.Env.Auth0Domain=$AUTH0_DOMAIN \\\\\\n--set AuthProxy.Env.Auth0Passwordless=$AUTH0_PASSWORDLESS \\\\\\n--set AuthProxy.Env.Auth0TokenAlg=$AUTH0_TOKEN_ALG \\\\\\n--set AuthProxy.Env.AuthenticationRequired=$AUTHENTICATION_REQUIRED \\\\\\n--set AuthProxy.Env.IPRanges=$process_ip_range \\\\\\n--set AuthProxy.Image.Repository=$ECR_REPO_AUTH0 \\\\\\n--set AuthProxy.Image.Tag=\"latest\" \\\\\\n--set Namespace=$KUBE_NAMESPACE \\\\\\n--set WebApp.AlternativeHealthCheck.enabled=\"true\" \\\\\\n--set WebApp.AlternativeHealthCheck.port=9999 \\\\\\n--set Secrets.Auth0.ClientId=$AUTH0_CLIENT_ID \\\\\\n--set Secrets.Auth0.ClientSecret=$AUTH0_CLIENT_SECRET \\\\\\n--set Secrets.Auth0.CookieSecret=$COOKIE_SECRET \\\\\\n--set ServiceAccount.RoleARN=$APP_ROLE_ARN \\\\\\n--set WebApp.Image.Repository=$ECR_REPO_WEBAPP \\\\\\n--set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n$custom_variables A full working example is available here Enabling Web Application Firewall (WAF) for Data Platform Apps There are two flags that need to be set to enable WAF for your application, Ingress.ModSec.enabled and GithubTeam To do this modify your app’s build-push-deploy-dev.yml and build-push-deploy-prod.yml GitHub action workflow files as follows: --set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n--set Ingress.ModSec.enabled=\"true\" \\\\\\n--set GithubTeam=\"your github team\" \\\\\\n$custom_variables The changes  will be will be applied to your Kubernetes namespace following a push/merge to the repository and the running of the workflow To disable –set Ingress.ModSec.enabled=“false” This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDeploy an RShiny app - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 855}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9135b340ae2e270a9d89eaef2064f988'}>,\n",
       " <Document: {'content': 'Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 856}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e67ddea1d45e13c1b4e2a256d9663d04'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools R Shiny app publishing Once you’ve built your Shiny app, you can make it available to users through the Analytical Platform.\\nWe have guidance for: Deploying your app Accessing your deployed app Managing your app users Managing and Monitoring Deployments as well as common issues faced during these stages of publishing your app. ⚠️ Note on deployment ⚠️ It is not currently possible to deploy new apps on the Analytical Platform, though we are working to make this functionality available again as soon as possible. In the meantime, you can still access and manage existing apps. If you have an existing app that requires urgent redeployment, please submit a request via GitHub.\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 857}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd501e46d322dcc5a769db89c679c8ec7'}>,\n",
       " <Document: {'content': 'We normally redeploy apps each Wednesday, where we have recevied a request by the Friday before. App deployment New apps It is not currently possible to deploy new apps on the Analytical Platform, though we are working to make this functionality available again as soon as possible. Existing apps Overview Your app environment is made up of two key elements: A GitHub workflow A Cloud Platform namespace In brief, the workflow builds and deploys your code as a docker container, and then deploys it to Cloud Platform’s kubernetes cluster, in your application’s namespace. The flow looks something like this: The rationale behind this change is to: Facilitate testing - including a working development app version and build artifacts like docker containers - before deploying to production Give teams more control over their workflows Restore the ability to deploy without the Analytical Platform team’s intervention Apps hosted on Cloud Platform After the move to Cloud Platform hosting for Analytical Platform apps, you’ll have two active deployments of your apps at all times.\\nThese are your ‘dev’ (development) and ‘prod’ (production) deployments. Be aware that both production and dev currently point to the same data. Your code repository within the ministryofjustice organisation was built from the data-platform-app-template repo , and has inherited the continuous integration and continuous delivery (CI/CD) pipelines (GitHub Action workflows) from that repo.\\nThese workflows will automatically build your docker images, push them to a remote image store, and then deploy the application based on how you’ve made your code changes: Deploying your application to your development environment is done through the -dev workflow, which will build-push-deploy when any pull request is made in the repo (e.g. feature-branch into main , or feature-branch into develop-branch ).\\nThe workflow can also be manually triggered, though this will only build and push the app, it will not deploy it: repo homepage > Actions (tab) > Run workflow (right hand side of the page). Opening any pull request will trigger a dev deployment.\\nAny subsequent pushes to that branch (or to any other open branches) will trigger dev deployments too.\\nIf you’re working on multiple PRs at once in the repo that you will need to coordinate pushes to your PRs so you can track your deployments.\\nYou can always cancel workflow runs and rerun deployments from the Actions page in the repo. Deploying your application to your production environment is done through the -prod workflow, which will build-push-deploy either: after a pull request is merged into main upon publishing a release You can view the status of your deployments either by checking the workflow runs in the Actions tab (repo-url/actions), or by checking out the deployments page (repo-url/deployments). The above describes how CI/CD will be set up by default in the ministryofjustice repo.\\nOnce you have ownership of the repo, you’ll have the ownership of the .github/workflow/ files too so you will be able to amend the processes and triggers so that they meet your needs. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 858}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '129b024e037373a1814495320f7e5b0d'}>,\n",
       " <Document: {'content': 'Your app deployment pipeline GitHub Actions has workflow definitions located in your repository under .github/workflows/ . By default, you will be provided with two workflows.\\nThe dev environment workflow is triggered when you open or contribute to a Pull Request from any branch (for example, into your main branch).\\nIts steps are: Check out the repository Authenticate to AWS Build a docker container from your development branch If the build is successful, push this container to a container registry Run helm against the development environment namespace. The production workflow behaves in the exact same way, containing the same steps, but will be triggered when a Pull Request is merged into main , and will deploy to the production namespace instead Managing published apps Manage app users If authentication is enabled and you choose to use email as the login option, user access management to app can be done through Control panel. To manage the users of your app: Login Control panel as app admin Go to the Analytical Platform control panel . Select the Webapps tab. Select the name of the app you want to manage or select Manage customers . Select the environment where app is deployed ( dev/prod ) To add app users: Enter the email addresses of the users in the box titled ‘Add app customers by entering their email addresses’ – multiple email addresses can be separated by spaces, tabs, commas and semicolons, and can contain capital letters. Select Add customer . To remove an app user, select Remove customer next to the email address of the customer you want to remove. Accessing the deployed app Apps hosted on Cloud Platform Your deployed app can be accessed at two URLs: prod is at: repository-name.apps.live.cloud-platform.service.justice.gov.uk dev is at: repository-name-dev.apps.live.cloud-platform.service.justice.gov.uk (where repository-name is the name of the relevant GitHub repository) By default, the user list on dev empty and you will need to add any users requiring access via control panel. Authenticating to your app For the dashboard apps using passwordless flow (email login), when accessing an app, you can choose whether to sign in using a one-time passcode (default) or an email magic link.\\nTo sign in with an email magic link, add /login?method=link to the end of the app’s URL, for example, https://apps.live.cloud-platform.service.justice.gov.uk/login?method=code . Troubleshooting app sign-in “That email address is not authorized for this app (or possibly another error occurred)” error, after entering email address Check that the user is authorised to access the app: Log in to the control panel . Navigate to the app detail page. Choose the right deployment environment (dev/prod) Check if the user’s email address is listed under ‘App customers’. If it is not, refer them to the app owner to obtain access. Check that the user is using the correct email address – there is sometimes confusion between @justice and @digital.justice email addresses. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 859}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5d2acd0c40d05a1bbe22c5e3218c8278'}>,\n",
       " <Document: {'content': '“Access denied” error, having entered email address and copied the one-time passcode into the login page Please follow the same steps above to check whether the user is in the customer list of the app. “IP x.x.x.x is not whitelisted” Check that the user is trying to access the app from one of the trusted networks listed on app’s app-detail from Control panel The app admin can modify the IP_Ranges on the app’s app-detail detail page. Other troubleshooting tips Check that they are trying to access the app using a URL beginning with https:// not http:// . Look for similar issues log in the data-platform-support repository . Try asking the user to clear their cookies by visiting https://alpha-analytics-moj.eu.auth0.com/logout and try again. In addition the AP team can: Check the Auth0 logs for the app in Kibana Check the Auth0 logs in the Auth0 console Advanced Editing the Dockerfile A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. In most cases, you will not need to change the Dockerfile when deploying your app. If your app uses packages that have additional system dependencies, you will need to add these in the Dockerfile . If you are unsure how to do this, contact the Analytical Platform team. A Dockerfile reference can be found in the Docker documentation . Getting details of current users of the app An RShiny app can find out who is using it.\\nThis can be useful to log an audit trail of significant events.\\nSpecifically, it can determine the email address that the user logged into the app with.\\nThis is sensitive data, so you must ensure that you are following all relevant information governance processes. The shiny-headers-demo repository contains an example of how to do this. These features are only available in the Analytical Platform version of shiny-server and the open source shiny server image provided by AP ( see details of both options below ). Finding current users’ email addresses You can obtain the logged in user’s email address by using the following code in the server function of your app: get ( \"HTTP_USER_EMAIL\" , envir = session $ request ) This line in shiny-headers-demo shows the code in context. NOTE : Ensure the latest AuthProxy image is being used or the user email header will not be accessible. This is defined in the helm upgrade command in the GitHub actions files that handle deployments - build-push-deploy-dev.yml and build-push-deploy-prod.yml . Check these files for AuthProxy.Image.Tag and set it to latest as the example below: --set AuthProxy.Image.Tag=\"latest\" \\\\ A full example in a working deployed app can be found here . Finding current users’ user profiles You can access the full user profile by making a request directly from the RShiny app to the auth-proxy’s /userinfo endpoint using the following code inside your server function. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 860}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '62b0e7edeeae247c5fa3ddd5302a40d1'}>,\n",
       " <Document: {'content': '# library(httr) # library(jsonlite) profile <- fromJSON ( content ( GET ( \"http://localhost:3001/userinfo\" , add_headers ( cookie = get ( \"HTTP_COOKIE\" , envir = session $ request ))), \"text\" )) This line shows the code in context. Example response { \"email\" : \"name@example.gov.uk\" , \"email_verified\" : true , \"user_id\" : \"email|12345121312\" , \"picture\" : \"https://s.gravatar.com/avatar/94deebe3b87fc5e9b3b4469112573cc0?s=480&r=pg&d=https%3A%2F%2Fcdn.auth0.com%2Favatars%2Fna.png\" , \"nickname\" : \"name\" , \"identities\" : [ { \"user_id\" : \"12345121312\" , \"provider\" : \"email\" , \"connection\" : \"email\" , \"isSocial\" : false } ], \"updated_at\" : \"2019-07-15T09:54:34.353Z\" , \"created_at\" : \"2018-07-12T14:26:45.663Z\" , \"name\" : \"name@example.gov.uk\" , \"last_ip\" : \"1.1.1.1\" , \"last_login\" : \"2019-07-15T09:54:34.353Z\" , \"logins_count\" : 20 , \"blocked_for\" : [], \"guardian_authenticators\" : [] } user-profile caching Due to the auth0 rate limit for /userinfo , the user-profile will be\\ncached for 10 minutes on auth-proxy. If somehow your app receives an exception, for example, token-expired, from the above call, you can add /userinfo?force=true to refresh the user-profle by force. Troubleshooting and monitoring Deploying locally If you have a MacBook, you can use Docker locally to test and troubleshoot your RShiny app.\\nYou can download Docker Desktop for Mac from the Docker website . To build and run your R Shiny app locally, follow the steps below: Clone your app’s repository to a new folder on your MacBook – this guarantees that the app will be built using the same code as on the Analytical Platform. You may need to create a new connection to GitHub with SSH . Open a terminal session and navigate to the directory containing the Dockerfile using the cd command. Build the Docker image by running: {bash}\\ndocker build . -t IMAGE:TAG where IMAGE is a name for the image, for example, my-docker-image , and TAG is the version number, for example, 0.1 . Run a Docker container created from the Docker image by running: {bash}\\ndocker run -p 80:9999 IMAGE:TAG Go to 127.0.0.1:80 to view the app. If the app does not work, follow the steps below to troubleshoot it: Start a bash session in a Docker container created from the Docker image by running: {bash}\\ndocker run -it -p 80:80 IMAGE:TAG bash Install the nano text editor by running: {bash}\\napt-get update\\napt-get install nano Open shiny-server.conf in the nano text editor by running: {bash}\\nnano /etc/shiny-server/shiny-server.conf Add the following lines at the beginning of shiny-server.conf : {bash}\\naccess_log /var/log/shiny-server/access.log tiny;\\npreserve_logs true; Write the changes by pressing Ctrl+O . Exit the nano text editor by pressing Ctrl+X . Increase the verbosity of logging and start the Shiny server by running: {bash}\\nexport SHINY_LOG_LEVEL=TRACE\\n/bin/shiny-server.sh Open a new terminal session. Start a new bash session in the Docker container by running: {bash}\\ndocker exec -it CONTAINER_ID bash You can find the CONTAINER_ID by running docker ps . View the logs by running: {bash}\\ncat /var/log/shiny-server/access.log For further details, see the Shiny server documentation . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 861}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a5d9cd543a6fb34ce55971c80d961008'}>,\n",
       " <Document: {'content': 'Managing and Monitoring Deployments Monitoring Deployments By default all applications deployed into Cloud platform have a basic level of monitoring which can be accessed on the following link Cloud Platform grafana namespace monitoring . Grafana dashboards are accessed using single-sign-on (SSO) via your GitHub account. This page shows all namespaces and can be refined by typing the name of your namespace in namespace drop down and selecting as needed. Note: Kubernetes defines Limits as the maximum amount of a resource to be used by a container. This means that the container can never consume more than the memory amount or CPU amount indicated. Requests, on the other hand, are the minimum guaranteed amount of a resource that is reserved for a container. A brief overview of namespaces Within Kubernetes a namespace provides a mechanism for isolating groups of resources within a single cluster, it can be thought of as a virtual cluster within the cluster. Your Application is deployed into its own namespace, this restricts access to your team and enables the setting of resource limits. Within the namespace are the various Kubernetes components: Pods the smallest deployable units of computing that you can create and manage in Kubernetes, usually one pod per function of your application ie web server, db server. Service is a method for exposing a network application that is running as one or more Pods in your cluster, basically simplifying the connections within your namespace. Deployment provides declarative updates for Pods and ReplicaSets. ReplicaSet’s their purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods. What information is displayed on the Grafana namespace chart: Namespace(s) usage on total cluster CPU in % - This is the total usage by all namespaces on the cluster Namespace(s) usage on total cluster RAM in % - This is the total usage by all namespaces on the cluster Kubernetes Resource Count - A simple count of resources running within your namespace. CPU usage by Pod - CPU usage per pod within your namespace. Memory usage by Pod - Memory usage per pod within your namespace. Nb of pods by state - Count of pods at various states  within your namespace. Nb of containers by pod - Number of containers within each pod. Replicas available by deployment - the number of replicas available to your deployment. Replicas unavailable by deployment - the number of replicas unavailable to your deployment. Persistent Volumes Capacity - If your application uses persistent volumes the total capacity is displayed here. Persistent Volumes Inodes - If your application uses persistent volumes the detail on the inodes is displayed here. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 862}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '747dc5905a5d786232f0ac3b2c214c46'}>,\n",
       " <Document: {'content': 'Further technical details can be found in the Cloud Platform’s Monitoring section of the user guidance and Configuring a dashboard using Grafana UI documentation Accessing logs You can access your applications logs in Cloud platform by following the the CP guidance Accessing Application Log Data Kibana on Cloud Platform Below are some notes to aid you in working with the Kibana service, on Cloud Platform. All logs from deployed apps can be viewed in Kibana . To view the logs for a specific app: Select live_kubernetes_cluster from the CHANGE INDEX PATTERN dropdown list. Select Add a filter . Select kubernetes.namespace_name . Select is as the operator. Insert the app’s name space by following the pattern data-platform-app-<app_name>-<dev/prod> . In order to filter out all the logs related health-check, you can put NOT log:  \"/healthz\" in the KQL field. Select Save . (Optional) - you can select to view only the log output by adding it from the Available Fields list in the left hand pane using the (+) button revealed on mouse hover Log messages are displayed in the message column. By default, Kibana only shows logs for the last 15 minutes.\\nIf no logs are available for that time range, you will receive the warning ‘No results match your search criteria’. To change the time range, select the clock icon in the menu bar.\\nThere are several presets or you can define a custom time range. Kibana also has experimental autocomplete and simple syntax tools that you can use to build custom searches.\\nTo enable these features, select Options_ from within the search bar, then toggle Turn on query features . Managing Deployments Cloud platform  allows teams to connect to the Kubernetes cluster and manage their applications, using kubectl , the command line tool for Kubernetes. To do this you will need to setup access through JupyterLab as follows: Go to Analytical Platform Control Panel ‘> Analytical Tools ’> open JupyterLab Data Science ‘> Terminal Installing Kubectl In the terminal session install kubectl curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" curl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\" echo \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check If valid, the output is: kubectl: OK chmod +x kubectl mkdir -p ~/.local/bin mv ./kubectl ~/.local/bin/kubectl which kubectl you should see: /home/jovyan/.local/bin/kubectl You should only need to carry out the above steps once. Setup Cloud Platform Access Follow the steps in CP’s Generating a Kubeconfig file documentation . As we are accessing via Jupyter Labs  step 7 “Move the config file to the location expected by kubectl” has to be carried out slightly differently. First upload the kubecfg.yaml from your local  PC to the Jupyter terminal session. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 863}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1fff2a5ea1b13e6494dc1668cb065181'}>,\n",
       " <Document: {'content': 'click on the upload arrow (see below) and select the kubecfg.yaml Then continue with the remaining steps in CP’s documentation Accessing the AWS console If required you can access the Cloud platform via AWS, please follow the guidance in Accessing the AWS console (read-only) Adding cron job to your application A cronjob for restarting your application can be setup easier by adding the following line in to your app’s development or production GitHub workflow: --set Cron.schedule=\"0 6 * * *\" crontab.guru can be used to setup the schedule. If you need to a cron job for the other jobs, more guides are available on the Cloud Platform kubernetes cronjobs Changing the resources available to the application When you deploy an update to the application the Kubernetes resources are built from a standard Helm chart with the following default memory/cpu resources: Limits -  CPU 25m.  Memory 1Gi Requests - CPU 3m.  Memory 1Gi To override this you will have to amend the GitHub  workflow in your application repository The file(s) to amend are .github/workflows/build-push-deploy-dev.yml or build-push-deploy-prod.yml Below is the section of code that calls the helm chart helm upgrade --install --wait --timeout 10m0s --namespace $KUBE_NAMESPACE $KUBE_NAMESPACE mojanalytics/webapp-cp \\\\\\n--set AuthProxy.Env.Auth0Domain=$AUTH0_DOMAIN \\\\\\n--set AuthProxy.Env.Auth0Passwordless=$AUTH0_PASSWORDLESS \\\\\\n--set AuthProxy.Env.Auth0TokenAlg=$AUTH0_TOKEN_ALG \\\\\\n--set AuthProxy.Env.AuthenticationRequired=$AUTHENTICATION_REQUIRED \\\\\\n--set AuthProxy.Env.IPRanges=$process_ip_range \\\\\\n--set AuthProxy.Image.Repository=$ECR_REPO_AUTH0 \\\\\\n--set AuthProxy.Image.Tag=\"latest\" \\\\\\n--set Namespace=$KUBE_NAMESPACE \\\\\\n--set Secrets.Auth0.ClientId=$AUTH0_CLIENT_ID \\\\\\n--set Secrets.Auth0.ClientSecret=$AUTH0_CLIENT_SECRET \\\\\\n--set Secrets.Auth0.CookieSecret=$COOKIE_SECRET \\\\\\n--set ServiceAccount.RoleARN=$APP_ROLE_ARN \\\\\\n--set WebApp.Image.Repository=$ECR_REPO_WEBAPP \\\\\\n--set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n$custom_variables ``` To change the resources  insert at  the end of the file just before the $custom_variables the following as appropriate, remembering to use the line continuation “\\\\” on the existing last line. `--set WebApp.resources.limits.cpu=100m \\\\`\\n`--set WebApp.resources.limits.memory=2Gi \\\\`\\n`--set WebApp.resources.requests.cpu=50m \\\\`\\n`--set WebApp.resources.requests.memory=2Gi \\\\`\\n`$custom_variables` Once the changes are pushed/merged to the repository the values will be applied to your Kubernetes namespace. Changing the number of instances (pods) on name space The number of app instances running on dev / prod is 1 by default, if users experience long response times from the application (apart from trying to improving the performance through the code itself) you can increase the number of instances to reduce the wating time on dev / prod GitHub workflows, for example :- --set ReplicaCount=3 Remember: “With great power comes great responsibility”  Your application’s namespace will be one of a number hosted on the same cluster, setting the values too high could crash the cluster! Storage guidance Guidance on storage within Cloud Platform can be found here Cloud platform storage Continuous Deployment Within Cloud platform your application is already automatically deployed by Github workflows but further guidance on continuous deployment within  CP can be found here Cloud Platform continuous deployment. Deploying Updates to the Application As the application is now hosted in Cloud Platform and GitHub workflow has been implemented, you will now be able to apply update to your application, full guidance can be found here Application deployment. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 864}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd51d9919c0c47dda2977f9e577cb26d6'}>,\n",
       " <Document: {'content': 'Other guidance Guidance on  managing Auth and Secrets through the Control Panel can be found Manage deployment settings of an app on Control panel. Shiny server There are a few choices for running a rshiny app on Cloud Platform. The team responsible for developing and maintaining the dashboard app has full control to choose the best way and practices to run the app by building their own Dockerfile. Since the shiny-framework uses websocket as the primary communication protocol between frontend and backend, no matter which choice you decide, the minimum capabilities required are: Keeping the connection live as long as possible e.g. implementing heart-beat mechanism. Being able to handle the lifecyle of session Providing a method for reconnecting when the websocket drops Working with auth-proxy, the AP component responsible for controlling user’s access,  unless the app is public facing or application handles authentication itself. The AP team offers two shiny-server solutions. AP version of shiny-server We developed a mini version of shiny-sever in nodejs, it provides a minimal implementation to support the required capabilities: Uses sockjs which supports heart-beat Create a new session for each new websocket connection Will try to reconnect to the shiny app automatically when the websocket connection drops If reconnection repeatedly fails and reaches the maximum number of attempts, a window will be appear asking the user to trigger a manual reconnect The majority of the shiny apps hosted on Cloud Platform use this version. The current tag for this docker image is: 593291632749.dkr.ecr.eu-west-1.amazonaws.com/rshiny:r4.1.3-shiny0.0.6 Open source shiny-server We also provide a solution for using the open source Shiny Server with a few minor tweaks to support USER_EMAIL and COOKIE headers.  The base docker image is defined here . The version of open source shiny server is defined by SHINY_SERVER_VERSION , currently set to 1.5.20.1002 . It offers more features than the AP shiny server and supports: Better reconnection behaviour: Reconnect and load existing session rather than creating a new session automatically If reconnection continues to fail, the manual reconnection pop-up will trigger an app reload rather than re-triggering the same reconnection behaviour Better session management: The session will be closed when the session is idle for a certain period of time This behaviour can result in:\\n- Session data (reactive values) is retained even after a reconnection happens\\n- Release resources e.g. memory linked to the session which avoids potential memory leaking It also provides more configuration options as outlined here . Note: options marked as “pro” are not available Instructions for using the open source shiny server image Example Dockerfile The following example can be used as the starting point when making your own Dockerfile # The base docker image\\nFROM ghcr.io/ministryofjustice/data-platform-rshiny-open-source-base:1.0.3\\n\\n# ** Optional step: only if some of R pakcages requires the system libraries which are not covered by base image\\n#   the one in the example below has been provided in base image.\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 865}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '36085d176f6a14e2924c0433c2257df6'}>,\n",
       " <Document: {'content': '# RUN apt-get update \\\\\\n#   && apt-get install -y --no-install-recommends \\\\\\n#     libglpk-dev\\n\\n# use renv for packages\\nADD renv.lock renv.lock\\n\\n# Install R packages\\nRUN R -e \"install.packages(\\'renv\\'); renv::restore()\"\\n\\n# ** Optional step: only if the app requires python packages\\n# Make sure reticulate uses the system Python\\n# ENV RETICULATE_PYTHON=\"/usr/bin/python3\"\\n# ensure requirements.txt exists (created automatically when making a venv in renv)\\n# COPY requirements.txt requirements.txt\\n# RUN python3 -m pip install -r requirements.txt\\n\\n# Add shiny app code\\nADD . .\\n\\nUSER 998 Instructions for switching from the AP shiny server to the open-source server If you already use the AP shiny server, and would like to switch to the open source server, the key changes you need to make are: Change the base docker image in your Dockerfile: FROM ghcr.io/ministryofjustice/data-platform-rshiny-open-source-base:1.0.3 If present, ensure the following redundant parts of your Dockerfile are removed: ENV PATH=\"/opt/shiny-server/bin:/opt/shiny-server/ext/node/bin:${PATH}\"\\nENV SHINY_APP=/srv/shiny-server\\nENV NODE_ENV=production RUN chown shiny:shiny /srv/shiny-server\\nCMD analytics-platform-shiny-server\\nEXPOSE 9999 Update GitHub actions work flows Assuming the app uses the GitHub actions workflows from AP, the following parameters for helm installation are required in both the build-push-deploy-dev.yml and build-push-deploy-prod.yml github action workflow files: WebApp.AlternativeHealthCheck.enabled=\"true\"\\nWebApp.AlternativeHealthCheck.port=9999 A complete example for installing the helm chart in the workflow below. These changes will need to be made in both: helm upgrade --install --wait --timeout 10m0s --namespace $KUBE_NAMESPACE $RELEASE_NAME mojanalytics/webapp-cp \\\\\\n--set AuthProxy.Env.Auth0Domain=$AUTH0_DOMAIN \\\\\\n--set AuthProxy.Env.Auth0Passwordless=$AUTH0_PASSWORDLESS \\\\\\n--set AuthProxy.Env.Auth0TokenAlg=$AUTH0_TOKEN_ALG \\\\\\n--set AuthProxy.Env.AuthenticationRequired=$AUTHENTICATION_REQUIRED \\\\\\n--set AuthProxy.Env.IPRanges=$process_ip_range \\\\\\n--set AuthProxy.Image.Repository=$ECR_REPO_AUTH0 \\\\\\n--set AuthProxy.Image.Tag=\"latest\" \\\\\\n--set Namespace=$KUBE_NAMESPACE \\\\\\n--set WebApp.AlternativeHealthCheck.enabled=\"true\" \\\\\\n--set WebApp.AlternativeHealthCheck.port=9999 \\\\\\n--set Secrets.Auth0.ClientId=$AUTH0_CLIENT_ID \\\\\\n--set Secrets.Auth0.ClientSecret=$AUTH0_CLIENT_SECRET \\\\\\n--set Secrets.Auth0.CookieSecret=$COOKIE_SECRET \\\\\\n--set ServiceAccount.RoleARN=$APP_ROLE_ARN \\\\\\n--set WebApp.Image.Repository=$ECR_REPO_WEBAPP \\\\\\n--set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n$custom_variables A full working example is available here Enabling Web Application Firewall (WAF) for Data Platform Apps There are two flags that need to be set to enable WAF for your application, Ingress.ModSec.enabled and GithubTeam To do this modify your app’s build-push-deploy-dev.yml and build-push-deploy-prod.yml GitHub action workflow files as follows: --set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n--set Ingress.ModSec.enabled=\"true\" \\\\\\n--set GithubTeam=\"your github team\" \\\\\\n$custom_variables The changes  will be will be applied to your Kubernetes namespace following a push/merge to the repository and the running of the workflow To disable –set Ingress.ModSec.enabled=“false” This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDeploy an RShiny app - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 866}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4068b35394414be6bc55dc905a46d88d'}>,\n",
       " <Document: {'content': 'Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 867}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f5a617e2b8bf3e97f303a94c8b4da304'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools R Shiny app publishing Once you’ve built your Shiny app, you can make it available to users through the Analytical Platform.\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 868}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b6d010c909f0a5eb3c74b9ee21db196a'}>,\n",
       " <Document: {'content': 'We have guidance for: Deploying your app Accessing your deployed app Managing your app users Managing and Monitoring Deployments as well as common issues faced during these stages of publishing your app. ⚠️ Note on deployment ⚠️ It is not currently possible to deploy new apps on the Analytical Platform, though we are working to make this functionality available again as soon as possible. In the meantime, you can still access and manage existing apps. If you have an existing app that requires urgent redeployment, please submit a request via GitHub.\\nWe normally redeploy apps each Wednesday, where we have recevied a request by the Friday before. App deployment New apps It is not currently possible to deploy new apps on the Analytical Platform, though we are working to make this functionality available again as soon as possible. Existing apps Overview Your app environment is made up of two key elements: A GitHub workflow A Cloud Platform namespace In brief, the workflow builds and deploys your code as a docker container, and then deploys it to Cloud Platform’s kubernetes cluster, in your application’s namespace. The flow looks something like this: The rationale behind this change is to: Facilitate testing - including a working development app version and build artifacts like docker containers - before deploying to production Give teams more control over their workflows Restore the ability to deploy without the Analytical Platform team’s intervention Apps hosted on Cloud Platform After the move to Cloud Platform hosting for Analytical Platform apps, you’ll have two active deployments of your apps at all times.\\nThese are your ‘dev’ (development) and ‘prod’ (production) deployments. Be aware that both production and dev currently point to the same data. Your code repository within the ministryofjustice organisation was built from the data-platform-app-template repo , and has inherited the continuous integration and continuous delivery (CI/CD) pipelines (GitHub Action workflows) from that repo.\\nThese workflows will automatically build your docker images, push them to a remote image store, and then deploy the application based on how you’ve made your code changes: Deploying your application to your development environment is done through the -dev workflow, which will build-push-deploy when any pull request is made in the repo (e.g. feature-branch into main , or feature-branch into develop-branch ).\\nThe workflow can also be manually triggered, though this will only build and push the app, it will not deploy it: repo homepage > Actions (tab) > Run workflow (right hand side of the page). Opening any pull request will trigger a dev deployment.\\nAny subsequent pushes to that branch (or to any other open branches) will trigger dev deployments too.\\nIf you’re working on multiple PRs at once in the repo that you will need to coordinate pushes to your PRs so you can track your deployments.\\nYou can always cancel workflow runs and rerun deployments from the Actions page in the repo. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 869}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '92b2bb3a27242c740eeffdb99b1ea273'}>,\n",
       " <Document: {'content': 'Deploying your application to your production environment is done through the -prod workflow, which will build-push-deploy either: after a pull request is merged into main upon publishing a release You can view the status of your deployments either by checking the workflow runs in the Actions tab (repo-url/actions), or by checking out the deployments page (repo-url/deployments). The above describes how CI/CD will be set up by default in the ministryofjustice repo.\\nOnce you have ownership of the repo, you’ll have the ownership of the .github/workflow/ files too so you will be able to amend the processes and triggers so that they meet your needs. Your app deployment pipeline GitHub Actions has workflow definitions located in your repository under .github/workflows/ . By default, you will be provided with two workflows.\\nThe dev environment workflow is triggered when you open or contribute to a Pull Request from any branch (for example, into your main branch).\\nIts steps are: Check out the repository Authenticate to AWS Build a docker container from your development branch If the build is successful, push this container to a container registry Run helm against the development environment namespace. The production workflow behaves in the exact same way, containing the same steps, but will be triggered when a Pull Request is merged into main , and will deploy to the production namespace instead Managing published apps Manage app users If authentication is enabled and you choose to use email as the login option, user access management to app can be done through Control panel. To manage the users of your app: Login Control panel as app admin Go to the Analytical Platform control panel . Select the Webapps tab. Select the name of the app you want to manage or select Manage customers . Select the environment where app is deployed ( dev/prod ) To add app users: Enter the email addresses of the users in the box titled ‘Add app customers by entering their email addresses’ – multiple email addresses can be separated by spaces, tabs, commas and semicolons, and can contain capital letters. Select Add customer . To remove an app user, select Remove customer next to the email address of the customer you want to remove. Accessing the deployed app Apps hosted on Cloud Platform Your deployed app can be accessed at two URLs: prod is at: repository-name.apps.live.cloud-platform.service.justice.gov.uk dev is at: repository-name-dev.apps.live.cloud-platform.service.justice.gov.uk (where repository-name is the name of the relevant GitHub repository) By default, the user list on dev empty and you will need to add any users requiring access via control panel. Authenticating to your app For the dashboard apps using passwordless flow (email login), when accessing an app, you can choose whether to sign in using a one-time passcode (default) or an email magic link.\\nTo sign in with an email magic link, add /login?method=link to the end of the app’s URL, for example, https://apps.live.cloud-platform.service.justice.gov.uk/login?method=code . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 870}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dd98764134c9ea4bf690bf131ac66a5f'}>,\n",
       " <Document: {'content': 'Troubleshooting app sign-in “That email address is not authorized for this app (or possibly another error occurred)” error, after entering email address Check that the user is authorised to access the app: Log in to the control panel . Navigate to the app detail page. Choose the right deployment environment (dev/prod) Check if the user’s email address is listed under ‘App customers’. If it is not, refer them to the app owner to obtain access. Check that the user is using the correct email address – there is sometimes confusion between @justice and @digital.justice email addresses. “Access denied” error, having entered email address and copied the one-time passcode into the login page Please follow the same steps above to check whether the user is in the customer list of the app. “IP x.x.x.x is not whitelisted” Check that the user is trying to access the app from one of the trusted networks listed on app’s app-detail from Control panel The app admin can modify the IP_Ranges on the app’s app-detail detail page. Other troubleshooting tips Check that they are trying to access the app using a URL beginning with https:// not http:// . Look for similar issues log in the data-platform-support repository . Try asking the user to clear their cookies by visiting https://alpha-analytics-moj.eu.auth0.com/logout and try again. In addition the AP team can: Check the Auth0 logs for the app in Kibana Check the Auth0 logs in the Auth0 console Advanced Editing the Dockerfile A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. In most cases, you will not need to change the Dockerfile when deploying your app. If your app uses packages that have additional system dependencies, you will need to add these in the Dockerfile . If you are unsure how to do this, contact the Analytical Platform team. A Dockerfile reference can be found in the Docker documentation . Getting details of current users of the app An RShiny app can find out who is using it.\\nThis can be useful to log an audit trail of significant events.\\nSpecifically, it can determine the email address that the user logged into the app with.\\nThis is sensitive data, so you must ensure that you are following all relevant information governance processes. The shiny-headers-demo repository contains an example of how to do this. These features are only available in the Analytical Platform version of shiny-server and the open source shiny server image provided by AP ( see details of both options below ). Finding current users’ email addresses You can obtain the logged in user’s email address by using the following code in the server function of your app: get ( \"HTTP_USER_EMAIL\" , envir = session $ request ) This line in shiny-headers-demo shows the code in context. NOTE : Ensure the latest AuthProxy image is being used or the user email header will not be accessible. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 871}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd0cd6ea27e9c8a5f40b7bb7f08736fb2'}>,\n",
       " <Document: {'content': 'This is defined in the helm upgrade command in the GitHub actions files that handle deployments - build-push-deploy-dev.yml and build-push-deploy-prod.yml . Check these files for AuthProxy.Image.Tag and set it to latest as the example below: --set AuthProxy.Image.Tag=\"latest\" \\\\ A full example in a working deployed app can be found here . Finding current users’ user profiles You can access the full user profile by making a request directly from the RShiny app to the auth-proxy’s /userinfo endpoint using the following code inside your server function. # library(httr) # library(jsonlite) profile <- fromJSON ( content ( GET ( \"http://localhost:3001/userinfo\" , add_headers ( cookie = get ( \"HTTP_COOKIE\" , envir = session $ request ))), \"text\" )) This line shows the code in context. Example response { \"email\" : \"name@example.gov.uk\" , \"email_verified\" : true , \"user_id\" : \"email|12345121312\" , \"picture\" : \"https://s.gravatar.com/avatar/94deebe3b87fc5e9b3b4469112573cc0?s=480&r=pg&d=https%3A%2F%2Fcdn.auth0.com%2Favatars%2Fna.png\" , \"nickname\" : \"name\" , \"identities\" : [ { \"user_id\" : \"12345121312\" , \"provider\" : \"email\" , \"connection\" : \"email\" , \"isSocial\" : false } ], \"updated_at\" : \"2019-07-15T09:54:34.353Z\" , \"created_at\" : \"2018-07-12T14:26:45.663Z\" , \"name\" : \"name@example.gov.uk\" , \"last_ip\" : \"1.1.1.1\" , \"last_login\" : \"2019-07-15T09:54:34.353Z\" , \"logins_count\" : 20 , \"blocked_for\" : [], \"guardian_authenticators\" : [] } user-profile caching Due to the auth0 rate limit for /userinfo , the user-profile will be\\ncached for 10 minutes on auth-proxy. If somehow your app receives an exception, for example, token-expired, from the above call, you can add /userinfo?force=true to refresh the user-profle by force. Troubleshooting and monitoring Deploying locally If you have a MacBook, you can use Docker locally to test and troubleshoot your RShiny app.\\nYou can download Docker Desktop for Mac from the Docker website . To build and run your R Shiny app locally, follow the steps below: Clone your app’s repository to a new folder on your MacBook – this guarantees that the app will be built using the same code as on the Analytical Platform. You may need to create a new connection to GitHub with SSH . Open a terminal session and navigate to the directory containing the Dockerfile using the cd command. Build the Docker image by running: {bash}\\ndocker build . -t IMAGE:TAG where IMAGE is a name for the image, for example, my-docker-image , and TAG is the version number, for example, 0.1 . Run a Docker container created from the Docker image by running: {bash}\\ndocker run -p 80:9999 IMAGE:TAG Go to 127.0.0.1:80 to view the app. If the app does not work, follow the steps below to troubleshoot it: Start a bash session in a Docker container created from the Docker image by running: {bash}\\ndocker run -it -p 80:80 IMAGE:TAG bash Install the nano text editor by running: {bash}\\napt-get update\\napt-get install nano Open shiny-server.conf in the nano text editor by running: {bash}\\nnano /etc/shiny-server/shiny-server.conf Add the following lines at the beginning of shiny-server.conf : {bash}\\naccess_log /var/log/shiny-server/access.log tiny;\\npreserve_logs true; Write the changes by pressing Ctrl+O . Exit the nano text editor by pressing Ctrl+X . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 872}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cc1b192562e75bd4fe701d30aa955c7b'}>,\n",
       " <Document: {'content': 'Increase the verbosity of logging and start the Shiny server by running: {bash}\\nexport SHINY_LOG_LEVEL=TRACE\\n/bin/shiny-server.sh Open a new terminal session. Start a new bash session in the Docker container by running: {bash}\\ndocker exec -it CONTAINER_ID bash You can find the CONTAINER_ID by running docker ps . View the logs by running: {bash}\\ncat /var/log/shiny-server/access.log For further details, see the Shiny server documentation . Managing and Monitoring Deployments Monitoring Deployments By default all applications deployed into Cloud platform have a basic level of monitoring which can be accessed on the following link Cloud Platform grafana namespace monitoring . Grafana dashboards are accessed using single-sign-on (SSO) via your GitHub account. This page shows all namespaces and can be refined by typing the name of your namespace in namespace drop down and selecting as needed. Note: Kubernetes defines Limits as the maximum amount of a resource to be used by a container. This means that the container can never consume more than the memory amount or CPU amount indicated. Requests, on the other hand, are the minimum guaranteed amount of a resource that is reserved for a container. A brief overview of namespaces Within Kubernetes a namespace provides a mechanism for isolating groups of resources within a single cluster, it can be thought of as a virtual cluster within the cluster. Your Application is deployed into its own namespace, this restricts access to your team and enables the setting of resource limits. Within the namespace are the various Kubernetes components: Pods the smallest deployable units of computing that you can create and manage in Kubernetes, usually one pod per function of your application ie web server, db server. Service is a method for exposing a network application that is running as one or more Pods in your cluster, basically simplifying the connections within your namespace. Deployment provides declarative updates for Pods and ReplicaSets. ReplicaSet’s their purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods. What information is displayed on the Grafana namespace chart: Namespace(s) usage on total cluster CPU in % - This is the total usage by all namespaces on the cluster Namespace(s) usage on total cluster RAM in % - This is the total usage by all namespaces on the cluster Kubernetes Resource Count - A simple count of resources running within your namespace. CPU usage by Pod - CPU usage per pod within your namespace. Memory usage by Pod - Memory usage per pod within your namespace. Nb of pods by state - Count of pods at various states  within your namespace. Nb of containers by pod - Number of containers within each pod. Replicas available by deployment - the number of replicas available to your deployment. Replicas unavailable by deployment - the number of replicas unavailable to your deployment. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 873}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '37e470b490a5867cd73892f7de59f578'}>,\n",
       " <Document: {'content': 'Persistent Volumes Capacity - If your application uses persistent volumes the total capacity is displayed here. Persistent Volumes Inodes - If your application uses persistent volumes the detail on the inodes is displayed here. Further technical details can be found in the Cloud Platform’s Monitoring section of the user guidance and Configuring a dashboard using Grafana UI documentation Accessing logs You can access your applications logs in Cloud platform by following the the CP guidance Accessing Application Log Data Kibana on Cloud Platform Below are some notes to aid you in working with the Kibana service, on Cloud Platform. All logs from deployed apps can be viewed in Kibana . To view the logs for a specific app: Select live_kubernetes_cluster from the CHANGE INDEX PATTERN dropdown list. Select Add a filter . Select kubernetes.namespace_name . Select is as the operator. Insert the app’s name space by following the pattern data-platform-app-<app_name>-<dev/prod> . In order to filter out all the logs related health-check, you can put NOT log:  \"/healthz\" in the KQL field. Select Save . (Optional) - you can select to view only the log output by adding it from the Available Fields list in the left hand pane using the (+) button revealed on mouse hover Log messages are displayed in the message column. By default, Kibana only shows logs for the last 15 minutes.\\nIf no logs are available for that time range, you will receive the warning ‘No results match your search criteria’. To change the time range, select the clock icon in the menu bar.\\nThere are several presets or you can define a custom time range. Kibana also has experimental autocomplete and simple syntax tools that you can use to build custom searches.\\nTo enable these features, select Options_ from within the search bar, then toggle Turn on query features . Managing Deployments Cloud platform  allows teams to connect to the Kubernetes cluster and manage their applications, using kubectl , the command line tool for Kubernetes. To do this you will need to setup access through JupyterLab as follows: Go to Analytical Platform Control Panel ‘> Analytical Tools ’> open JupyterLab Data Science ‘> Terminal Installing Kubectl In the terminal session install kubectl curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" curl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\" echo \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check If valid, the output is: kubectl: OK chmod +x kubectl mkdir -p ~/.local/bin mv ./kubectl ~/.local/bin/kubectl which kubectl you should see: /home/jovyan/.local/bin/kubectl You should only need to carry out the above steps once. Setup Cloud Platform Access Follow the steps in CP’s Generating a Kubeconfig file documentation . As we are accessing via Jupyter Labs  step 7 “Move the config file to the location expected by kubectl” has to be carried out slightly differently. First upload the kubecfg.yaml from your local  PC to the Jupyter terminal session. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 874}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '56e1a52cf3548eb5c3940101fe33abe8'}>,\n",
       " <Document: {'content': 'click on the upload arrow (see below) and select the kubecfg.yaml Then continue with the remaining steps in CP’s documentation Accessing the AWS console If required you can access the Cloud platform via AWS, please follow the guidance in Accessing the AWS console (read-only) Adding cron job to your application A cronjob for restarting your application can be setup easier by adding the following line in to your app’s development or production GitHub workflow: --set Cron.schedule=\"0 6 * * *\" crontab.guru can be used to setup the schedule. If you need to a cron job for the other jobs, more guides are available on the Cloud Platform kubernetes cronjobs Changing the resources available to the application When you deploy an update to the application the Kubernetes resources are built from a standard Helm chart with the following default memory/cpu resources: Limits -  CPU 25m.  Memory 1Gi Requests - CPU 3m.  Memory 1Gi To override this you will have to amend the GitHub  workflow in your application repository The file(s) to amend are .github/workflows/build-push-deploy-dev.yml or build-push-deploy-prod.yml Below is the section of code that calls the helm chart helm upgrade --install --wait --timeout 10m0s --namespace $KUBE_NAMESPACE $KUBE_NAMESPACE mojanalytics/webapp-cp \\\\\\n--set AuthProxy.Env.Auth0Domain=$AUTH0_DOMAIN \\\\\\n--set AuthProxy.Env.Auth0Passwordless=$AUTH0_PASSWORDLESS \\\\\\n--set AuthProxy.Env.Auth0TokenAlg=$AUTH0_TOKEN_ALG \\\\\\n--set AuthProxy.Env.AuthenticationRequired=$AUTHENTICATION_REQUIRED \\\\\\n--set AuthProxy.Env.IPRanges=$process_ip_range \\\\\\n--set AuthProxy.Image.Repository=$ECR_REPO_AUTH0 \\\\\\n--set AuthProxy.Image.Tag=\"latest\" \\\\\\n--set Namespace=$KUBE_NAMESPACE \\\\\\n--set Secrets.Auth0.ClientId=$AUTH0_CLIENT_ID \\\\\\n--set Secrets.Auth0.ClientSecret=$AUTH0_CLIENT_SECRET \\\\\\n--set Secrets.Auth0.CookieSecret=$COOKIE_SECRET \\\\\\n--set ServiceAccount.RoleARN=$APP_ROLE_ARN \\\\\\n--set WebApp.Image.Repository=$ECR_REPO_WEBAPP \\\\\\n--set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n$custom_variables ``` To change the resources  insert at  the end of the file just before the $custom_variables the following as appropriate, remembering to use the line continuation “\\\\” on the existing last line. `--set WebApp.resources.limits.cpu=100m \\\\`\\n`--set WebApp.resources.limits.memory=2Gi \\\\`\\n`--set WebApp.resources.requests.cpu=50m \\\\`\\n`--set WebApp.resources.requests.memory=2Gi \\\\`\\n`$custom_variables` Once the changes are pushed/merged to the repository the values will be applied to your Kubernetes namespace. Changing the number of instances (pods) on name space The number of app instances running on dev / prod is 1 by default, if users experience long response times from the application (apart from trying to improving the performance through the code itself) you can increase the number of instances to reduce the wating time on dev / prod GitHub workflows, for example :- --set ReplicaCount=3 Remember: “With great power comes great responsibility”  Your application’s namespace will be one of a number hosted on the same cluster, setting the values too high could crash the cluster! Storage guidance Guidance on storage within Cloud Platform can be found here Cloud platform storage Continuous Deployment Within Cloud platform your application is already automatically deployed by Github workflows but further guidance on continuous deployment within  CP can be found here Cloud Platform continuous deployment. Deploying Updates to the Application As the application is now hosted in Cloud Platform and GitHub workflow has been implemented, you will now be able to apply update to your application, full guidance can be found here Application deployment. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 875}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd51d9919c0c47dda2977f9e577cb26d6'}>,\n",
       " <Document: {'content': 'Other guidance Guidance on  managing Auth and Secrets through the Control Panel can be found Manage deployment settings of an app on Control panel. Shiny server There are a few choices for running a rshiny app on Cloud Platform. The team responsible for developing and maintaining the dashboard app has full control to choose the best way and practices to run the app by building their own Dockerfile. Since the shiny-framework uses websocket as the primary communication protocol between frontend and backend, no matter which choice you decide, the minimum capabilities required are: Keeping the connection live as long as possible e.g. implementing heart-beat mechanism. Being able to handle the lifecyle of session Providing a method for reconnecting when the websocket drops Working with auth-proxy, the AP component responsible for controlling user’s access,  unless the app is public facing or application handles authentication itself. The AP team offers two shiny-server solutions. AP version of shiny-server We developed a mini version of shiny-sever in nodejs, it provides a minimal implementation to support the required capabilities: Uses sockjs which supports heart-beat Create a new session for each new websocket connection Will try to reconnect to the shiny app automatically when the websocket connection drops If reconnection repeatedly fails and reaches the maximum number of attempts, a window will be appear asking the user to trigger a manual reconnect The majority of the shiny apps hosted on Cloud Platform use this version. The current tag for this docker image is: 593291632749.dkr.ecr.eu-west-1.amazonaws.com/rshiny:r4.1.3-shiny0.0.6 Open source shiny-server We also provide a solution for using the open source Shiny Server with a few minor tweaks to support USER_EMAIL and COOKIE headers.  The base docker image is defined here . The version of open source shiny server is defined by SHINY_SERVER_VERSION , currently set to 1.5.20.1002 . It offers more features than the AP shiny server and supports: Better reconnection behaviour: Reconnect and load existing session rather than creating a new session automatically If reconnection continues to fail, the manual reconnection pop-up will trigger an app reload rather than re-triggering the same reconnection behaviour Better session management: The session will be closed when the session is idle for a certain period of time This behaviour can result in:\\n- Session data (reactive values) is retained even after a reconnection happens\\n- Release resources e.g. memory linked to the session which avoids potential memory leaking It also provides more configuration options as outlined here . Note: options marked as “pro” are not available Instructions for using the open source shiny server image Example Dockerfile The following example can be used as the starting point when making your own Dockerfile # The base docker image\\nFROM ghcr.io/ministryofjustice/data-platform-rshiny-open-source-base:1.0.3\\n\\n# ** Optional step: only if some of R pakcages requires the system libraries which are not covered by base image\\n#   the one in the example below has been provided in base image.\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 876}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '36085d176f6a14e2924c0433c2257df6'}>,\n",
       " <Document: {'content': '# RUN apt-get update \\\\\\n#   && apt-get install -y --no-install-recommends \\\\\\n#     libglpk-dev\\n\\n# use renv for packages\\nADD renv.lock renv.lock\\n\\n# Install R packages\\nRUN R -e \"install.packages(\\'renv\\'); renv::restore()\"\\n\\n# ** Optional step: only if the app requires python packages\\n# Make sure reticulate uses the system Python\\n# ENV RETICULATE_PYTHON=\"/usr/bin/python3\"\\n# ensure requirements.txt exists (created automatically when making a venv in renv)\\n# COPY requirements.txt requirements.txt\\n# RUN python3 -m pip install -r requirements.txt\\n\\n# Add shiny app code\\nADD . .\\n\\nUSER 998 Instructions for switching from the AP shiny server to the open-source server If you already use the AP shiny server, and would like to switch to the open source server, the key changes you need to make are: Change the base docker image in your Dockerfile: FROM ghcr.io/ministryofjustice/data-platform-rshiny-open-source-base:1.0.3 If present, ensure the following redundant parts of your Dockerfile are removed: ENV PATH=\"/opt/shiny-server/bin:/opt/shiny-server/ext/node/bin:${PATH}\"\\nENV SHINY_APP=/srv/shiny-server\\nENV NODE_ENV=production RUN chown shiny:shiny /srv/shiny-server\\nCMD analytics-platform-shiny-server\\nEXPOSE 9999 Update GitHub actions work flows Assuming the app uses the GitHub actions workflows from AP, the following parameters for helm installation are required in both the build-push-deploy-dev.yml and build-push-deploy-prod.yml github action workflow files: WebApp.AlternativeHealthCheck.enabled=\"true\"\\nWebApp.AlternativeHealthCheck.port=9999 A complete example for installing the helm chart in the workflow below. These changes will need to be made in both: helm upgrade --install --wait --timeout 10m0s --namespace $KUBE_NAMESPACE $RELEASE_NAME mojanalytics/webapp-cp \\\\\\n--set AuthProxy.Env.Auth0Domain=$AUTH0_DOMAIN \\\\\\n--set AuthProxy.Env.Auth0Passwordless=$AUTH0_PASSWORDLESS \\\\\\n--set AuthProxy.Env.Auth0TokenAlg=$AUTH0_TOKEN_ALG \\\\\\n--set AuthProxy.Env.AuthenticationRequired=$AUTHENTICATION_REQUIRED \\\\\\n--set AuthProxy.Env.IPRanges=$process_ip_range \\\\\\n--set AuthProxy.Image.Repository=$ECR_REPO_AUTH0 \\\\\\n--set AuthProxy.Image.Tag=\"latest\" \\\\\\n--set Namespace=$KUBE_NAMESPACE \\\\\\n--set WebApp.AlternativeHealthCheck.enabled=\"true\" \\\\\\n--set WebApp.AlternativeHealthCheck.port=9999 \\\\\\n--set Secrets.Auth0.ClientId=$AUTH0_CLIENT_ID \\\\\\n--set Secrets.Auth0.ClientSecret=$AUTH0_CLIENT_SECRET \\\\\\n--set Secrets.Auth0.CookieSecret=$COOKIE_SECRET \\\\\\n--set ServiceAccount.RoleARN=$APP_ROLE_ARN \\\\\\n--set WebApp.Image.Repository=$ECR_REPO_WEBAPP \\\\\\n--set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n$custom_variables A full working example is available here Enabling Web Application Firewall (WAF) for Data Platform Apps There are two flags that need to be set to enable WAF for your application, Ingress.ModSec.enabled and GithubTeam To do this modify your app’s build-push-deploy-dev.yml and build-push-deploy-prod.yml GitHub action workflow files as follows: --set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n--set Ingress.ModSec.enabled=\"true\" \\\\\\n--set GithubTeam=\"your github team\" \\\\\\n$custom_variables The changes  will be will be applied to your Kubernetes namespace following a push/merge to the repository and the running of the workflow To disable –set Ingress.ModSec.enabled=“false” This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDeploy an RShiny app - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 877}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4068b35394414be6bc55dc905a46d88d'}>,\n",
       " <Document: {'content': 'Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 878}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f5a617e2b8bf3e97f303a94c8b4da304'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools R Shiny app publishing Once you’ve built your Shiny app, you can make it available to users through the Analytical Platform.\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 879}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b6d010c909f0a5eb3c74b9ee21db196a'}>,\n",
       " <Document: {'content': 'We have guidance for: Deploying your app Accessing your deployed app Managing your app users Managing and Monitoring Deployments as well as common issues faced during these stages of publishing your app. ⚠️ Note on deployment ⚠️ It is not currently possible to deploy new apps on the Analytical Platform, though we are working to make this functionality available again as soon as possible. In the meantime, you can still access and manage existing apps. If you have an existing app that requires urgent redeployment, please submit a request via GitHub.\\nWe normally redeploy apps each Wednesday, where we have recevied a request by the Friday before. App deployment New apps It is not currently possible to deploy new apps on the Analytical Platform, though we are working to make this functionality available again as soon as possible. Existing apps Overview Your app environment is made up of two key elements: A GitHub workflow A Cloud Platform namespace In brief, the workflow builds and deploys your code as a docker container, and then deploys it to Cloud Platform’s kubernetes cluster, in your application’s namespace. The flow looks something like this: The rationale behind this change is to: Facilitate testing - including a working development app version and build artifacts like docker containers - before deploying to production Give teams more control over their workflows Restore the ability to deploy without the Analytical Platform team’s intervention Apps hosted on Cloud Platform After the move to Cloud Platform hosting for Analytical Platform apps, you’ll have two active deployments of your apps at all times.\\nThese are your ‘dev’ (development) and ‘prod’ (production) deployments. Be aware that both production and dev currently point to the same data. Your code repository within the ministryofjustice organisation was built from the data-platform-app-template repo , and has inherited the continuous integration and continuous delivery (CI/CD) pipelines (GitHub Action workflows) from that repo.\\nThese workflows will automatically build your docker images, push them to a remote image store, and then deploy the application based on how you’ve made your code changes: Deploying your application to your development environment is done through the -dev workflow, which will build-push-deploy when any pull request is made in the repo (e.g. feature-branch into main , or feature-branch into develop-branch ).\\nThe workflow can also be manually triggered, though this will only build and push the app, it will not deploy it: repo homepage > Actions (tab) > Run workflow (right hand side of the page). Opening any pull request will trigger a dev deployment.\\nAny subsequent pushes to that branch (or to any other open branches) will trigger dev deployments too.\\nIf you’re working on multiple PRs at once in the repo that you will need to coordinate pushes to your PRs so you can track your deployments.\\nYou can always cancel workflow runs and rerun deployments from the Actions page in the repo. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 880}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '92b2bb3a27242c740eeffdb99b1ea273'}>,\n",
       " <Document: {'content': 'Deploying your application to your production environment is done through the -prod workflow, which will build-push-deploy either: after a pull request is merged into main upon publishing a release You can view the status of your deployments either by checking the workflow runs in the Actions tab (repo-url/actions), or by checking out the deployments page (repo-url/deployments). The above describes how CI/CD will be set up by default in the ministryofjustice repo.\\nOnce you have ownership of the repo, you’ll have the ownership of the .github/workflow/ files too so you will be able to amend the processes and triggers so that they meet your needs. Your app deployment pipeline GitHub Actions has workflow definitions located in your repository under .github/workflows/ . By default, you will be provided with two workflows.\\nThe dev environment workflow is triggered when you open or contribute to a Pull Request from any branch (for example, into your main branch).\\nIts steps are: Check out the repository Authenticate to AWS Build a docker container from your development branch If the build is successful, push this container to a container registry Run helm against the development environment namespace. The production workflow behaves in the exact same way, containing the same steps, but will be triggered when a Pull Request is merged into main , and will deploy to the production namespace instead Managing published apps Manage app users If authentication is enabled and you choose to use email as the login option, user access management to app can be done through Control panel. To manage the users of your app: Login Control panel as app admin Go to the Analytical Platform control panel . Select the Webapps tab. Select the name of the app you want to manage or select Manage customers . Select the environment where app is deployed ( dev/prod ) To add app users: Enter the email addresses of the users in the box titled ‘Add app customers by entering their email addresses’ – multiple email addresses can be separated by spaces, tabs, commas and semicolons, and can contain capital letters. Select Add customer . To remove an app user, select Remove customer next to the email address of the customer you want to remove. Accessing the deployed app Apps hosted on Cloud Platform Your deployed app can be accessed at two URLs: prod is at: repository-name.apps.live.cloud-platform.service.justice.gov.uk dev is at: repository-name-dev.apps.live.cloud-platform.service.justice.gov.uk (where repository-name is the name of the relevant GitHub repository) By default, the user list on dev empty and you will need to add any users requiring access via control panel. Authenticating to your app For the dashboard apps using passwordless flow (email login), when accessing an app, you can choose whether to sign in using a one-time passcode (default) or an email magic link.\\nTo sign in with an email magic link, add /login?method=link to the end of the app’s URL, for example, https://apps.live.cloud-platform.service.justice.gov.uk/login?method=code . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 881}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dd98764134c9ea4bf690bf131ac66a5f'}>,\n",
       " <Document: {'content': 'Troubleshooting app sign-in “That email address is not authorized for this app (or possibly another error occurred)” error, after entering email address Check that the user is authorised to access the app: Log in to the control panel . Navigate to the app detail page. Choose the right deployment environment (dev/prod) Check if the user’s email address is listed under ‘App customers’. If it is not, refer them to the app owner to obtain access. Check that the user is using the correct email address – there is sometimes confusion between @justice and @digital.justice email addresses. “Access denied” error, having entered email address and copied the one-time passcode into the login page Please follow the same steps above to check whether the user is in the customer list of the app. “IP x.x.x.x is not whitelisted” Check that the user is trying to access the app from one of the trusted networks listed on app’s app-detail from Control panel The app admin can modify the IP_Ranges on the app’s app-detail detail page. Other troubleshooting tips Check that they are trying to access the app using a URL beginning with https:// not http:// . Look for similar issues log in the data-platform-support repository . Try asking the user to clear their cookies by visiting https://alpha-analytics-moj.eu.auth0.com/logout and try again. In addition the AP team can: Check the Auth0 logs for the app in Kibana Check the Auth0 logs in the Auth0 console Advanced Editing the Dockerfile A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. In most cases, you will not need to change the Dockerfile when deploying your app. If your app uses packages that have additional system dependencies, you will need to add these in the Dockerfile . If you are unsure how to do this, contact the Analytical Platform team. A Dockerfile reference can be found in the Docker documentation . Getting details of current users of the app An RShiny app can find out who is using it.\\nThis can be useful to log an audit trail of significant events.\\nSpecifically, it can determine the email address that the user logged into the app with.\\nThis is sensitive data, so you must ensure that you are following all relevant information governance processes. The shiny-headers-demo repository contains an example of how to do this. These features are only available in the Analytical Platform version of shiny-server and the open source shiny server image provided by AP ( see details of both options below ). Finding current users’ email addresses You can obtain the logged in user’s email address by using the following code in the server function of your app: get ( \"HTTP_USER_EMAIL\" , envir = session $ request ) This line in shiny-headers-demo shows the code in context. NOTE : Ensure the latest AuthProxy image is being used or the user email header will not be accessible. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 882}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd0cd6ea27e9c8a5f40b7bb7f08736fb2'}>,\n",
       " <Document: {'content': 'This is defined in the helm upgrade command in the GitHub actions files that handle deployments - build-push-deploy-dev.yml and build-push-deploy-prod.yml . Check these files for AuthProxy.Image.Tag and set it to latest as the example below: --set AuthProxy.Image.Tag=\"latest\" \\\\ A full example in a working deployed app can be found here . Finding current users’ user profiles You can access the full user profile by making a request directly from the RShiny app to the auth-proxy’s /userinfo endpoint using the following code inside your server function. # library(httr) # library(jsonlite) profile <- fromJSON ( content ( GET ( \"http://localhost:3001/userinfo\" , add_headers ( cookie = get ( \"HTTP_COOKIE\" , envir = session $ request ))), \"text\" )) This line shows the code in context. Example response { \"email\" : \"name@example.gov.uk\" , \"email_verified\" : true , \"user_id\" : \"email|12345121312\" , \"picture\" : \"https://s.gravatar.com/avatar/94deebe3b87fc5e9b3b4469112573cc0?s=480&r=pg&d=https%3A%2F%2Fcdn.auth0.com%2Favatars%2Fna.png\" , \"nickname\" : \"name\" , \"identities\" : [ { \"user_id\" : \"12345121312\" , \"provider\" : \"email\" , \"connection\" : \"email\" , \"isSocial\" : false } ], \"updated_at\" : \"2019-07-15T09:54:34.353Z\" , \"created_at\" : \"2018-07-12T14:26:45.663Z\" , \"name\" : \"name@example.gov.uk\" , \"last_ip\" : \"1.1.1.1\" , \"last_login\" : \"2019-07-15T09:54:34.353Z\" , \"logins_count\" : 20 , \"blocked_for\" : [], \"guardian_authenticators\" : [] } user-profile caching Due to the auth0 rate limit for /userinfo , the user-profile will be\\ncached for 10 minutes on auth-proxy. If somehow your app receives an exception, for example, token-expired, from the above call, you can add /userinfo?force=true to refresh the user-profle by force. Troubleshooting and monitoring Deploying locally If you have a MacBook, you can use Docker locally to test and troubleshoot your RShiny app.\\nYou can download Docker Desktop for Mac from the Docker website . To build and run your R Shiny app locally, follow the steps below: Clone your app’s repository to a new folder on your MacBook – this guarantees that the app will be built using the same code as on the Analytical Platform. You may need to create a new connection to GitHub with SSH . Open a terminal session and navigate to the directory containing the Dockerfile using the cd command. Build the Docker image by running: {bash}\\ndocker build . -t IMAGE:TAG where IMAGE is a name for the image, for example, my-docker-image , and TAG is the version number, for example, 0.1 . Run a Docker container created from the Docker image by running: {bash}\\ndocker run -p 80:9999 IMAGE:TAG Go to 127.0.0.1:80 to view the app. If the app does not work, follow the steps below to troubleshoot it: Start a bash session in a Docker container created from the Docker image by running: {bash}\\ndocker run -it -p 80:80 IMAGE:TAG bash Install the nano text editor by running: {bash}\\napt-get update\\napt-get install nano Open shiny-server.conf in the nano text editor by running: {bash}\\nnano /etc/shiny-server/shiny-server.conf Add the following lines at the beginning of shiny-server.conf : {bash}\\naccess_log /var/log/shiny-server/access.log tiny;\\npreserve_logs true; Write the changes by pressing Ctrl+O . Exit the nano text editor by pressing Ctrl+X . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 883}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cc1b192562e75bd4fe701d30aa955c7b'}>,\n",
       " <Document: {'content': 'Increase the verbosity of logging and start the Shiny server by running: {bash}\\nexport SHINY_LOG_LEVEL=TRACE\\n/bin/shiny-server.sh Open a new terminal session. Start a new bash session in the Docker container by running: {bash}\\ndocker exec -it CONTAINER_ID bash You can find the CONTAINER_ID by running docker ps . View the logs by running: {bash}\\ncat /var/log/shiny-server/access.log For further details, see the Shiny server documentation . Managing and Monitoring Deployments Monitoring Deployments By default all applications deployed into Cloud platform have a basic level of monitoring which can be accessed on the following link Cloud Platform grafana namespace monitoring . Grafana dashboards are accessed using single-sign-on (SSO) via your GitHub account. This page shows all namespaces and can be refined by typing the name of your namespace in namespace drop down and selecting as needed. Note: Kubernetes defines Limits as the maximum amount of a resource to be used by a container. This means that the container can never consume more than the memory amount or CPU amount indicated. Requests, on the other hand, are the minimum guaranteed amount of a resource that is reserved for a container. A brief overview of namespaces Within Kubernetes a namespace provides a mechanism for isolating groups of resources within a single cluster, it can be thought of as a virtual cluster within the cluster. Your Application is deployed into its own namespace, this restricts access to your team and enables the setting of resource limits. Within the namespace are the various Kubernetes components: Pods the smallest deployable units of computing that you can create and manage in Kubernetes, usually one pod per function of your application ie web server, db server. Service is a method for exposing a network application that is running as one or more Pods in your cluster, basically simplifying the connections within your namespace. Deployment provides declarative updates for Pods and ReplicaSets. ReplicaSet’s their purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods. What information is displayed on the Grafana namespace chart: Namespace(s) usage on total cluster CPU in % - This is the total usage by all namespaces on the cluster Namespace(s) usage on total cluster RAM in % - This is the total usage by all namespaces on the cluster Kubernetes Resource Count - A simple count of resources running within your namespace. CPU usage by Pod - CPU usage per pod within your namespace. Memory usage by Pod - Memory usage per pod within your namespace. Nb of pods by state - Count of pods at various states  within your namespace. Nb of containers by pod - Number of containers within each pod. Replicas available by deployment - the number of replicas available to your deployment. Replicas unavailable by deployment - the number of replicas unavailable to your deployment. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 884}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '37e470b490a5867cd73892f7de59f578'}>,\n",
       " <Document: {'content': 'Persistent Volumes Capacity - If your application uses persistent volumes the total capacity is displayed here. Persistent Volumes Inodes - If your application uses persistent volumes the detail on the inodes is displayed here. Further technical details can be found in the Cloud Platform’s Monitoring section of the user guidance and Configuring a dashboard using Grafana UI documentation Accessing logs You can access your applications logs in Cloud platform by following the the CP guidance Accessing Application Log Data Kibana on Cloud Platform Below are some notes to aid you in working with the Kibana service, on Cloud Platform. All logs from deployed apps can be viewed in Kibana . To view the logs for a specific app: Select live_kubernetes_cluster from the CHANGE INDEX PATTERN dropdown list. Select Add a filter . Select kubernetes.namespace_name . Select is as the operator. Insert the app’s name space by following the pattern data-platform-app-<app_name>-<dev/prod> . In order to filter out all the logs related health-check, you can put NOT log:  \"/healthz\" in the KQL field. Select Save . (Optional) - you can select to view only the log output by adding it from the Available Fields list in the left hand pane using the (+) button revealed on mouse hover Log messages are displayed in the message column. By default, Kibana only shows logs for the last 15 minutes.\\nIf no logs are available for that time range, you will receive the warning ‘No results match your search criteria’. To change the time range, select the clock icon in the menu bar.\\nThere are several presets or you can define a custom time range. Kibana also has experimental autocomplete and simple syntax tools that you can use to build custom searches.\\nTo enable these features, select Options_ from within the search bar, then toggle Turn on query features . Managing Deployments Cloud platform  allows teams to connect to the Kubernetes cluster and manage their applications, using kubectl , the command line tool for Kubernetes. To do this you will need to setup access through JupyterLab as follows: Go to Analytical Platform Control Panel ‘> Analytical Tools ’> open JupyterLab Data Science ‘> Terminal Installing Kubectl In the terminal session install kubectl curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" curl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\" echo \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check If valid, the output is: kubectl: OK chmod +x kubectl mkdir -p ~/.local/bin mv ./kubectl ~/.local/bin/kubectl which kubectl you should see: /home/jovyan/.local/bin/kubectl You should only need to carry out the above steps once. Setup Cloud Platform Access Follow the steps in CP’s Generating a Kubeconfig file documentation . As we are accessing via Jupyter Labs  step 7 “Move the config file to the location expected by kubectl” has to be carried out slightly differently. First upload the kubecfg.yaml from your local  PC to the Jupyter terminal session. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 885}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '56e1a52cf3548eb5c3940101fe33abe8'}>,\n",
       " <Document: {'content': 'click on the upload arrow (see below) and select the kubecfg.yaml Then continue with the remaining steps in CP’s documentation Accessing the AWS console If required you can access the Cloud platform via AWS, please follow the guidance in Accessing the AWS console (read-only) Adding cron job to your application A cronjob for restarting your application can be setup easier by adding the following line in to your app’s development or production GitHub workflow: --set Cron.schedule=\"0 6 * * *\" crontab.guru can be used to setup the schedule. If you need to a cron job for the other jobs, more guides are available on the Cloud Platform kubernetes cronjobs Changing the resources available to the application When you deploy an update to the application the Kubernetes resources are built from a standard Helm chart with the following default memory/cpu resources: Limits -  CPU 25m.  Memory 1Gi Requests - CPU 3m.  Memory 1Gi To override this you will have to amend the GitHub  workflow in your application repository The file(s) to amend are .github/workflows/build-push-deploy-dev.yml or build-push-deploy-prod.yml Below is the section of code that calls the helm chart helm upgrade --install --wait --timeout 10m0s --namespace $KUBE_NAMESPACE $KUBE_NAMESPACE mojanalytics/webapp-cp \\\\\\n--set AuthProxy.Env.Auth0Domain=$AUTH0_DOMAIN \\\\\\n--set AuthProxy.Env.Auth0Passwordless=$AUTH0_PASSWORDLESS \\\\\\n--set AuthProxy.Env.Auth0TokenAlg=$AUTH0_TOKEN_ALG \\\\\\n--set AuthProxy.Env.AuthenticationRequired=$AUTHENTICATION_REQUIRED \\\\\\n--set AuthProxy.Env.IPRanges=$process_ip_range \\\\\\n--set AuthProxy.Image.Repository=$ECR_REPO_AUTH0 \\\\\\n--set AuthProxy.Image.Tag=\"latest\" \\\\\\n--set Namespace=$KUBE_NAMESPACE \\\\\\n--set Secrets.Auth0.ClientId=$AUTH0_CLIENT_ID \\\\\\n--set Secrets.Auth0.ClientSecret=$AUTH0_CLIENT_SECRET \\\\\\n--set Secrets.Auth0.CookieSecret=$COOKIE_SECRET \\\\\\n--set ServiceAccount.RoleARN=$APP_ROLE_ARN \\\\\\n--set WebApp.Image.Repository=$ECR_REPO_WEBAPP \\\\\\n--set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n$custom_variables ``` To change the resources  insert at  the end of the file just before the $custom_variables the following as appropriate, remembering to use the line continuation “\\\\” on the existing last line. `--set WebApp.resources.limits.cpu=100m \\\\`\\n`--set WebApp.resources.limits.memory=2Gi \\\\`\\n`--set WebApp.resources.requests.cpu=50m \\\\`\\n`--set WebApp.resources.requests.memory=2Gi \\\\`\\n`$custom_variables` Once the changes are pushed/merged to the repository the values will be applied to your Kubernetes namespace. Changing the number of instances (pods) on name space The number of app instances running on dev / prod is 1 by default, if users experience long response times from the application (apart from trying to improving the performance through the code itself) you can increase the number of instances to reduce the wating time on dev / prod GitHub workflows, for example :- --set ReplicaCount=3 Remember: “With great power comes great responsibility”  Your application’s namespace will be one of a number hosted on the same cluster, setting the values too high could crash the cluster! Storage guidance Guidance on storage within Cloud Platform can be found here Cloud platform storage Continuous Deployment Within Cloud platform your application is already automatically deployed by Github workflows but further guidance on continuous deployment within  CP can be found here Cloud Platform continuous deployment. Deploying Updates to the Application As the application is now hosted in Cloud Platform and GitHub workflow has been implemented, you will now be able to apply update to your application, full guidance can be found here Application deployment. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 886}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd51d9919c0c47dda2977f9e577cb26d6'}>,\n",
       " <Document: {'content': 'Other guidance Guidance on  managing Auth and Secrets through the Control Panel can be found Manage deployment settings of an app on Control panel. Shiny server There are a few choices for running a rshiny app on Cloud Platform. The team responsible for developing and maintaining the dashboard app has full control to choose the best way and practices to run the app by building their own Dockerfile. Since the shiny-framework uses websocket as the primary communication protocol between frontend and backend, no matter which choice you decide, the minimum capabilities required are: Keeping the connection live as long as possible e.g. implementing heart-beat mechanism. Being able to handle the lifecyle of session Providing a method for reconnecting when the websocket drops Working with auth-proxy, the AP component responsible for controlling user’s access,  unless the app is public facing or application handles authentication itself. The AP team offers two shiny-server solutions. AP version of shiny-server We developed a mini version of shiny-sever in nodejs, it provides a minimal implementation to support the required capabilities: Uses sockjs which supports heart-beat Create a new session for each new websocket connection Will try to reconnect to the shiny app automatically when the websocket connection drops If reconnection repeatedly fails and reaches the maximum number of attempts, a window will be appear asking the user to trigger a manual reconnect The majority of the shiny apps hosted on Cloud Platform use this version. The current tag for this docker image is: 593291632749.dkr.ecr.eu-west-1.amazonaws.com/rshiny:r4.1.3-shiny0.0.6 Open source shiny-server We also provide a solution for using the open source Shiny Server with a few minor tweaks to support USER_EMAIL and COOKIE headers.  The base docker image is defined here . The version of open source shiny server is defined by SHINY_SERVER_VERSION , currently set to 1.5.20.1002 . It offers more features than the AP shiny server and supports: Better reconnection behaviour: Reconnect and load existing session rather than creating a new session automatically If reconnection continues to fail, the manual reconnection pop-up will trigger an app reload rather than re-triggering the same reconnection behaviour Better session management: The session will be closed when the session is idle for a certain period of time This behaviour can result in:\\n- Session data (reactive values) is retained even after a reconnection happens\\n- Release resources e.g. memory linked to the session which avoids potential memory leaking It also provides more configuration options as outlined here . Note: options marked as “pro” are not available Instructions for using the open source shiny server image Example Dockerfile The following example can be used as the starting point when making your own Dockerfile # The base docker image\\nFROM ghcr.io/ministryofjustice/data-platform-rshiny-open-source-base:1.0.3\\n\\n# ** Optional step: only if some of R pakcages requires the system libraries which are not covered by base image\\n#   the one in the example below has been provided in base image.\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 887}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '36085d176f6a14e2924c0433c2257df6'}>,\n",
       " <Document: {'content': '# RUN apt-get update \\\\\\n#   && apt-get install -y --no-install-recommends \\\\\\n#     libglpk-dev\\n\\n# use renv for packages\\nADD renv.lock renv.lock\\n\\n# Install R packages\\nRUN R -e \"install.packages(\\'renv\\'); renv::restore()\"\\n\\n# ** Optional step: only if the app requires python packages\\n# Make sure reticulate uses the system Python\\n# ENV RETICULATE_PYTHON=\"/usr/bin/python3\"\\n# ensure requirements.txt exists (created automatically when making a venv in renv)\\n# COPY requirements.txt requirements.txt\\n# RUN python3 -m pip install -r requirements.txt\\n\\n# Add shiny app code\\nADD . .\\n\\nUSER 998 Instructions for switching from the AP shiny server to the open-source server If you already use the AP shiny server, and would like to switch to the open source server, the key changes you need to make are: Change the base docker image in your Dockerfile: FROM ghcr.io/ministryofjustice/data-platform-rshiny-open-source-base:1.0.3 If present, ensure the following redundant parts of your Dockerfile are removed: ENV PATH=\"/opt/shiny-server/bin:/opt/shiny-server/ext/node/bin:${PATH}\"\\nENV SHINY_APP=/srv/shiny-server\\nENV NODE_ENV=production RUN chown shiny:shiny /srv/shiny-server\\nCMD analytics-platform-shiny-server\\nEXPOSE 9999 Update GitHub actions work flows Assuming the app uses the GitHub actions workflows from AP, the following parameters for helm installation are required in both the build-push-deploy-dev.yml and build-push-deploy-prod.yml github action workflow files: WebApp.AlternativeHealthCheck.enabled=\"true\"\\nWebApp.AlternativeHealthCheck.port=9999 A complete example for installing the helm chart in the workflow below. These changes will need to be made in both: helm upgrade --install --wait --timeout 10m0s --namespace $KUBE_NAMESPACE $RELEASE_NAME mojanalytics/webapp-cp \\\\\\n--set AuthProxy.Env.Auth0Domain=$AUTH0_DOMAIN \\\\\\n--set AuthProxy.Env.Auth0Passwordless=$AUTH0_PASSWORDLESS \\\\\\n--set AuthProxy.Env.Auth0TokenAlg=$AUTH0_TOKEN_ALG \\\\\\n--set AuthProxy.Env.AuthenticationRequired=$AUTHENTICATION_REQUIRED \\\\\\n--set AuthProxy.Env.IPRanges=$process_ip_range \\\\\\n--set AuthProxy.Image.Repository=$ECR_REPO_AUTH0 \\\\\\n--set AuthProxy.Image.Tag=\"latest\" \\\\\\n--set Namespace=$KUBE_NAMESPACE \\\\\\n--set WebApp.AlternativeHealthCheck.enabled=\"true\" \\\\\\n--set WebApp.AlternativeHealthCheck.port=9999 \\\\\\n--set Secrets.Auth0.ClientId=$AUTH0_CLIENT_ID \\\\\\n--set Secrets.Auth0.ClientSecret=$AUTH0_CLIENT_SECRET \\\\\\n--set Secrets.Auth0.CookieSecret=$COOKIE_SECRET \\\\\\n--set ServiceAccount.RoleARN=$APP_ROLE_ARN \\\\\\n--set WebApp.Image.Repository=$ECR_REPO_WEBAPP \\\\\\n--set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n$custom_variables A full working example is available here Enabling Web Application Firewall (WAF) for Data Platform Apps There are two flags that need to be set to enable WAF for your application, Ingress.ModSec.enabled and GithubTeam To do this modify your app’s build-push-deploy-dev.yml and build-push-deploy-prod.yml GitHub action workflow files as follows: --set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n--set Ingress.ModSec.enabled=\"true\" \\\\\\n--set GithubTeam=\"your github team\" \\\\\\n$custom_variables The changes  will be will be applied to your Kubernetes namespace following a push/merge to the repository and the running of the workflow To disable –set Ingress.ModSec.enabled=“false” This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDeploy an RShiny app - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 888}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4068b35394414be6bc55dc905a46d88d'}>,\n",
       " <Document: {'content': 'Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 889}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f5a617e2b8bf3e97f303a94c8b4da304'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools R Shiny app publishing Once you’ve built your Shiny app, you can make it available to users through the Analytical Platform.\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 890}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b6d010c909f0a5eb3c74b9ee21db196a'}>,\n",
       " <Document: {'content': 'We have guidance for: Deploying your app Accessing your deployed app Managing your app users Managing and Monitoring Deployments as well as common issues faced during these stages of publishing your app. ⚠️ Note on deployment ⚠️ It is not currently possible to deploy new apps on the Analytical Platform, though we are working to make this functionality available again as soon as possible. In the meantime, you can still access and manage existing apps. If you have an existing app that requires urgent redeployment, please submit a request via GitHub.\\nWe normally redeploy apps each Wednesday, where we have recevied a request by the Friday before. App deployment New apps It is not currently possible to deploy new apps on the Analytical Platform, though we are working to make this functionality available again as soon as possible. Existing apps Overview Your app environment is made up of two key elements: A GitHub workflow A Cloud Platform namespace In brief, the workflow builds and deploys your code as a docker container, and then deploys it to Cloud Platform’s kubernetes cluster, in your application’s namespace. The flow looks something like this: The rationale behind this change is to: Facilitate testing - including a working development app version and build artifacts like docker containers - before deploying to production Give teams more control over their workflows Restore the ability to deploy without the Analytical Platform team’s intervention Apps hosted on Cloud Platform After the move to Cloud Platform hosting for Analytical Platform apps, you’ll have two active deployments of your apps at all times.\\nThese are your ‘dev’ (development) and ‘prod’ (production) deployments. Be aware that both production and dev currently point to the same data. Your code repository within the ministryofjustice organisation was built from the data-platform-app-template repo , and has inherited the continuous integration and continuous delivery (CI/CD) pipelines (GitHub Action workflows) from that repo.\\nThese workflows will automatically build your docker images, push them to a remote image store, and then deploy the application based on how you’ve made your code changes: Deploying your application to your development environment is done through the -dev workflow, which will build-push-deploy when any pull request is made in the repo (e.g. feature-branch into main , or feature-branch into develop-branch ).\\nThe workflow can also be manually triggered, though this will only build and push the app, it will not deploy it: repo homepage > Actions (tab) > Run workflow (right hand side of the page). Opening any pull request will trigger a dev deployment.\\nAny subsequent pushes to that branch (or to any other open branches) will trigger dev deployments too.\\nIf you’re working on multiple PRs at once in the repo that you will need to coordinate pushes to your PRs so you can track your deployments.\\nYou can always cancel workflow runs and rerun deployments from the Actions page in the repo. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 891}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '92b2bb3a27242c740eeffdb99b1ea273'}>,\n",
       " <Document: {'content': 'Deploying your application to your production environment is done through the -prod workflow, which will build-push-deploy either: after a pull request is merged into main upon publishing a release You can view the status of your deployments either by checking the workflow runs in the Actions tab (repo-url/actions), or by checking out the deployments page (repo-url/deployments). The above describes how CI/CD will be set up by default in the ministryofjustice repo.\\nOnce you have ownership of the repo, you’ll have the ownership of the .github/workflow/ files too so you will be able to amend the processes and triggers so that they meet your needs. Your app deployment pipeline GitHub Actions has workflow definitions located in your repository under .github/workflows/ . By default, you will be provided with two workflows.\\nThe dev environment workflow is triggered when you open or contribute to a Pull Request from any branch (for example, into your main branch).\\nIts steps are: Check out the repository Authenticate to AWS Build a docker container from your development branch If the build is successful, push this container to a container registry Run helm against the development environment namespace. The production workflow behaves in the exact same way, containing the same steps, but will be triggered when a Pull Request is merged into main , and will deploy to the production namespace instead Managing published apps Manage app users If authentication is enabled and you choose to use email as the login option, user access management to app can be done through Control panel. To manage the users of your app: Login Control panel as app admin Go to the Analytical Platform control panel . Select the Webapps tab. Select the name of the app you want to manage or select Manage customers . Select the environment where app is deployed ( dev/prod ) To add app users: Enter the email addresses of the users in the box titled ‘Add app customers by entering their email addresses’ – multiple email addresses can be separated by spaces, tabs, commas and semicolons, and can contain capital letters. Select Add customer . To remove an app user, select Remove customer next to the email address of the customer you want to remove. Accessing the deployed app Apps hosted on Cloud Platform Your deployed app can be accessed at two URLs: prod is at: repository-name.apps.live.cloud-platform.service.justice.gov.uk dev is at: repository-name-dev.apps.live.cloud-platform.service.justice.gov.uk (where repository-name is the name of the relevant GitHub repository) By default, the user list on dev empty and you will need to add any users requiring access via control panel. Authenticating to your app For the dashboard apps using passwordless flow (email login), when accessing an app, you can choose whether to sign in using a one-time passcode (default) or an email magic link.\\nTo sign in with an email magic link, add /login?method=link to the end of the app’s URL, for example, https://apps.live.cloud-platform.service.justice.gov.uk/login?method=code . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 892}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dd98764134c9ea4bf690bf131ac66a5f'}>,\n",
       " <Document: {'content': 'Troubleshooting app sign-in “That email address is not authorized for this app (or possibly another error occurred)” error, after entering email address Check that the user is authorised to access the app: Log in to the control panel . Navigate to the app detail page. Choose the right deployment environment (dev/prod) Check if the user’s email address is listed under ‘App customers’. If it is not, refer them to the app owner to obtain access. Check that the user is using the correct email address – there is sometimes confusion between @justice and @digital.justice email addresses. “Access denied” error, having entered email address and copied the one-time passcode into the login page Please follow the same steps above to check whether the user is in the customer list of the app. “IP x.x.x.x is not whitelisted” Check that the user is trying to access the app from one of the trusted networks listed on app’s app-detail from Control panel The app admin can modify the IP_Ranges on the app’s app-detail detail page. Other troubleshooting tips Check that they are trying to access the app using a URL beginning with https:// not http:// . Look for similar issues log in the data-platform-support repository . Try asking the user to clear their cookies by visiting https://alpha-analytics-moj.eu.auth0.com/logout and try again. In addition the AP team can: Check the Auth0 logs for the app in Kibana Check the Auth0 logs in the Auth0 console Advanced Editing the Dockerfile A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. In most cases, you will not need to change the Dockerfile when deploying your app. If your app uses packages that have additional system dependencies, you will need to add these in the Dockerfile . If you are unsure how to do this, contact the Analytical Platform team. A Dockerfile reference can be found in the Docker documentation . Getting details of current users of the app An RShiny app can find out who is using it.\\nThis can be useful to log an audit trail of significant events.\\nSpecifically, it can determine the email address that the user logged into the app with.\\nThis is sensitive data, so you must ensure that you are following all relevant information governance processes. The shiny-headers-demo repository contains an example of how to do this. These features are only available in the Analytical Platform version of shiny-server and the open source shiny server image provided by AP ( see details of both options below ). Finding current users’ email addresses You can obtain the logged in user’s email address by using the following code in the server function of your app: get ( \"HTTP_USER_EMAIL\" , envir = session $ request ) This line in shiny-headers-demo shows the code in context. NOTE : Ensure the latest AuthProxy image is being used or the user email header will not be accessible. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 893}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd0cd6ea27e9c8a5f40b7bb7f08736fb2'}>,\n",
       " <Document: {'content': 'This is defined in the helm upgrade command in the GitHub actions files that handle deployments - build-push-deploy-dev.yml and build-push-deploy-prod.yml . Check these files for AuthProxy.Image.Tag and set it to latest as the example below: --set AuthProxy.Image.Tag=\"latest\" \\\\ A full example in a working deployed app can be found here . Finding current users’ user profiles You can access the full user profile by making a request directly from the RShiny app to the auth-proxy’s /userinfo endpoint using the following code inside your server function. # library(httr) # library(jsonlite) profile <- fromJSON ( content ( GET ( \"http://localhost:3001/userinfo\" , add_headers ( cookie = get ( \"HTTP_COOKIE\" , envir = session $ request ))), \"text\" )) This line shows the code in context. Example response { \"email\" : \"name@example.gov.uk\" , \"email_verified\" : true , \"user_id\" : \"email|12345121312\" , \"picture\" : \"https://s.gravatar.com/avatar/94deebe3b87fc5e9b3b4469112573cc0?s=480&r=pg&d=https%3A%2F%2Fcdn.auth0.com%2Favatars%2Fna.png\" , \"nickname\" : \"name\" , \"identities\" : [ { \"user_id\" : \"12345121312\" , \"provider\" : \"email\" , \"connection\" : \"email\" , \"isSocial\" : false } ], \"updated_at\" : \"2019-07-15T09:54:34.353Z\" , \"created_at\" : \"2018-07-12T14:26:45.663Z\" , \"name\" : \"name@example.gov.uk\" , \"last_ip\" : \"1.1.1.1\" , \"last_login\" : \"2019-07-15T09:54:34.353Z\" , \"logins_count\" : 20 , \"blocked_for\" : [], \"guardian_authenticators\" : [] } user-profile caching Due to the auth0 rate limit for /userinfo , the user-profile will be\\ncached for 10 minutes on auth-proxy. If somehow your app receives an exception, for example, token-expired, from the above call, you can add /userinfo?force=true to refresh the user-profle by force. Troubleshooting and monitoring Deploying locally If you have a MacBook, you can use Docker locally to test and troubleshoot your RShiny app.\\nYou can download Docker Desktop for Mac from the Docker website . To build and run your R Shiny app locally, follow the steps below: Clone your app’s repository to a new folder on your MacBook – this guarantees that the app will be built using the same code as on the Analytical Platform. You may need to create a new connection to GitHub with SSH . Open a terminal session and navigate to the directory containing the Dockerfile using the cd command. Build the Docker image by running: {bash}\\ndocker build . -t IMAGE:TAG where IMAGE is a name for the image, for example, my-docker-image , and TAG is the version number, for example, 0.1 . Run a Docker container created from the Docker image by running: {bash}\\ndocker run -p 80:9999 IMAGE:TAG Go to 127.0.0.1:80 to view the app. If the app does not work, follow the steps below to troubleshoot it: Start a bash session in a Docker container created from the Docker image by running: {bash}\\ndocker run -it -p 80:80 IMAGE:TAG bash Install the nano text editor by running: {bash}\\napt-get update\\napt-get install nano Open shiny-server.conf in the nano text editor by running: {bash}\\nnano /etc/shiny-server/shiny-server.conf Add the following lines at the beginning of shiny-server.conf : {bash}\\naccess_log /var/log/shiny-server/access.log tiny;\\npreserve_logs true; Write the changes by pressing Ctrl+O . Exit the nano text editor by pressing Ctrl+X . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 894}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cc1b192562e75bd4fe701d30aa955c7b'}>,\n",
       " <Document: {'content': 'Increase the verbosity of logging and start the Shiny server by running: {bash}\\nexport SHINY_LOG_LEVEL=TRACE\\n/bin/shiny-server.sh Open a new terminal session. Start a new bash session in the Docker container by running: {bash}\\ndocker exec -it CONTAINER_ID bash You can find the CONTAINER_ID by running docker ps . View the logs by running: {bash}\\ncat /var/log/shiny-server/access.log For further details, see the Shiny server documentation . Managing and Monitoring Deployments Monitoring Deployments By default all applications deployed into Cloud platform have a basic level of monitoring which can be accessed on the following link Cloud Platform grafana namespace monitoring . Grafana dashboards are accessed using single-sign-on (SSO) via your GitHub account. This page shows all namespaces and can be refined by typing the name of your namespace in namespace drop down and selecting as needed. Note: Kubernetes defines Limits as the maximum amount of a resource to be used by a container. This means that the container can never consume more than the memory amount or CPU amount indicated. Requests, on the other hand, are the minimum guaranteed amount of a resource that is reserved for a container. A brief overview of namespaces Within Kubernetes a namespace provides a mechanism for isolating groups of resources within a single cluster, it can be thought of as a virtual cluster within the cluster. Your Application is deployed into its own namespace, this restricts access to your team and enables the setting of resource limits. Within the namespace are the various Kubernetes components: Pods the smallest deployable units of computing that you can create and manage in Kubernetes, usually one pod per function of your application ie web server, db server. Service is a method for exposing a network application that is running as one or more Pods in your cluster, basically simplifying the connections within your namespace. Deployment provides declarative updates for Pods and ReplicaSets. ReplicaSet’s their purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods. What information is displayed on the Grafana namespace chart: Namespace(s) usage on total cluster CPU in % - This is the total usage by all namespaces on the cluster Namespace(s) usage on total cluster RAM in % - This is the total usage by all namespaces on the cluster Kubernetes Resource Count - A simple count of resources running within your namespace. CPU usage by Pod - CPU usage per pod within your namespace. Memory usage by Pod - Memory usage per pod within your namespace. Nb of pods by state - Count of pods at various states  within your namespace. Nb of containers by pod - Number of containers within each pod. Replicas available by deployment - the number of replicas available to your deployment. Replicas unavailable by deployment - the number of replicas unavailable to your deployment. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 895}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '37e470b490a5867cd73892f7de59f578'}>,\n",
       " <Document: {'content': 'Persistent Volumes Capacity - If your application uses persistent volumes the total capacity is displayed here. Persistent Volumes Inodes - If your application uses persistent volumes the detail on the inodes is displayed here. Further technical details can be found in the Cloud Platform’s Monitoring section of the user guidance and Configuring a dashboard using Grafana UI documentation Accessing logs You can access your applications logs in Cloud platform by following the the CP guidance Accessing Application Log Data Kibana on Cloud Platform Below are some notes to aid you in working with the Kibana service, on Cloud Platform. All logs from deployed apps can be viewed in Kibana . To view the logs for a specific app: Select live_kubernetes_cluster from the CHANGE INDEX PATTERN dropdown list. Select Add a filter . Select kubernetes.namespace_name . Select is as the operator. Insert the app’s name space by following the pattern data-platform-app-<app_name>-<dev/prod> . In order to filter out all the logs related health-check, you can put NOT log:  \"/healthz\" in the KQL field. Select Save . (Optional) - you can select to view only the log output by adding it from the Available Fields list in the left hand pane using the (+) button revealed on mouse hover Log messages are displayed in the message column. By default, Kibana only shows logs for the last 15 minutes.\\nIf no logs are available for that time range, you will receive the warning ‘No results match your search criteria’. To change the time range, select the clock icon in the menu bar.\\nThere are several presets or you can define a custom time range. Kibana also has experimental autocomplete and simple syntax tools that you can use to build custom searches.\\nTo enable these features, select Options_ from within the search bar, then toggle Turn on query features . Managing Deployments Cloud platform  allows teams to connect to the Kubernetes cluster and manage their applications, using kubectl , the command line tool for Kubernetes. To do this you will need to setup access through JupyterLab as follows: Go to Analytical Platform Control Panel ‘> Analytical Tools ’> open JupyterLab Data Science ‘> Terminal Installing Kubectl In the terminal session install kubectl curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" curl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\" echo \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check If valid, the output is: kubectl: OK chmod +x kubectl mkdir -p ~/.local/bin mv ./kubectl ~/.local/bin/kubectl which kubectl you should see: /home/jovyan/.local/bin/kubectl You should only need to carry out the above steps once. Setup Cloud Platform Access Follow the steps in CP’s Generating a Kubeconfig file documentation . As we are accessing via Jupyter Labs  step 7 “Move the config file to the location expected by kubectl” has to be carried out slightly differently. First upload the kubecfg.yaml from your local  PC to the Jupyter terminal session. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 896}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '56e1a52cf3548eb5c3940101fe33abe8'}>,\n",
       " <Document: {'content': 'click on the upload arrow (see below) and select the kubecfg.yaml Then continue with the remaining steps in CP’s documentation Accessing the AWS console If required you can access the Cloud platform via AWS, please follow the guidance in Accessing the AWS console (read-only) Adding cron job to your application A cronjob for restarting your application can be setup easier by adding the following line in to your app’s development or production GitHub workflow: --set Cron.schedule=\"0 6 * * *\" crontab.guru can be used to setup the schedule. If you need to a cron job for the other jobs, more guides are available on the Cloud Platform kubernetes cronjobs Changing the resources available to the application When you deploy an update to the application the Kubernetes resources are built from a standard Helm chart with the following default memory/cpu resources: Limits -  CPU 25m.  Memory 1Gi Requests - CPU 3m.  Memory 1Gi To override this you will have to amend the GitHub  workflow in your application repository The file(s) to amend are .github/workflows/build-push-deploy-dev.yml or build-push-deploy-prod.yml Below is the section of code that calls the helm chart helm upgrade --install --wait --timeout 10m0s --namespace $KUBE_NAMESPACE $KUBE_NAMESPACE mojanalytics/webapp-cp \\\\\\n--set AuthProxy.Env.Auth0Domain=$AUTH0_DOMAIN \\\\\\n--set AuthProxy.Env.Auth0Passwordless=$AUTH0_PASSWORDLESS \\\\\\n--set AuthProxy.Env.Auth0TokenAlg=$AUTH0_TOKEN_ALG \\\\\\n--set AuthProxy.Env.AuthenticationRequired=$AUTHENTICATION_REQUIRED \\\\\\n--set AuthProxy.Env.IPRanges=$process_ip_range \\\\\\n--set AuthProxy.Image.Repository=$ECR_REPO_AUTH0 \\\\\\n--set AuthProxy.Image.Tag=\"latest\" \\\\\\n--set Namespace=$KUBE_NAMESPACE \\\\\\n--set Secrets.Auth0.ClientId=$AUTH0_CLIENT_ID \\\\\\n--set Secrets.Auth0.ClientSecret=$AUTH0_CLIENT_SECRET \\\\\\n--set Secrets.Auth0.CookieSecret=$COOKIE_SECRET \\\\\\n--set ServiceAccount.RoleARN=$APP_ROLE_ARN \\\\\\n--set WebApp.Image.Repository=$ECR_REPO_WEBAPP \\\\\\n--set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n$custom_variables ``` To change the resources  insert at  the end of the file just before the $custom_variables the following as appropriate, remembering to use the line continuation “\\\\” on the existing last line. `--set WebApp.resources.limits.cpu=100m \\\\`\\n`--set WebApp.resources.limits.memory=2Gi \\\\`\\n`--set WebApp.resources.requests.cpu=50m \\\\`\\n`--set WebApp.resources.requests.memory=2Gi \\\\`\\n`$custom_variables` Once the changes are pushed/merged to the repository the values will be applied to your Kubernetes namespace. Changing the number of instances (pods) on name space The number of app instances running on dev / prod is 1 by default, if users experience long response times from the application (apart from trying to improving the performance through the code itself) you can increase the number of instances to reduce the wating time on dev / prod GitHub workflows, for example :- --set ReplicaCount=3 Remember: “With great power comes great responsibility”  Your application’s namespace will be one of a number hosted on the same cluster, setting the values too high could crash the cluster! Storage guidance Guidance on storage within Cloud Platform can be found here Cloud platform storage Continuous Deployment Within Cloud platform your application is already automatically deployed by Github workflows but further guidance on continuous deployment within  CP can be found here Cloud Platform continuous deployment. Deploying Updates to the Application As the application is now hosted in Cloud Platform and GitHub workflow has been implemented, you will now be able to apply update to your application, full guidance can be found here Application deployment. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 897}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd51d9919c0c47dda2977f9e577cb26d6'}>,\n",
       " <Document: {'content': 'Other guidance Guidance on  managing Auth and Secrets through the Control Panel can be found Manage deployment settings of an app on Control panel. Shiny server There are a few choices for running a rshiny app on Cloud Platform. The team responsible for developing and maintaining the dashboard app has full control to choose the best way and practices to run the app by building their own Dockerfile. Since the shiny-framework uses websocket as the primary communication protocol between frontend and backend, no matter which choice you decide, the minimum capabilities required are: Keeping the connection live as long as possible e.g. implementing heart-beat mechanism. Being able to handle the lifecyle of session Providing a method for reconnecting when the websocket drops Working with auth-proxy, the AP component responsible for controlling user’s access,  unless the app is public facing or application handles authentication itself. The AP team offers two shiny-server solutions. AP version of shiny-server We developed a mini version of shiny-sever in nodejs, it provides a minimal implementation to support the required capabilities: Uses sockjs which supports heart-beat Create a new session for each new websocket connection Will try to reconnect to the shiny app automatically when the websocket connection drops If reconnection repeatedly fails and reaches the maximum number of attempts, a window will be appear asking the user to trigger a manual reconnect The majority of the shiny apps hosted on Cloud Platform use this version. The current tag for this docker image is: 593291632749.dkr.ecr.eu-west-1.amazonaws.com/rshiny:r4.1.3-shiny0.0.6 Open source shiny-server We also provide a solution for using the open source Shiny Server with a few minor tweaks to support USER_EMAIL and COOKIE headers.  The base docker image is defined here . The version of open source shiny server is defined by SHINY_SERVER_VERSION , currently set to 1.5.20.1002 . It offers more features than the AP shiny server and supports: Better reconnection behaviour: Reconnect and load existing session rather than creating a new session automatically If reconnection continues to fail, the manual reconnection pop-up will trigger an app reload rather than re-triggering the same reconnection behaviour Better session management: The session will be closed when the session is idle for a certain period of time This behaviour can result in:\\n- Session data (reactive values) is retained even after a reconnection happens\\n- Release resources e.g. memory linked to the session which avoids potential memory leaking It also provides more configuration options as outlined here . Note: options marked as “pro” are not available Instructions for using the open source shiny server image Example Dockerfile The following example can be used as the starting point when making your own Dockerfile # The base docker image\\nFROM ghcr.io/ministryofjustice/data-platform-rshiny-open-source-base:1.0.3\\n\\n# ** Optional step: only if some of R pakcages requires the system libraries which are not covered by base image\\n#   the one in the example below has been provided in base image.\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 898}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '36085d176f6a14e2924c0433c2257df6'}>,\n",
       " <Document: {'content': '# RUN apt-get update \\\\\\n#   && apt-get install -y --no-install-recommends \\\\\\n#     libglpk-dev\\n\\n# use renv for packages\\nADD renv.lock renv.lock\\n\\n# Install R packages\\nRUN R -e \"install.packages(\\'renv\\'); renv::restore()\"\\n\\n# ** Optional step: only if the app requires python packages\\n# Make sure reticulate uses the system Python\\n# ENV RETICULATE_PYTHON=\"/usr/bin/python3\"\\n# ensure requirements.txt exists (created automatically when making a venv in renv)\\n# COPY requirements.txt requirements.txt\\n# RUN python3 -m pip install -r requirements.txt\\n\\n# Add shiny app code\\nADD . .\\n\\nUSER 998 Instructions for switching from the AP shiny server to the open-source server If you already use the AP shiny server, and would like to switch to the open source server, the key changes you need to make are: Change the base docker image in your Dockerfile: FROM ghcr.io/ministryofjustice/data-platform-rshiny-open-source-base:1.0.3 If present, ensure the following redundant parts of your Dockerfile are removed: ENV PATH=\"/opt/shiny-server/bin:/opt/shiny-server/ext/node/bin:${PATH}\"\\nENV SHINY_APP=/srv/shiny-server\\nENV NODE_ENV=production RUN chown shiny:shiny /srv/shiny-server\\nCMD analytics-platform-shiny-server\\nEXPOSE 9999 Update GitHub actions work flows Assuming the app uses the GitHub actions workflows from AP, the following parameters for helm installation are required in both the build-push-deploy-dev.yml and build-push-deploy-prod.yml github action workflow files: WebApp.AlternativeHealthCheck.enabled=\"true\"\\nWebApp.AlternativeHealthCheck.port=9999 A complete example for installing the helm chart in the workflow below. These changes will need to be made in both: helm upgrade --install --wait --timeout 10m0s --namespace $KUBE_NAMESPACE $RELEASE_NAME mojanalytics/webapp-cp \\\\\\n--set AuthProxy.Env.Auth0Domain=$AUTH0_DOMAIN \\\\\\n--set AuthProxy.Env.Auth0Passwordless=$AUTH0_PASSWORDLESS \\\\\\n--set AuthProxy.Env.Auth0TokenAlg=$AUTH0_TOKEN_ALG \\\\\\n--set AuthProxy.Env.AuthenticationRequired=$AUTHENTICATION_REQUIRED \\\\\\n--set AuthProxy.Env.IPRanges=$process_ip_range \\\\\\n--set AuthProxy.Image.Repository=$ECR_REPO_AUTH0 \\\\\\n--set AuthProxy.Image.Tag=\"latest\" \\\\\\n--set Namespace=$KUBE_NAMESPACE \\\\\\n--set WebApp.AlternativeHealthCheck.enabled=\"true\" \\\\\\n--set WebApp.AlternativeHealthCheck.port=9999 \\\\\\n--set Secrets.Auth0.ClientId=$AUTH0_CLIENT_ID \\\\\\n--set Secrets.Auth0.ClientSecret=$AUTH0_CLIENT_SECRET \\\\\\n--set Secrets.Auth0.CookieSecret=$COOKIE_SECRET \\\\\\n--set ServiceAccount.RoleARN=$APP_ROLE_ARN \\\\\\n--set WebApp.Image.Repository=$ECR_REPO_WEBAPP \\\\\\n--set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n$custom_variables A full working example is available here Enabling Web Application Firewall (WAF) for Data Platform Apps There are two flags that need to be set to enable WAF for your application, Ingress.ModSec.enabled and GithubTeam To do this modify your app’s build-push-deploy-dev.yml and build-push-deploy-prod.yml GitHub action workflow files as follows: --set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n--set Ingress.ModSec.enabled=\"true\" \\\\\\n--set GithubTeam=\"your github team\" \\\\\\n$custom_variables The changes  will be will be applied to your Kubernetes namespace following a push/merge to the repository and the running of the workflow To disable –set Ingress.ModSec.enabled=“false” This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDeploy an RShiny app - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 899}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4068b35394414be6bc55dc905a46d88d'}>,\n",
       " <Document: {'content': 'Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 900}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f5a617e2b8bf3e97f303a94c8b4da304'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools R Shiny app publishing Once you’ve built your Shiny app, you can make it available to users through the Analytical Platform.\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 901}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b6d010c909f0a5eb3c74b9ee21db196a'}>,\n",
       " <Document: {'content': 'We have guidance for: Deploying your app Accessing your deployed app Managing your app users Managing and Monitoring Deployments as well as common issues faced during these stages of publishing your app. ⚠️ Note on deployment ⚠️ It is not currently possible to deploy new apps on the Analytical Platform, though we are working to make this functionality available again as soon as possible. In the meantime, you can still access and manage existing apps. If you have an existing app that requires urgent redeployment, please submit a request via GitHub.\\nWe normally redeploy apps each Wednesday, where we have recevied a request by the Friday before. App deployment New apps It is not currently possible to deploy new apps on the Analytical Platform, though we are working to make this functionality available again as soon as possible. Existing apps Overview Your app environment is made up of two key elements: A GitHub workflow A Cloud Platform namespace In brief, the workflow builds and deploys your code as a docker container, and then deploys it to Cloud Platform’s kubernetes cluster, in your application’s namespace. The flow looks something like this: The rationale behind this change is to: Facilitate testing - including a working development app version and build artifacts like docker containers - before deploying to production Give teams more control over their workflows Restore the ability to deploy without the Analytical Platform team’s intervention Apps hosted on Cloud Platform After the move to Cloud Platform hosting for Analytical Platform apps, you’ll have two active deployments of your apps at all times.\\nThese are your ‘dev’ (development) and ‘prod’ (production) deployments. Be aware that both production and dev currently point to the same data. Your code repository within the ministryofjustice organisation was built from the data-platform-app-template repo , and has inherited the continuous integration and continuous delivery (CI/CD) pipelines (GitHub Action workflows) from that repo.\\nThese workflows will automatically build your docker images, push them to a remote image store, and then deploy the application based on how you’ve made your code changes: Deploying your application to your development environment is done through the -dev workflow, which will build-push-deploy when any pull request is made in the repo (e.g. feature-branch into main , or feature-branch into develop-branch ).\\nThe workflow can also be manually triggered, though this will only build and push the app, it will not deploy it: repo homepage > Actions (tab) > Run workflow (right hand side of the page). Opening any pull request will trigger a dev deployment.\\nAny subsequent pushes to that branch (or to any other open branches) will trigger dev deployments too.\\nIf you’re working on multiple PRs at once in the repo that you will need to coordinate pushes to your PRs so you can track your deployments.\\nYou can always cancel workflow runs and rerun deployments from the Actions page in the repo. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 902}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '92b2bb3a27242c740eeffdb99b1ea273'}>,\n",
       " <Document: {'content': 'Deploying your application to your production environment is done through the -prod workflow, which will build-push-deploy either: after a pull request is merged into main upon publishing a release You can view the status of your deployments either by checking the workflow runs in the Actions tab (repo-url/actions), or by checking out the deployments page (repo-url/deployments). The above describes how CI/CD will be set up by default in the ministryofjustice repo.\\nOnce you have ownership of the repo, you’ll have the ownership of the .github/workflow/ files too so you will be able to amend the processes and triggers so that they meet your needs. Your app deployment pipeline GitHub Actions has workflow definitions located in your repository under .github/workflows/ . By default, you will be provided with two workflows.\\nThe dev environment workflow is triggered when you open or contribute to a Pull Request from any branch (for example, into your main branch).\\nIts steps are: Check out the repository Authenticate to AWS Build a docker container from your development branch If the build is successful, push this container to a container registry Run helm against the development environment namespace. The production workflow behaves in the exact same way, containing the same steps, but will be triggered when a Pull Request is merged into main , and will deploy to the production namespace instead Managing published apps Manage app users If authentication is enabled and you choose to use email as the login option, user access management to app can be done through Control panel. To manage the users of your app: Login Control panel as app admin Go to the Analytical Platform control panel . Select the Webapps tab. Select the name of the app you want to manage or select Manage customers . Select the environment where app is deployed ( dev/prod ) To add app users: Enter the email addresses of the users in the box titled ‘Add app customers by entering their email addresses’ – multiple email addresses can be separated by spaces, tabs, commas and semicolons, and can contain capital letters. Select Add customer . To remove an app user, select Remove customer next to the email address of the customer you want to remove. Accessing the deployed app Apps hosted on Cloud Platform Your deployed app can be accessed at two URLs: prod is at: repository-name.apps.live.cloud-platform.service.justice.gov.uk dev is at: repository-name-dev.apps.live.cloud-platform.service.justice.gov.uk (where repository-name is the name of the relevant GitHub repository) By default, the user list on dev empty and you will need to add any users requiring access via control panel. Authenticating to your app For the dashboard apps using passwordless flow (email login), when accessing an app, you can choose whether to sign in using a one-time passcode (default) or an email magic link.\\nTo sign in with an email magic link, add /login?method=link to the end of the app’s URL, for example, https://apps.live.cloud-platform.service.justice.gov.uk/login?method=code . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 903}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dd98764134c9ea4bf690bf131ac66a5f'}>,\n",
       " <Document: {'content': 'Troubleshooting app sign-in “That email address is not authorized for this app (or possibly another error occurred)” error, after entering email address Check that the user is authorised to access the app: Log in to the control panel . Navigate to the app detail page. Choose the right deployment environment (dev/prod) Check if the user’s email address is listed under ‘App customers’. If it is not, refer them to the app owner to obtain access. Check that the user is using the correct email address – there is sometimes confusion between @justice and @digital.justice email addresses. “Access denied” error, having entered email address and copied the one-time passcode into the login page Please follow the same steps above to check whether the user is in the customer list of the app. “IP x.x.x.x is not whitelisted” Check that the user is trying to access the app from one of the trusted networks listed on app’s app-detail from Control panel The app admin can modify the IP_Ranges on the app’s app-detail detail page. Other troubleshooting tips Check that they are trying to access the app using a URL beginning with https:// not http:// . Look for similar issues log in the data-platform-support repository . Try asking the user to clear their cookies by visiting https://alpha-analytics-moj.eu.auth0.com/logout and try again. In addition the AP team can: Check the Auth0 logs for the app in Kibana Check the Auth0 logs in the Auth0 console Advanced Editing the Dockerfile A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. In most cases, you will not need to change the Dockerfile when deploying your app. If your app uses packages that have additional system dependencies, you will need to add these in the Dockerfile . If you are unsure how to do this, contact the Analytical Platform team. A Dockerfile reference can be found in the Docker documentation . Getting details of current users of the app An RShiny app can find out who is using it.\\nThis can be useful to log an audit trail of significant events.\\nSpecifically, it can determine the email address that the user logged into the app with.\\nThis is sensitive data, so you must ensure that you are following all relevant information governance processes. The shiny-headers-demo repository contains an example of how to do this. These features are only available in the Analytical Platform version of shiny-server and the open source shiny server image provided by AP ( see details of both options below ). Finding current users’ email addresses You can obtain the logged in user’s email address by using the following code in the server function of your app: get ( \"HTTP_USER_EMAIL\" , envir = session $ request ) This line in shiny-headers-demo shows the code in context. NOTE : Ensure the latest AuthProxy image is being used or the user email header will not be accessible. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 904}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd0cd6ea27e9c8a5f40b7bb7f08736fb2'}>,\n",
       " <Document: {'content': 'This is defined in the helm upgrade command in the GitHub actions files that handle deployments - build-push-deploy-dev.yml and build-push-deploy-prod.yml . Check these files for AuthProxy.Image.Tag and set it to latest as the example below: --set AuthProxy.Image.Tag=\"latest\" \\\\ A full example in a working deployed app can be found here . Finding current users’ user profiles You can access the full user profile by making a request directly from the RShiny app to the auth-proxy’s /userinfo endpoint using the following code inside your server function. # library(httr) # library(jsonlite) profile <- fromJSON ( content ( GET ( \"http://localhost:3001/userinfo\" , add_headers ( cookie = get ( \"HTTP_COOKIE\" , envir = session $ request ))), \"text\" )) This line shows the code in context. Example response { \"email\" : \"name@example.gov.uk\" , \"email_verified\" : true , \"user_id\" : \"email|12345121312\" , \"picture\" : \"https://s.gravatar.com/avatar/94deebe3b87fc5e9b3b4469112573cc0?s=480&r=pg&d=https%3A%2F%2Fcdn.auth0.com%2Favatars%2Fna.png\" , \"nickname\" : \"name\" , \"identities\" : [ { \"user_id\" : \"12345121312\" , \"provider\" : \"email\" , \"connection\" : \"email\" , \"isSocial\" : false } ], \"updated_at\" : \"2019-07-15T09:54:34.353Z\" , \"created_at\" : \"2018-07-12T14:26:45.663Z\" , \"name\" : \"name@example.gov.uk\" , \"last_ip\" : \"1.1.1.1\" , \"last_login\" : \"2019-07-15T09:54:34.353Z\" , \"logins_count\" : 20 , \"blocked_for\" : [], \"guardian_authenticators\" : [] } user-profile caching Due to the auth0 rate limit for /userinfo , the user-profile will be\\ncached for 10 minutes on auth-proxy. If somehow your app receives an exception, for example, token-expired, from the above call, you can add /userinfo?force=true to refresh the user-profle by force. Troubleshooting and monitoring Deploying locally If you have a MacBook, you can use Docker locally to test and troubleshoot your RShiny app.\\nYou can download Docker Desktop for Mac from the Docker website . To build and run your R Shiny app locally, follow the steps below: Clone your app’s repository to a new folder on your MacBook – this guarantees that the app will be built using the same code as on the Analytical Platform. You may need to create a new connection to GitHub with SSH . Open a terminal session and navigate to the directory containing the Dockerfile using the cd command. Build the Docker image by running: {bash}\\ndocker build . -t IMAGE:TAG where IMAGE is a name for the image, for example, my-docker-image , and TAG is the version number, for example, 0.1 . Run a Docker container created from the Docker image by running: {bash}\\ndocker run -p 80:9999 IMAGE:TAG Go to 127.0.0.1:80 to view the app. If the app does not work, follow the steps below to troubleshoot it: Start a bash session in a Docker container created from the Docker image by running: {bash}\\ndocker run -it -p 80:80 IMAGE:TAG bash Install the nano text editor by running: {bash}\\napt-get update\\napt-get install nano Open shiny-server.conf in the nano text editor by running: {bash}\\nnano /etc/shiny-server/shiny-server.conf Add the following lines at the beginning of shiny-server.conf : {bash}\\naccess_log /var/log/shiny-server/access.log tiny;\\npreserve_logs true; Write the changes by pressing Ctrl+O . Exit the nano text editor by pressing Ctrl+X . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 905}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cc1b192562e75bd4fe701d30aa955c7b'}>,\n",
       " <Document: {'content': 'Increase the verbosity of logging and start the Shiny server by running: {bash}\\nexport SHINY_LOG_LEVEL=TRACE\\n/bin/shiny-server.sh Open a new terminal session. Start a new bash session in the Docker container by running: {bash}\\ndocker exec -it CONTAINER_ID bash You can find the CONTAINER_ID by running docker ps . View the logs by running: {bash}\\ncat /var/log/shiny-server/access.log For further details, see the Shiny server documentation . Managing and Monitoring Deployments Monitoring Deployments By default all applications deployed into Cloud platform have a basic level of monitoring which can be accessed on the following link Cloud Platform grafana namespace monitoring . Grafana dashboards are accessed using single-sign-on (SSO) via your GitHub account. This page shows all namespaces and can be refined by typing the name of your namespace in namespace drop down and selecting as needed. Note: Kubernetes defines Limits as the maximum amount of a resource to be used by a container. This means that the container can never consume more than the memory amount or CPU amount indicated. Requests, on the other hand, are the minimum guaranteed amount of a resource that is reserved for a container. A brief overview of namespaces Within Kubernetes a namespace provides a mechanism for isolating groups of resources within a single cluster, it can be thought of as a virtual cluster within the cluster. Your Application is deployed into its own namespace, this restricts access to your team and enables the setting of resource limits. Within the namespace are the various Kubernetes components: Pods the smallest deployable units of computing that you can create and manage in Kubernetes, usually one pod per function of your application ie web server, db server. Service is a method for exposing a network application that is running as one or more Pods in your cluster, basically simplifying the connections within your namespace. Deployment provides declarative updates for Pods and ReplicaSets. ReplicaSet’s their purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods. What information is displayed on the Grafana namespace chart: Namespace(s) usage on total cluster CPU in % - This is the total usage by all namespaces on the cluster Namespace(s) usage on total cluster RAM in % - This is the total usage by all namespaces on the cluster Kubernetes Resource Count - A simple count of resources running within your namespace. CPU usage by Pod - CPU usage per pod within your namespace. Memory usage by Pod - Memory usage per pod within your namespace. Nb of pods by state - Count of pods at various states  within your namespace. Nb of containers by pod - Number of containers within each pod. Replicas available by deployment - the number of replicas available to your deployment. Replicas unavailable by deployment - the number of replicas unavailable to your deployment. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 906}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '37e470b490a5867cd73892f7de59f578'}>,\n",
       " <Document: {'content': 'Persistent Volumes Capacity - If your application uses persistent volumes the total capacity is displayed here. Persistent Volumes Inodes - If your application uses persistent volumes the detail on the inodes is displayed here. Further technical details can be found in the Cloud Platform’s Monitoring section of the user guidance and Configuring a dashboard using Grafana UI documentation Accessing logs You can access your applications logs in Cloud platform by following the the CP guidance Accessing Application Log Data Kibana on Cloud Platform Below are some notes to aid you in working with the Kibana service, on Cloud Platform. All logs from deployed apps can be viewed in Kibana . To view the logs for a specific app: Select live_kubernetes_cluster from the CHANGE INDEX PATTERN dropdown list. Select Add a filter . Select kubernetes.namespace_name . Select is as the operator. Insert the app’s name space by following the pattern data-platform-app-<app_name>-<dev/prod> . In order to filter out all the logs related health-check, you can put NOT log:  \"/healthz\" in the KQL field. Select Save . (Optional) - you can select to view only the log output by adding it from the Available Fields list in the left hand pane using the (+) button revealed on mouse hover Log messages are displayed in the message column. By default, Kibana only shows logs for the last 15 minutes.\\nIf no logs are available for that time range, you will receive the warning ‘No results match your search criteria’. To change the time range, select the clock icon in the menu bar.\\nThere are several presets or you can define a custom time range. Kibana also has experimental autocomplete and simple syntax tools that you can use to build custom searches.\\nTo enable these features, select Options_ from within the search bar, then toggle Turn on query features . Managing Deployments Cloud platform  allows teams to connect to the Kubernetes cluster and manage their applications, using kubectl , the command line tool for Kubernetes. To do this you will need to setup access through JupyterLab as follows: Go to Analytical Platform Control Panel ‘> Analytical Tools ’> open JupyterLab Data Science ‘> Terminal Installing Kubectl In the terminal session install kubectl curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" curl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\" echo \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check If valid, the output is: kubectl: OK chmod +x kubectl mkdir -p ~/.local/bin mv ./kubectl ~/.local/bin/kubectl which kubectl you should see: /home/jovyan/.local/bin/kubectl You should only need to carry out the above steps once. Setup Cloud Platform Access Follow the steps in CP’s Generating a Kubeconfig file documentation . As we are accessing via Jupyter Labs  step 7 “Move the config file to the location expected by kubectl” has to be carried out slightly differently. First upload the kubecfg.yaml from your local  PC to the Jupyter terminal session. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 907}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '56e1a52cf3548eb5c3940101fe33abe8'}>,\n",
       " <Document: {'content': 'click on the upload arrow (see below) and select the kubecfg.yaml Then continue with the remaining steps in CP’s documentation Accessing the AWS console If required you can access the Cloud platform via AWS, please follow the guidance in Accessing the AWS console (read-only) Adding cron job to your application A cronjob for restarting your application can be setup easier by adding the following line in to your app’s development or production GitHub workflow: --set Cron.schedule=\"0 6 * * *\" crontab.guru can be used to setup the schedule. If you need to a cron job for the other jobs, more guides are available on the Cloud Platform kubernetes cronjobs Changing the resources available to the application When you deploy an update to the application the Kubernetes resources are built from a standard Helm chart with the following default memory/cpu resources: Limits -  CPU 25m.  Memory 1Gi Requests - CPU 3m.  Memory 1Gi To override this you will have to amend the GitHub  workflow in your application repository The file(s) to amend are .github/workflows/build-push-deploy-dev.yml or build-push-deploy-prod.yml Below is the section of code that calls the helm chart helm upgrade --install --wait --timeout 10m0s --namespace $KUBE_NAMESPACE $KUBE_NAMESPACE mojanalytics/webapp-cp \\\\\\n--set AuthProxy.Env.Auth0Domain=$AUTH0_DOMAIN \\\\\\n--set AuthProxy.Env.Auth0Passwordless=$AUTH0_PASSWORDLESS \\\\\\n--set AuthProxy.Env.Auth0TokenAlg=$AUTH0_TOKEN_ALG \\\\\\n--set AuthProxy.Env.AuthenticationRequired=$AUTHENTICATION_REQUIRED \\\\\\n--set AuthProxy.Env.IPRanges=$process_ip_range \\\\\\n--set AuthProxy.Image.Repository=$ECR_REPO_AUTH0 \\\\\\n--set AuthProxy.Image.Tag=\"latest\" \\\\\\n--set Namespace=$KUBE_NAMESPACE \\\\\\n--set Secrets.Auth0.ClientId=$AUTH0_CLIENT_ID \\\\\\n--set Secrets.Auth0.ClientSecret=$AUTH0_CLIENT_SECRET \\\\\\n--set Secrets.Auth0.CookieSecret=$COOKIE_SECRET \\\\\\n--set ServiceAccount.RoleARN=$APP_ROLE_ARN \\\\\\n--set WebApp.Image.Repository=$ECR_REPO_WEBAPP \\\\\\n--set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n$custom_variables ``` To change the resources  insert at  the end of the file just before the $custom_variables the following as appropriate, remembering to use the line continuation “\\\\” on the existing last line. `--set WebApp.resources.limits.cpu=100m \\\\`\\n`--set WebApp.resources.limits.memory=2Gi \\\\`\\n`--set WebApp.resources.requests.cpu=50m \\\\`\\n`--set WebApp.resources.requests.memory=2Gi \\\\`\\n`$custom_variables` Once the changes are pushed/merged to the repository the values will be applied to your Kubernetes namespace. Changing the number of instances (pods) on name space The number of app instances running on dev / prod is 1 by default, if users experience long response times from the application (apart from trying to improving the performance through the code itself) you can increase the number of instances to reduce the wating time on dev / prod GitHub workflows, for example :- --set ReplicaCount=3 Remember: “With great power comes great responsibility”  Your application’s namespace will be one of a number hosted on the same cluster, setting the values too high could crash the cluster! Storage guidance Guidance on storage within Cloud Platform can be found here Cloud platform storage Continuous Deployment Within Cloud platform your application is already automatically deployed by Github workflows but further guidance on continuous deployment within  CP can be found here Cloud Platform continuous deployment. Deploying Updates to the Application As the application is now hosted in Cloud Platform and GitHub workflow has been implemented, you will now be able to apply update to your application, full guidance can be found here Application deployment. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 908}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd51d9919c0c47dda2977f9e577cb26d6'}>,\n",
       " <Document: {'content': 'Other guidance Guidance on  managing Auth and Secrets through the Control Panel can be found Manage deployment settings of an app on Control panel. Shiny server There are a few choices for running a rshiny app on Cloud Platform. The team responsible for developing and maintaining the dashboard app has full control to choose the best way and practices to run the app by building their own Dockerfile. Since the shiny-framework uses websocket as the primary communication protocol between frontend and backend, no matter which choice you decide, the minimum capabilities required are: Keeping the connection live as long as possible e.g. implementing heart-beat mechanism. Being able to handle the lifecyle of session Providing a method for reconnecting when the websocket drops Working with auth-proxy, the AP component responsible for controlling user’s access,  unless the app is public facing or application handles authentication itself. The AP team offers two shiny-server solutions. AP version of shiny-server We developed a mini version of shiny-sever in nodejs, it provides a minimal implementation to support the required capabilities: Uses sockjs which supports heart-beat Create a new session for each new websocket connection Will try to reconnect to the shiny app automatically when the websocket connection drops If reconnection repeatedly fails and reaches the maximum number of attempts, a window will be appear asking the user to trigger a manual reconnect The majority of the shiny apps hosted on Cloud Platform use this version. The current tag for this docker image is: 593291632749.dkr.ecr.eu-west-1.amazonaws.com/rshiny:r4.1.3-shiny0.0.6 Open source shiny-server We also provide a solution for using the open source Shiny Server with a few minor tweaks to support USER_EMAIL and COOKIE headers.  The base docker image is defined here . The version of open source shiny server is defined by SHINY_SERVER_VERSION , currently set to 1.5.20.1002 . It offers more features than the AP shiny server and supports: Better reconnection behaviour: Reconnect and load existing session rather than creating a new session automatically If reconnection continues to fail, the manual reconnection pop-up will trigger an app reload rather than re-triggering the same reconnection behaviour Better session management: The session will be closed when the session is idle for a certain period of time This behaviour can result in:\\n- Session data (reactive values) is retained even after a reconnection happens\\n- Release resources e.g. memory linked to the session which avoids potential memory leaking It also provides more configuration options as outlined here . Note: options marked as “pro” are not available Instructions for using the open source shiny server image Example Dockerfile The following example can be used as the starting point when making your own Dockerfile # The base docker image\\nFROM ghcr.io/ministryofjustice/data-platform-rshiny-open-source-base:1.0.3\\n\\n# ** Optional step: only if some of R pakcages requires the system libraries which are not covered by base image\\n#   the one in the example below has been provided in base image.\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 909}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '36085d176f6a14e2924c0433c2257df6'}>,\n",
       " <Document: {'content': '# RUN apt-get update \\\\\\n#   && apt-get install -y --no-install-recommends \\\\\\n#     libglpk-dev\\n\\n# use renv for packages\\nADD renv.lock renv.lock\\n\\n# Install R packages\\nRUN R -e \"install.packages(\\'renv\\'); renv::restore()\"\\n\\n# ** Optional step: only if the app requires python packages\\n# Make sure reticulate uses the system Python\\n# ENV RETICULATE_PYTHON=\"/usr/bin/python3\"\\n# ensure requirements.txt exists (created automatically when making a venv in renv)\\n# COPY requirements.txt requirements.txt\\n# RUN python3 -m pip install -r requirements.txt\\n\\n# Add shiny app code\\nADD . .\\n\\nUSER 998 Instructions for switching from the AP shiny server to the open-source server If you already use the AP shiny server, and would like to switch to the open source server, the key changes you need to make are: Change the base docker image in your Dockerfile: FROM ghcr.io/ministryofjustice/data-platform-rshiny-open-source-base:1.0.3 If present, ensure the following redundant parts of your Dockerfile are removed: ENV PATH=\"/opt/shiny-server/bin:/opt/shiny-server/ext/node/bin:${PATH}\"\\nENV SHINY_APP=/srv/shiny-server\\nENV NODE_ENV=production RUN chown shiny:shiny /srv/shiny-server\\nCMD analytics-platform-shiny-server\\nEXPOSE 9999 Update GitHub actions work flows Assuming the app uses the GitHub actions workflows from AP, the following parameters for helm installation are required in both the build-push-deploy-dev.yml and build-push-deploy-prod.yml github action workflow files: WebApp.AlternativeHealthCheck.enabled=\"true\"\\nWebApp.AlternativeHealthCheck.port=9999 A complete example for installing the helm chart in the workflow below. These changes will need to be made in both: helm upgrade --install --wait --timeout 10m0s --namespace $KUBE_NAMESPACE $RELEASE_NAME mojanalytics/webapp-cp \\\\\\n--set AuthProxy.Env.Auth0Domain=$AUTH0_DOMAIN \\\\\\n--set AuthProxy.Env.Auth0Passwordless=$AUTH0_PASSWORDLESS \\\\\\n--set AuthProxy.Env.Auth0TokenAlg=$AUTH0_TOKEN_ALG \\\\\\n--set AuthProxy.Env.AuthenticationRequired=$AUTHENTICATION_REQUIRED \\\\\\n--set AuthProxy.Env.IPRanges=$process_ip_range \\\\\\n--set AuthProxy.Image.Repository=$ECR_REPO_AUTH0 \\\\\\n--set AuthProxy.Image.Tag=\"latest\" \\\\\\n--set Namespace=$KUBE_NAMESPACE \\\\\\n--set WebApp.AlternativeHealthCheck.enabled=\"true\" \\\\\\n--set WebApp.AlternativeHealthCheck.port=9999 \\\\\\n--set Secrets.Auth0.ClientId=$AUTH0_CLIENT_ID \\\\\\n--set Secrets.Auth0.ClientSecret=$AUTH0_CLIENT_SECRET \\\\\\n--set Secrets.Auth0.CookieSecret=$COOKIE_SECRET \\\\\\n--set ServiceAccount.RoleARN=$APP_ROLE_ARN \\\\\\n--set WebApp.Image.Repository=$ECR_REPO_WEBAPP \\\\\\n--set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n$custom_variables A full working example is available here Enabling Web Application Firewall (WAF) for Data Platform Apps There are two flags that need to be set to enable WAF for your application, Ingress.ModSec.enabled and GithubTeam To do this modify your app’s build-push-deploy-dev.yml and build-push-deploy-prod.yml GitHub action workflow files as follows: --set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n--set Ingress.ModSec.enabled=\"true\" \\\\\\n--set GithubTeam=\"your github team\" \\\\\\n$custom_variables The changes  will be will be applied to your Kubernetes namespace following a push/merge to the repository and the running of the workflow To disable –set Ingress.ModSec.enabled=“false” This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDeploy an RShiny app - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 910}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4068b35394414be6bc55dc905a46d88d'}>,\n",
       " <Document: {'content': 'Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 911}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f5a617e2b8bf3e97f303a94c8b4da304'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools R Shiny app publishing Once you’ve built your Shiny app, you can make it available to users through the Analytical Platform.\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 912}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b6d010c909f0a5eb3c74b9ee21db196a'}>,\n",
       " <Document: {'content': 'We have guidance for: Deploying your app Accessing your deployed app Managing your app users Managing and Monitoring Deployments as well as common issues faced during these stages of publishing your app. ⚠️ Note on deployment ⚠️ It is not currently possible to deploy new apps on the Analytical Platform, though we are working to make this functionality available again as soon as possible. In the meantime, you can still access and manage existing apps. If you have an existing app that requires urgent redeployment, please submit a request via GitHub.\\nWe normally redeploy apps each Wednesday, where we have recevied a request by the Friday before. App deployment New apps It is not currently possible to deploy new apps on the Analytical Platform, though we are working to make this functionality available again as soon as possible. Existing apps Overview Your app environment is made up of two key elements: A GitHub workflow A Cloud Platform namespace In brief, the workflow builds and deploys your code as a docker container, and then deploys it to Cloud Platform’s kubernetes cluster, in your application’s namespace. The flow looks something like this: The rationale behind this change is to: Facilitate testing - including a working development app version and build artifacts like docker containers - before deploying to production Give teams more control over their workflows Restore the ability to deploy without the Analytical Platform team’s intervention Apps hosted on Cloud Platform After the move to Cloud Platform hosting for Analytical Platform apps, you’ll have two active deployments of your apps at all times.\\nThese are your ‘dev’ (development) and ‘prod’ (production) deployments. Be aware that both production and dev currently point to the same data. Your code repository within the ministryofjustice organisation was built from the data-platform-app-template repo , and has inherited the continuous integration and continuous delivery (CI/CD) pipelines (GitHub Action workflows) from that repo.\\nThese workflows will automatically build your docker images, push them to a remote image store, and then deploy the application based on how you’ve made your code changes: Deploying your application to your development environment is done through the -dev workflow, which will build-push-deploy when any pull request is made in the repo (e.g. feature-branch into main , or feature-branch into develop-branch ).\\nThe workflow can also be manually triggered, though this will only build and push the app, it will not deploy it: repo homepage > Actions (tab) > Run workflow (right hand side of the page). Opening any pull request will trigger a dev deployment.\\nAny subsequent pushes to that branch (or to any other open branches) will trigger dev deployments too.\\nIf you’re working on multiple PRs at once in the repo that you will need to coordinate pushes to your PRs so you can track your deployments.\\nYou can always cancel workflow runs and rerun deployments from the Actions page in the repo. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 913}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '92b2bb3a27242c740eeffdb99b1ea273'}>,\n",
       " <Document: {'content': 'Deploying your application to your production environment is done through the -prod workflow, which will build-push-deploy either: after a pull request is merged into main upon publishing a release You can view the status of your deployments either by checking the workflow runs in the Actions tab (repo-url/actions), or by checking out the deployments page (repo-url/deployments). The above describes how CI/CD will be set up by default in the ministryofjustice repo.\\nOnce you have ownership of the repo, you’ll have the ownership of the .github/workflow/ files too so you will be able to amend the processes and triggers so that they meet your needs. Your app deployment pipeline GitHub Actions has workflow definitions located in your repository under .github/workflows/ . By default, you will be provided with two workflows.\\nThe dev environment workflow is triggered when you open or contribute to a Pull Request from any branch (for example, into your main branch).\\nIts steps are: Check out the repository Authenticate to AWS Build a docker container from your development branch If the build is successful, push this container to a container registry Run helm against the development environment namespace. The production workflow behaves in the exact same way, containing the same steps, but will be triggered when a Pull Request is merged into main , and will deploy to the production namespace instead Managing published apps Manage app users If authentication is enabled and you choose to use email as the login option, user access management to app can be done through Control panel. To manage the users of your app: Login Control panel as app admin Go to the Analytical Platform control panel . Select the Webapps tab. Select the name of the app you want to manage or select Manage customers . Select the environment where app is deployed ( dev/prod ) To add app users: Enter the email addresses of the users in the box titled ‘Add app customers by entering their email addresses’ – multiple email addresses can be separated by spaces, tabs, commas and semicolons, and can contain capital letters. Select Add customer . To remove an app user, select Remove customer next to the email address of the customer you want to remove. Accessing the deployed app Apps hosted on Cloud Platform Your deployed app can be accessed at two URLs: prod is at: repository-name.apps.live.cloud-platform.service.justice.gov.uk dev is at: repository-name-dev.apps.live.cloud-platform.service.justice.gov.uk (where repository-name is the name of the relevant GitHub repository) By default, the user list on dev empty and you will need to add any users requiring access via control panel. Authenticating to your app For the dashboard apps using passwordless flow (email login), when accessing an app, you can choose whether to sign in using a one-time passcode (default) or an email magic link.\\nTo sign in with an email magic link, add /login?method=link to the end of the app’s URL, for example, https://apps.live.cloud-platform.service.justice.gov.uk/login?method=code . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 914}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dd98764134c9ea4bf690bf131ac66a5f'}>,\n",
       " <Document: {'content': 'Troubleshooting app sign-in “That email address is not authorized for this app (or possibly another error occurred)” error, after entering email address Check that the user is authorised to access the app: Log in to the control panel . Navigate to the app detail page. Choose the right deployment environment (dev/prod) Check if the user’s email address is listed under ‘App customers’. If it is not, refer them to the app owner to obtain access. Check that the user is using the correct email address – there is sometimes confusion between @justice and @digital.justice email addresses. “Access denied” error, having entered email address and copied the one-time passcode into the login page Please follow the same steps above to check whether the user is in the customer list of the app. “IP x.x.x.x is not whitelisted” Check that the user is trying to access the app from one of the trusted networks listed on app’s app-detail from Control panel The app admin can modify the IP_Ranges on the app’s app-detail detail page. Other troubleshooting tips Check that they are trying to access the app using a URL beginning with https:// not http:// . Look for similar issues log in the data-platform-support repository . Try asking the user to clear their cookies by visiting https://alpha-analytics-moj.eu.auth0.com/logout and try again. In addition the AP team can: Check the Auth0 logs for the app in Kibana Check the Auth0 logs in the Auth0 console Advanced Editing the Dockerfile A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. In most cases, you will not need to change the Dockerfile when deploying your app. If your app uses packages that have additional system dependencies, you will need to add these in the Dockerfile . If you are unsure how to do this, contact the Analytical Platform team. A Dockerfile reference can be found in the Docker documentation . Getting details of current users of the app An RShiny app can find out who is using it.\\nThis can be useful to log an audit trail of significant events.\\nSpecifically, it can determine the email address that the user logged into the app with.\\nThis is sensitive data, so you must ensure that you are following all relevant information governance processes. The shiny-headers-demo repository contains an example of how to do this. These features are only available in the Analytical Platform version of shiny-server and the open source shiny server image provided by AP ( see details of both options below ). Finding current users’ email addresses You can obtain the logged in user’s email address by using the following code in the server function of your app: get ( \"HTTP_USER_EMAIL\" , envir = session $ request ) This line in shiny-headers-demo shows the code in context. NOTE : Ensure the latest AuthProxy image is being used or the user email header will not be accessible. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 915}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd0cd6ea27e9c8a5f40b7bb7f08736fb2'}>,\n",
       " <Document: {'content': 'This is defined in the helm upgrade command in the GitHub actions files that handle deployments - build-push-deploy-dev.yml and build-push-deploy-prod.yml . Check these files for AuthProxy.Image.Tag and set it to latest as the example below: --set AuthProxy.Image.Tag=\"latest\" \\\\ A full example in a working deployed app can be found here . Finding current users’ user profiles You can access the full user profile by making a request directly from the RShiny app to the auth-proxy’s /userinfo endpoint using the following code inside your server function. # library(httr) # library(jsonlite) profile <- fromJSON ( content ( GET ( \"http://localhost:3001/userinfo\" , add_headers ( cookie = get ( \"HTTP_COOKIE\" , envir = session $ request ))), \"text\" )) This line shows the code in context. Example response { \"email\" : \"name@example.gov.uk\" , \"email_verified\" : true , \"user_id\" : \"email|12345121312\" , \"picture\" : \"https://s.gravatar.com/avatar/94deebe3b87fc5e9b3b4469112573cc0?s=480&r=pg&d=https%3A%2F%2Fcdn.auth0.com%2Favatars%2Fna.png\" , \"nickname\" : \"name\" , \"identities\" : [ { \"user_id\" : \"12345121312\" , \"provider\" : \"email\" , \"connection\" : \"email\" , \"isSocial\" : false } ], \"updated_at\" : \"2019-07-15T09:54:34.353Z\" , \"created_at\" : \"2018-07-12T14:26:45.663Z\" , \"name\" : \"name@example.gov.uk\" , \"last_ip\" : \"1.1.1.1\" , \"last_login\" : \"2019-07-15T09:54:34.353Z\" , \"logins_count\" : 20 , \"blocked_for\" : [], \"guardian_authenticators\" : [] } user-profile caching Due to the auth0 rate limit for /userinfo , the user-profile will be\\ncached for 10 minutes on auth-proxy. If somehow your app receives an exception, for example, token-expired, from the above call, you can add /userinfo?force=true to refresh the user-profle by force. Troubleshooting and monitoring Deploying locally If you have a MacBook, you can use Docker locally to test and troubleshoot your RShiny app.\\nYou can download Docker Desktop for Mac from the Docker website . To build and run your R Shiny app locally, follow the steps below: Clone your app’s repository to a new folder on your MacBook – this guarantees that the app will be built using the same code as on the Analytical Platform. You may need to create a new connection to GitHub with SSH . Open a terminal session and navigate to the directory containing the Dockerfile using the cd command. Build the Docker image by running: {bash}\\ndocker build . -t IMAGE:TAG where IMAGE is a name for the image, for example, my-docker-image , and TAG is the version number, for example, 0.1 . Run a Docker container created from the Docker image by running: {bash}\\ndocker run -p 80:9999 IMAGE:TAG Go to 127.0.0.1:80 to view the app. If the app does not work, follow the steps below to troubleshoot it: Start a bash session in a Docker container created from the Docker image by running: {bash}\\ndocker run -it -p 80:80 IMAGE:TAG bash Install the nano text editor by running: {bash}\\napt-get update\\napt-get install nano Open shiny-server.conf in the nano text editor by running: {bash}\\nnano /etc/shiny-server/shiny-server.conf Add the following lines at the beginning of shiny-server.conf : {bash}\\naccess_log /var/log/shiny-server/access.log tiny;\\npreserve_logs true; Write the changes by pressing Ctrl+O . Exit the nano text editor by pressing Ctrl+X . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 916}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cc1b192562e75bd4fe701d30aa955c7b'}>,\n",
       " <Document: {'content': 'Increase the verbosity of logging and start the Shiny server by running: {bash}\\nexport SHINY_LOG_LEVEL=TRACE\\n/bin/shiny-server.sh Open a new terminal session. Start a new bash session in the Docker container by running: {bash}\\ndocker exec -it CONTAINER_ID bash You can find the CONTAINER_ID by running docker ps . View the logs by running: {bash}\\ncat /var/log/shiny-server/access.log For further details, see the Shiny server documentation . Managing and Monitoring Deployments Monitoring Deployments By default all applications deployed into Cloud platform have a basic level of monitoring which can be accessed on the following link Cloud Platform grafana namespace monitoring . Grafana dashboards are accessed using single-sign-on (SSO) via your GitHub account. This page shows all namespaces and can be refined by typing the name of your namespace in namespace drop down and selecting as needed. Note: Kubernetes defines Limits as the maximum amount of a resource to be used by a container. This means that the container can never consume more than the memory amount or CPU amount indicated. Requests, on the other hand, are the minimum guaranteed amount of a resource that is reserved for a container. A brief overview of namespaces Within Kubernetes a namespace provides a mechanism for isolating groups of resources within a single cluster, it can be thought of as a virtual cluster within the cluster. Your Application is deployed into its own namespace, this restricts access to your team and enables the setting of resource limits. Within the namespace are the various Kubernetes components: Pods the smallest deployable units of computing that you can create and manage in Kubernetes, usually one pod per function of your application ie web server, db server. Service is a method for exposing a network application that is running as one or more Pods in your cluster, basically simplifying the connections within your namespace. Deployment provides declarative updates for Pods and ReplicaSets. ReplicaSet’s their purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods. What information is displayed on the Grafana namespace chart: Namespace(s) usage on total cluster CPU in % - This is the total usage by all namespaces on the cluster Namespace(s) usage on total cluster RAM in % - This is the total usage by all namespaces on the cluster Kubernetes Resource Count - A simple count of resources running within your namespace. CPU usage by Pod - CPU usage per pod within your namespace. Memory usage by Pod - Memory usage per pod within your namespace. Nb of pods by state - Count of pods at various states  within your namespace. Nb of containers by pod - Number of containers within each pod. Replicas available by deployment - the number of replicas available to your deployment. Replicas unavailable by deployment - the number of replicas unavailable to your deployment. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 917}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '37e470b490a5867cd73892f7de59f578'}>,\n",
       " <Document: {'content': 'Persistent Volumes Capacity - If your application uses persistent volumes the total capacity is displayed here. Persistent Volumes Inodes - If your application uses persistent volumes the detail on the inodes is displayed here. Further technical details can be found in the Cloud Platform’s Monitoring section of the user guidance and Configuring a dashboard using Grafana UI documentation Accessing logs You can access your applications logs in Cloud platform by following the the CP guidance Accessing Application Log Data Kibana on Cloud Platform Below are some notes to aid you in working with the Kibana service, on Cloud Platform. All logs from deployed apps can be viewed in Kibana . To view the logs for a specific app: Select live_kubernetes_cluster from the CHANGE INDEX PATTERN dropdown list. Select Add a filter . Select kubernetes.namespace_name . Select is as the operator. Insert the app’s name space by following the pattern data-platform-app-<app_name>-<dev/prod> . In order to filter out all the logs related health-check, you can put NOT log:  \"/healthz\" in the KQL field. Select Save . (Optional) - you can select to view only the log output by adding it from the Available Fields list in the left hand pane using the (+) button revealed on mouse hover Log messages are displayed in the message column. By default, Kibana only shows logs for the last 15 minutes.\\nIf no logs are available for that time range, you will receive the warning ‘No results match your search criteria’. To change the time range, select the clock icon in the menu bar.\\nThere are several presets or you can define a custom time range. Kibana also has experimental autocomplete and simple syntax tools that you can use to build custom searches.\\nTo enable these features, select Options_ from within the search bar, then toggle Turn on query features . Managing Deployments Cloud platform  allows teams to connect to the Kubernetes cluster and manage their applications, using kubectl , the command line tool for Kubernetes. To do this you will need to setup access through JupyterLab as follows: Go to Analytical Platform Control Panel ‘> Analytical Tools ’> open JupyterLab Data Science ‘> Terminal Installing Kubectl In the terminal session install kubectl curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" curl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\" echo \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check If valid, the output is: kubectl: OK chmod +x kubectl mkdir -p ~/.local/bin mv ./kubectl ~/.local/bin/kubectl which kubectl you should see: /home/jovyan/.local/bin/kubectl You should only need to carry out the above steps once. Setup Cloud Platform Access Follow the steps in CP’s Generating a Kubeconfig file documentation . As we are accessing via Jupyter Labs  step 7 “Move the config file to the location expected by kubectl” has to be carried out slightly differently. First upload the kubecfg.yaml from your local  PC to the Jupyter terminal session. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 918}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '56e1a52cf3548eb5c3940101fe33abe8'}>,\n",
       " <Document: {'content': 'click on the upload arrow (see below) and select the kubecfg.yaml Then continue with the remaining steps in CP’s documentation Accessing the AWS console If required you can access the Cloud platform via AWS, please follow the guidance in Accessing the AWS console (read-only) Adding cron job to your application A cronjob for restarting your application can be setup easier by adding the following line in to your app’s development or production GitHub workflow: --set Cron.schedule=\"0 6 * * *\" crontab.guru can be used to setup the schedule. If you need to a cron job for the other jobs, more guides are available on the Cloud Platform kubernetes cronjobs Changing the resources available to the application When you deploy an update to the application the Kubernetes resources are built from a standard Helm chart with the following default memory/cpu resources: Limits -  CPU 25m.  Memory 1Gi Requests - CPU 3m.  Memory 1Gi To override this you will have to amend the GitHub  workflow in your application repository The file(s) to amend are .github/workflows/build-push-deploy-dev.yml or build-push-deploy-prod.yml Below is the section of code that calls the helm chart helm upgrade --install --wait --timeout 10m0s --namespace $KUBE_NAMESPACE $KUBE_NAMESPACE mojanalytics/webapp-cp \\\\\\n--set AuthProxy.Env.Auth0Domain=$AUTH0_DOMAIN \\\\\\n--set AuthProxy.Env.Auth0Passwordless=$AUTH0_PASSWORDLESS \\\\\\n--set AuthProxy.Env.Auth0TokenAlg=$AUTH0_TOKEN_ALG \\\\\\n--set AuthProxy.Env.AuthenticationRequired=$AUTHENTICATION_REQUIRED \\\\\\n--set AuthProxy.Env.IPRanges=$process_ip_range \\\\\\n--set AuthProxy.Image.Repository=$ECR_REPO_AUTH0 \\\\\\n--set AuthProxy.Image.Tag=\"latest\" \\\\\\n--set Namespace=$KUBE_NAMESPACE \\\\\\n--set Secrets.Auth0.ClientId=$AUTH0_CLIENT_ID \\\\\\n--set Secrets.Auth0.ClientSecret=$AUTH0_CLIENT_SECRET \\\\\\n--set Secrets.Auth0.CookieSecret=$COOKIE_SECRET \\\\\\n--set ServiceAccount.RoleARN=$APP_ROLE_ARN \\\\\\n--set WebApp.Image.Repository=$ECR_REPO_WEBAPP \\\\\\n--set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n$custom_variables ``` To change the resources  insert at  the end of the file just before the $custom_variables the following as appropriate, remembering to use the line continuation “\\\\” on the existing last line. `--set WebApp.resources.limits.cpu=100m \\\\`\\n`--set WebApp.resources.limits.memory=2Gi \\\\`\\n`--set WebApp.resources.requests.cpu=50m \\\\`\\n`--set WebApp.resources.requests.memory=2Gi \\\\`\\n`$custom_variables` Once the changes are pushed/merged to the repository the values will be applied to your Kubernetes namespace. Changing the number of instances (pods) on name space The number of app instances running on dev / prod is 1 by default, if users experience long response times from the application (apart from trying to improving the performance through the code itself) you can increase the number of instances to reduce the wating time on dev / prod GitHub workflows, for example :- --set ReplicaCount=3 Remember: “With great power comes great responsibility”  Your application’s namespace will be one of a number hosted on the same cluster, setting the values too high could crash the cluster! Storage guidance Guidance on storage within Cloud Platform can be found here Cloud platform storage Continuous Deployment Within Cloud platform your application is already automatically deployed by Github workflows but further guidance on continuous deployment within  CP can be found here Cloud Platform continuous deployment. Deploying Updates to the Application As the application is now hosted in Cloud Platform and GitHub workflow has been implemented, you will now be able to apply update to your application, full guidance can be found here Application deployment. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 919}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd51d9919c0c47dda2977f9e577cb26d6'}>,\n",
       " <Document: {'content': 'Other guidance Guidance on  managing Auth and Secrets through the Control Panel can be found Manage deployment settings of an app on Control panel. Shiny server There are a few choices for running a rshiny app on Cloud Platform. The team responsible for developing and maintaining the dashboard app has full control to choose the best way and practices to run the app by building their own Dockerfile. Since the shiny-framework uses websocket as the primary communication protocol between frontend and backend, no matter which choice you decide, the minimum capabilities required are: Keeping the connection live as long as possible e.g. implementing heart-beat mechanism. Being able to handle the lifecyle of session Providing a method for reconnecting when the websocket drops Working with auth-proxy, the AP component responsible for controlling user’s access,  unless the app is public facing or application handles authentication itself. The AP team offers two shiny-server solutions. AP version of shiny-server We developed a mini version of shiny-sever in nodejs, it provides a minimal implementation to support the required capabilities: Uses sockjs which supports heart-beat Create a new session for each new websocket connection Will try to reconnect to the shiny app automatically when the websocket connection drops If reconnection repeatedly fails and reaches the maximum number of attempts, a window will be appear asking the user to trigger a manual reconnect The majority of the shiny apps hosted on Cloud Platform use this version. The current tag for this docker image is: 593291632749.dkr.ecr.eu-west-1.amazonaws.com/rshiny:r4.1.3-shiny0.0.6 Open source shiny-server We also provide a solution for using the open source Shiny Server with a few minor tweaks to support USER_EMAIL and COOKIE headers.  The base docker image is defined here . The version of open source shiny server is defined by SHINY_SERVER_VERSION , currently set to 1.5.20.1002 . It offers more features than the AP shiny server and supports: Better reconnection behaviour: Reconnect and load existing session rather than creating a new session automatically If reconnection continues to fail, the manual reconnection pop-up will trigger an app reload rather than re-triggering the same reconnection behaviour Better session management: The session will be closed when the session is idle for a certain period of time This behaviour can result in:\\n- Session data (reactive values) is retained even after a reconnection happens\\n- Release resources e.g. memory linked to the session which avoids potential memory leaking It also provides more configuration options as outlined here . Note: options marked as “pro” are not available Instructions for using the open source shiny server image Example Dockerfile The following example can be used as the starting point when making your own Dockerfile # The base docker image\\nFROM ghcr.io/ministryofjustice/data-platform-rshiny-open-source-base:1.0.3\\n\\n# ** Optional step: only if some of R pakcages requires the system libraries which are not covered by base image\\n#   the one in the example below has been provided in base image.\\n', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 920}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '36085d176f6a14e2924c0433c2257df6'}>,\n",
       " <Document: {'content': '# RUN apt-get update \\\\\\n#   && apt-get install -y --no-install-recommends \\\\\\n#     libglpk-dev\\n\\n# use renv for packages\\nADD renv.lock renv.lock\\n\\n# Install R packages\\nRUN R -e \"install.packages(\\'renv\\'); renv::restore()\"\\n\\n# ** Optional step: only if the app requires python packages\\n# Make sure reticulate uses the system Python\\n# ENV RETICULATE_PYTHON=\"/usr/bin/python3\"\\n# ensure requirements.txt exists (created automatically when making a venv in renv)\\n# COPY requirements.txt requirements.txt\\n# RUN python3 -m pip install -r requirements.txt\\n\\n# Add shiny app code\\nADD . .\\n\\nUSER 998 Instructions for switching from the AP shiny server to the open-source server If you already use the AP shiny server, and would like to switch to the open source server, the key changes you need to make are: Change the base docker image in your Dockerfile: FROM ghcr.io/ministryofjustice/data-platform-rshiny-open-source-base:1.0.3 If present, ensure the following redundant parts of your Dockerfile are removed: ENV PATH=\"/opt/shiny-server/bin:/opt/shiny-server/ext/node/bin:${PATH}\"\\nENV SHINY_APP=/srv/shiny-server\\nENV NODE_ENV=production RUN chown shiny:shiny /srv/shiny-server\\nCMD analytics-platform-shiny-server\\nEXPOSE 9999 Update GitHub actions work flows Assuming the app uses the GitHub actions workflows from AP, the following parameters for helm installation are required in both the build-push-deploy-dev.yml and build-push-deploy-prod.yml github action workflow files: WebApp.AlternativeHealthCheck.enabled=\"true\"\\nWebApp.AlternativeHealthCheck.port=9999 A complete example for installing the helm chart in the workflow below. These changes will need to be made in both: helm upgrade --install --wait --timeout 10m0s --namespace $KUBE_NAMESPACE $RELEASE_NAME mojanalytics/webapp-cp \\\\\\n--set AuthProxy.Env.Auth0Domain=$AUTH0_DOMAIN \\\\\\n--set AuthProxy.Env.Auth0Passwordless=$AUTH0_PASSWORDLESS \\\\\\n--set AuthProxy.Env.Auth0TokenAlg=$AUTH0_TOKEN_ALG \\\\\\n--set AuthProxy.Env.AuthenticationRequired=$AUTHENTICATION_REQUIRED \\\\\\n--set AuthProxy.Env.IPRanges=$process_ip_range \\\\\\n--set AuthProxy.Image.Repository=$ECR_REPO_AUTH0 \\\\\\n--set AuthProxy.Image.Tag=\"latest\" \\\\\\n--set Namespace=$KUBE_NAMESPACE \\\\\\n--set WebApp.AlternativeHealthCheck.enabled=\"true\" \\\\\\n--set WebApp.AlternativeHealthCheck.port=9999 \\\\\\n--set Secrets.Auth0.ClientId=$AUTH0_CLIENT_ID \\\\\\n--set Secrets.Auth0.ClientSecret=$AUTH0_CLIENT_SECRET \\\\\\n--set Secrets.Auth0.CookieSecret=$COOKIE_SECRET \\\\\\n--set ServiceAccount.RoleARN=$APP_ROLE_ARN \\\\\\n--set WebApp.Image.Repository=$ECR_REPO_WEBAPP \\\\\\n--set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n$custom_variables A full working example is available here Enabling Web Application Firewall (WAF) for Data Platform Apps There are two flags that need to be set to enable WAF for your application, Ingress.ModSec.enabled and GithubTeam To do this modify your app’s build-push-deploy-dev.yml and build-push-deploy-prod.yml GitHub action workflow files as follows: --set WebApp.Image.Tag=$NEW_TAG_V \\\\\\n--set WebApp.Name=$KUBE_NAMESPACE \\\\\\n--set Ingress.ModSec.enabled=\"true\" \\\\\\n--set GithubTeam=\"your github team\" \\\\\\n$custom_variables The changes  will be will be applied to your Kubernetes namespace following a push/merge to the repository and the running of the workflow To disable –set Ingress.ModSec.enabled=“false” This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nManage deployment settings of an app on Control panel - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 921}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3b53d83e83d649cb9668873ece5adf50'}>,\n",
       " <Document: {'content': 'Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 922}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '77b329a04cdaaec1738007ecfa242975'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 923}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7a1bbf0601e5e6586cff17ef311f612'}>,\n",
       " <Document: {'content': 'Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Manage deployment settings of an app on Control panel After you complete the process of registering an app through Control panel and initialise the infrastructure resource under Cloud platform , you can start to manage the deployment settings through the Control panel . The settings are stored as GitHub secrets or environment variables depending on whether the setting contains sensitive information or not. This section is to explain each setting and how the value of the setting affect the app. Permission required The user:\\n- Has been granted as the app’s admin on Control panel\\n- Has been the memember of GitHub admin team If you do not satisfy the above requirements, ask someone in your team who has admin permissions on Control panel and GitHub to grant you the permissions. Context You can have multiple deployment environments for an app for different purposes, minumum is the production environment, then you can have extra ones, e.g. one for testing purpose. What is an application environment? Environments contain the following parts:\\n- A namepace with other required resources on Cloud Platform’s cluster, e.g. an ECR repo for storing your app’s docker images\\n- An environment on GitHub repo\\n- Deployment settings under each GitHub environment and can be managed through Control panel\\n- An ingress (app URL) for you to access the deployed app in each environment\\nThe docker image of app will be built and pushed into app’s ECR, then be deployed with deployment settings on its namespace via GitHub workflow and can be accessed via the app’s URL. By default, 2 deployment environments are provided :-\\n- dev environment: used for testing changes to your application and can be used as the staging environment before releasing a new version to the production environment.\\n- namespace: data-platform-app-<repo-name>-dev - dev environment on GitHub repo\\n- app URL: <repo_name>-dev.apps.live.cloud-platform.service.justice.gov.uk - prod environment:  the production environment where the live application sits\\n- namespace: data-platform-app-<repo-name>-prod - prod enviroment on GitHub repo\\n- app URL: <repo_name>.apps.live.cloud-platform.service.justice.gov.uk All the deployment settings linked to each deployment environment will be displayed and can be managed under the app-detail page on Control panel. Introduction to the settings First, the following flag can be used for switching on/off the user-access-control to the app | Setting | Format | Description |\\n|———|———|———————————————|\\n| AUTHENTICATION_REQUIRED | GitHub environment var | True/False indicate whether your app needs user-access-control, editable| If the value of flag is True , then an auth0 client is required which can be created by clicking the button Create auth0 client under the deployment environment you choose.\\nThe auth0-client is responsible for providing the integration with different login options, for example below:\\n- passwordless flow: allow user to gain access by one-time magic link. more detail is here . This approach allows external users to be able to access app via their email. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 924}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4e023d62f9cf7bd0b59f15126945497'}>,\n",
       " <Document: {'content': \"This flow appears as email in the AUTH0_CONNECTIONS field.\\n- GitHub\\n- nomis login (HMPPS Auth): allow the user login with their NOMIS credentials, the guide about how to set it up will be provided soon. The credentials of the auth0-client with other related settings need to be available in the app’s deployment environment and be stored as GitHub secrets and environment vars which is explained in the following section Authentication related settings Command Format Description AUTH0_DOMAIN GitHub environment var The domain of the auth0 tenant, not editable AUTH0_CLIENT_ID GitHub secret The client_id of the auth0-client, not editable AUTH0_CLIENT_SECRET GitHub secret The client_secret of the auth0-client, not editable AUTH0_CONNECTIONS stored in auth0 platform The list of login opions the app can choose, only superuser can edit it AUTH0_PASSWORDLESS GitHub environment var only True if email is choosen from AUTH0_CONNECTIONS , not editable If you choose to use email ( passwordless flow),  then you can manage the app’s customers for each deployment environment through clicking Manage customers button on app-list page . Right now we only provide customer management for email login option. If you choose other options like nomis or github etc, then it means your app will be open to any users who have nomis credential or who has a GitHub account and has joined the moj-analytical-services GitHub org.  Further user management and control is required under app-level if the default scope of users is wider than the target audience of the app. IP whitelist You can configure whether your app needs extra protection from internet environment by setting the allowed IP_RANGES (the list VPN managed in MoJ). You can set up this option even if your app is public facing ( AUTHENTICATION_REQUIRED is False ) Command Format Description IP_RANGES GitHub secret The list of MoJ VPNS being allowed to this app, editable Self-defined secrets or environment vars If the app has its own settings and the value of the setting depends on which deployment environment the app is running in, you can create secret (sensitive value e.g., credential or api-key) or environment variable (non-sensitive value) through the app-detail page. The GitHub name of self-defined secrets and environment vars will be have XXX_ as prefix which differentiate themselves with other system secrets and vars,\\nbut the prefix XXX_ will be removed when GitHub workflow (dev/prod) pass the their values to application. We strongly recommend to define one environment variable for indicating which deployment environment the app is running, is dev environment or production environment?\\nfor example, naming the variable as ENV ,  then give it a string value of dev on dev environment,  a string value of prod on prod environment.\\nIn your application code,  retrieve the value of ENV by using Sys.getenv('ENV') for R, os.environ.get('ENV') for python. Other Github secrets and environment vars Other secrets and vars which are not mentioned in the section, if they are not the ones you defined, e.g. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 925}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '75f2c60036946b5a36bd3272be00c9fe'}>,\n",
       " <Document: {'content': 'having ECR or KUBE or AWS as part of the name,  then they are created during the process of initialising the infrastructure resources on CP and are required by the deployment workflows too.  They are maintained or updated usually through the terraforms of CP. Want to make change to AUTHENTICATION_REQUIRED flag? If the flag was off ( False for AUTHENTICATION_REQUIRED ) and you switch it on,  then an auth0-client is required to be created by clicking Create auth0 client on app-detail page on Control panel. If the client is missing,  a red warning flag will be displayed on the page to remind you If the flag was on ( True for AUTHENTICATION_REQUIRED ) and you swtich it off,  then existing auth0-client becomes redundant and please do remove it by clicking the Remove the auth0 client to save resource on auth0 platform. Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? As the deployment settings are stored in the GitHub repo and not in Control panel,  any changes made on the GitHub repo will be reflected back in Control panel. DO NOT change the value of the settings mentioned in this section on GitHub repo directly, feel free to change self-defined settings. When will the changes made on Control panel will be applied to the deployment pipeline? The changes will be applied when the next deployment workflow is triggered. This page was last reviewed on 2 June 2023.\\n\\nIt needs to be reviewed again on 2 October 2023\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 2 October 2023\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nManage deployment settings of an app on Control panel - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 926}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '79a4c569555e7f0366aee89d702d8194'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 927}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Manage deployment settings of an app on Control panel After you complete the process of registering an app through Control panel and initialise the infrastructure resource under Cloud platform , you can start to manage the deployment settings through the Control panel . The settings are stored as GitHub secrets or environment variables depending on whether the setting contains sensitive information or not. This section is to explain each setting and how the value of the setting affect the app. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 928}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ac7f619af64f01da46e791f85dda72b'}>,\n",
       " <Document: {'content': 'Permission required The user:\\n- Has been granted as the app’s admin on Control panel\\n- Has been the memember of GitHub admin team If you do not satisfy the above requirements, ask someone in your team who has admin permissions on Control panel and GitHub to grant you the permissions. Context You can have multiple deployment environments for an app for different purposes, minumum is the production environment, then you can have extra ones, e.g. one for testing purpose. What is an application environment? Environments contain the following parts:\\n- A namepace with other required resources on Cloud Platform’s cluster, e.g. an ECR repo for storing your app’s docker images\\n- An environment on GitHub repo\\n- Deployment settings under each GitHub environment and can be managed through Control panel\\n- An ingress (app URL) for you to access the deployed app in each environment\\nThe docker image of app will be built and pushed into app’s ECR, then be deployed with deployment settings on its namespace via GitHub workflow and can be accessed via the app’s URL. By default, 2 deployment environments are provided :-\\n- dev environment: used for testing changes to your application and can be used as the staging environment before releasing a new version to the production environment.\\n- namespace: data-platform-app-<repo-name>-dev - dev environment on GitHub repo\\n- app URL: <repo_name>-dev.apps.live.cloud-platform.service.justice.gov.uk - prod environment:  the production environment where the live application sits\\n- namespace: data-platform-app-<repo-name>-prod - prod enviroment on GitHub repo\\n- app URL: <repo_name>.apps.live.cloud-platform.service.justice.gov.uk All the deployment settings linked to each deployment environment will be displayed and can be managed under the app-detail page on Control panel. Introduction to the settings First, the following flag can be used for switching on/off the user-access-control to the app | Setting | Format | Description |\\n|———|———|———————————————|\\n| AUTHENTICATION_REQUIRED | GitHub environment var | True/False indicate whether your app needs user-access-control, editable| If the value of flag is True , then an auth0 client is required which can be created by clicking the button Create auth0 client under the deployment environment you choose.\\nThe auth0-client is responsible for providing the integration with different login options, for example below:\\n- passwordless flow: allow user to gain access by one-time magic link. more detail is here . This approach allows external users to be able to access app via their email. This flow appears as email in the AUTH0_CONNECTIONS field.\\n- GitHub\\n- nomis login (HMPPS Auth): allow the user login with their NOMIS credentials, the guide about how to set it up will be provided soon. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 929}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8b70bce0102cb77a19d27b350007304'}>,\n",
       " <Document: {'content': \"The credentials of the auth0-client with other related settings need to be available in the app’s deployment environment and be stored as GitHub secrets and environment vars which is explained in the following section Authentication related settings Command Format Description AUTH0_DOMAIN GitHub environment var The domain of the auth0 tenant, not editable AUTH0_CLIENT_ID GitHub secret The client_id of the auth0-client, not editable AUTH0_CLIENT_SECRET GitHub secret The client_secret of the auth0-client, not editable AUTH0_CONNECTIONS stored in auth0 platform The list of login opions the app can choose, only superuser can edit it AUTH0_PASSWORDLESS GitHub environment var only True if email is choosen from AUTH0_CONNECTIONS , not editable If you choose to use email ( passwordless flow),  then you can manage the app’s customers for each deployment environment through clicking Manage customers button on app-list page . Right now we only provide customer management for email login option. If you choose other options like nomis or github etc, then it means your app will be open to any users who have nomis credential or who has a GitHub account and has joined the moj-analytical-services GitHub org.  Further user management and control is required under app-level if the default scope of users is wider than the target audience of the app. IP whitelist You can configure whether your app needs extra protection from internet environment by setting the allowed IP_RANGES (the list VPN managed in MoJ). You can set up this option even if your app is public facing ( AUTHENTICATION_REQUIRED is False ) Command Format Description IP_RANGES GitHub secret The list of MoJ VPNS being allowed to this app, editable Self-defined secrets or environment vars If the app has its own settings and the value of the setting depends on which deployment environment the app is running in, you can create secret (sensitive value e.g., credential or api-key) or environment variable (non-sensitive value) through the app-detail page. The GitHub name of self-defined secrets and environment vars will be have XXX_ as prefix which differentiate themselves with other system secrets and vars,\\nbut the prefix XXX_ will be removed when GitHub workflow (dev/prod) pass the their values to application. We strongly recommend to define one environment variable for indicating which deployment environment the app is running, is dev environment or production environment?\\nfor example, naming the variable as ENV ,  then give it a string value of dev on dev environment,  a string value of prod on prod environment.\\nIn your application code,  retrieve the value of ENV by using Sys.getenv('ENV') for R, os.environ.get('ENV') for python. Other Github secrets and environment vars Other secrets and vars which are not mentioned in the section, if they are not the ones you defined, e.g. having ECR or KUBE or AWS as part of the name,  then they are created during the process of initialising the infrastructure resources on CP and are required by the deployment workflows too.  They are maintained or updated usually through the terraforms of CP. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 930}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c242b274c6b53d805a6f0d69f8b5ca43'}>,\n",
       " <Document: {'content': 'Want to make change to AUTHENTICATION_REQUIRED flag? If the flag was off ( False for AUTHENTICATION_REQUIRED ) and you switch it on,  then an auth0-client is required to be created by clicking Create auth0 client on app-detail page on Control panel. If the client is missing,  a red warning flag will be displayed on the page to remind you If the flag was on ( True for AUTHENTICATION_REQUIRED ) and you swtich it off,  then existing auth0-client becomes redundant and please do remove it by clicking the Remove the auth0 client to save resource on auth0 platform. Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? As the deployment settings are stored in the GitHub repo and not in Control panel,  any changes made on the GitHub repo will be reflected back in Control panel. DO NOT change the value of the settings mentioned in this section on GitHub repo directly, feel free to change self-defined settings. When will the changes made on Control panel will be applied to the deployment pipeline? The changes will be applied when the next deployment workflow is triggered. This page was last reviewed on 2 June 2023.\\n\\nIt needs to be reviewed again on 2 October 2023\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 2 October 2023\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nManage deployment settings of an app on Control panel - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 931}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd4d73b48f1458ca1adf8c28f5cd80d75'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 932}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Manage deployment settings of an app on Control panel After you complete the process of registering an app through Control panel and initialise the infrastructure resource under Cloud platform , you can start to manage the deployment settings through the Control panel . The settings are stored as GitHub secrets or environment variables depending on whether the setting contains sensitive information or not. This section is to explain each setting and how the value of the setting affect the app. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 933}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ac7f619af64f01da46e791f85dda72b'}>,\n",
       " <Document: {'content': 'Permission required The user:\\n- Has been granted as the app’s admin on Control panel\\n- Has been the memember of GitHub admin team If you do not satisfy the above requirements, ask someone in your team who has admin permissions on Control panel and GitHub to grant you the permissions. Context You can have multiple deployment environments for an app for different purposes, minumum is the production environment, then you can have extra ones, e.g. one for testing purpose. What is an application environment? Environments contain the following parts:\\n- A namepace with other required resources on Cloud Platform’s cluster, e.g. an ECR repo for storing your app’s docker images\\n- An environment on GitHub repo\\n- Deployment settings under each GitHub environment and can be managed through Control panel\\n- An ingress (app URL) for you to access the deployed app in each environment\\nThe docker image of app will be built and pushed into app’s ECR, then be deployed with deployment settings on its namespace via GitHub workflow and can be accessed via the app’s URL. By default, 2 deployment environments are provided :-\\n- dev environment: used for testing changes to your application and can be used as the staging environment before releasing a new version to the production environment.\\n- namespace: data-platform-app-<repo-name>-dev - dev environment on GitHub repo\\n- app URL: <repo_name>-dev.apps.live.cloud-platform.service.justice.gov.uk - prod environment:  the production environment where the live application sits\\n- namespace: data-platform-app-<repo-name>-prod - prod enviroment on GitHub repo\\n- app URL: <repo_name>.apps.live.cloud-platform.service.justice.gov.uk All the deployment settings linked to each deployment environment will be displayed and can be managed under the app-detail page on Control panel. Introduction to the settings First, the following flag can be used for switching on/off the user-access-control to the app | Setting | Format | Description |\\n|———|———|———————————————|\\n| AUTHENTICATION_REQUIRED | GitHub environment var | True/False indicate whether your app needs user-access-control, editable| If the value of flag is True , then an auth0 client is required which can be created by clicking the button Create auth0 client under the deployment environment you choose.\\nThe auth0-client is responsible for providing the integration with different login options, for example below:\\n- passwordless flow: allow user to gain access by one-time magic link. more detail is here . This approach allows external users to be able to access app via their email. This flow appears as email in the AUTH0_CONNECTIONS field.\\n- GitHub\\n- nomis login (HMPPS Auth): allow the user login with their NOMIS credentials, the guide about how to set it up will be provided soon. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 934}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8b70bce0102cb77a19d27b350007304'}>,\n",
       " <Document: {'content': \"The credentials of the auth0-client with other related settings need to be available in the app’s deployment environment and be stored as GitHub secrets and environment vars which is explained in the following section Authentication related settings Command Format Description AUTH0_DOMAIN GitHub environment var The domain of the auth0 tenant, not editable AUTH0_CLIENT_ID GitHub secret The client_id of the auth0-client, not editable AUTH0_CLIENT_SECRET GitHub secret The client_secret of the auth0-client, not editable AUTH0_CONNECTIONS stored in auth0 platform The list of login opions the app can choose, only superuser can edit it AUTH0_PASSWORDLESS GitHub environment var only True if email is choosen from AUTH0_CONNECTIONS , not editable If you choose to use email ( passwordless flow),  then you can manage the app’s customers for each deployment environment through clicking Manage customers button on app-list page . Right now we only provide customer management for email login option. If you choose other options like nomis or github etc, then it means your app will be open to any users who have nomis credential or who has a GitHub account and has joined the moj-analytical-services GitHub org.  Further user management and control is required under app-level if the default scope of users is wider than the target audience of the app. IP whitelist You can configure whether your app needs extra protection from internet environment by setting the allowed IP_RANGES (the list VPN managed in MoJ). You can set up this option even if your app is public facing ( AUTHENTICATION_REQUIRED is False ) Command Format Description IP_RANGES GitHub secret The list of MoJ VPNS being allowed to this app, editable Self-defined secrets or environment vars If the app has its own settings and the value of the setting depends on which deployment environment the app is running in, you can create secret (sensitive value e.g., credential or api-key) or environment variable (non-sensitive value) through the app-detail page. The GitHub name of self-defined secrets and environment vars will be have XXX_ as prefix which differentiate themselves with other system secrets and vars,\\nbut the prefix XXX_ will be removed when GitHub workflow (dev/prod) pass the their values to application. We strongly recommend to define one environment variable for indicating which deployment environment the app is running, is dev environment or production environment?\\nfor example, naming the variable as ENV ,  then give it a string value of dev on dev environment,  a string value of prod on prod environment.\\nIn your application code,  retrieve the value of ENV by using Sys.getenv('ENV') for R, os.environ.get('ENV') for python. Other Github secrets and environment vars Other secrets and vars which are not mentioned in the section, if they are not the ones you defined, e.g. having ECR or KUBE or AWS as part of the name,  then they are created during the process of initialising the infrastructure resources on CP and are required by the deployment workflows too.  They are maintained or updated usually through the terraforms of CP. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 935}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c242b274c6b53d805a6f0d69f8b5ca43'}>,\n",
       " <Document: {'content': 'Want to make change to AUTHENTICATION_REQUIRED flag? If the flag was off ( False for AUTHENTICATION_REQUIRED ) and you switch it on,  then an auth0-client is required to be created by clicking Create auth0 client on app-detail page on Control panel. If the client is missing,  a red warning flag will be displayed on the page to remind you If the flag was on ( True for AUTHENTICATION_REQUIRED ) and you swtich it off,  then existing auth0-client becomes redundant and please do remove it by clicking the Remove the auth0 client to save resource on auth0 platform. Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? As the deployment settings are stored in the GitHub repo and not in Control panel,  any changes made on the GitHub repo will be reflected back in Control panel. DO NOT change the value of the settings mentioned in this section on GitHub repo directly, feel free to change self-defined settings. When will the changes made on Control panel will be applied to the deployment pipeline? The changes will be applied when the next deployment workflow is triggered. This page was last reviewed on 2 June 2023.\\n\\nIt needs to be reviewed again on 2 October 2023\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 2 October 2023\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nManage deployment settings of an app on Control panel - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 936}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd4d73b48f1458ca1adf8c28f5cd80d75'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 937}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Manage deployment settings of an app on Control panel After you complete the process of registering an app through Control panel and initialise the infrastructure resource under Cloud platform , you can start to manage the deployment settings through the Control panel . The settings are stored as GitHub secrets or environment variables depending on whether the setting contains sensitive information or not. This section is to explain each setting and how the value of the setting affect the app. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 938}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ac7f619af64f01da46e791f85dda72b'}>,\n",
       " <Document: {'content': 'Permission required The user:\\n- Has been granted as the app’s admin on Control panel\\n- Has been the memember of GitHub admin team If you do not satisfy the above requirements, ask someone in your team who has admin permissions on Control panel and GitHub to grant you the permissions. Context You can have multiple deployment environments for an app for different purposes, minumum is the production environment, then you can have extra ones, e.g. one for testing purpose. What is an application environment? Environments contain the following parts:\\n- A namepace with other required resources on Cloud Platform’s cluster, e.g. an ECR repo for storing your app’s docker images\\n- An environment on GitHub repo\\n- Deployment settings under each GitHub environment and can be managed through Control panel\\n- An ingress (app URL) for you to access the deployed app in each environment\\nThe docker image of app will be built and pushed into app’s ECR, then be deployed with deployment settings on its namespace via GitHub workflow and can be accessed via the app’s URL. By default, 2 deployment environments are provided :-\\n- dev environment: used for testing changes to your application and can be used as the staging environment before releasing a new version to the production environment.\\n- namespace: data-platform-app-<repo-name>-dev - dev environment on GitHub repo\\n- app URL: <repo_name>-dev.apps.live.cloud-platform.service.justice.gov.uk - prod environment:  the production environment where the live application sits\\n- namespace: data-platform-app-<repo-name>-prod - prod enviroment on GitHub repo\\n- app URL: <repo_name>.apps.live.cloud-platform.service.justice.gov.uk All the deployment settings linked to each deployment environment will be displayed and can be managed under the app-detail page on Control panel. Introduction to the settings First, the following flag can be used for switching on/off the user-access-control to the app | Setting | Format | Description |\\n|———|———|———————————————|\\n| AUTHENTICATION_REQUIRED | GitHub environment var | True/False indicate whether your app needs user-access-control, editable| If the value of flag is True , then an auth0 client is required which can be created by clicking the button Create auth0 client under the deployment environment you choose.\\nThe auth0-client is responsible for providing the integration with different login options, for example below:\\n- passwordless flow: allow user to gain access by one-time magic link. more detail is here . This approach allows external users to be able to access app via their email. This flow appears as email in the AUTH0_CONNECTIONS field.\\n- GitHub\\n- nomis login (HMPPS Auth): allow the user login with their NOMIS credentials, the guide about how to set it up will be provided soon. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 939}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8b70bce0102cb77a19d27b350007304'}>,\n",
       " <Document: {'content': \"The credentials of the auth0-client with other related settings need to be available in the app’s deployment environment and be stored as GitHub secrets and environment vars which is explained in the following section Authentication related settings Command Format Description AUTH0_DOMAIN GitHub environment var The domain of the auth0 tenant, not editable AUTH0_CLIENT_ID GitHub secret The client_id of the auth0-client, not editable AUTH0_CLIENT_SECRET GitHub secret The client_secret of the auth0-client, not editable AUTH0_CONNECTIONS stored in auth0 platform The list of login opions the app can choose, only superuser can edit it AUTH0_PASSWORDLESS GitHub environment var only True if email is choosen from AUTH0_CONNECTIONS , not editable If you choose to use email ( passwordless flow),  then you can manage the app’s customers for each deployment environment through clicking Manage customers button on app-list page . Right now we only provide customer management for email login option. If you choose other options like nomis or github etc, then it means your app will be open to any users who have nomis credential or who has a GitHub account and has joined the moj-analytical-services GitHub org.  Further user management and control is required under app-level if the default scope of users is wider than the target audience of the app. IP whitelist You can configure whether your app needs extra protection from internet environment by setting the allowed IP_RANGES (the list VPN managed in MoJ). You can set up this option even if your app is public facing ( AUTHENTICATION_REQUIRED is False ) Command Format Description IP_RANGES GitHub secret The list of MoJ VPNS being allowed to this app, editable Self-defined secrets or environment vars If the app has its own settings and the value of the setting depends on which deployment environment the app is running in, you can create secret (sensitive value e.g., credential or api-key) or environment variable (non-sensitive value) through the app-detail page. The GitHub name of self-defined secrets and environment vars will be have XXX_ as prefix which differentiate themselves with other system secrets and vars,\\nbut the prefix XXX_ will be removed when GitHub workflow (dev/prod) pass the their values to application. We strongly recommend to define one environment variable for indicating which deployment environment the app is running, is dev environment or production environment?\\nfor example, naming the variable as ENV ,  then give it a string value of dev on dev environment,  a string value of prod on prod environment.\\nIn your application code,  retrieve the value of ENV by using Sys.getenv('ENV') for R, os.environ.get('ENV') for python. Other Github secrets and environment vars Other secrets and vars which are not mentioned in the section, if they are not the ones you defined, e.g. having ECR or KUBE or AWS as part of the name,  then they are created during the process of initialising the infrastructure resources on CP and are required by the deployment workflows too.  They are maintained or updated usually through the terraforms of CP. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 940}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c242b274c6b53d805a6f0d69f8b5ca43'}>,\n",
       " <Document: {'content': 'Want to make change to AUTHENTICATION_REQUIRED flag? If the flag was off ( False for AUTHENTICATION_REQUIRED ) and you switch it on,  then an auth0-client is required to be created by clicking Create auth0 client on app-detail page on Control panel. If the client is missing,  a red warning flag will be displayed on the page to remind you If the flag was on ( True for AUTHENTICATION_REQUIRED ) and you swtich it off,  then existing auth0-client becomes redundant and please do remove it by clicking the Remove the auth0 client to save resource on auth0 platform. Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? As the deployment settings are stored in the GitHub repo and not in Control panel,  any changes made on the GitHub repo will be reflected back in Control panel. DO NOT change the value of the settings mentioned in this section on GitHub repo directly, feel free to change self-defined settings. When will the changes made on Control panel will be applied to the deployment pipeline? The changes will be applied when the next deployment workflow is triggered. This page was last reviewed on 2 June 2023.\\n\\nIt needs to be reviewed again on 2 October 2023\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 2 October 2023\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nManage deployment settings of an app on Control panel - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 941}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd4d73b48f1458ca1adf8c28f5cd80d75'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 942}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Manage deployment settings of an app on Control panel After you complete the process of registering an app through Control panel and initialise the infrastructure resource under Cloud platform , you can start to manage the deployment settings through the Control panel . The settings are stored as GitHub secrets or environment variables depending on whether the setting contains sensitive information or not. This section is to explain each setting and how the value of the setting affect the app. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 943}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ac7f619af64f01da46e791f85dda72b'}>,\n",
       " <Document: {'content': 'Permission required The user:\\n- Has been granted as the app’s admin on Control panel\\n- Has been the memember of GitHub admin team If you do not satisfy the above requirements, ask someone in your team who has admin permissions on Control panel and GitHub to grant you the permissions. Context You can have multiple deployment environments for an app for different purposes, minumum is the production environment, then you can have extra ones, e.g. one for testing purpose. What is an application environment? Environments contain the following parts:\\n- A namepace with other required resources on Cloud Platform’s cluster, e.g. an ECR repo for storing your app’s docker images\\n- An environment on GitHub repo\\n- Deployment settings under each GitHub environment and can be managed through Control panel\\n- An ingress (app URL) for you to access the deployed app in each environment\\nThe docker image of app will be built and pushed into app’s ECR, then be deployed with deployment settings on its namespace via GitHub workflow and can be accessed via the app’s URL. By default, 2 deployment environments are provided :-\\n- dev environment: used for testing changes to your application and can be used as the staging environment before releasing a new version to the production environment.\\n- namespace: data-platform-app-<repo-name>-dev - dev environment on GitHub repo\\n- app URL: <repo_name>-dev.apps.live.cloud-platform.service.justice.gov.uk - prod environment:  the production environment where the live application sits\\n- namespace: data-platform-app-<repo-name>-prod - prod enviroment on GitHub repo\\n- app URL: <repo_name>.apps.live.cloud-platform.service.justice.gov.uk All the deployment settings linked to each deployment environment will be displayed and can be managed under the app-detail page on Control panel. Introduction to the settings First, the following flag can be used for switching on/off the user-access-control to the app | Setting | Format | Description |\\n|———|———|———————————————|\\n| AUTHENTICATION_REQUIRED | GitHub environment var | True/False indicate whether your app needs user-access-control, editable| If the value of flag is True , then an auth0 client is required which can be created by clicking the button Create auth0 client under the deployment environment you choose.\\nThe auth0-client is responsible for providing the integration with different login options, for example below:\\n- passwordless flow: allow user to gain access by one-time magic link. more detail is here . This approach allows external users to be able to access app via their email. This flow appears as email in the AUTH0_CONNECTIONS field.\\n- GitHub\\n- nomis login (HMPPS Auth): allow the user login with their NOMIS credentials, the guide about how to set it up will be provided soon. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 944}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8b70bce0102cb77a19d27b350007304'}>,\n",
       " <Document: {'content': \"The credentials of the auth0-client with other related settings need to be available in the app’s deployment environment and be stored as GitHub secrets and environment vars which is explained in the following section Authentication related settings Command Format Description AUTH0_DOMAIN GitHub environment var The domain of the auth0 tenant, not editable AUTH0_CLIENT_ID GitHub secret The client_id of the auth0-client, not editable AUTH0_CLIENT_SECRET GitHub secret The client_secret of the auth0-client, not editable AUTH0_CONNECTIONS stored in auth0 platform The list of login opions the app can choose, only superuser can edit it AUTH0_PASSWORDLESS GitHub environment var only True if email is choosen from AUTH0_CONNECTIONS , not editable If you choose to use email ( passwordless flow),  then you can manage the app’s customers for each deployment environment through clicking Manage customers button on app-list page . Right now we only provide customer management for email login option. If you choose other options like nomis or github etc, then it means your app will be open to any users who have nomis credential or who has a GitHub account and has joined the moj-analytical-services GitHub org.  Further user management and control is required under app-level if the default scope of users is wider than the target audience of the app. IP whitelist You can configure whether your app needs extra protection from internet environment by setting the allowed IP_RANGES (the list VPN managed in MoJ). You can set up this option even if your app is public facing ( AUTHENTICATION_REQUIRED is False ) Command Format Description IP_RANGES GitHub secret The list of MoJ VPNS being allowed to this app, editable Self-defined secrets or environment vars If the app has its own settings and the value of the setting depends on which deployment environment the app is running in, you can create secret (sensitive value e.g., credential or api-key) or environment variable (non-sensitive value) through the app-detail page. The GitHub name of self-defined secrets and environment vars will be have XXX_ as prefix which differentiate themselves with other system secrets and vars,\\nbut the prefix XXX_ will be removed when GitHub workflow (dev/prod) pass the their values to application. We strongly recommend to define one environment variable for indicating which deployment environment the app is running, is dev environment or production environment?\\nfor example, naming the variable as ENV ,  then give it a string value of dev on dev environment,  a string value of prod on prod environment.\\nIn your application code,  retrieve the value of ENV by using Sys.getenv('ENV') for R, os.environ.get('ENV') for python. Other Github secrets and environment vars Other secrets and vars which are not mentioned in the section, if they are not the ones you defined, e.g. having ECR or KUBE or AWS as part of the name,  then they are created during the process of initialising the infrastructure resources on CP and are required by the deployment workflows too.  They are maintained or updated usually through the terraforms of CP. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 945}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c242b274c6b53d805a6f0d69f8b5ca43'}>,\n",
       " <Document: {'content': 'Want to make change to AUTHENTICATION_REQUIRED flag? If the flag was off ( False for AUTHENTICATION_REQUIRED ) and you switch it on,  then an auth0-client is required to be created by clicking Create auth0 client on app-detail page on Control panel. If the client is missing,  a red warning flag will be displayed on the page to remind you If the flag was on ( True for AUTHENTICATION_REQUIRED ) and you swtich it off,  then existing auth0-client becomes redundant and please do remove it by clicking the Remove the auth0 client to save resource on auth0 platform. Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? As the deployment settings are stored in the GitHub repo and not in Control panel,  any changes made on the GitHub repo will be reflected back in Control panel. DO NOT change the value of the settings mentioned in this section on GitHub repo directly, feel free to change self-defined settings. When will the changes made on Control panel will be applied to the deployment pipeline? The changes will be applied when the next deployment workflow is triggered. This page was last reviewed on 2 June 2023.\\n\\nIt needs to be reviewed again on 2 October 2023\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 2 October 2023\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nManage deployment settings of an app on Control panel - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 946}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd4d73b48f1458ca1adf8c28f5cd80d75'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 947}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Manage deployment settings of an app on Control panel After you complete the process of registering an app through Control panel and initialise the infrastructure resource under Cloud platform , you can start to manage the deployment settings through the Control panel . The settings are stored as GitHub secrets or environment variables depending on whether the setting contains sensitive information or not. This section is to explain each setting and how the value of the setting affect the app. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 948}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ac7f619af64f01da46e791f85dda72b'}>,\n",
       " <Document: {'content': 'Permission required The user:\\n- Has been granted as the app’s admin on Control panel\\n- Has been the memember of GitHub admin team If you do not satisfy the above requirements, ask someone in your team who has admin permissions on Control panel and GitHub to grant you the permissions. Context You can have multiple deployment environments for an app for different purposes, minumum is the production environment, then you can have extra ones, e.g. one for testing purpose. What is an application environment? Environments contain the following parts:\\n- A namepace with other required resources on Cloud Platform’s cluster, e.g. an ECR repo for storing your app’s docker images\\n- An environment on GitHub repo\\n- Deployment settings under each GitHub environment and can be managed through Control panel\\n- An ingress (app URL) for you to access the deployed app in each environment\\nThe docker image of app will be built and pushed into app’s ECR, then be deployed with deployment settings on its namespace via GitHub workflow and can be accessed via the app’s URL. By default, 2 deployment environments are provided :-\\n- dev environment: used for testing changes to your application and can be used as the staging environment before releasing a new version to the production environment.\\n- namespace: data-platform-app-<repo-name>-dev - dev environment on GitHub repo\\n- app URL: <repo_name>-dev.apps.live.cloud-platform.service.justice.gov.uk - prod environment:  the production environment where the live application sits\\n- namespace: data-platform-app-<repo-name>-prod - prod enviroment on GitHub repo\\n- app URL: <repo_name>.apps.live.cloud-platform.service.justice.gov.uk All the deployment settings linked to each deployment environment will be displayed and can be managed under the app-detail page on Control panel. Introduction to the settings First, the following flag can be used for switching on/off the user-access-control to the app | Setting | Format | Description |\\n|———|———|———————————————|\\n| AUTHENTICATION_REQUIRED | GitHub environment var | True/False indicate whether your app needs user-access-control, editable| If the value of flag is True , then an auth0 client is required which can be created by clicking the button Create auth0 client under the deployment environment you choose.\\nThe auth0-client is responsible for providing the integration with different login options, for example below:\\n- passwordless flow: allow user to gain access by one-time magic link. more detail is here . This approach allows external users to be able to access app via their email. This flow appears as email in the AUTH0_CONNECTIONS field.\\n- GitHub\\n- nomis login (HMPPS Auth): allow the user login with their NOMIS credentials, the guide about how to set it up will be provided soon. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 949}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8b70bce0102cb77a19d27b350007304'}>,\n",
       " <Document: {'content': \"The credentials of the auth0-client with other related settings need to be available in the app’s deployment environment and be stored as GitHub secrets and environment vars which is explained in the following section Authentication related settings Command Format Description AUTH0_DOMAIN GitHub environment var The domain of the auth0 tenant, not editable AUTH0_CLIENT_ID GitHub secret The client_id of the auth0-client, not editable AUTH0_CLIENT_SECRET GitHub secret The client_secret of the auth0-client, not editable AUTH0_CONNECTIONS stored in auth0 platform The list of login opions the app can choose, only superuser can edit it AUTH0_PASSWORDLESS GitHub environment var only True if email is choosen from AUTH0_CONNECTIONS , not editable If you choose to use email ( passwordless flow),  then you can manage the app’s customers for each deployment environment through clicking Manage customers button on app-list page . Right now we only provide customer management for email login option. If you choose other options like nomis or github etc, then it means your app will be open to any users who have nomis credential or who has a GitHub account and has joined the moj-analytical-services GitHub org.  Further user management and control is required under app-level if the default scope of users is wider than the target audience of the app. IP whitelist You can configure whether your app needs extra protection from internet environment by setting the allowed IP_RANGES (the list VPN managed in MoJ). You can set up this option even if your app is public facing ( AUTHENTICATION_REQUIRED is False ) Command Format Description IP_RANGES GitHub secret The list of MoJ VPNS being allowed to this app, editable Self-defined secrets or environment vars If the app has its own settings and the value of the setting depends on which deployment environment the app is running in, you can create secret (sensitive value e.g., credential or api-key) or environment variable (non-sensitive value) through the app-detail page. The GitHub name of self-defined secrets and environment vars will be have XXX_ as prefix which differentiate themselves with other system secrets and vars,\\nbut the prefix XXX_ will be removed when GitHub workflow (dev/prod) pass the their values to application. We strongly recommend to define one environment variable for indicating which deployment environment the app is running, is dev environment or production environment?\\nfor example, naming the variable as ENV ,  then give it a string value of dev on dev environment,  a string value of prod on prod environment.\\nIn your application code,  retrieve the value of ENV by using Sys.getenv('ENV') for R, os.environ.get('ENV') for python. Other Github secrets and environment vars Other secrets and vars which are not mentioned in the section, if they are not the ones you defined, e.g. having ECR or KUBE or AWS as part of the name,  then they are created during the process of initialising the infrastructure resources on CP and are required by the deployment workflows too.  They are maintained or updated usually through the terraforms of CP. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 950}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c242b274c6b53d805a6f0d69f8b5ca43'}>,\n",
       " <Document: {'content': 'Want to make change to AUTHENTICATION_REQUIRED flag? If the flag was off ( False for AUTHENTICATION_REQUIRED ) and you switch it on,  then an auth0-client is required to be created by clicking Create auth0 client on app-detail page on Control panel. If the client is missing,  a red warning flag will be displayed on the page to remind you If the flag was on ( True for AUTHENTICATION_REQUIRED ) and you swtich it off,  then existing auth0-client becomes redundant and please do remove it by clicking the Remove the auth0 client to save resource on auth0 platform. Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? As the deployment settings are stored in the GitHub repo and not in Control panel,  any changes made on the GitHub repo will be reflected back in Control panel. DO NOT change the value of the settings mentioned in this section on GitHub repo directly, feel free to change self-defined settings. When will the changes made on Control panel will be applied to the deployment pipeline? The changes will be applied when the next deployment workflow is triggered. This page was last reviewed on 2 June 2023.\\n\\nIt needs to be reviewed again on 2 October 2023\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 2 October 2023\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nManage deployment settings of an app on Control panel - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 951}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd4d73b48f1458ca1adf8c28f5cd80d75'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 952}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Manage deployment settings of an app on Control panel After you complete the process of registering an app through Control panel and initialise the infrastructure resource under Cloud platform , you can start to manage the deployment settings through the Control panel . The settings are stored as GitHub secrets or environment variables depending on whether the setting contains sensitive information or not. This section is to explain each setting and how the value of the setting affect the app. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 953}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ac7f619af64f01da46e791f85dda72b'}>,\n",
       " <Document: {'content': 'Permission required The user:\\n- Has been granted as the app’s admin on Control panel\\n- Has been the memember of GitHub admin team If you do not satisfy the above requirements, ask someone in your team who has admin permissions on Control panel and GitHub to grant you the permissions. Context You can have multiple deployment environments for an app for different purposes, minumum is the production environment, then you can have extra ones, e.g. one for testing purpose. What is an application environment? Environments contain the following parts:\\n- A namepace with other required resources on Cloud Platform’s cluster, e.g. an ECR repo for storing your app’s docker images\\n- An environment on GitHub repo\\n- Deployment settings under each GitHub environment and can be managed through Control panel\\n- An ingress (app URL) for you to access the deployed app in each environment\\nThe docker image of app will be built and pushed into app’s ECR, then be deployed with deployment settings on its namespace via GitHub workflow and can be accessed via the app’s URL. By default, 2 deployment environments are provided :-\\n- dev environment: used for testing changes to your application and can be used as the staging environment before releasing a new version to the production environment.\\n- namespace: data-platform-app-<repo-name>-dev - dev environment on GitHub repo\\n- app URL: <repo_name>-dev.apps.live.cloud-platform.service.justice.gov.uk - prod environment:  the production environment where the live application sits\\n- namespace: data-platform-app-<repo-name>-prod - prod enviroment on GitHub repo\\n- app URL: <repo_name>.apps.live.cloud-platform.service.justice.gov.uk All the deployment settings linked to each deployment environment will be displayed and can be managed under the app-detail page on Control panel. Introduction to the settings First, the following flag can be used for switching on/off the user-access-control to the app | Setting | Format | Description |\\n|———|———|———————————————|\\n| AUTHENTICATION_REQUIRED | GitHub environment var | True/False indicate whether your app needs user-access-control, editable| If the value of flag is True , then an auth0 client is required which can be created by clicking the button Create auth0 client under the deployment environment you choose.\\nThe auth0-client is responsible for providing the integration with different login options, for example below:\\n- passwordless flow: allow user to gain access by one-time magic link. more detail is here . This approach allows external users to be able to access app via their email. This flow appears as email in the AUTH0_CONNECTIONS field.\\n- GitHub\\n- nomis login (HMPPS Auth): allow the user login with their NOMIS credentials, the guide about how to set it up will be provided soon. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 954}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8b70bce0102cb77a19d27b350007304'}>,\n",
       " <Document: {'content': \"The credentials of the auth0-client with other related settings need to be available in the app’s deployment environment and be stored as GitHub secrets and environment vars which is explained in the following section Authentication related settings Command Format Description AUTH0_DOMAIN GitHub environment var The domain of the auth0 tenant, not editable AUTH0_CLIENT_ID GitHub secret The client_id of the auth0-client, not editable AUTH0_CLIENT_SECRET GitHub secret The client_secret of the auth0-client, not editable AUTH0_CONNECTIONS stored in auth0 platform The list of login opions the app can choose, only superuser can edit it AUTH0_PASSWORDLESS GitHub environment var only True if email is choosen from AUTH0_CONNECTIONS , not editable If you choose to use email ( passwordless flow),  then you can manage the app’s customers for each deployment environment through clicking Manage customers button on app-list page . Right now we only provide customer management for email login option. If you choose other options like nomis or github etc, then it means your app will be open to any users who have nomis credential or who has a GitHub account and has joined the moj-analytical-services GitHub org.  Further user management and control is required under app-level if the default scope of users is wider than the target audience of the app. IP whitelist You can configure whether your app needs extra protection from internet environment by setting the allowed IP_RANGES (the list VPN managed in MoJ). You can set up this option even if your app is public facing ( AUTHENTICATION_REQUIRED is False ) Command Format Description IP_RANGES GitHub secret The list of MoJ VPNS being allowed to this app, editable Self-defined secrets or environment vars If the app has its own settings and the value of the setting depends on which deployment environment the app is running in, you can create secret (sensitive value e.g., credential or api-key) or environment variable (non-sensitive value) through the app-detail page. The GitHub name of self-defined secrets and environment vars will be have XXX_ as prefix which differentiate themselves with other system secrets and vars,\\nbut the prefix XXX_ will be removed when GitHub workflow (dev/prod) pass the their values to application. We strongly recommend to define one environment variable for indicating which deployment environment the app is running, is dev environment or production environment?\\nfor example, naming the variable as ENV ,  then give it a string value of dev on dev environment,  a string value of prod on prod environment.\\nIn your application code,  retrieve the value of ENV by using Sys.getenv('ENV') for R, os.environ.get('ENV') for python. Other Github secrets and environment vars Other secrets and vars which are not mentioned in the section, if they are not the ones you defined, e.g. having ECR or KUBE or AWS as part of the name,  then they are created during the process of initialising the infrastructure resources on CP and are required by the deployment workflows too.  They are maintained or updated usually through the terraforms of CP. \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 955}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c242b274c6b53d805a6f0d69f8b5ca43'}>,\n",
       " <Document: {'content': 'Want to make change to AUTHENTICATION_REQUIRED flag? If the flag was off ( False for AUTHENTICATION_REQUIRED ) and you switch it on,  then an auth0-client is required to be created by clicking Create auth0 client on app-detail page on Control panel. If the client is missing,  a red warning flag will be displayed on the page to remind you If the flag was on ( True for AUTHENTICATION_REQUIRED ) and you swtich it off,  then existing auth0-client becomes redundant and please do remove it by clicking the Remove the auth0 client to save resource on auth0 platform. Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? As the deployment settings are stored in the GitHub repo and not in Control panel,  any changes made on the GitHub repo will be reflected back in Control panel. DO NOT change the value of the settings mentioned in this section on GitHub repo directly, feel free to change self-defined settings. When will the changes made on Control panel will be applied to the deployment pipeline? The changes will be applied when the next deployment workflow is triggered. This page was last reviewed on 2 June 2023.\\n\\nIt needs to be reviewed again on 2 October 2023\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 2 October 2023\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDeploy a static web app - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 956}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3929a8eef4d250744f37c4e116c2e67'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 957}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Deploying a static webapp It is not currently possible to deploy new apps on the Analytical Platform, though we are working to make this functionality available again as soon as possible. In the meantime, you can still access and manage existing apps. If you have an existing app that requires urgent redeployment, please submit a request via GitHub. We normally redeploy apps each Wednesday, where we have recevied a request by the Friday before. Manage existing apps Manage app users To grant access to someone, in the Control Panel wepapps tab find your App and click “Manage App”. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 958}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6da9c6f8427071606d0c2f9b0894de6f'}>,\n",
       " <Document: {'content': 'In the ‘App customers’ section you can let people view your app by putting one or more email addresses in the text box and clicking “Add customer”. Access the app The URL for the app will be\\n- dev deployment environment: the <respository-name>-dev followed by apps.live.cloud-platform.service.justice.gov.uk .\\n- prod deployment environment: <respository-name> followed by apps.live.cloud-platform.service.justice.gov.uk . So for the example project above “static-web-deploy”, the deployment URL will be\\n- dev: https://static-web-deploy-dev.apps.live.cloud-platform.service.justice.gov.uk - prod: https://static-web-deploy.apps.live.cloud-platform.service.justice.gov.uk Note that characters that are not compatible with website URLs are converted. So, repositories with underscores in their name (e.g. repository_name.apps... ) will be converted to dashes for the URL (e.g. repository_name.apps... ). This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDeploy a static web app - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 959}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '595a3100bfea1714af49a679ee75d700'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 960}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Deploying a static webapp It is not currently possible to deploy new apps on the Analytical Platform, though we are working to make this functionality available again as soon as possible. In the meantime, you can still access and manage existing apps. If you have an existing app that requires urgent redeployment, please submit a request via GitHub. We normally redeploy apps each Wednesday, where we have recevied a request by the Friday before. Manage existing apps Manage app users To grant access to someone, in the Control Panel wepapps tab find your App and click “Manage App”. In the ‘App customers’ section you can let people view your app by putting one or more email addresses in the text box and clicking “Add customer”. Access the app The URL for the app will be\\n- dev deployment environment: the <respository-name>-dev followed by apps.live.cloud-platform.service.justice.gov.uk .\\n- prod deployment environment: <respository-name> followed by apps.live.cloud-platform.service.justice.gov.uk . So for the example project above “static-web-deploy”, the deployment URL will be\\n- dev: https://static-web-deploy-dev.apps.live.cloud-platform.service.justice.gov.uk - prod: https://static-web-deploy.apps.live.cloud-platform.service.justice.gov.uk Note that characters that are not compatible with website URLs are converted. So, repositories with underscores in their name (e.g. repository_name.apps... ) will be converted to dashes for the URL (e.g. repository_name.apps... ). This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 961}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '19224d9927956e99c33b4169bc4b3de1'}>,\n",
       " <Document: {'content': 'This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nDeploy a static web app - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 962}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd49ec442b021acf3916608c5f366fdfe'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 963}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Deploying a static webapp It is not currently possible to deploy new apps on the Analytical Platform, though we are working to make this functionality available again as soon as possible. In the meantime, you can still access and manage existing apps. If you have an existing app that requires urgent redeployment, please submit a request via GitHub. We normally redeploy apps each Wednesday, where we have recevied a request by the Friday before. Manage existing apps Manage app users To grant access to someone, in the Control Panel wepapps tab find your App and click “Manage App”. In the ‘App customers’ section you can let people view your app by putting one or more email addresses in the text box and clicking “Add customer”. Access the app The URL for the app will be\\n- dev deployment environment: the <respository-name>-dev followed by apps.live.cloud-platform.service.justice.gov.uk .\\n- prod deployment environment: <respository-name> followed by apps.live.cloud-platform.service.justice.gov.uk . So for the example project above “static-web-deploy”, the deployment URL will be\\n- dev: https://static-web-deploy-dev.apps.live.cloud-platform.service.justice.gov.uk - prod: https://static-web-deploy.apps.live.cloud-platform.service.justice.gov.uk Note that characters that are not compatible with website URLs are converted. So, repositories with underscores in their name (e.g. repository_name.apps... ) will be converted to dashes for the URL (e.g. repository_name.apps... ). This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 964}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '19224d9927956e99c33b4169bc4b3de1'}>,\n",
       " <Document: {'content': 'This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nGit and GitHub - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 965}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6957f893d657328b4446a2c6bb5cf3b9'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 966}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Git and GitHub What is git? Git is a distributed version control system. It allows you to track changes in files and directories, and to collaborate with others on code development. What is GitHub? GitHub is a online hosting platform for git. It also provides a wide range of other developer tools , covering: automation security project management package and website hosting You can read more about the benefits of GitHub and why we use it. All code written on the Analytical Platform should be stored in a git repository on GitHub in the MoJ Analytical Services organisation. This includes R scripts, Python scripts and Jupyter notebooks. Get started If you’re new to git or GitHub, you may find it useful to take a look at the learning resources before getting started. When you’re ready, you’ll need to set up GitHub to work with git in RStudio and JupyterLab on the Analytical Platform. This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 967}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dfbf936f86d0a8ebda45d30ae9b519b1'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nSetup GitHub - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 968}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c3ef010a61996fb496e36cbbf461b272'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 969}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Set up GitHub To set up GitHub for use with git in RStudio or JupyterLab, you’ll need to: Create an SSH key . Add the SSH key to GitHub . Configure your username and email in git on the Analytical Platform . Create an SSH key You can create an SSH key in RStudio or JupyterLab. You must create a separate SSH key for each tool that you use. RStudio To create an SSH key in RStudio: Open RStudio from the Analytical Platform control panel. In the menu bar, select Tools then Global Options… In the options window, select Git/SVN in the navigation menu. Select Create RSA key… Select Create . Select Close when the information window appears. Select View public key . Copy the SSH key to the clipboard by pressing Ctrl+C on Windows or ⌘C on Mac. JupyterLab To create an SSH key in JupyterLab, follow the steps below: Open JupyerLab from the Analytical Platform control panel. Select the + icon in the file browser to open a new Launcher tab. Select Terminal from the ‘Other’ section. Create an SSH key by running: ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" Here, you should substitute the email address you used to sign up to GitHub. When prompted to enter a file in which to save the key, press Enter to accept the default location. When prompted to enter a passphrase, press Enter to not set a passphrase. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 970}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e6ea24d599834e6e31f0e3d9733edffb'}>,\n",
       " <Document: {'content': \"View the SSH key by running: cat /home/jovyan/.ssh/id_rsa.pub Select the SSH key and copy it to the clipboard by pressing Ctrl+C on windows or ⌘C on Mac. Add the SSH key to GitHub To add the SSH key to GitHub, you should follow the guidance from GitHub . Configure your username and email in git on the Analytical Platform To configure your username and email in git on the Analytical Platform using RStudio or JupyterLab, follow the steps below: Open a new terminal: In RStudio, select Tools in the menu bar and then Shell… In JupyterLap, select the + icon in the file browser and then select Terminal from the Other section in the new Launcher tab. Configure your username by running: git config --global user.name 'Your Name' Here, you should substitute your name. Configure your email address by runnung: git config --global user.email 'your_email@example.com' Here, you should substitute the email address you used to sign up to GitHub. This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nCreate a new project in GitHub - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? \", 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 971}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '62348dbe83589da25ad775e57cfd9e95'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 972}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Create a new project in GitHub In GitHub, you can use a repository to store and collaborate on all of your project’s code. You can also use: pull requests to propose changes to your code and receive reviews from others issues to plan and track your work Create a new repository To create a new repository, you should: Navigate to the MoJ Analytical Services GitHub organisation page. In the repositories section, select New . Configure the properties of your repository, in line with the information below. Select Create repository . Repository template You may find it helpful to use a repository template, such as the moj-analytical-services/template-repository , instead of starting with a blank repository. Templates are often pre-configured with initial files and workflows, and sensible defaults that enable you to get going more quickly. You can also create your own template repositories that you can reuse and share with others. Owner You should select moj-analytical-services as the Owner of the repository. This should be selected by default. If you accidentally create a repository in your personal GitHub account or a different organisation, you can transfer it to the MoJ Analytical Services GitHub organisation. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 973}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fc9961a48ca2d787a4b36ed6494619ef'}>,\n",
       " <Document: {'content': 'Repository name When choosing a name for your repository, we recommend: keeping it short and simple making it descriptive and meaningful avoiding abbreviations and acronyms using dashes to separate words and not full stops or underscores Description We recommend providing a description for your repository that will help others understand what it is for. Repository visibility You can create a repository that is public, internal or private. Public repositories are visible to anyone on the internet. Internal repositories are visible to anyone within the MoJ GitHub enterprise. This includes members of the MoJ Analytical Services organisation, the Ministry of Justice organisation and the > Criminal Injuries Compensation Authority organisation. Private repositories are only visible to you and other people that you share access with. Where possible, you should make your code open and reusable so others can benefit from it . In practice, this is not always possible and you should take care to avoid publishing any information that is sensitive or should not be in the public domain. This may include: credentials and secrets data code relating to the development of policy that has not been announced code that could enable people to exploit systems or processes In such cases, we recommend that you create an internal repository. Generally, you shouldn’t use private repositories, unless you are working on a project that is particularly sensitive. If you are unsure what visibility setting to use, contact the Analytical Platform team . README We recommend that you add a README file when creating your repository. You can use a README file to tell other people why your project is useful, what they can do with your project, and how they can use it. .gitignore A .gitignore file allows you tell git to ignore certain files and directories , such as configuration files and temporary files. We recommend that you select a .gitignore template for the language you are going to use for your project. .gitignore templates are available for R and Python. Licence If your repository is public, you may wish to include a license to tell other people what they can do with your code. At MoJ we use the MIT License . Manage access to a repository Once you have created a repository, you can give other people access to it. This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 974}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '985e1931ace9a09d0fd1ec232e20c5b2'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nManage access in GitHub - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 975}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '285f1b7d7ce4fd3217912ae3ea7d30c6'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 976}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Manage access in GitHub Access to repositories in GitHub is managed via teams . You can set up teams in any way you like. Commonly, teams in GitHub correspond directly to team in the organisation or to a group of people that are working together on the same project. You can also nest teams in GitHub to mirror organisational structures and cascade permissions. You cannot provide direct access to repositories, except for outside collaborators . If you give a member direct access to a repository, they will automatically be added to a team with the same permissions. About teams Invidividuals can be a member of a team or a maintainer of a team. Maintainers can add and remove members from the team, and change the role of members in the team. It is often useful for a team to have at least one maintainer. To add a maintainer to a team, contact the Operations Engineering Team . If your team has no maintainers, you can also ask the Operations Engineering Team to add and remove members on your behalf. Create a team To create a team: Navigate to the MoJ Analytical Services GitHub organisation. Select Teams . Select New team . Configure the team information and select Create team . We recommend that you set the team visibility to visible so other members of the organisation can see it. You can also create nested team structures by selecting a parent team. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 977}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3e3babec309e12e4a06c5e85ba0b4407'}>,\n",
       " <Document: {'content': 'Give a team access to a repository To give a team access to a repository, you must be an admin for the repository. When giving teams access to a repository, you can assign one of five roles: read, triage, write, maintain or admin. The read role allows members to read and clone repositories, and to open and comment on issues and pull requests. The triage role has the same permissions as the read role and also allows members to manage issues and pull requests. The write role has the same permissions as the triage role and also allows members to push to repositories. The maintain role has the same permissions as the write role and also allows members to manage some repository settings. The admin role has full permissions on the repository. Only members with the admin role can manage access to the repository. For more detailed information, see the GitHub guidance on the permissions for each role . To give a team access to a repository: Navigate to the repository that you want to give the team access to. Select Settings . Select Collaborators and teams . Select Add teams . Search for the team you want to give access to and select it from the drop-down menu. Choose the role you want to assign to the team. Select Add to this repository . If your repository is internal or public, all other members of the organisation will have read access by default. If you want to give everyone in the organisation a higher level of access, you can use the @moj-analytical-services/everyone team. All members of the organisation (excluding outside collaborators) are added to this team automatically . This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nCollaborate on a project - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 978}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5bbda6a86be854008485d6697cd19d09'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 979}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Collaborate on a project Working on a branch One of the most useful aspects of git is ‘branching’.  This involves a few extra steps, but it enables some really important benefits: Allows you to separate out work in progress from completed work.  This means there is always a single ‘latest’ definitive working version of the code, that everyone agrees is the ‘master copy’. Enables you and collaborators to work on the same project and files concurrently, resolving conflicts if you edit the same parts of the same files. Enables you to coordinate work on several new features or bugs at once, keeping track of how the code has changed and why, and whether it’s been quality assured. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 980}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '967e2d473f6cf800c7818e273dc045a3'}>,\n",
       " <Document: {'content': 'Creates intutitive, tagged ‘undo points’ which allow you to revert back to previous version of the project e.g. we may wish to revert to the exact code that was tagged ‘model run 2015Q1’. We therefore highly recommend using branches.  (Up until now, we’ve been working on a single branch called ‘master’.) Step 1 (optional):  Create an Issue in github that describes the piece of work you’re about to do (the purpose of the branch) Github ‘issues’ are a central place to maintain a ‘to do’ list for a project, and to discuss them with your team.  ‘Issues’ can be bug fixes (such as ‘fix divide by zero errors in output tables’), or features (e.g. ‘add a percentage change column to output table’), or anything else you want. By using issues, you can keep track of who is working on what.  If you use issues, you automatically preserve a record of why changes were made to code.  So you can see when a line of code was last changed, and which issue it related to, and who wrote it. Step 2:  Create a new branch in R Studio and tell Github about its existence Create a branch with a name of your choosing.  The branch is essentially a label for the segment of work you’re doing.  If you’re working on an issue, it often makes sense to name the branch after the issue. To create a branch, you need to enter the following two commands into the shell: git checkout -b my_branch_name .  Substitute my_branch_name for a name of your choosing. This command simultaneously creates the branch and switches to it, so you are immediately working on it. git push -u origin my_branch_name .  This tells github.com about the existence of the new branch. Step 3:  Make some changes to address the Github issue, and push (sync) them with Github Make changes to the code, commit them, and push them to Github. Step 4: View changes on Github and create pull request You can now view the changes in Github. Github recognises that you’ve synced some code on a branch, and asks you whether you want to merge these changes onto the main ‘master’ branch. You merge the changes using something called a ‘pull request’.  A ‘pull request’ is a set of suggested changes to your project.  You can merge these changes in yourself, or you can ask another collaborator to review the changes. One way of using this process is for quality assurance. For instance, a team may agree that each pull request must be reviewed by a second team member before it is merged.  The code on the main ‘master’ branch is then considered to be quality assured at all times. Pull requests also allow you and others working on the project to leave comments and feedback about the code. You can also leave comments that reference issues on the issue log (by writing # followed by the issue number). ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 981}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '22c5bbd7331e19e7d17f843bf9ecae73'}>,\n",
       " <Document: {'content': 'For example you might comment saying “This pull request now fixes issue #102 and completes task #103”. Step 5:  Sync the changes you made on github.com with your local platform When you merged the pull request, you made changes to your files on Github.  Your personal version of the project in your R Studio hasn’t changed, and is unaware of these changes. The final step is therefore to switch back to the ‘master’ branch in R Studio, and ‘Pull’ the code.  ‘Pulling’ makes R Studio check for changes on Github, and update your local files to incorporate any changes. This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nWork with git in RStudio - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 982}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c3d603bad38c0155ee791bdd87b7a6bb'}>,\n",
       " <Document: {'content': 'Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 983}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e8ca0924c3a77c45321578c4e5b38fd'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Work with git in RStudio Below are point and click steps you can use to sync with your new GitHub repo in RStudio. You can also use the command line . Step 1: Navigate to your platform R Studio and make a copy of the Github project in your R Studio In this step, we create a copy of the definitive GitHub project in your personal R Studio workspace. This means you have a version of the project which you can work on and change. Follow the steps in this gif: (Note: we now recommend making repositories Internal which is not shown in this gif) Notes: When you copy the link to the repo from Github, ensure you use the ssh link, which starts git@github.com as opposed to the https one, which starts https://github.com/ . If this is your first time cloning a repo from Github you may be prompted to answer if you want to continue. Type yes and click enter. Step 2: Edit your files and track them using Git Edit your files as usual using R Studio. Once you’re happy with your changes, Git enables you to create a ‘commit’. Each git commit creates a snapshot of your personal files on the Platform. You can can always undo changes to your work by reverting back to any of the snapshots. This ‘snapshotting’ ability is why git is a ‘verson control’ system. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 984}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6d7de379ee582f91153c6ed8a5ad565d'}>,\n",
       " <Document: {'content': 'In the following gif, we demonstrate changing a single file, staging the changes, and committing them. In reality, each commit would typically include changes to a number of different files, rather than the single file shown in the gif. Notes: ‘committing’ does not sync your changes with github.com. It just creates a snapshot of your personal files in your platform disk. Git will only become aware of changes you’ve made after you’ve saved the file as shown in the gif. Unsaved changes are signified when the filename in the code editor tab is red with an asterisk. Step 3: Sync (‘push’) your work with github.com In R Studio, click the ‘Push’ button (the green up arrow). This will send any change you have committed to the definitive version of the project on Github. You can then navigate to the project on Github in your web browser and you should see the changes. Notes: After pushing, make sure you refresh the GitHub page in your web browser to see changes. That’s it! If you’re working on a personal project, and are not collaborating with others, those three basic steps will allow you to apply version control to your work with Github This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nWork with git and GitHub in JupyterLab - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 985}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6f678211127ea4ece082e72deb2681d5'}>,\n",
       " <Document: {'content': 'Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 986}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe131d5d8242c0aa698bf051be627cdc'}>,\n",
       " <Document: {'content': 'Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Work with git in JupyterLab There is no git interface built into JupyterLab. You should use the command line instead. This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 987}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd26483ba19246f42c79161b7c67eaaa7'}>,\n",
       " <Document: {'content': 'View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nWork with git on the command line - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 988}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9a2a12697ae0b7d0aa378c617ae5e0be'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 989}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Work with git on the command line The command line is the text interface to your Analytical Platform tools. When googling, it may also be referred to as the shell, terminal, or console (and perhaps other names). In Jupyter, you can get the command line by selecting ‘Terminal’ from the launcher screen (the + button in the top left of JupyterLab). You can also use all these commands in RStudio by going to Tools -> Terminal -> New Terminal. Once you are comfortable using the Terminal (in either R Studio or Jupyter) you can run all Git commands from the command line. If you are quite new to the command line, there are a few commands you may find useful to know, in addition to the git commands described later in this section: mkdir : create a new directory/folder cd : change directory touch : create a file ls : list files For example, to create a new python script, main.py in a new folder, scripts you would do: > mkdir scripts > cd scripts > touch main.py > ls main.py You can go back a directory using cd .. and back to your home directory with cd ~ . ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 990}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'afbf181634724b3a5034531b09340ccb'}>,\n",
       " <Document: {'content': 'Some other commands you may wish to use are: rm <filename> : delete file(s) cp <filename> <new_location> : copy a file from current location to a new one mv <filename> <new_location> : move a file from current location to a new one It is a good idea to avoid the use of whitespace in file, folder and repository names, but if you have included a space you can escape it using a backslash (e.g. cd directory\\\\ with\\\\ spaces ). You can also hit the tab key to autocomplete if your file or directory already exists. Make a copy of a GitHub project (‘cloning’) Use your browser to go to the repository you want to copy. Click on ‘Code’ and select the ‘SSH’ tab. You’ll see a link. Click on the button to its right (the overlapping rectangles) to copy that link. In the command line, navigate to the directory where you want to keep your copy of the project. Type git clone followed by the link you’ve just copied from GitHub. So to clone this guidance enter: git clone git@github.com:moj-analytical-services/user-guidance.git Add files to your next commit (‘staging’) Add changed files to your next commit with: git add <filename1> <filename2> This is known as ‘staging’ the files. You can also type git add . to add all changed files to your next commit. Before you do this, use git status to check which files will be added. Commit files ‘Commit’ the files you’ve added: git commit . After calling this command, you need to provide a commit message. R Studio provides a popup. Jupyter will start an editor where you write the message, before saving and exiting it. To commit and add a message in one command, use git commit -m \"Your commit message\" . This is useful if you’re only including a short commit message. Sync work with GitHub (‘pushing’) ‘Push’ your commits to GitHub: git push origin <branch_name> . The default branch name is main . If you’re pushing to this your command would be git push origin main . This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nInstall packages from GitHub - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 991}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '39dc05eff988102e4790e9428be58aef'}>,\n",
       " <Document: {'content': 'Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 992}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9ba4b74f4fa634ec10c04b4f7c578f23'}>,\n",
       " <Document: {'content': '2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 993}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7a1bbf0601e5e6586cff17ef311f612'}>,\n",
       " <Document: {'content': 'Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Install packages from GitHub R packages When the visibility of a repository containing an R package is set to internal or private, you need to authenticate to GitHub to access it from R. Otherwise, you will get a 404 error. The remotes package and the renv package allow you to install R packages using the same SSH credentials you use to close repositories from GitHub. To install a package from an internal or private repository using remotes , you can use the following code: remotes :: install_git ( \"git@github.com:moj-analytical-services/repository-name.git\" ) Alternatively, if you are using renv , you can use the following code: renv :: install ( \"git@github.com:moj-analytical-services/repository-name.git\" ) In both cases, you should replace repository-name with the name of your repository. You can use the same approach to install packages from any other public repository. This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nSecurity in GitHub - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 994}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e128ba79550228656b6f57d7a89c5e2b'}>,\n",
       " <Document: {'content': 'Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 995}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9deed949d002aa9f7bb62f9e06fa76a9'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools Security in GitHub Protecting information in GitHub GitHub should primarily be used to store code. Generally, you should not store any data in GitHub, especially sensitive data. Instead, you should use warehouse data sources . You can use the following approaches to reduce the risk of accidentally publishing sensitive data to GitHub. See also repository visibility and managing access in GitHub . What How it’s configured Reasoning How to override Publishing data files (.csv, .xlsx, etc.) gitignore file You should not store data in GitHub. Manually add the file using git add -f <filename> Publishing file archives (.zip, .tar, .7z, .gz, .bz, .rar, etc.) gitignore file It’s better to unzip file archives and commit their raw contents so files can be tracked individually. This helps prevent data from being accidentally published within a file archive. Manually add the file using git add -f <filename> Publishing large files (>5MB) Pre-commit hook Large files are likely to be data. Do not run pre-commit hooks using git commit --no-verify Publishing Jupyter notebooks nbstripout as a pre-commit hook Jupyter notebook outputs often contain data. Disable nbstripout using ENABLE_NBSTRIPOUT=false; git commit Pushing to repositories outside the MoJ Analytical Services GitHub organistion Pre-push hook You should only store code in the MoJ Analytical Services GitHub organisation. Force push using git push -f <remote> <branch> You should also not store secrets in GitHub, including passwords, credentials and keys. You can use parameters to securely store secrets on the Analytical Platform. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 996}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '950dcf21b2210f3d19ae9b17166caa3d'}>,\n",
       " <Document: {'content': 'Accidentally publishing data to GitHub If you accidentally publish sensitive data to GitHub, you should: follow the GitHub guidance on removing sensitive data from a repository report a security incident and follow any instructions given by the security team If you need further support contact the Data Engineering team in the #ask-data-engineering Slack channel. This page was last reviewed on 30 January 2023.\\n\\nIt needs to be reviewed again on 30 January 2024\\nby the page owner #analytical-platform-support . This page was set to be reviewed before 30 January 2024\\nby the page owner #analytical-platform-support .\\nThis might mean the content is out of date. View source Report problem GitHub Repo Get in touch by email Get in touch by Slack All content is available under the Open Government Licence v3.0 , except where otherwise stated © Crown copyright\\n\\nOrganisation Management - Analytical Platform User Guide Skip to main content Analytical Platform User Guide Menu Control panel Table of contents Search (via Google) Search Overview Intended users Benefits Quickstart guide Before you begin 1. Read Terms of Use 2. Create Slack account 3. Create GitHub account 4. Access the Analytical Platform 5. Set up JupyterLab 6. Set up RStudio Data Data FAQs Where do I find out what data is already on the Platform? How do I gain access to existing data? Where should I store my own data? How do I read/write data from an s3 bucket? How do I query a database on the Platform? I am running into memory issues, what should I do? How do I create my own Athena database? Home directories Amazon S3 What is Amazon S3? Working with Amazon S3 buckets Interacting with Amazon S3 via the Analytical Platform Curated databases SQL quickguide Database structure Data types Commands Functions Amazon Athena Accessing Amazon Athena Previewing tables Working with tables Using RStudio Using JupyterLab Using the Athena UI Querying databases from the AP R - dbtools R - Rdbtools Python - pydbtools Databases User-maintained databases Using mojap_*_timestamp filters Joining temporal-schema tables Using databases and data for apps Guidance on using our databases for analysis Guidance on using databases / data for deployed apps Data Discovery and Documentation Exporting data to other platforms Tools Create a Derived Table What is Create a Derived Table? Database Access Standard database access Your Data Engineering Database Access project access file Rstudio Set Up Clone the repository using the RStudio GUI Clone the repository using the terminal Setting up a Python virtual environment Show indent guides in RStudio Collaborating with Git Creating branches Updating your branch with main Data Modelling Concepts - placeholder Project Structure Domains Databases Standard directory structure and naming conventions Data modelling Models What is a model? Model properties Where can I define configs? Config inheritance Materialisations Source and Ref Functions Sources Adding a new source The ref function Seeds What are seeds? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 997}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '355a40e85bb9c5eb5391190b36072f3e'}>,\n",
       " <Document: {'content': 'Tests Available tests Custom generic tests Singular tests Configuring tests Macros Linting YAML files Folded style > Literal style | Quick Reference Contents Glossary Set up Virtual environment set up Git commands dbt commands yamllint commands sqlfluff commands Moving models to production: Tips Linting SQL files .sqlfluffignore Deploying to Dev Helpful commands Using the + prefix How to use the incremental materialisation with the append strategy Troubleshooting Contents dbt artefacts General troubleshooting tips Delete dev models instructions Troubleshooting list Scheduling to Prod Prod Scheduling dbt-athena Upgrade Guidance Table of contents Test set up Test prod models Test dev models Insert external_location SQLFluff linting changes Update your branch with the dbt-athena upgrade S3 location change for seeds License Control panel Analytical tools RStudio R package management Why use a package manager? Renv Conda Packrat R’s install.packages() Upgrading RStudio Why should I upgrade? 2.2.6 to 3.0.12 JupyterLab Python package management venv and pip Basic usage Using a project that has a requirements.txt Library conflicts & warnings Airflow Airflow Concepts Why use Airflow What is Airflow What is Kubernetes Airflow Environments Airflow Pipeline Image Pipeline DAG Pipeline Component Responsibilities When not to use an Airflow Airflow Instructions Image Pipeline Tips on writing the code Environment Variables Dockerfile Test Docker image (optional) DAG and Role pipeline Define the DAG Define the IAM Policy Validate from the command line (optional) Deploy the changes Troubleshooting Airflow Pipelines Data Uploader Why use the Uploader? Uploader flowchart Data Uploader pre-requisites Login page Front page Step 1 of 4: Data governance requirements Step 2 of 4: Choose database Step 3 of 4: Choose table Step 4 of 4: Check your inputs before uploading your data Upload complete Getting access to uploaded data Limitations and awareness Apps Enabling alerting Alert config Current configured alerts R Shiny app publishing App deployment Managing published apps Advanced Troubleshooting and monitoring Managing and Monitoring Deployments Shiny server Manage deployment settings of an app on Control panel Permission required Context Introduction to the settings Want to make change to AUTHENTICATION_REQUIRED flag? Can I make changes(add/remove/update) the secrets/vars on GitHub repo directly? When will the changes made on Control panel will be applied to the deployment pipeline? ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 998}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952b1c5739374521f6e2a5426a554'}>,\n",
       " <Document: {'content': 'Deploying a static webapp Manage existing apps Access the app Git and GitHub Set up GitHub Create a new project in GitHub Manage access in GitHub Collaborate on a project Work with git in RStudio Work with git in JupyterLab Work with git on the command line Install packages from GitHub Security in GitHub GitHub organisation management Processes and practices Automation Contact Learning resources Acceptable use policy Scope Who this policy applies to General principles GitHub Parameters - working with secrets About parameters S3 bucket A file which is not committed to git GitHub repositories - DO NOT use for secrets Information governance Data management Data movements Reporting security incidents How to get Support Summary Introduction Routes of support How to ask for support: creating a reproducible example Raising issues Common Errors and Solutions Annexes Memory limits Benefits of GitHub Reproducible Analytical Pipelines Step by step guide to setup Two Factor Authentication Infrastructure Migration - step by step instructions Shared Responsibility Model Documentation Onboarding and offboarding Platform maintenance Platform security Tools and packages Appendix Athena workgroup upgrade Background How do I prepare for the new version? How do I use the testing workgroup? How can I check if my queries are running on the correct version? What if I’m using create-a-derived-table for my work? What should I do if I get stuck? Where can I find out more about Athena engine version 3? Running your app within Jupyter Running Plotly Dash apps Troubleshooting Migrating to botor Table of Contents Installation Usage Migrating from s3tools GitHub organisation management The MoJ Analytical Services GitHub organisation is managed by the Operations Engineering Team. Processes and practices Members Adding a member To add a member to the organisation, you can either: Follow the instructions in the GitHub documentation. To do this, you must be an organisation owner. Submit a request in the #ask-operations-engineering Slack channel. You do not need to add members to the everyone team manually, as this will be done automatically . Outside collaborators An outside collaborator is a person who is not a member of the organisation, but has access to one or more repositories. We use outside collaborators for external users, such as analysts from other government departments and partner organisations. Outside collaborators are managed in Terraform in the moj-analytical-services/github-outside-collaborators repository. Adding an outside collaborator To add an outside collaborator to the GitHub organisation, you can either submit an issue in the moj-analytical-services/github-outside-collaborators repository or manually make changes to the relevant Terraform files . You will need to provide: the GitHub username of the user the name of the user the email of the user the organisation of the user the reason the user needs access to the organisation your name and email a review date the level of permission needed by the user a list of repositories the user should have access to Automation All automated processes are managed in the moj-analytical-services/operations-engineering repository. ', 'content_type': 'text', 'score': None, 'meta': {'pdf_path': 'data-platform-firebreak-llm-unstructured/data/economy.pdf', '_split_id': 999}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '35a3ae631d5331508d81564548459d6d'}>,\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.nodes import PreProcessor\n",
    "# Create a list containing single document\n",
    "\n",
    "docs = [doc]\n",
    "\n",
    "processor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=True,\n",
    "    split_by=\"word\",\n",
    "    split_length=500,\n",
    "    split_respect_sentence_boundary=True,\n",
    "    split_overlap=0,\n",
    "    language=\"en\",\n",
    ")\n",
    "\n",
    "preprocessed_docs = processor.process(docs)\n",
    "\n",
    "print(len(preprocessed_docs))\n",
    "\n",
    "# a smaller chunked document\n",
    "\n",
    "preprocessed_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80db73d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating BM25 representation...: 100%|██████████| 408/408 [00:00<00:00, 2948.37 docs/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore(use_bm25=True)\n",
    "document_store.write_documents(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d126d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.nodes import BM25Retriever\n",
    "retriever = BM25Retriever(document_store, top_k=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fddd09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_template = PromptTemplate(prompt=\n",
    "  \"\"\" Using the information contained in the context, answer only the question asked without adding question suggestions\n",
    "  If the answer cannot be inferred from the context, reply: '\\I don't know'.\n",
    "  Context: {join(documents)};\n",
    "  Question: {query}\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f184bca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_node = PromptNode(\n",
    "    model_name_or_path=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    api_key=HF_TOKEN,\n",
    "    default_prompt_template=qa_template,\n",
    "    max_length=500,\n",
    "    model_kwargs={\"model_max_length\": 5000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d436cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
    "rag_pipeline.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"retriever\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4fed9af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "print_answer = lambda out: pprint(out[\"results\"][0].strip())\n",
    "def print_answer_with_score(output):\n",
    "    if output[\"results\"]:\n",
    "        top_result = output[\"results\"][0].strip()  # Get the top result\n",
    "        top_document = output[\"documents\"][0]  # Get the top document\n",
    "\n",
    "        score = top_document.score  # Extract the score of the top document\n",
    "        pprint(f\"{top_result}\")\n",
    "    else:\n",
    "        pprint(\"No results found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04d81d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Answer: Access to curated databases is granted via the database access '\n",
      " 'repository . Have a read through the guidance on there. If you are finding '\n",
      " 'the process a little tricky, please ask for help in #ask-data-engineering . '\n",
      " 'A data engineer will happily guide you through things. If you are looking '\n",
      " 'for access to a user created bucket, then the admin of that bucket should be '\n",
      " 'able to grant you access. If you don’t know who the admin is, or they are '\n",
      " 'not able to grant you access, then ask in the #analytical-platform-support '\n",
      " 'Slack channel or via GitHub .')\n"
     ]
    }
   ],
   "source": [
    "print_answer_with_score(rag_pipeline.run(query=\"How do I gain access to existing data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46c17730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The context provides information about the data discovery tool in the '\n",
      " 'Analytical Platform. This tool is used for data discovery and documentation, '\n",
      " 'and users have access to it by default. Basic metadata is provided '\n",
      " 'automatically by data engineers, while documentation is provided by users. '\n",
      " 'Instructions on how to add material are contained within the app. The data '\n",
      " \"discovery tool's documentation is contained in the data discovery tool, and \"\n",
      " 'access to the tool is governed via GitHub. The page was last reviewed on 10 '\n",
      " 'March 2023 and needs to be reviewed again on 10 March 2024 by the page owner '\n",
      " '#ask-data-engineering.')\n"
     ]
    }
   ],
   "source": [
    "print_answer_with_score(rag_pipeline.run(query=\"data discovery\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0225eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Answer: The key risk that you need to manage is the chance of accidentally '\n",
      " 'adding the file with the secret to your git repository. You could add the '\n",
      " 'filename to a .gitignore file, but it’s probably more reliable if you store '\n",
      " 'it in a directory outside the repository entirely. This technique is limited '\n",
      " 'to personal use. Whilst you could share the file securely with colleagues, '\n",
      " 'you do lose control and oversight of what they do with it. And it is not '\n",
      " 'suitable for apps or Airflow pipelines. For these needs, store the secret in '\n",
      " 'the cloud using parameters or an S3 bucket. GitHub repositories - DO NOT use '\n",
      " 'for secrets Storing secrets in GitHub repositories is prohibited for '\n",
      " 'Analytical Platform users. Secrets should not be stored in code, even if the '\n",
      " 'GitHub repository is private. For example, never write code like this: '\n",
      " 'password <- \"AIzaSyAqDsnMnOTAyNKXKt3HRuIvLTCctaFVCLQ\". Instead, the secrets '\n",
      " 'should be stored in the cloud using parameters or an S3 bucket. The '\n",
      " 'Analytical Platform uses AWS Parameter Store to store the parameters in an '\n",
      " 'encrypted form.')\n"
     ]
    }
   ],
   "source": [
    "print_answer_with_score(rag_pipeline.run(query=\"where to store secrets\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0207923c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Answer: If you’ve lost your platform 2FA code because e.g. you’ve broken or '\n",
      " 'lost your phone, please contact the Analytical Platform team and we will '\n",
      " 'reset it for you.')\n"
     ]
    }
   ],
   "source": [
    "print_answer_with_score(rag_pipeline.run(query=\"how to reset my 2fa\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cff44c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
